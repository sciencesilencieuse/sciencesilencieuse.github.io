[
{
	"uri": "https://sciencesilencieuse.github.io/maths/algebre/",
	"title": "Algèbre",
	"tags": [],
	"description": "",
	"content": " Algèbre Équations Jusque là, rien de bien folichon\u0026hellip; Mais si on libère la variable de nos polynômes en lui permettant de se balader dans l\u0026rsquo;espace des complexes, tout devient plus sympa. Déjà, plus d\u0026rsquo;histoires de 0, 1 ou 2 solutions pour un polynôme de degré 2\u0026hellip; En complexe, un polynôme de degré 2 a toujours 2 solutions. C\u0026rsquo;est quand même plus propre comme ça. Et d\u0026rsquo;ailleurs, un polynôme de degré n a toujours n racines et peut donc se factoriser en n polynômes de degré 1. C\u0026rsquo;est le théorème fondamental de l\u0026rsquo;algèbre.\nnote\nJolie série de vidéos sur les complexes.\nIdentités remarquables Algèbre de Boole L\u0026rsquo;algèbre de Boole permet d\u0026rsquo;algébriser la logique. C\u0026rsquo;est le langage naturelle des circuits électroniques à la base de l\u0026rsquo;informatique.\nAlgèbre linéaire L\u0026rsquo;algèbre linéaire est un outil fondamental pour la physique et en particulier pour la physique quantique dont c\u0026rsquo;est la base même.\nApplication de l\u0026rsquo;algèbre linéaire (et de la géométrie projective) pour déceler et corriger le plus efficacement possible les erreurs dans un message.\nCombinatoire / graphes tip\nCours sur les graphes donné à des élèves de TSI1 en informatique.\nPetit jeu sur un graphe inventé par Hamilton. Le but est de trouver un chemin hamiltonien consistant à ne visiter qu\u0026rsquo;une seule fois chaque sommet avant de retourner au sommet initial. Il appela le jeu \u0026ldquo;The Icosian Game\u0026rdquo; car les 20 sommets forment un icosaèdre régulier. Si le challenge n\u0026rsquo;est pas suffisant, ajoutez à la ville de départ un, deux ou trois sommets avant de commencer à chercher le chemin.\nThéorie des groupes info\nLa théorie des groupes découpe et structure les représentations physiques que l’on se fait du monde. Elle prescrit le choix même des grandeurs physique. Son importance est phénoménale.\nQuelques pages dédiées ici.\nMultiplication de polynômes et FFT "
},
{
	"uri": "https://sciencesilencieuse.github.io/info/algorithmique/",
	"title": "Algorithmique",
	"tags": [],
	"description": "",
	"content": " Algorithmique Généralités La notion de complexité permet de classer des algorithmes réalisant la même tâche comme dans la vidéo suivante. C\u0026rsquo;est finalement une sorte d\u0026rsquo;échelle de raffinement ; descendre d\u0026rsquo;un barreau la complexité temporelle ou spatiale demande le plus souvent de pousser la réflexion plus loin voire de repenser entièrement le problème. Et c\u0026rsquo;est ainsi qu\u0026rsquo;une recherche de doublons dans une liste se transforme en la chasse de cycles sur un graphe dans l\u0026rsquo;ultime mouture pour atteindre le Graal ($\\mathcal{O}(n)$ en temps et $\\mathcal{O}(1)$ en espace sans mutation).\nLa notion est aussi au centre d\u0026rsquo;une des plus importantes conjectures des mathématiques : $\\mathrm{P}\\stackrel{?}{=}\\mathrm{NP}$.\nGrossièrement, la conjecture revient à se demander si l\u0026rsquo;ensemble $\\mathrm{NP}$ des problèmes dont les solutions sont facilement vérifiables (en temps polynomial) est le même que l\u0026rsquo;ensemble $\\mathrm{P}$ des problèmes faciles (qu\u0026rsquo;on peut résoudre en temps polynomial).\nPrenons l\u0026rsquo;exemple classique du voyageur de commerce.\nLe problème (dans sa version décisionnelle) demande s\u0026rsquo;il existe un trajet de longueur inférieure à une valeur $K$ passant une et une seule fois par un ensemble de villes. Pour valider une solution, il suffit donc de vérifier que chaque ville est bien là une et une seule fois, calculer la distance et la comparer à $K$. Cela se fait bien sûr en temps polynomial rendant le problème $\\mathrm{NP}$.\nPar contre, aucun algorithme connu à ce jour ne sait aboutir à la solution en un temps mieux qu\u0026rsquo;exponentiel\u0026hellip;\nOr si $\\mathrm{P=NP}$, cela voudrait dire qu\u0026rsquo;un algorithme polynomial existe bel et bien.\nLa grande majorité des chercheurs se rangent dans le camp pessimiste $\\mathrm{P≠NP}$.\nLe meilleur algorithme (non quantique) permettant de résoudre de manière exacte le problème du voyageur de commerce, l\u0026rsquo;algorithme de Held-Karp, utilise la programmation dynamique dont la logique est présentée dans la vidéo suivante.\nUne implémentation de l\u0026rsquo;algorithme de Held-Karp est donnée ci-dessous. Elle utilise la technique du masquage pour parcourir tous les sous-ensembles de villes visitées possibles et pour pouvoir tester rapidement l\u0026rsquo;appartenance ou non d\u0026rsquo;une ville à ce sous-ensemble.\nCode def tsp_chemin(distance, depart=0): \u0026#34;\u0026#34;\u0026#34; Résout le problème du voyageur de commerce (TSP) en utilisant une programmation dynamique basée sur les masques binaires. Cette fonction calcule, pour chaque sous-ensemble de villes et pour chaque ville terminale, le coût minimal pour parcourir l\u0026#39;ensemble des villes spécifié par le masque et terminer dans cette ville. Elle permet également de reconstruire le chemin optimal en stockant, pour chaque état, la ville précédente. Paramètres ---------- distance : list[list[float]] Matrice des distances entre les villes, où distance[i][j] représente la distance allant de la ville i à la ville j. depart : int, optionnel Indice de la ville de départ (par défaut 0). Retourne -------- tuple Un tuple (dp, parent) contenant : - dp : list[list[float]] Tableau à deux dimensions de taille (2^n x n) où dp[masque][j] est le coût minimum pour visiter l\u0026#39;ensemble de villes indiqué par le masque (représenté en binaire) et terminer à la ville j. - parent : list[list[int]] Tableau à deux dimensions de taille (2^n x n) permettant de reconstruire le chemin optimal. Pour chaque état (masque, j), parent[masque][j] stocke l\u0026#39;indice de la ville qui précède j dans le chemin optimal. Description ----------- L\u0026#39;algorithme initialise une table dp avec des valeurs infinies, sauf pour l\u0026#39;état correspondant au départ. Il parcourt ensuite tous les masques possibles (correspondant aux 2^n sous-ensembles de villes) et, pour chaque ville déjà visitée, il tente de mettre à jour le coût minimal pour atteindre chaque ville non encore visitée. La structure parent est utilisée pour enregistrer les transitions effectuées, ce qui permet ultérieurement de reconstruire le chemin optimal. Exemple d\u0026#39;utilisation --------------------- \u0026gt;\u0026gt;\u0026gt; distance = [ ... [0, 10, 15, 20], ... [10, 0, 35, 25], ... [15, 35, 0, 30], ... [20, 25, 30, 0] ... ] \u0026gt;\u0026gt;\u0026gt; dp, parent = tsp_chemin(distance, depart=0) \u0026#34;\u0026#34;\u0026#34; n = len(distance) INF = float(\u0026#39;inf\u0026#39;) dp = [[INF] * n for _ in range(1 \u0026lt;\u0026lt; n)] parent = [[-1] * n for _ in range(1 \u0026lt;\u0026lt; n)] dp[1 \u0026lt;\u0026lt; depart][depart] = 0 for masque in range(1 \u0026lt;\u0026lt; n): # 2^n possibilités (chaque ville est visitée ou non visitée) for i in range(n): if masque \u0026amp; (1 \u0026lt;\u0026lt; i): # si la ville i est visitée for j in range(n): if not (masque \u0026amp; (1 \u0026lt;\u0026lt; j)): # si la ville j n\u0026#39;est pas encore visitée nv_masque = masque | (1 \u0026lt;\u0026lt; j) if dp[nv_masque][j] \u0026gt; dp[masque][i] + distance[i][j]: dp[nv_masque][j] = dp[masque][i] + distance[i][j] parent[nv_masque][j] = i return dp, parent Comme à chaque fois en programmation dynamique, la matrice dp contient toutes les informations nécessaires et pour reconstruire le plus petit chemin et obtenir son coût, on peut utiliser le code suivant :\nCode def reconstr_chemin(parent, masque, dernier): chemin = [] while dernier != -1: chemin.append(dernier) temp = parent[masque][dernier] masque = masque \u0026amp; ~(1 \u0026lt;\u0026lt; dernier) dernier = temp chemin.reverse() return chemin dp, parent = tsp(distance, depart=0) masque_final = (1 \u0026lt;\u0026lt; len(distance)) - 1 for i in range(len(distance)): coût_total = dp[masque_final][i] + distance[i][0] if coût_total \u0026lt; coût_min: coût_min = coût_total chemin_min = reconstr_chemin(parent, masque_final, i) chemin_min.append(0) # Fermer le cycle Il est basée sur la relation de récurrence suivante : $C(S,i) = \\min_{j \\in S,\\ j \\neq i} \\left\\{ C\\left(S \\setminus \\{i\\}, j\\right) + \\mathrm{d}(j,i) \\right\\}$ où $C(S,i)$ représente la plus petite distance pour parcourir chaque ville de l\u0026rsquo;ensemble $S$ et terminer dans la ville $i$ et $d(i,j)$ est la distance entre la ville $i$ et la ville $j$.\nSa complexité temporelle est en $\\mathcal{O(n^22^n)}$. En effet, tous les sous-ensembles de villes sont inspectés $\\mathcal{O}(2^n)$ et pour chacun, on parcourt toutes les paires ville de départ - ville d\u0026rsquo;arrivée possibles $\\mathcal{O}(n)\\times\\mathcal{O}(n)$ sans jamais faire autre chose que des opérations en temps constant $\\mathcal{O}(1)$ pour chaque paire.\nUn algorithme de programmation dynamique ne se laisse pas approcher facilement, particulièrement dans sa forme non-récursive avec sa grosse matrice mystérieuse. Mais c\u0026rsquo;est plutôt rare que la forme itérative d\u0026rsquo;un algorithme vole la vedette à sa forme récursive tant la récursivité sait insuffler un air de magie aux algorithmes en permettant à de tout petits codes de réaliser des tâches étonnamment complexes.\nFonction centrale du programme ci-dessus :\ndef Hanoi(n, depart, cible, inter): if n == 1: deplacement(depart,cible) else : Hanoi(n-1,depart,inter,cible) Hanoi(1,depart,cible,inter) Hanoi(n-1,inter,cible,depart) Hanoi(6, 0, 2,1) Quelques algorithmes remarquables Les tris sont omniprésents en informatique et Tim Roughgarden (auteur des géniaux Algorithms illuminated) en parle même comme de la “mère de tous les problèmes algorithmiques”.\nCi-dessous, vous pourrez comparer l\u0026rsquo;efficacité de différents tris par comparaison en les voyant se débattre courageusement avec les données.\nQuicksort Un code Python possible pour la version détaillée dans la vidéo :\nCode from random import randint def partition(L, g, d): p = L[g] i = g+1 for j in range(g+1,d+1): if L[j] \u0026lt; p: permute(L,i,j) i += 1 permute(L,g,i-1) return i-1 def tri_rapide_gd(L, g, d): if g \u0026lt; d: pivot = randint(g,d) permute(L,pivot,g) pivot = partition(L, g, d) tri_rapide(L, g, pivot-1) tri_rapide(L, pivot+1, d) return L def tri_rapide(L): g = 0 d = len(L)-1 tri_rapide_gd(L, g, d) La danse hongroise ci-dessous implémente une version de l\u0026rsquo;algorithme sans hasard :\nDans cette version, le premier élément est systématiquement choisi comme pivot (chapeau noir) et la partition marche un peu différemment :\nLe chapeau rouge i est d'abord donné au dernier élément de la partition puis comparé au pivot p. Si le chapeau rouge est plus petit que le pivot, les deux éléments échangent leur place et le chapeau rouge est donné au plus proche voisin du côté du pivot. Sinon le pivot reste à sa place et le chapeau rouge est là aussi donné au voisin immédiat le plus proche du pivot. Lorsque le pivot récupère le chapeau rouge, il est bien placé. On coupe ensuite la liste en deux sur le pivot et on recommence récursivement. Un code Python possible pour cette variante :\nCode def partition(L): p = 0 # pivot (chapeau noir) i = len(L)-1 # chapeau rouge while i != p: if (L[p]-L[i])*(p-i) \u0026lt; 0: L[p],L[i] = L[i],L[p] p,i = i,p i -= (i-p)//abs(i-p) return p def tri_rapide(L): if len(L) \u0026lt;= 1: return L else: p = partition(L) L[:p] = tri_rapide(L[:p]) L[p+1:] = tri_rapide(L[p+1:]) return L Un tirage non aléatoire du pivot rend l\u0026rsquo;algorithme moins robuste sur certaines entrées. Avec le choix systématique du premier élément (par exemple), on se retrouve avec une complexité en $O(n^2)$ sur des données déjà triées ou presque. Le tirage aléatoire rend extrêmement peu probable un scénario \u0026ldquo;défavorable\u0026rdquo;.\nSi en plus de choisir le premier élément comme pivot, on ne s\u0026rsquo;occupe même plus d\u0026rsquo;avoir un tri en place (c\u0026rsquo;est-à-dire qu\u0026rsquo;on s\u0026rsquo;autorise des copies de la liste de départ), Quicksort peut s\u0026rsquo;écrire de manière très compacte :\ndef triRapide(elements): if len(elements) \u0026lt;= 1: return elements else: pivot = elements[0] plusPetit = triRapide([e for e in elements[1:] if e \u0026lt;= pivot]) plusGrand = triRapide([e for e in elements[1:] if e \u0026gt; pivot]) return plusPetit + [pivot] + plusGrand Mais attention, l\u0026rsquo;élégance de ce code cache sa lâcheté puisqu\u0026rsquo;en ne rangeant plus en place, il esquive la principale difficulté d\u0026rsquo;un algorithme de tri. S\u0026rsquo;interdire la création de nouvelles listes oblige en effet à gérer astucieusement les permutations au sein de la liste de départ. Et si cela donne souvent des codes plus verbeux, il n\u0026rsquo;en sont pas moins beaucoup plus fins.\nAlgorithmes gloutons Les algorithmes gloutons sont très faciles à comprendre et à implémenter. Si on devait leur trouver un défaut, elle est à chercher du côté de leur correction, pas toujours simple à démontrer\u0026hellip;\nSont présentés ci-dessous deux algorithmes gloutons très similaires qui permettent de résoudre des problèmes d\u0026rsquo;optimisation sur des graphes :\nDijkstra cherche le plus court chemin entre deux nœuds. Prim construit l\u0026rsquo;arbre couvrant de poids minimal. Parcours d\u0026rsquo;un graphe Pour inventorier les sommets d\u0026rsquo;un graphe, deux stratégies sont possibles :\nle parcours en largeur (BFS), et le parcours en profondeur (DFS). Le code est identique entre les deux algorithmes à la structure de donnée utilisée près ! On utilise une file (queue en anglais) pour le parcours en largeur et une pile pour le parcours en profondeur.\nAvec sa logique FIFO (premier arrivé, premier parti, comme dans une file d\u0026rsquo;attente), BFS explore le graphe de proche en proche.\nfrom collections import deque # deque est une file à double extrémités (double-ended queue) avec la primitive popleft() pour retirer à gauche comme une file et pop() pour retirer à droite comme une pile. def BFS(G,depart): \u0026#34;\u0026#34;\u0026#34; graphe G(S,A) représenté par une liste d\u0026#39;adjacence implémentée par un dictionnaire et depart un sommet de S \u0026#34;\u0026#34;\u0026#34; file = deque() file.append(depart) Vus = {s : False for s in G} Sommets = [] while file: # tant que la file n\u0026#39;est pas vide sommet = file.popleft() # méthode de la classe deque permettant de défiler if not Vus[sommet]: file += G[sommet] Vus[sommet] = True Sommets.append(sommet) return Sommets Avec sa logique LIFO (dernier, arrivée, premier parti, comme pour une pile d\u0026rsquo;assiette pendant la vaisselle), DFS s\u0026rsquo;enfonce dans une branche du graphe jusqu\u0026rsquo;à son extrémité puis revient à la dernière jonction rencontrée et explore une nouvelle branche, et ainsi de suite.\ndef DFS(G,depart): pile = deque() pile.append(depart) Vus = {s : False for s in G} Sommets = [] while pile: sommet = pile.pop() if not Vus[sommet]: pile += G[sommet] Vus[sommet] = True Sommets.append(sommet) return Sommets Comme la récursivité utilise naturellement une pile via la pile d\u0026rsquo;exécution, il y a une écriture récursive naturelle de la recherche en profondeur.\ndef DFS(G,depart,Vus,Sommets=[]): Sommets.append(depart) Vus[depart] = True for sommet in G[depart]: if not Vus[sommet]: DFS(G,sommet,Vus,Sommets) return Sommets On peut voir Dijkstra comme un parcours de graphe où la structure de donnée n\u0026rsquo;est plus une file ou une pile mais une file de priorité (qui donne en $\\mathcal{O}(1)$ l\u0026rsquo;élément stocké de plus grand score).\nPour implémenter une file de priorité, on peut utiliser un tas (heap), arbre binaire presque ordonné dont la racine est l\u0026rsquo;élément prioritaire.\nUne implémentation possible de Dijkstra :\nCode import heapq def Dijkstra(G, depart): \u0026#34;\u0026#34;\u0026#34; Calcule les plus courts chemins dans un graphe pondéré à partir d\u0026#39;un sommet de départ. Paramètres : - G : dict, représentation du graphe où les clés sont les sommets et les valeurs sont des dictionnaires {voisin: poids}. - depart : le sommet de départ. Retourne : - predecesseurs : dict, permettant de reconstruire les plus courts chemins. - distances : dict, distances minimales depuis le sommet de départ. \u0026#34;\u0026#34;\u0026#34; if depart not in G: raise ValueError(\u0026#34;Le sommet de départ n\u0026#39;est pas présent dans le graphe\u0026#34;) distances = {sommet: float(\u0026#39;infinity\u0026#39;) for sommet in G} predecesseurs = {sommet: None for sommet in G} distances[depart] = 0 tas = [(0, depart)] while tas: distance_actuelle, sommet_actuel = heapq.heappop(tas) # On ignore l\u0026#39;entrée obsolète si un meilleur chemin a été trouvé if distance_actuelle \u0026gt; distances[sommet_actuel]: continue for voisin, poids in G[sommet_actuel].items(): nouvelle_distance = distance_actuelle + poids if nouvelle_distance \u0026lt; distances[voisin]: distances[voisin] = nouvelle_distance predecesseurs[voisin] = sommet_actuel heapq.heappush(tas, (nouvelle_distance, voisin)) return predecesseurs, distances Algorithme d\u0026rsquo;Euclide Une des plus vieux algorithmes ($\\approx$-300).\nLa suite de Fibonacci est un peu la tarte à la crème du mathématicien de série télévisée avec ses spirales dans les coquillages\u0026hellip; Mais c\u0026rsquo;est bien elle qui montre le bout de son nez là où on ne l\u0026rsquo;attendait pas vraiment dans cette vidéo sur la complexité de l\u0026rsquo;algorithme d\u0026rsquo;Euclide.\nOn peut utiliser le pulvérisateur (algorithme d\u0026rsquo;Euclide étendu) ci-dessous. Le code est corrigé par rapport à celui de la vidéo qui présente une erreur dans la cas de base (on doit retourner b, 0, 1 comme Euclide bien sûr et non a,0,1).\nEt le petit programme qui donne la décomposition en fraction continue :\nAlgorithme X Polyominos et Sudoku ont le droit à leur algorithme au nom aguicheur : X.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique1/",
	"title": "Calcul des propositions",
	"tags": [],
	"description": "",
	"content": " info\nNotes de lecture du livre La logique pas à pas de Jacques Duparc que je paraphrase allégrement.\nCalcul des propositions Syntaxe Sémantique Preuve Le calcul des propositions (ou calcul propositionnel ou logique des propositions) permet de modéliser des raisonnements simples. Son pouvoir expressif est assez limité mais c\u0026rsquo;est sur ses fondations que se construisent les logiques plus évoluées.\nSyntaxe Langage Le langage du calcul des propositions est constitué de variables propositionnelles $P$, $Q$, $R$,..., de connecteurs logiques $\\neg, \\lor, \\land, \\rightarrow, \\leftrightarrow$ et de parenthèses. On désigna par $VAR=\\Set{P,Q,R\\dots}$ l\u0026rsquo;ensemble des variables propositionnelles, potentiellement infini.\nLe connectecteur logique $\\neg$ est unaire (ou d'arité 1) puisqu'il transforme à lui seul une formule en une autre formule. C'est le symbole de négation et il se dit non. Les autres connecteurs logiques sont binaires (d'arité 2) puisqu'ils lient deux formules en une nouvelle. $\\lor$ est le symbole de disjonction (\"ou\") $\\land$ est le symbole de conjonction (\"et\") $\\rightarrow$ est le symbole d'implication (\"mplique\") $\\leftrightarrow$ est le symbole de double implication ou d'équivalence (\"si et seulement si\" ou \"équivaut à\") Le langage du calcul des propositions est alors l\u0026rsquo;ensemble suivant : $\\mathcal{L} = VAR\\cup\\Set{\\neg,\\lor,\\land,\\rightarrow,\\leftrightarrow,(,)}$\nFormules On peut représenter les formules du calcul propositionnel par des arbres dont les feuilles sont des variables propositionnelles et les nœuds sont des connecteurs.\nL'ensemble $\\mathcal{F}$ des formules du calcul propositionnel est le plus petit ensemble d'arbres qui\ncontient chaque arbre réduit à sa racine qui est une variable propositionnelle. chaque fois qu'il contient des formules $\\phi$ et $\\psi$ contient également les formules suivantes\u0026nbsp;: La hauteur d\u0026rsquo;une formule est la longueur de sa plus longue branche.\ntip\nC\u0026rsquo;est une définition par récurrence (ou inductive) : on a d\u0026rsquo;abord défini les feuilles, cas de base de hauteur 0, puis on a donné la recette pour passer d\u0026rsquo;un arbre de hauteur $n$ à un arbre de hauteur $n+1$.\nUne sous formule d\u0026rsquo;une formule $\\phi$ est un sous-arbre de $\\phi$ dont l\u0026rsquo;un des nœuds de $\\phi$ est la racine.\nLa formule de hauteur 5 ci-dessus contient :\n5 feuilles, sous-formules de hauteur 0 (en vert), 3 sous-formules de hauteur 1 (en rouge), 2 sous-formules de hauteur 2 (en bleu), 1 sous-formule de hauteur 3 (en rose), 1 sous-formule de hauteur 4 (en jaune). Lorsqu\u0026rsquo;une sous-formule apparaît plusieurs fois dans une formule, on dit qu\u0026rsquo;elle a plusieurs occurences.\nLinéarisation d\u0026rsquo;une formule La linéarisation d\u0026rsquo;une formule $\\theta$ peut s\u0026rsquo;obtenir par induction :\nune feuille $\\color{#007100}P$ se linéarise en $\\color{#007100}P$ : $\\theta=\\color{#007100}P$ les linéarisations des sous-formules suivantes sont données respectivement par $\\theta=\\left(\\color{#B51700}{\\neg}\\color{#007100}{\\phi}\\color{#000} \\right)$, $\\theta=\\left(\\color{#007100}{\\phi} \\color{#B51700}{\\lor} \\color{#007100}{\\psi} \\color{#000} \\right)$, $\\theta=\\left( \\color{#007100}{\\phi} \\color{#B51700}{\\land} \\color{#007100}{\\psi} \\color{#000} \\right)$, $\\theta=\\left( \\color{#007100}{\\phi} \\color{#B51700}{\\rightarrow} \\color{#007100}{\\psi} \\color{#000} \\right)$, $\\theta=\\left( \\color{#007100}{\\phi} \\color{#B51700}{\\leftrightarrow} \\color{#007100} \\psi \\color{#000} \\right)$. Exemple :\nLa linéarisation de cette formule $\\phi$ donne :\n$$\\displaystyle \\phi = ( \\color{#B51700}{\\neg}\\color{#000} ( \\color{#007100}{P} \\color{#B51700}{\\lor} \\color{#007100}{R}\\color{#000} ) ) \\color{#B51700}{\\land} \\color{#000} (\\color{#007100}{P} \\color{#B51700}{\\lor}\\color{#000} ( ( \\color{#B51700}{\\neg} \\color{#007100}{R}\\color{#000} ) \\color{#B51700}{\\rightarrow} \\color{#000}( ( \\color{#B51700}{\\neg} \\color{#007100}{Q}\\color{#000} ) \\color{#B51700}{\\leftrightarrow} \\color{#007100}{P} \\color{#000} ) )) $$\nnote\nLa syntaxe, c\u0026rsquo;est l\u0026rsquo;articulation des symboles. La sémantique, c\u0026rsquo;est ce que ça raconte.\nSuite : la sémantique "
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/geometrie/",
	"title": "Géométrie",
	"tags": [],
	"description": "",
	"content": " Géométrie La géométrie est avec l\u0026rsquo;arithmétique une des plus vieilles branches des mathématiques (l\u0026rsquo;arithmétique pour tenir les comptes et la géométrie pour mesurer les terres).\nGéométrie et physique sont intimement liées. Le développement de la géométrie a en partie été poussé par les questionnements des astronomes (sur la position des astres, leur mouvement, leur taille, leur distance, etc.) et des ingénieurs (pour construire des trucs qui tiennent debout). Et une explication géométrique des lois fondamentales de l\u0026rsquo;univers attire toujours certains chercheurs, plus de 2000 ans après Platon.\nTriangle : Pythagore, Thalès, droite d\u0026rsquo;Euler Cercle et triangle : trigonométrie, angle inscrit Aires et Volumes Curse of dimensionality Poursuites "
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/algebre/groupes/",
	"title": "Groupes",
	"tags": [],
	"description": "",
	"content": " Miettes de théorie des groupes pour physicien Trois sources majoritairement plagiées pour les pages qui suivent :\nle précieux livre de Wu-Ki Tung «Group Theory in Physics» le fabuleux cours en ligne de Bertrand Delamotte : «Un soupçon de théorie des groupes: groupe des rotations et groupe de Poincaré» les très clairs et fournis cours en ligne de Jean-Bernard Zuber : «Invariances en physique et théorie des groupes» et «Introduction à la théorie des groupes et de leurs représentations» La théorie des groupes découpe et structure les représentations physiques que l’on se fait du monde. Elle prescrit le choix même des grandeurs physique. Son importance est phénoménale.\nMais il faut abandonner tout espoir de trouver ici rigueur mathématique ou exhaustivité. Vous serez en échange comblés de graves approximations, voir errances, émaillées d’«inclartés». Je tente seulement de faire apparaître, en paraphrasant souvent mal mes source, les angles de ce qu’il a été pour moi important de comprendre. Les deux premiers chapitres décrivent les résultats généraux obtenus sur les groupes discrets. Leur rédaction s’appuie grandement sur le Wu-Ki Tung, très très grandement\u0026hellip;\nLes deux premiers chapitres décrivent les résultats généraux obtenus sur les groupes discrets. Leur rédaction s’appuie grandement sur le Wu-Ki Tung, très très grandement\u0026hellip;\nLes groupes discrets Leurs représentations "
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/algebre/groupes/groupe1/",
	"title": "Groupes discrets",
	"tags": [],
	"description": "",
	"content": " Les groupes Retour sommaire\nUn groupe est un club privé, un entre-soi monarchique d’éléments se reproduisant entre eux\u0026hellip;\nGénéralités Définition d\u0026rsquo;un groupe Comme son nom l’indique, un groupe désigne un ensemble d’éléments. Mais pour que cet ensemble soit promu groupe, on doit le munir d’une loi de composition interne (un truc qui dit comment les éléments jouent entre eux et rien qu’entre eux). Quand on compose un élément du groupe avec un autre élément du groupe, on obtient encore un élément du groupe. C’est surtout ça un groupe !\nTechniquement, un groupe $G=\\{a,b,c\\}$ est bien un groupe si\u0026nbsp;: $\\forall (a,b) \\in G^2,\\ a\\cdot b \\in G$ La loi de composition interne est associative, c’est-à-dire\n$ (a\\cdot b)\\cdot c = a\\cdot (b\\cdot c) $ $G$ contient un élément neutre $e$ tel que\n$\\forall a \\in G,\\ a\\cdot e = a$ Pour chaque $a \\in G,$ il existe un élément symétrique $a^{-1}$ tel que $a\\cdot a^{-1} = e$ Remarque : on dira indifféremment \u0026ldquo;loi de composition interne\u0026rdquo; ou \u0026ldquo;loi de multiplication interne\u0026rdquo;.\nExemple de groupe ultra simple\u0026nbsp;: $\\boldsymbol{C_2} = \\{1,-1\\}$ avec la multiplication ordinaire comme loi de composition interne. En effet, $1\\times1,\\; 1\\times(-1),\\; (-1)\\times(-1)$ font tous partie de $\\boldsymbol{C_2}$ puisque le résultat est toujours 1 ou -1. La permutation entre deux éléments associée à l’identité forme un groupe en tout point similaire, c’est aussi $\\boldsymbol{C_2}$. De la même façon, l’identité et l’inversion spatiale (parité), i.e. $x \\mapsto -x,$ forment encore $\\boldsymbol{C_2}$. Un même groupe peut donc être décrit différemment ! Groupes et symétries Groupes et symétries sont fortement liés, ce qui explique l’importance de leur étude en physique, étant donné que les symétries sont au centre de notre compréhension physique du monde.\nImaginons un système $\\mathcal{S}$ laissé invariant par deux symétries différentes $A$ et $B$. La loi de composition interne $A\\cdot B$ correspond, dans le cas des symétries, à l’application successive de $A$ et de $B$ sur le système. Et l’application successive des symétries sur le système continue nécessairement à le laisser invariant, donc $A\\cdot B$ est aussi une symétrie du système. La loi de composition interne étant interprétée comme une succession d’opérations de symétrie, l’associativité en découle : on aura forcément $A\\cdot (B\\cdot C) = (A\\cdot B)\\cdot C$ (en gardant l’ordre).\nEnfin, laisser $\\mathcal{S}$ tel quel constitue l’élément neutre de toutes les symétries et chaque action d’une symétrie peut être inversée.\nLes opérations de symétrie sur un système forment donc toujours un groupe !\nVocabulaire L’ordre ou le cardinal d’un groupe est son nombre d’éléments. Un groupe est dit abélien lorsque la loi de composition est commutative, c’est-à-dire $ab = ba, \\forall a,b \\in G$. $\\boldsymbol{C_2}$ est à l’évidence abélien.\nExemple : Le plus petit groupe non abélien est le groupe des symétries du triangle équilatéral, appelé groupe diédral $\\boldsymbol{D_3}$. Il y a 6 transformations qui laissent le triangle invariant : l’identité ($e$), les rotations de $\\frac{2\\pi}{3}$ et $\\frac{4\\pi}{3}$ (appelées $R_1$ et $R_2$), et les réflexions par rapport aux hauteurs (notées $S_A, S_B, S_C$). Elles sont toutes inversibles et la table de multiplication entre ces transformations finit de prouver qu’il s’agit bien d’un groupe.\nLe caractère non abélien est évident si on compare, par exemple, $R_1 S_A$ (qui amène $A$ en $C$, $B$ en $B$ et $C$ en $A$, en appliquant les transformations de la droite vers la gauche) et $S_A R_1$ (qui amène $A$ en $B$, $B$ en $A$ et $C$ en $C$).\nLa table de multiplication d’un groupe permet de savoir comment chacun des éléments interagit et permet donc de le décrire entièrement. Pour $\\boldsymbol{D_3}$, la table de multiplication est la suivante :\nDeux grandes dynasties de groupe se partagent le royaume :\nles groupes cycliques qui peuvent décomposer tous les groupes abéliens. les groupes symétriques auxquels peuvent se rapporter tout groupe d’ordre fini. Groupes cycliques et groupes symétriques Groupe cyclique Le groupe cyclique $\\boldsymbol{C_n}$ (dont on a déjà côtoyé un membre avec $\\boldsymbol{C_2}$) a la structure générale $\\{e,a,a^2,\\ldots,a^{n-1};a^n=e\\}$ avec $n$ un entier positif quelconque. On le note aussi $\u0026lt;a\u0026gt;$.\na générant tous les éléments du groupe est appelé\u0026hellip; générateur du groupe.\nOn appelle période ou ordre d’un élément $a$ d’un groupe le plus petit entier positif $m$ tel que $a^m=e$. Si $m$ n’existe pas, $a$ est dit d’ordre infini. Le générateur $a$ de $\\boldsymbol{C_n}$ est donc, par définition, de période (ou d’ordre) $n$.\nL’ordre d’un groupe cyclique coïncide avec l’ordre de son générateur (ce qui explique l’utilisation du termes ordre au lieu de période pour un élément).\nTous les groupes cycliques sont abéliens (car $a^i a^j=a^j a^i=a^{i+j}$).\nLes racines n-ième de l’unité $\\{\\exp{^{i2\\pi/n}}\\}$ munies de la règle usuelle de multiplication sont l’exemple concret le plus direct d’un groupe cyclique. Les lignes et colonnes de la table de multiplication de ces groupes sont en permutation circulaire (telle que l’ordre reste le même) les unes par rapport aux autres, d’où son nom.\nTable de multiplication de $\\boldsymbol{C_2}$\u0026nbsp;: Table de multiplication de $\\boldsymbol{C_3}$\u0026nbsp;: Groupe symétrique Le groupe symétrique $\\boldsymbol{S_n}$ est formé de toutes les permutations possibles entre $n$ éléments différents et est donc d’ordre $n!$.\nOn peut représenter de manière générale les permutations de $n$ éléments sur deux lignes :\n$p=\\left(\\begin{array}{ccccc} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; \\cdots \u0026amp; n \\\\ p_1 \u0026amp; p_2 \u0026amp; p_3 \u0026amp; \\cdots \u0026amp; p_n \\end{array}\\right)$\nUne permutation suivie d’une seconde en forme bien sûr une troisième, ce qui définit la loi de composition du groupe. L’identité correspond à l’absence de permutation :\n$ e=\\left(\\begin{array}{lllll} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; \\cdots \u0026amp; n \\\\ 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; \\cdots \u0026amp; n \\end{array}\\right) $\nEt l’inverse de p est logiquement :\n$ p^{-1}=\\left(\\begin{array}{ccccc} p_1 \u0026amp; p_2 \u0026amp; p_3 \u0026amp; \\cdots \u0026amp; p_n \\\\ 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; \\cdots \u0026amp; n \\end{array}\\right) $\nExemple : $\\boldsymbol{S_3}$ est d’ordre $3!=6$. Les 6 permutations sont :\n$$ \\begin{aligned} \u0026amp; \\left(\\begin{array}{lll} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{array}\\right) \\\\ \u0026amp; \\left(\\begin{array}{lll} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{array}\\right),\\left(\\begin{array}{lll} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{array}\\right),\\left(\\begin{array}{lll} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{array}\\right) \\\\ \u0026amp; \\left(\\begin{array}{lll} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{array}\\right),\\left(\\begin{array}{lll} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{array}\\right) \\end{aligned} $$\nLe premier élément n’est autre que $e$ (rien n’a bougé). Les trois éléments suivants sont obtenus en permutant deux éléments du trio et en laissant le troisième tranquille. Les deux derniers éléments sont obtenus en permutant circulairement les trois éléments.\nPour une écriture plus compacte, on décompose les permutations en 1-cycle (pas de permutations), 2-cycle (permutations 2 à 2), 3-cycle (permutations circulaires à 3 éléments), etc.\nCela donne pour les 6 permutations du groupe $\\boldsymbol{S_3}$\u0026nbsp;:\n$$\\begin{aligned} \u0026amp; e=(1)(2)(3) \\\\ \u0026amp; (12)(3),(1)(23),(2)(31), \\\\ \u0026amp; (123),(321) \\end{aligned} $$\nL’ordre d’écriture est indifférent et il est d’usage d’omettre les 1-cycle ou éléments non permutés ($(12)(3)=(12)$).\nLorsqu’on multiplie deux permutations, on part de la permutation la plus à droite et on regarde où arrive successivement chacun des éléments.\nExemple : $(23)\\cdot(13)=\\,?$\n$1\\rightarrow\\,?$ : $(13)$ est tel que $1\\rightarrow 3$ et $(23)$ est tel que $3\\rightarrow 2$, donc 1 est envoyé sur 2 ($1\\rightarrow 2$) par le produit des permutations. $2\\rightarrow\\,?$ : $(13)$ est tel que $2\\rightarrow 2$ et $(23)$ est tel que $2\\rightarrow 3$, donc 2 est envoyé sur 3 ($2\\rightarrow 3$) par le produit des permutations. $3\\rightarrow\\,?$ : $(13)$ est tel que $3\\rightarrow 1$ et $(23)$ est tel que $1\\rightarrow 1$, donc 3 est envoyé sur 1 ($3\\rightarrow 1$) par le produit des permutations. On obtient bien la permutation cyclique $(123)$ dans laquelle $1\\rightarrow 2\\rightarrow 3\\rightarrow 1$. Donc $(23)\\cdot (13) = (123)$.\nAutre exemple : $(321)\\cdot (12) = (1)(23)(2) = (23)$\nSous-groupes, morphismes, classes et groupes quotients Sous-groupes Un sous-ensemble de $G$ qui forme un groupe avec la même loi de multiplication est un sous-groupe de $G$. Tout élément $a$ différent de $e$ d’un groupe $G$ d’ordre fini forme un sous-groupe cyclique de $G$.\nPreuve : $a\\cdot a=a^2$ est dans $G$ par propriété des groupes et vaut soit $e$ soit un élément différent de $a$ (car seul $a\\cdot e$ donne $a$).\nDe même, si $a^2≠e$, alors $a^2\\cdot a=a^3$ vaut soit $e$, soit un élément différent à la fois de $a$ et $a^2$ puisqu’ils sont tous deux différents de $e$. En continuant ainsi, on obtient un ensemble ${a,a^2,a^3,a^4,a^5,\\ldots}$ s’arrêtant pour $a^p$ valant $e$ (et cela arrive nécessairement puisque le groupe $G$ est fini). On obtient alors un groupe cyclique d’ordre $p$.\nMorphismes Un homomorphisme entre un groupe $G$ et un groupe $G^{\\prime}$ est une application envoyant les éléments de $G$ vers $G^{\\prime}$ tout en préservant la loi de composition\u0026nbsp;: si $g_i \\in G \\mapsto g_i^{\\prime} \\in G^{\\prime}$ et $g_1g_2=g_3$, alors $g_1^{\\prime} g_2^{\\prime}=g_3^{\\prime}$.\nQuand l’application est bijective, un élément pour un élément, on parle d’isomorphisme (on le note symboliquement $G \\simeq G^{\\prime}$).\nSi on appelle $f$ l’homomorphisme de $G$ vers $G^{\\prime}$, on a :\n$f(g_1)f(g_2) = f(g_1g_2)$ par définition, $f(g)f(e) = f(g)$ donc $f(e) = e'$ (l’élément neutre est préservé), $f(g^{-1}) = f^{-1}(g)$ car $f(g^{-1})f(g) = f(e) = e'$. Cela montre que :\nL’image de l'homomorphisme $f$, notée $\\mathrm{Im}(f) \\equiv f(G) \\equiv \\{ f(g) ; g \\in G \\}$, est un sous-groupe de $G'$. Preuve : $e' \\in \\mathrm{Im}(f)$, Pour tout $g_1, g_2 \\in G$, $f(g_1)f(g_2) = f(g_1g_2) \\in \\mathrm{Im}(f)$, Pour tout $g \\in G$, $f(g^{-1}) = f(g)^{-1} \\in \\mathrm{Im}(f)$. Construisons la table de multiplication du groupe $\\boldsymbol{S_3}$ :\nOn remarque qu’en identifiant les deux permutations circulaires aux rotations de $\\boldsymbol{D_3}$ et les 2-cycles aux réflexions, on retrouve exactement la même table de multiplication. Cela montre que $\\boldsymbol{S_3}$ et $\\boldsymbol{D_3}$ sont isomorphes ($\\boldsymbol{S_3} \\simeq \\boldsymbol{D_3}$).\nL’exemple précédent est généralisable\u0026nbsp;: Théorème de Cayley\u0026nbsp; Tout groupe $G$ d’ordre $n$ est isomorphe à un sous-groupe de $\\boldsymbol{S_n}$.\nPreuve : Les éléments de $G$ sont étiquetés $\\{g_i \\, ; i = 1, \\dots, n\\}$.\nPour un élément $a \\in G$, l’élément $a g_i$ est un élément de $G$.\nOn peut très bien appeler $a_i$ l’indice entier qui désigne cet élément : $g_{a_i} \\equiv a g_i$, ce qui détermine une séquence de nombres entiers $\\left(a_1, \\cdots, a_n\\right)$.\nComme $a g_i=a g_k$ seulement pour $i=k$ (suffit de multiplier par $a^{-1}$ pour s’en assurer), tous les entiers $\\left\\{a_1, \\cdots, a_n\\right\\}$ sont différents. Ils forment donc une permutation de $(1,2, \\cdots, n)$.\nPlus simplement, on peut voir l’action de $a$ sur l’ensemble des $g\\in G$ comme une translation (à gauche) des éléments du groupe ($ga$ serait la translation à droite, différente par défaut).\nOn peut par conséquent envoyer $G$ sur $\\boldsymbol{S_n}$ :\n$ a \\in G \\longrightarrow p_a=\\left(\\begin{array}{cccc} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; n \\\\ a_1 \u0026amp; a_2 \u0026amp; \\cdots \u0026amp; a_n \\end{array}\\right) \\in \\boldsymbol{S_n} $\nSi $ab=c$ dans $G$, on a :\n$\\begin{aligned} p_a p_b\u0026amp;=\\left(\\begin{array}{cccc} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; n \\\\ a_1 \u0026amp; a_2 \u0026amp; \\cdots \u0026amp; a_n \\end{array}\\right) \\cdot\\left(\\begin{array}{cccc} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; n \\\\ b_1 \u0026amp; b_2 \u0026amp; \\cdots \u0026amp; b_n \\end{array}\\right) \\\\ \u0026amp; =\\left(\\begin{array}{llll} b_1 \u0026amp; b_2 \u0026amp; \\cdots \u0026amp; b_n \\\\ a_{b_1} \u0026amp; a_{b_2} \u0026amp; \\cdots \u0026amp; a_{b_n} \\end{array}\\right) \\cdot\\left(\\begin{array}{llll} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; n \\\\ b_1 \u0026amp; b_2 \u0026amp; \\cdots \u0026amp; b_n \\end{array}\\right)=\\left(\\begin{array}{cccc} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; n \\\\ a_{b_1} \u0026amp; a_{b_2} \u0026amp; \\cdots \u0026amp; a_{b_n} \\end{array}\\right) \\end{aligned} $\nOr $g_{a_{b_i}}=a g_{b_i}=a\\left(b g_i\\right)=(a b) g_i=c g_i=g_{c_i}$, donc :\n$p_a p_b=p_c=\\left(\\begin{array}{cccc} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; n \\\\ c_1 \u0026amp; c_2 \u0026amp; \\cdots \u0026amp; c_n \\end{array}\\right) \\in S_n$\nOn a finalement montré que l’application $a \\in G \\longrightarrow p_a \\in \\boldsymbol{S_n}$ préserve la loi de composition interne, ie c’est un homomorphisme.\nEt comme on a commencé par dire que l’application envoyait un antécédent sur une image unique, il s’agit d’un isomorphisme.\nL’ensemble des $p_a$ (pour tous les $a$ de $G$) forme donc un sous-groupe de $\\boldsymbol{S_n}$ isomorphe à $G$.\nClasses Deux éléments $a$ et $b$ de $G$ sont dit conjugués s’il existe un troisième élément $p$ de $G$ tel que $b=pap^{-1}$. On écrit $b\\sim a$ car il s’agit d’une relation d’équivalence. Une relation d’équivalence se doit d’être symétrique ($a\\sim b \\Rightarrow b\\sim a$), réflexive ($a\\sim a$) et transitive (si on a $a\\sim b$ et $b\\sim c$ alors $a\\sim c$), ce qu’on vérifie bien avec la relation de conjugaison.\nDes éléments conjugués les uns par rapport aux autres forment une classe de conjugaison. L’identité (élément neutre) forme une classe à elle seule. Dans un groupe symétrique, les cycle d’une même longueur appartienne à une même classe. Preuve : soit $p$ un cycle de longueur donnée et $q$ une permutation quelconque du même groupe de symétrie alors :\n$ \\begin{aligned} q p q^{-1} \u0026amp; =\\left(q_i \\leftarrow i\\right)\\left(p_i \\leftarrow i\\right)\\left(i \\leftarrow q_i\\right) \\\\ \u0026amp; =\\left(q_i \\leftarrow i\\right)\\left(p_i \\leftarrow q_i\\right) \\\\ \u0026amp; =\\left(q_{p_i} \\leftarrow p_i\\right)\\left(p_i \\leftarrow q_i\\right) \\\\ \u0026amp; =\\left(q_{p_i} \\leftarrow q_i\\right) \\\\ \u0026amp; =q[p] \\end{aligned} $\nOn n’a fait qu’étiqueter différemment la permutation $p$ :\n$ \\left(\\begin{array}{ccccc} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; \\cdots \u0026amp; n \\\\ p_1 \u0026amp; p_2 \u0026amp; p_3 \u0026amp; \\cdots \u0026amp; p_n \\end{array}\\right) $ devient $ \\left(\\begin{array}{ccccc} q_1 \u0026amp; q_2 \u0026amp; q_3 \u0026amp; \\cdots \u0026amp; q_n \\\\ p_{q_1} \u0026amp; p_{q_2} \u0026amp; p_{q_3} \u0026amp; \\cdots \u0026amp; p_{q_n} \\end{array}\\right) $.\nL’ordre change mais pas ce que devient chacune des valeurs. La structure des cycles est donc nécessairement la même.\nExemple : $\\boldsymbol{S_3}$ est donc constitué de 3 classes :\n$e$ $\\{(12),(23),(13)\\}$ $\\{(123),(321)\\}$ Comme l’isomorphisme entre $\\boldsymbol{D_3}$ et $\\boldsymbol{S_3}$ l’impose, ces classes trouves bien leurs correspondances dans $\\boldsymbol{S_3}$ puisqu’on peut vérifier que l’identité, les deux rotations et les trois réflexions forment là encore trois classes de conjugaison. Chaque élément d’un groupe appartient à une et une seule classe puisque l’action de conjugaison consiste en une translation à gauche et une translation à droite (composer tous les éléments du groupe par un même élément revient à les décaler (ou permuter) tous de la même façon) et donc associe un élément conjugué différent à chaque élément différent du groupe (c’est une bijection).\nEt d’autre part, deux classes différentes sont disjointes par transitivité (si elle ne sont pas disjointes, elles coïncident). Par conséquent, l’union de toutes les classes d’un groupe reforme le groupe, ou dit autrement, les classes forment une partition du groupe.\nUn sous-groupe $H$ de $G$ est dit invariant, ou normal, ou distingué, s’il est identique à ses sous-groupes conjugués ($H=g^{-1} H g$ pour $g\\in G$).\nRemarque :\nun sous-groupe invariant est nécessairement une union de classes conjuguées dont l’identité (un groupe doit la contenir).\nExemple : Le sous-groupe ${e, (123), (321)}$ de $\\boldsymbol{S_3}$ forme un sous-groupe invariant puisqu’il contient l’identité et la classe entière des 3-cycles. Tout élément conjugué de cet ensemble appartient à une de ces deux classes et se trouve donc dans l’ensemble de départ.\nLes classes de conjugaison ne sont pas les seules à savoir découper un groupe. Leurs concurrentes : les classes latérales dites à gauche ou à droite issues d’un sous-groupe donné.\nCette partition diffère de la précédente sur deux points : elle n’est pas nécessairement unique et les classes latérales entrant dans la partition comportent chacune le même nombre d’éléments.\nSoit $H=\\{h_1,h_2,\\ldots\\}$ un sous-groupe de $G$ et $p$ un élément de $G$ (qui n’est pas dans $H$). Alors l’ensemble $pH=\\{ph_1,ph_2,\\ldots\\}$ est appelé classe à gauche de $G$ suivant $H$.\nDe même, $Hp$ est une classe à droite suivant $H$.\nRemarques :\nsi $p$ est dans $H$, on récupère $H$ ($pH=H=Hp$) par définition d’un sous-groupe. une classe à gauche (ou à droite), comme une classe tout court, n’est généralement pas un groupe (on doit contenir l’identité pour en être un !). Deux classes à gauche (ou à droite) d’un même sous-groupe soit coïncident complètement, soit n’ont aucun élément en commun.\nPreuve : Soient $pH$ et $qH$ deux classes à gauche et supposons qu’on ait, pour un certain $h_i$, et un certain $h_j$ pris dans $H$, $ph_i=qh_j$, donc $pH$ et $qH$ ont au moins un élément en commun.\nComme $q^{-1}p=h_j h_i^{-1}$, $q^{-1}p$ est un élément de $H$.\nPar conséquent, $q^{-1}pH=H$ (puisque $H$ est un sous-groupe donc un groupe).\nConclusion : $pH=qH$.\nPour ne pas être dans ce cas, il ne faut aucun $h_i$ et $h_j$ tels que $ph_i=qh_j$, autrement dit, $pH$ et $qH$ doivent être disjoints.\nEt chaque classe à gauche (ou à droite) suivant un sous-groupe $H$ a nécessairement autant d’éléments que $H$.\nOn en déduit que pour un sous-groupe $H$ d’ordre $n_H$, l’ensemble des classes à gauche (ou à droite) forme une partition des éléments de $H$ en ensembles disjoints de $n_H$ éléments chacun.\nExemple : Le sous-groupe $\\left\\{H_1: e,(123),(321)\\right\\}$ de $\\boldsymbol{S_3}$ possède une classe à gauche $\\{M:(12),(23),(31)\\}$ obtenue en multipliant à gauche les éléments de $H_1$ par $(12)$, ou par $(23)$, ou encore par $(31)$.\nLe sous-groupe $\\left\\{H_2: e,(12)\\right\\}$ de $\\boldsymbol{S_3}$ possède, lui, deux classes à gauche :\n$\\left\\{M_1:(23),(321)\\right\\}$ obtenue en multipliant à gauche les éléments de $H_2$ soit par $(23)$, soit par $(321)$\u0026nbsp;; $\\left\\{M_2:(31),(123)\\right\\}$ obtenue en multipliant à gauche les éléments de $H_2$ soit par $(31)$, soit par $(123)$. Le théorème de Lagrange en découle :\nl’ordre d’un groupe fini doit être un multiple entier de l’ordre de n’importe lequel de ses sous-groupes.\nCela entraîne que tout groupe d’ordre premier est cyclique et donc abélien. Plutôt joli, non ?\nPreuve : Soit $a$ un élément de $G$ différent de $e$.\nAlors $a$ forme un sous-groupe cyclique de $G$ d’ordre au moins 2.\nOr cet ordre doit diviser l’ordre $G$.\nSeule solution, il vaut l’ordre $G$, ce qui implique que $G$ soit cyclique et par conséquent abélien.\nGroupes quotients Les classes à gauche ou à droite issues d’un sous-groupe invariant sont particulièrement simples et utiles. Déjà, classes à gauche et classes à droite coïncident ($pHp^{-1}=H$ implique $pH=Hp$). De plus, la partition obtenue est unique et une «factorisation» de $G$ basée sur cette partition devient naturelle.\nL’ensemble des classes issues d’un sous-groupe invariant $H$ d’un groupe $G$ a la propriété de former lui-même un groupe, appelé groupe quotient $G/H$, d’ordre $n_G/n_H$.\nPreuve : La loi de composition interne entre deux classes latérales $pH$ et $qH$ est définie comme l’ensemble des produits $ph_iqh_j=(pq)h_k$ avec $h_k=(q^{-1}h_iq)h_j$ appartenant bien à $H$ (puisque $H$ est un sous-groupe invariant).\nPlus simplement\u0026nbsp;: $pHqH=pqH$. $H=eH$ joue le rôle de l’élément neutre. $p^{-1}H$ est l’inverse de $pH$. $pH\\cdot(qH\\cdot rH)=(pH\\cdot qH)\\cdot rH=(pqr)H$ Exemple 1 : Considérons $\\mathbb{Z}/2\\mathbb{Z}$ où $\\mathbb{Z}$ est l’ensemble des entiers relatifs munis de l’addition comme loi de composition interne. $2\\mathbb{Z}$ est donc l’ensemble des entiers relatifs pairs. Et par conséquent, $\\mathbb{Z}/2\\mathbb{Z}$ est formé de deux sous-groupes distincts : les entiers pairs et les entiers impairs. Le groupe quotient $\\mathbb{Z}/2\\mathbb{Z}$ est donc isomorphe au groupe cyclique à deux éléments $\\boldsymbol{C_2}$. Il correspond aussi à l’ensemble $\\{0,1\\}$ muni de l’addition modulo 2.\nOn peut généraliser en disant que $\\mathbb{Z}/2\\mathbb{Z}$ est isomorphe au groupe cyclique $\\boldsymbol{C_n}$ et correspond aussi à l’ensemble des restes dans la division euclidienne de $k$ par $n$, soit l’ensemble $\\{0,1,\\ldots,n\\}$ muni de l’addition modulo $n$.\nL’exemple précédent permet de mieux comprendre pourquoi $G/H$ se lit $G$ modulo $H$.\nExemple 2 : Dans le cas de $\\boldsymbol{S_3}$, $H=\\{e,(123),(321)\\}$ est un sous-groupe invariant.\n$G/H$ contient deux éléments : $H$ et $M=\\{(12),(23),(31)\\}$.\n$H$ est l’ensemble des permutations paires à 3 éléments (l’identité correspond à aucune permutation et les 3 cycles à des permutations doubles). On appelle aussi $H$ $\\boldsymbol{A_3}$, groupe alterné d’ordre 3.\n$M$ est l’ensemble des permutations impaires.\nOn voit facilement que la composition de deux permutations impaires ou paires donne une permutation paire alors qu’une composition mixte donne une permutation impaire :\n$HM=MH=M$, $HH=H$ et $MM=H$.\nOn en déduit que $G/H$ est isomorphe à $\\boldsymbol{C_2}$ (H est envoyé sur l’identité et $\\boldsymbol{M}$ correspond à l’autre élément).\nLe morphisme $f: G \\rightarrow G / H, g \\mapsto g H$ est appelé morphisme canonique ou projection canonique.\nLe deuxième exemple illustre un théorème qui va se révéler bien utile mais définissons d’abord le noyau d’un homomorphisme :\nSoit f un homomorphisme de $G$ à $G^{\\prime}$. On appelle noyau $K$ de cet homomorphisme l’ensemble des éléments de $G$ qui sont envoyé sur l’élément neutre de $G’$ ($K=\\{g \\in G ; g \\stackrel{f}{\\longmapsto} e^{\\prime} \\in G^{\\prime}\\}$).\nThéorème d’isomorphisme (premier) :\nSoit $f$ un homomorphisme de $G$ à $G^{\\prime}$ de noyau $K$.\n$K$ forme alors un sous-groupe invariant de $G$.\nLe groupe quotient $G/K$ est isomorphe à $G^{\\prime}$.\nAutrement dit, on rend $f$ injectif en quotientant $G$ par son noyau.\nCela se note symboliquement $G / K \\simeq G^{\\prime}$ ou encore $G / \\operatorname{Ker}(f) \\simeq f(G)$ où $\\operatorname{Ker}(f)$ désigne le noyau de $f$ et $f(G)$ est l’image de $f$.\nPreuve :\nMontrons que $K$ est un sous-groupe\u0026nbsp;:\npour $a$ et $b$ dans $K$, $a \\cdot b \\xrightarrow{f} e^{\\prime} \\cdot e^{\\prime}=e^{\\prime}$, donc $a\\cdot b$ est aussi dans $K$. La préservation de la loi de composition par l’homomorphisme assure que pour $g \\xrightarrow{f} g^{\\prime}$, on a aussi $e \\xrightarrow{f} e^{\\prime}$ et $g^{-1} \\xrightarrow{f} g^{\\prime-1}$. D’où $e \\in K$, et si $a\\in K$, alors $a^{-1}$ est aussi dans $K$ (car $a^{-1} \\xrightarrow{f} e^{\\prime-1}=e^{\\prime}$). Montrons que $K$ est invariant\u0026nbsp;:\nprenons $a$ dans $K$ et $g$ dans $G$. $g a g^{-1} \\xrightarrow{f} g^{\\prime} e^{\\prime} g^{\\prime-1}=e^{\\prime}$. Donc $g a g^{-1} \\in K$ pour tout $g\\in G$. Montrons que $G/K$ est isomorphe à $G^{\\prime}$\u0026nbsp;: Les éléments du groupe quotient $G/K$ sont les classes latérales $pK$.\nConsidérons l’application envoyant les classes latérales vers l’image de $f$, $p K \\xrightarrow{\\rho} f(p)=p^{\\prime} \\in G^{\\prime}$.\n$\\rho$ est bien définie\u0026nbsp;: si $pK=qK$ pour $p,q\\in K$, alors $q^{-1}p$ est aussi dans $K$ (puisque $K$ est un groupe) et donc comme $f$ est un homomorphisme\u0026nbsp;: $ \\begin{aligned} f\\left(q^{-1} p\\right) \u0026amp; =1 \\\\ \u0026amp; =f\\left(q^{-1}\\right) f(p) \\\\ \u0026amp; =f^{-1}(q) f(p) \\end{aligned} $\nEt finalement, $f(q)=f(p)$.\n$\\rho$ est bien un homomorphisme\u0026nbsp;: $ \\begin{aligned} \\rho(p K \\cdot q K) \u0026amp; =\\rho(p q K) \\\\ \u0026amp; =f(p q) \\\\ \u0026amp; =f(p) f(q) \\\\ \u0026amp; =\\rho(p K) \\rho(q K) \\end{aligned} $\nSi $\\rho(p K)=\\rho(q K)$ alors $\\rho\\left(q^{-1} p K\\right)=\\rho\\left(q^{-1} K \\cdot p K\\right)$ (le groupe quotient est un groupe), et par action de groupe de l’homomorphisme, $\\rho\\left(q^{-1} K \\cdot p K\\right)=\\rho\\left(q^{-1} K\\right) \\rho(p K)=\\rho^{-1}(q K) \\rho(p K)=e^{\\prime}$, ce qui implique $q^{-1} p K=K$ ou $qK=pK$. L’application est bien bijective.\nLe dessin ci-dessus illustre le cas d’un groupe $G$ contenant 15 éléments avec un noyau $K$ en contenant 3. $G/K$ et $G^{\\prime}$ sont alors tous deux d’ordre 5.\nExemple : on a vu dans un exemple précédent que l’homomorphisme de $\\boldsymbol{S_3}$ sur $\\boldsymbol{C_2}$ devient un isomorphisme si on quotiente $\\boldsymbol{S_3}$ par le sous groupe invariant $H=\\{e,(123),(321)\\}$ : $\\boldsymbol{S_3} / H \\simeq \\boldsymbol{C_2}$. Or $H$ est bien le noyau $K$ de l’homomorphisme comme le prévoit le théorème d’isomorphisme.\nImage et noyau nous disent beaucoup sur les homomorphismes.\nEn effet :\nPour un homomorphisme $f$ tel que $G \\xrightarrow{f} G^{\\prime}$ :\n$f$ est injectif si et seulement si $\\operatorname{Ker}(f)=e$ $f$ est surjectif si et seulement si $\\operatorname{Im}(f)=G^{\\prime}$ Produit direct de deux groupes Soit $H_1$ et $H_2$ deux sous-groupes du groupe $G$ avec les deux propriétés suivantes :\nles éléments de $H_1$ commutent avec les éléments de $H_2$. chaque élément de $g\\in G$ peut s’écrire $g=h_1h_2$ avec $h_1\\in H_1$ et $h_2\\in H_2$. $G$ est alors le produit direct de $H_1$ et $H_2$\u0026nbsp;: $G=H_1 \\otimes H_2$. Exemple : Décomposons $\\boldsymbol{C_6}=\\{e=a^6, a, a^2, a^3, a^4, a^5\\}$ en $\\color{red} H_1=\\{e, a^3\\}$ et $\\color{blue} H_2=\\{e, a^2,a^4\\}$.\nComme $\\boldsymbol{C_6}$ est abélien, le premier critère est respecté.\nEt on a : $e = \\color{red}e\\color{blue}e$, $a = \\color{red}a^3\\color{blue}a^4$, $a^2 = \\color{red}e\\color{blue}a^2$, $a^3 = \\color{red}a^3\\color{blue}e$, $a^4 = \\color{red}e\\color{blue}a^4$, $a^5 = \\color{red}a^3\\color{blue}a^2$.\n$H_1 \\simeq C_2$ et $H_2 \\simeq C_3$ donc $C_6 \\simeq C_2 \\otimes C_3$.\nSi $G=H_1 \\otimes H_2$, alors $H_1$ et $H_2$ doivent être invariants.\nPreuve : Pour $a_1\\in H_1$, $g a_1 g^{-1}=h_1 h_2 a_1\\left(h_1 h_2\\right)^{-1}=h_1 h_2 a_1 h_2^{-1} h_1^{-1}=h_1 a_1 h_1^{-1} \\in H_1$ (et on peut bien sûr faire pareil avec $a_2$ dans $H_2$).\nOn peut donc construire les groupes quotient $G/H_1$ et $G/H_2$. On montre alors que $G / H_1 \\simeq H_2$ et $G / H_2 \\simeq H_1$, ce qui éclaire un peu plus le terme de groupe quotient.\nChapitre suivant\u0026nbsp;: les représentations\nRetour sommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/bayes/",
	"title": "Inférences bayésiennes",
	"tags": [],
	"description": "",
	"content": " Inférences bayésiennes L\u0026rsquo;intelligence artificielle vise à simuler l\u0026rsquo;intelligence humaine. Mais la compréhension du fonctionnement de notre cerveau en est elle-même encore qu\u0026rsquo;à ses prémices.\nTel un programme informatique, le cerveau produit de nouvelles informations à partir de l\u0026rsquo;information qu\u0026rsquo;il reçoit (les données). Un changement de paradigme a eu lieu récemment dans la communauté scentifique concernant la modalité même de ce traitement de l\u0026rsquo;information par le cerveau. Stanislas Dehaene, un psychologue spécialisé en neuropsychologie, résume cette nouvelle façon d\u0026rsquo;apréhender l\u0026rsquo;information dans ses leçons au Collège de France :\nUn vaste courant récent des sciences cognitives s’appuie sur la théorie mathématique de l’inférence bayésienne pour modéliser une très grande diversité de phénomènes psychologiques : perception, inférence statistique, prise de décision, apprentissage, traitement du langage\u0026hellip; La rapidité avec laquelle cette théorie envahit et unifie divers domaines de la cognition, la simplicité de ses fondements axiomatiques, et la profondeur de ses conclusions justifient de parler d’une véritable « révolution bayésienne » en sciences cognitives.\nPour résumer, la théorie bayésienne fournit un modèle mathématique de la manière optimale de mener un raisonnement plausible en présence d’incertitudes. Dès la naissance, le bébé semble doté de compétences pour ce type de raisonnement probabiliste. L’inférence bayésienne rend également bien compte des processus de perception : étant donné des entrées ambigües, le cerveau en reconstruit l’interprétation la plus probable. La règle de Bayes indique comment combiner, de façon optimale, les a priori issus de notre évolution ou de notre mémoire avec les données reçues du monde extérieur. En cela, elle offre une nouvelle vision de l’apprentissage qui dépasse le dilemme classique entre théories empiristes et nativistes. Enfin, de nombreuses décisions humaines semblent résulter d’une approximation de la règle bayésienne d’accumulation d’évidence, combinée à une estimation de la valeur attendue des conséquences de nos choix. Dans la mesure où les principes de l’inférence bayésienne sont ainsi partagés par de multiples domaines de la cognition, il se pourrait que l’architecture du cortex ait évolué pour approximer ce type de calcul probabiliste à grande vitesse, et de façon massivement parallèle. L’algorithme utilisé pourrait expliquer non seulement l’organisation du cortex en couches, mais aussi la manière dont notre cerveau anticipe sur le monde extérieur (codage prédictif) et dont il répond à la nouveauté (propagation des signaux d’erreur).\nDigression sur le raisonnement avec une autre vidéo d\u0026rsquo;Hygiéne Mentale.\nBibliothécaire vs agriculteur On trouve dans le livre \u0026ldquo;Système 1 / Système 2. Les deux vitesses de la pensée\u0026rdquo; du prix Nobel d\u0026rsquo;économie Daniel Kahneman une expérience illustrant ce que peut nous apporter une meilleur familiarité avec le bayésianisme.\nL\u0026rsquo;expérience consistait à présenter à un amphi le portrait suivant :\nSteve est très timide et réservé, toujours prêt à rendre service, mais sans vraiment s'intéresser aux gens ou à la réalité. Personnalité docile et méticuleuse, il a besoin d'ordre et de structure, et se passionne pour les détails. Puis on demande à l\u0026rsquo;audience si Steve est plus susceptible d\u0026rsquo;être bibliothécaire ou agriculteur.\nUne large majorité répond alors bibliothécaire tant le portrait est proche du stéréotype associé à cette profession.\nEt c\u0026rsquo;était aussi le cas de ChatGPT 3.5 au départ (rappelons qu\u0026rsquo;il ne s\u0026rsquo;agit pas d\u0026rsquo;une IA qui raisonne mais d\u0026rsquo;une IA produisant un discours visant à satisfaire l\u0026rsquo;utilisateur) :\nMais sachant qu\u0026rsquo;aux Etats-Unis, où l\u0026rsquo;étude a été menée, il y a au moins 20 fois plus d\u0026rsquo;agriculteurs que de bibliothécaires, les chances que Steve soit bibliothécaire sont minces.\nUn raisonnement bayésien nous aurait prémuni d\u0026rsquo;une conclusion trop hâtive sur la profession de Steve. Il consiste non pas à déduire une probabilité d\u0026rsquo;une information donnée mais à mettre à jour une probabilité a priori à partir de cette information.\nLa probabilité a priori de l\u0026rsquo;hypothèse $H$ est $\\textcolor{#0076BA}{P(H)}$.\nIci, l\u0026rsquo;hypothèse est que Steve est bibliothécaire et la probabilité a priori vaut 4,8% $\\left(\\frac{1}{20+1}\\right)$.\nLa probabilité a priori de l\u0026rsquo;hypothèse contraire $\\overline{H}$ vaut $\\textcolor{#56C1FF}{P(\\overline{H})}=1-\\textcolor{#0076BA}{P(H)}$.\nIci, cela correspond à un Steve agriculteur puisqu\u0026rsquo;on suppose qu\u0026rsquo;il est soit bibliothécaire, soit agriculteur. La probabilité a priori d\u0026rsquo;un Steve agriculteur vaut 95,2%.\nLe portrait de Steve est l\u0026rsquo;information nouvelle apportée $I$, elle modifie la plausibilité que Steve soit bibliothécaire tel un curseur qui va tirer la probabilité dans un sens ou dans l\u0026rsquo;autre. La probabilité de l\u0026rsquo;information $I$ est notée $\\textcolor{#1DB100}{P(I)}$.\nLa probabilité a posteriori correspond alors à la probabilité de $H$ sachant $I$, qu\u0026rsquo;on note $\\textcolor{#CB297B}{P(H|I)}$ (que devient la probabilité de $H$ une fois qu\u0026rsquo;on a connaissance de l\u0026rsquo;information $I$).\nC\u0026rsquo;est le théorème de Bayes qui va nous permettre de mettre à jour la probabilité a priori pour obtenir la probabilité a posteriori :\n$ \\displaystyle \\textcolor{#CB297B}{P(H|I)}=\\frac{ \\textcolor{#FF644E}{P(I|H)}\\times \\textcolor{#0076BA}{P(H)}}{\\textcolor{#1DB100}{P(I)}}=\\frac{ \\textcolor{#FF644E}{P(I|H)}\\times \\textcolor{#0076BA}{P(H)}}{ \\textcolor{#FF644E}{P(I|H)}\\times \\textcolor{#0076BA}{P(H)} + \\textcolor{#00A89D}{P(I|\\overline{H})} \\times \\textcolor{#56C1FF}{P(\\overline{H})}} $ $\\textcolor{#FF644E}{P(I|H)}$ est la probabilité d\u0026rsquo;avoir $I$ si $H$ est vraie (probabilité que Steve corresponde à la description s\u0026rsquo;il est bibliothécaire).\n$\\textcolor{#00A89D}{P(I|\\overline{H})}$ est la probabilité d\u0026rsquo;avoir $I$ si $H$ est fausse (probabilité que Steve corresponde à la description s\u0026rsquo;il est agriculteur).\nCes deux probabilités ne sont pas connues, mais on peut les estimer !\nL\u0026rsquo;illustration ci-dessous permet de se rendre compte que le théorème de Bayes est finalement assez trivial : Et encore plus trivial avec le plus grand effort pédagogique fait ci-dessous :\nOn peut utiliser la représentation graphique décrite dans la vidéo d\u0026rsquo;introduction pour nous aider à faire les calculs en fonction de nos estimations. C\u0026rsquo;est ce qui est fait dans l\u0026rsquo;appliquette Geogebra ci-dessous (cliquer pour l\u0026rsquo;ouvrir).\nEn supposant que le portrait corresponde à 40% des bibliothécaires et à 5% des agriculteurs, que vaut la probabilité a posteriori que Steve soit bibliothécaire ?\nLa probabilté que le portrait corresponde à un bibliothécaire devrait être combien de fois supérieure à celle qu\u0026rsquo;il corresponde à un agriculteur pour faire basculer la probabilité a posteriori que Steve soit bibliothécaire au-delà de 50% ? Comparez au ratio bibliothécaires sur agriculteurs.\nMoralité, le portrait correspond probablement plus à un agriculteur !\ninfo\nLes fréquentistes et les bayésiens interprètent les probabilités différemment.\nPour les fréquentistes, une probabilité est la limite vers laquelle tendrait une fréquence mesurée sur un échantillon lorsqu\u0026rsquo;on fait tendre la taille de l\u0026rsquo;échantillon vers l\u0026rsquo;infini.\nPour les bayésiens, une probabilité mesure un degré de conviction qui est mis à jour à chaque nouvelle information obtenue.\nAutre célèbre exemple de portrait tiré du même livre de Kahneman (et toujours avec ChatGPT comme cobaye) :\nje pleure pic.twitter.com/d9qL7qcp2I\n\u0026mdash; Dr. Juliette (@FerryDanini) February 12, 2023 Le paradoxe des deux enfants Un couple a deux enfants. On nous informe que l\u0026rsquo;un des deux est une fille. Quelle est la probabilté que les deux enfants soient des filles ?\nUtilisez le théorème de Bayes pour répondre en précisant d\u0026rsquo;abord $H$ et $I$, puis en déterminant les différentes probabilités $P(H)$, $P(I|H)$ et $P(I)$.\nVérifier votre résultat en complétant le code python suivant afin qu\u0026rsquo;il simule une expérience sur 100000 couples de deux enfants.\nUn couple a deux enfants. On apperçoit une fille dans le jardin. Quelle est la probabilté que les deux enfants soient des filles ?\nLa réponse change-t-elle ?\nCorrection (cliquer pour afficher) Oui : dans l'hypothèse où il y a deux filles (1 chance sur 4), il y a 100% de chance de voir une fille dans le jardin. Et dans l'hypothèse où il n'y a pas deux filles ($\\bar{H}=3/4$), il y a 1 chance sur 3 de voir une fille dans le jardin (il faut à la fois que les parents aient un garçon et une fille, 2 chance sur 3, et il faut que cela soit la fille dans le jardin, 1 chance sur 2). On a donc $P(H|I)=\\frac{1\\times 1/4}{1\\times 1/4 + 1/3\\times 3/4}=\\frac{1}{2}$. Comment modifier le code pour qu\u0026rsquo;il corresponde à cette situation ?\nCorrection (cliquer pour afficher) from random import randint vufillejardin = 0 deuxfilles = 0 n = 100000 enfants = [None,None] for i in range(n): enfants[0] = randint(0,1) enfants[1] = randint(0,1) if enfants[randint(0,1)] == 1: # on voit une fille dans le jardin (on tire au sort l'enfant) vufillejardin += 1 if enfants[0] == enfants[1]: # deux filles deuxfilles += 1 P = deuxfilles/vufillejardin*100 print(f\"Probabilité qe les parents aient deux filles sachant qu'on a vu une fille dans le jardin : {P:.1f}%\") note\nla probabilité a priori d\u0026rsquo;avoir deux filles vaut 1/4.\nL\u0026rsquo;information apportée tire cette probabilité vers le haut.\nElle tire plus dans le second cas que dans le premier, car l\u0026rsquo;information est plus précise ; un tirage a été fait.\nDans le premier cas, l\u0026rsquo;information supplémentaire laisse trois possibilités équiprobables (on sait seulement que le couple ne peut pas avoir deux garçons), alors que dans le second, il n\u0026rsquo;y en a plus que deux (l\u0026rsquo;autre enfant est soit une fille, soit un garçon) !\nProblème de Monty Hall Dans un ancien jeu télévisé américain, présenté par Monty Hall, un candidat devait choisir une porte parmi trois. Derrière l\u0026rsquo;une d\u0026rsquo;elles se cache une voiture et derrière les deux autres une chèvre.\nAprès que le candidat ait indiqué son choix, Monty ouvre une des deux autres portes derrière laquelle il sait que se trouve une chèvre (s\u0026rsquo;il y a une chèvre derrière les deux portes non choisies, il en choisit une au hasard).\nIl demande ensuite au candidat s\u0026rsquo;il veut garder sa porte ou s\u0026rsquo;il veut choisir l\u0026rsquo;autre porte.\nDoit-il changer de porte ?\nSupposons que vous ayez préalablement choisi la porte 1 et que Monty ouvre la porte 2. Montrez en utilisant le théorème de Bayes que le candidat a intérêt à changer de porte.\nPour raisonner, on va prendre pour hypothèse $H$ \u0026ldquo;il y a une voiture derrière la porte 3\u0026rdquo; et pour information $I$ : \u0026ldquo;Monty ouvre la porte 2\u0026rdquo;.\nL\u0026rsquo;argument de l\u0026rsquo;Apocalypse Le philosophe Nick Bolstrom présente sa version du \u0026ldquo;Doomsday Argument\u0026rdquo; à peu près ainsi :\n1re étape :\nImaginez un univers constitué de 100 boites habitées chacune par un humain.\nL\u0026rsquo;extérieur des boites est peint en bleu pour 90 d\u0026rsquo;entre elles et en rouge pour les 10 autres.\nChaque personne connaît la situation et on leur demande de deviner la couleur de leur boite.\nQue répondez-vous ?\nCorrection (cliquer pour afficher) Si vous supposez qu'il y a 90% de chance que votre boite soit bleue, vous êtes SSA (self-sampling assumption) dans la terminologie de Bostrom. Et si vous pensez plutôt qu'il n'y a que 50% de chance qu'elle soit bleue, vous échappez à la conclusion du Doomsday. 2e étape :\nOn modifie un peu l\u0026rsquo;expérience en remplaçant la couleur des boites par une numérotation entre 1 et 100 (le numéro est, là encore, peint à l\u0026rsquo;extérieur).\nMaintenant, un dieu bizarre lance une pièce. Si ça tombe sur face, il crée une personne dans chaque boite et si ça tombe sur pile, il ne crée des personnes que dans les boites 1 à 10.\nVous vous retrouvez dans une de ces boites et on vous demande s\u0026rsquo;il y a 10 ou 100 personnes dans l\u0026rsquo;univers.\nN\u0026rsquo;ayant pas d\u0026rsquo;information supplémentaire, que répondez-vous ?\nEt si on vous demande d\u0026rsquo;estimer la probabilité que le numéro de votre pièce soit entre 1 et 10 selon chacune des deux possibilités pour le pile ou face ?\nSupposons maintenant que vous sortiez de votre boite pour découvrir que son numéro est le 7.\nOn vous demande alors d\u0026rsquo;estimer la probabilité que la pièce soit tombée sur pile maintenant que vous connaissez le numéro de votre boite.\nCorrection (cliquer pour afficher) $$ \\color{#006C65} \\begin{aligned} P(\\text{Pile}|7)\u0026amp;=\\frac{P(7|\\text{Pile})P(\\text{pile})}{P(7|\\text{Pile})P(\\text{pile})+P(7|\\text{face})P(\\text{face})}\\\\ \u0026amp;=\\frac{1/10\\times 1/2}{1/10\\times 1/2 + 1/100\\times 1/2}\\\\ \u0026amp;=\\frac{10}{11}\\\\ \u0026amp;=91\\% \\end{aligned} $$\n3e étape :\nOn transpose ces résultats à la situation actuelle sur Terre.\nPosons les deux hypothèses rivales suivantes :\napocalypes précoce : l\u0026rsquo;humanité va s\u0026rsquo;éteindre dans le prochain siècle et la quantité totale d\u0026rsquo;humain ayant existé sera d\u0026rsquo;environ 200 milliards. apocalypse tardive : l\u0026rsquo;humanité va survivre le prochain siècle et coloniser la galaxie. Le nombre total d\u0026rsquo;humain ayant existé s\u0026rsquo;élèvera à 200 mille milliards. Quelle probabilité a priori attribuez-vous à chacun de ces scénarios ?\nVous n\u0026rsquo;êtes pas loin d\u0026rsquo;être l\u0026rsquo;humain n°100 milliards (en terme d\u0026rsquo;ordre de naissance).\nComme le théorème de Bayes va-t-il faire glisser les probabilités attribuées à chacun des scénarios sachant cela (faire le parallèle avec l\u0026rsquo;univers des boites numérotées).\nCommentaire (cliquer pour afficher) On peut échapper à cette conclusion sinistre en rejetant SSA. On peut en effet considérer qu'il y a plus de chance qu'il y ait 100 personnes que 10 car le fait que j'existe devient alors plus probable.\nSans connaître le numéro de la boite, on peut donc penser qu'il y a 10 fois plus de chances que dieu ait tiré face. Cela rééquilibre a posteriori les deux hypothèses car maintenant, $P(\\text{Pile}|7)$ vaut 1/2 et de même, les deux scénarios d'apocalypse retrouvent leurs probabilités a priori. Nick Bostrom a aussi développé des arguments semblables sur la probabilité que l\u0026rsquo;on vive dans une simulation.\nBayes et cote Le raisonnement de la question 2 suggère de réexprimer le théorème de Bayes en terme de cotes, ce qui va permettre de simplifier à la fois son calcul et son interprétation.\nLa cote d\u0026rsquo;un événement (odds en anglais) est le ratio de la probabilité que l\u0026rsquo;événement se produise par la probabilité qu\u0026rsquo;il ne se produise pas. On l\u0026rsquo;exprime en général comme une paire de nombres (le numérateur et le dénominateur).\nPar exemple, si un évènement a une probabilité de 5% de se produire, il a donc aussi une probabilité de 95% de ne pas se produire et sa cote est alors de 5 contre 95 (ou 1 contre 19 qu\u0026rsquo;on peut aussi noter $1:19$).\nSi un pari consiste à obtenir un 5 ou un 6 au dé, que vaut alors sa cote ?\nÀ quelle cote correspond une probabilité de 50% ?\nL\u0026rsquo;utilisation des cotes est très commune pour les paris sportifs en Angleterre.\nThéorème de Bayes exprimé en termes de cote :\nLa cote de $H$ sachant $I$ vaut la cote de $H$ multipliée par le facteur de Bayes $\\left(\\frac{P(I|H)}{P(I|\\overline{H})}\\right)$. Le facteur de Bayes mesure le mérite relatif des deux hypothèses $H$ et $\\bar{H}$, le rapport de leurs vraisemblances.\nVérifions avec Steve :\nLa cote de $H$ correspond au ratio bibliothécaires/agriculteurs (soit 1/20) et le facteur de Bayes correspond à combien de fois le portrait $I$ correspond plus à un bibliothécaire qu\u0026rsquo;à un agriculteur. Si le facteur de Bayes vaut 20, on trouve une cote de 1 pour la cote a posteriori, ce qui correspond bien à une probabilité de 50% que Steve soit bibliothécaire.\nReprenez la première questions du paradoxe des deux filles en utilisant les cotes.\nInférence bayésienne et diagnostic médical Le psychologue Gerd Gigerenzer présente le problème suivant dans un séminaire de statistique à des gynécologues en activité :\nUne femme de 50 ans sans symptôme passe une mammographie de routine. L'examen se révèle positif. Alarmée, elle veut savoir avec quelle certitude cela implique qu'elle a un cancer du sein.\nÀ part le résultat du test, vous ne savez rien sur cette femme.\nLa prévalence des cancers du sein est de 1% chez les femmes de cet âge.\nLa sensibilité du test est de 90%.\nEt sa spécificité est de 91%.\nParmi les femmes dont le test est positif, combien sont atteintes d'un cancer du sein\u0026nbsp;?\nA : 9 sur 10 ; B : 8 sur 10 ; C : 1 sur 10 ; D : 1 sur 100 Un peu de vocabulaire :\nLa sensibilité d\u0026rsquo;un test mesure sa capacité à donner un résultat positif lorsqu\u0026rsquo;une hypothèse est vérifiée = capacité à détecter un maximum de malades (avoir le moins possible de faux négatifs). La spécificité d\u0026rsquo;un test mesure sa capacité à donner une résultat négatif lorsque l\u0026rsquo;hypothèse n\u0026rsquo;est pas vérifiée = capacité à ne détecter que les malades (avoir le moins possible de faux positifs). En notant VP et FP les vrais et les faux positifs, et VN et FN les vrais et faux négatifs, on a :\nMalade Non malade Test positif VP FP Test négatif FN VN sensibilité $=\\frac{VP}{VP+FN}$\nspécificité $=\\frac{VN}{VN+FP}$\ninfo\nEn bon bayésien, il ne faut pas considérer qu\u0026rsquo;un test détermine si on a une maladie, ni même qu\u0026rsquo;il détermine les chances d\u0026rsquo;avoir une maladie.\nTout ce qu\u0026rsquo;il fait, c\u0026rsquo;est mettre à jour les chances d\u0026rsquo;avoir une maladie !\nQue vaut le facteur de Bayes dans cet exemple ?\nRéponse (cliquer pour afficher) $\\displaystyle \\frac{P(+|\\text{Cancer})}{P(+|\\overline{\\text{Cancer}})}=\\frac{\\text{probabilité de vrais positifs}}{\\text{probabilité de faux positifs}}=\\frac{\\text{sensibilité}}{\\text{1-spécificité}} $ Le théorème de Bayes version cote devient donc :\n$\\displaystyle cote(\\text{cancer}|+)=\\frac{\\text{sensibilité}}{\\text{1-spécificité}}\\times cote(\\text{cancer}) $\nQuelle est la bonne réponse (les cotes permettent de l\u0026rsquo;estimer facilement) ?\nPlus de la moitié des docteurs présents ont choisi la réponse A, ce qui est très à côté de la plaque, et seulement 1 sur 5 ont choisi la bonne réponse\u0026hellip;\nChangeons la prévalence à 10 % :\nprévalence 10% sensibilité 90% spécificité 91% Que devient la probabilité d\u0026rsquo;avoir un cancer en cas de test positif ?\nPassons-la maintenant à 0,1 % :\nprévalence 0,1% sensibilité 90% spécificité 91% Que vaut $P(cancer|+)$ maintenant ? Augmentons la spécificité à 99 % et reprenant une prévalence de 1 %.\nprévalence 1% sensibilité 90% spécificité 99% Que devient la probabilité ?\nEt si le test est négatif ?\nReprendre les données de départ et trouvez la probabilité de ne pas avoir de cancer si on a été testé négatif.\nEt si on passe un second test négatif ?\nMoustiques Trois maladies virales peuvent être transmises par les moustiques :\ndengue chikungunya zika Elles provoquent des symptômes qui peuvent être assez proches, ce qui les rend difficiles à différencier directement.\nIci on s’intéresse à la mise en place d’une aide statistique au diagnostic. Pour cela, on va s’appuyer sur des données obtenues chez des personnes dont le diagnostic a pu être certifié par des examens biologiques. Pour simplifier, on supposera que ces caractères apparaissent indépendamment chez les personnes infectées.\nsymptômes Dengue Chikungunya Zika Fièvre 95% 75% 75% Courbatures 75% 95% 50% Douleurs oculaires 50% 25% 50% Déficit globules blancs 50% 50% 25% Hémorragie 25% 5% 5% À partir de ces données, on veut déterminer les probabilités de chaque maladie selon les symptômes présentés et dans des conditions différentes.\nLa forme du théorème de Bayes la plus pratique est dans ce cas :\n$\\displaystyle P(\\text{maladie}|\\text{symptômes})=\\frac{P(\\text{symptômes}|\\text{maladie})\\times P(\\text{maladie})}{P(\\text{symptômes})} $\nVous êtes internes au service de maladies infectieuses du CHU de Limoges et une personne se présente avec à la fois de la fièvre, pas de courbatures et des douleurs oculaires. La personne revient d’un pays dans lequel aucune des trois maladies n’est épidémique. On considère donc a priori que les trois maladies sont équiprobables.\nCalculer la probabilité que la personne présente ces 3 symptômes ensemble pour chacune des maladies $P(\\text{symptômes}|\\text{maladie})$.\nCalculer la probabilité d\u0026rsquo;avoir ces symptômes quelle que soit la maladie $P(\\text{symptômes})$ aide : $P(\\text{symptômes})=\\sum_{\\text{maladie}}P(\\text{symptômes}|\\text{maladie})\\times P(\\text{maladie})$\nQuelles sont les probabilités a posteriori de chaque maladie si cette personne présente ces symptômes ? Quel est selon vous le diagnostic le plus probable dans ce cas ?\nSi vous apprenez maintenant qu\u0026rsquo;en fait la personne revient d\u0026rsquo;un pays dans lequel sévit une épidémie de Chikungunya. A priori, il y a 90 % de chances qu’elle ait été infectée par le Chikungunya et 5 % par chacune des deux autres maladies. Quel est le diagnostic le plus probable dans ce cas ?\nspam\nUn des premiers programmes de filtrage bayésien du courrier électronique était le programme iFile de Jason Rennie, publié en 1996.\nLe principe, analogue à celui du diagnostic médical, repose sur le fait que les mots du dictionnaire ont des probabilités différentes d’apparaître dans les spams et dans les courriers légitimes.\nLe filtre de détection des spams ne connaît pas à l’avance les probabilités d’apparition de ces mots, c’est pourquoi il lui faut une phase d’apprentissage pour les évaluer. Cette phase d’apprentissage est analogue à la phase de calibrage du test médical étudié ci-dessus.\nL’apprentissage se fait à partir de l’observation du comportement des utilisateurs, qui doivent indiquer manuellement si un message est un spam ou non. Pour chaque mot de chaque message « appris », le filtre ajustera les probabilités de rencontrer ce mot dans un spam ou dans un courrier légitime et le stockera dans sa base de données.\nOn note $P(M|S)$ la probabilité qu’un spam contienne le mot $M$ et $P(M|\\overline{S})$ la probabilité qu’un courrier légitime contienne le mot $M$. Ces deux probabilités sont estimées au cours de la phase d’apprentissage, tout comme la probabilité $P(S)$ qu’un message quelconque soit un spam (analogue à la prévalence $P(\\text{maladie})$ dans le test médical).\nUne fois ces valeurs déterminées, la formule de Bayes permet de calculer la probabilité qu’un message donné soit un spam sachant qu’il contient le mot $M$ selon la formule :\n$\\displaystyle P(S|M)=\\frac{P(M\\cap S)}{P(M)}=\\frac{P(M|S)P(S)}{P(M|S)P(S)+P(M|\\bar{S})P(\\bar{S})}$\nCette probabilité est comparée à un seuil ; si elle est supérieure au seuil, le filtre classera ce message dans les spams.\nDans la réalité, on travaille non pas sur un seul mot $M$, mais sur un stock de mots, en faisant l\u0026rsquo;hypothèse naïve que les mots présents dans un message sont indépendants les uns des autres. Cela est faux dans les langages naturels, où par exemple la probabilité de trouver un adjectif est influencée par celle de trouver un nom. De plus, cette technique de filtrage, connue sous le nom de filtrage bayésien naïf, ne tient pas compte du sens des mots, alors qu’il a une incidence sur la présence simultanée de certains mots à l\u0026rsquo;intérieur du message. Par exemple, la présence du mot « anniversaire » n’est pas indépendante de celle du mot « joyeux ».\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/",
	"title": "Logique",
	"tags": [],
	"description": "",
	"content": " Logique La logique est à l\u0026rsquo;intersection de l\u0026rsquo;informatique, des mathématiques et de la philosophie.\nCalcul des propositions Syntaxe Sémantique Preuve Logique modale Syntaxe et sémantique Systèmes logiques Différentes logiques modales Le paradoxe de Berry rend non calculable la complexité de Kolmogorov. Et cette non calculabilité tue dans l\u0026rsquo;œuf le rêve du jeune Ray Solomonoff d\u0026rsquo;un super algorithme capable de résoudre l\u0026rsquo;intégralité des problèmes scientifiques 😢\nVoir une chaussure blanche participe-t-il à prouver que tous les corbeaux sont noirs ?\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/meca/",
	"title": "Mécanique",
	"tags": [],
	"description": "",
	"content": " Mécanique Énergie cinétique et travail Une approche historique sur l\u0026rsquo;émergence du concept d\u0026rsquo;énergie en mécanique, de Descartes à Einstein.\nPourquoi une énergie cinétique en $v^2$ ? Trois démonstrations de la formule de l\u0026rsquo;énergie cinétique.\nJe trouve la deuxième (celle de Johann Bernoulli) fabuleuse d\u0026rsquo;intuition géométrique (elle utilise l\u0026rsquo;escargot de Pythagore).\nLagrange et Hamilton Les objets de la mécanique classique dans sa formulation newtonienne sont des vecteurs à 3 dimensions qui évoluent dans le bon vieil espace physique euclidien $\\mathbb{R}^3$.\nMais la fin du 18e et la première moitié du 19e siècle ont vu naître de nouvelles formulations de la mécanique où l\u0026rsquo;arène même dans laquelle s\u0026rsquo;ébattent les objets physiques étudiés est chamboulée. On peut littéralement parler de nouvelles façons de voir.\nSouhaitant s\u0026rsquo;affranchir de la géométrie (quel souhait étrange), Lagrange vise une approche purement analytique (il ventait l\u0026rsquo;absence de tout schéma dans son manuel de mécanique\u0026hellip;). Son idée centrale fut de basculer de l\u0026rsquo;étude de $N$ systèmes comme $N$ vecteurs dans $\\mathbb{R}^3$ à un seul système composite, un seul vecteur, évoluant dans $\\mathbb{R}^{3N}$. Toute configuration des $N$ systèmes devient un unique point dans l\u0026rsquo;espace $\\mathbb{R}^{3N}$ appelé alors espace des configurations. Et l\u0026rsquo;évolution de l\u0026rsquo;ensemble des $N$ objets dessine un chemin unique dans l\u0026rsquo;espace des configurations.\nHamilton souhaite, lui, retrouver la source géométrique de la mécanique mais tout en conservant une approche analytique. L\u0026rsquo;arène d\u0026rsquo;Hamilton ne doit plus seulement décrire la position du système mais son état physique entier, ce qui nécessite de savoir comment le système bouge. On a besoin pour cela de l\u0026rsquo;espace des phases dont chaque point informe à la fois de la position et de la quantité de mouvement de chacun des $N$ objets du système. L\u0026rsquo;espace des phases est donc de dimension $2\\times 3N$ !\nPanorama du principe de moindre action De Newton à Hamilton en sautant Lagrange Chute libre et paraboles Étudier la chute libre, c\u0026rsquo;est jouer avec des paraboles.\nForce de Coriolis La force de Coriolis n\u0026rsquo;est pas une force mais traduit juste la galère d\u0026rsquo;aller droit dans un référentiel tournant.\nComprendre cet effet permet d\u0026rsquo;expliquer les rotations des masses d\u0026rsquo;air cycloniques et anticycloniques ou encore d\u0026rsquo;en savoir plus sur ces troyens qui peuplent l\u0026rsquo;orbite de Jupiter.\nTunnels traversant la Terre Petits exercices de niveau licence/prépa traitant de voyages autour ou à travers la Terre et durant à chaque fois 42 min (nombre geek par excellence depuis que Douglas Adams en a fait la réponse à toute chose dans Le Guide du voyageur galactique).\nForces de marée De manière \u0026ldquo;amusante\u0026rdquo;, le ratio des contributions de la Lune et du Soleil aux forces de marée sur Terre est donné par le ratio de leurs masses volumiques respectives :\n$$\\frac{F_\\text{maréee,Lune}}{F_\\text{maréee,Soleil}}\\approx\\frac{\\rho_\\text{Lune}}{\\rho_\\text{Soleil}}$$\nEn effet, les forces de marée exercées par un astre sur la Terre sont proportionnelles au gradient de son champ gravitationnel, donc à $\\displaystyle\\frac{M_\\text{Astre}}{d_\\text{Astre-Terre}^3}$ .\nMais on sait aussi que la Lune et le Soleil sont à peu près vu sous le même diamètre apparent de 30\u0026rsquo; depuis la Terre (un indice parmi d\u0026rsquo;autres : les éclipses solaires parfois annulaires, parfois totales). Par conséquent, les distances des deux astres sont dans le rapport de leurs rayons (Thalès) :\n$$\\frac{d_\\text{Terre-Lune}}{d_\\text{Terre-Soleil}}\\approx\\frac{r_\\text{Lune}}{r_\\text{Soleil}}$$\nOn a donc bien au final :\n$$ \\begin{aligned} \\frac{F_\\text{maréee,Lune}}{F_\\text{maréee,Soleil}} \u0026amp;=\\frac{M_\\text{Lune}}{d_\\text{Lune-Terre}^3}\\left/\\frac{M_\\text{Soleil}}{d_\\text{Soleil-Terre}^3}\\right.\\\\ \u0026amp;=\\frac{M_\\text{Lune}}{M_\\text{Soleil}}\\times \\frac{d_\\text{Soleil-Terre}^3}{d_\\text{Lune-Terre}^3}\\\\ \u0026amp;\\approx \\frac{M_\\text{Lune}}{M_\\text{Soleil}}\\times \\frac{r_\\text{Soleil}^3}{r_\\text{Lune}^3}\\\\ \u0026amp;=\\frac{M_\\text{Lune}}{r_\\text{Lune}^3}\\left/\\frac{M_\\text{Soleil}}{r_\\text{Soleil}^3}\\right.\\\\ \u0026amp;=\\frac{\\rho_\\text{Lune}}{\\rho_\\text{Soleil}} \\end{aligned} $$\nEt avec $\\rho_\\text{Lune} = \\pu{3,3 g*cm-3}$ et $\\rho_\\text{Soleil} = \\pu{1,4 g*cm-3}$, on retrouve que la contribution du Soleil aux forces de marée représente environ 42% (comme par hasard) de celle de la Lune.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/quantique/fondations/",
	"title": "Notions fondamentales",
	"tags": [],
	"description": "",
	"content": " Notions fondamentales en mécanique quantique Le cinquième Conseil international de Solvay, en 1927, avait pour thème « Électrons et Photons » et porta principalement sur la mécanique quantique. 17 des 29 personnalités présentes à ce congrès étaient ou allaient devenir prix Nobel\u0026hellip;\nC\u0026rsquo;est à cette occasion qu\u0026rsquo;eurent lieu les échanges entre les représentants de l\u0026rsquo;« école de Copenhague » (Bohr, Heisenberg, Ehrenfest, etc.), partisans d\u0026rsquo;une mécanique quantique probabiliste, et les adeptes d\u0026rsquo;une approche déterministe (Einstein, Schrödinger, de Broglie notamment). Ce fut le début d\u0026rsquo;une longue controverse d\u0026rsquo;une vingtaine d\u0026rsquo;années entre les deux principaux protagonistes, Bohr et Einstein.\nAujourd’hui, l’interprétation de Copenhague est le cadre de référence le plus couramment enseigné, même si d’autres lectures (décohérence, réaliste, informationnelle…) gagnent du terrain.\nLes lignes qui suivent en résument brièvement les axiomes et en illustrent quelques conséquences fondamentales, avec un prisme information quantique.\nSource : Knee, G. C. « Isolation of the Conceptual Ingredients of Quantum Theory by Toy Theory Comparison », mémoire de Master of Science, Imperial College London, soutenu le 20 septembre 2010 🌐\nUn jeu d\u0026rsquo;axiomes L\u0026rsquo;interprétation de la mécanique correspondant à l\u0026rsquo;école de Copenhague, repose sur un jeu d\u0026rsquo;axiomes1 :\nAxiome 1\u0026nbsp;: État quantique\nÀ un instant fixé, l’état d’un système quantique est un vecteur $\\lvert \\psi\\rangle$ dans un espace de Hilbert $\\mathcal{H}$. Axiome 2\u0026nbsp;: Évolution unitaire\nL’évolution d’un système quantique est toujours unitaire, c’est-à-dire décrite par un opérateur unitaire $\\hat U$ (tel que $\\hat U^\\dagger = \\hat U^{-1}$), parfois appelé « porte », agissant selon $\\lvert \\psi\\rangle \\;\\longmapsto\\;\\hat U \\lvert \\psi\\rangle$.\nEn particulier, l’évolution dans le temps obéit à l’équation de Schrödinger $$\\mathrm{i}\\,\\hbar\\frac{\\partial}{\\partial t}\\lvert \\psi\\rangle \\;=\\; \\hat H \\lvert \\psi\\rangle ,$$\noù $\\hat H$ est l’opérateur Hamiltonien.\nAxiome 3\u0026nbsp;: Règle de Born\nUne mesure sur un système quantique est décrite par un opérateur hermitien $\\hat A$ (i.e. $\\hat A = \\hat A^\\dagger$), appelé \"observable\".\nCette observable possède une décomposition spectrale $$\\hat A \\;=\\; \\sum_i \\lambda_i \\,\\lvert e_i\\rangle\\langle e_i\\rvert$$\noù les valeurs propres $\\lambda_i$ sont les résultats possibles de la mesure et les projecteurs $\\lvert e_i\\rangle\\langle e_i\\rvert$ en sont les opérateurs associés.\nSi l’on mesure l’état $\\lvert \\psi\\rangle$, la probabilité d’obtenir le résultat $i$ est $$p(i) \\;=\\; \\langle \\psi \\rvert e_i\\rangle\\langle e_i \\lvert \\psi\\rangle$$\nAxiome 4\u0026nbsp;: Postulat de réduction du paquet d'onde\nAprès la mesure, l’état du système s’effondre sur l’état propre associé au résultat obtenu. Si l’issue $i$ est observée, l’état devient $$\\frac{\\lvert e_i\\rangle\\langle e_i \\lvert \\psi\\rangle}{\\sqrt{p(i)}}$$\nAxiome 5\u0026nbsp;: Relation de complétude\nLes projecteurs satisfont la relation $$\\sum_i \\lvert e_i\\rangle\\langle e_i\\rvert \\;=\\; \\mathbb{I}$$\noù $\\mathbb{I}$ est l’opérateur identité sur $\\mathcal{H}$.\nInformation quantique Le bit classique Un bit est la plus petite unité d’information : il distingue entre deux possibilités (le résultat d’un pile ou face, l’état marche/arrêt d’une lampe, le niveau de tension haut/bas dans un circuit, etc.). On note habituellement ces possibilités « zéro logique » et « un logique » : $0, 1$.\nPour représenter plus d’information, on considère une chaîne de bits : avec $n$ bits, on a $2^n$ possibilités (par exemple, pour 3 bits : $000,001, \\dots,111$).\nLe qubit L’analogue quantique du bit est le qubit. C’est le système quantique non trivial le plus simple. À l’issue d’une mesure, il distingue également deux possibilités, correspondant aux éléments d’une base orthonormale de l’espace de Hilbert à deux dimensions $\\mathcal{H}_2$.\nOn note $\\lvert0\\rangle$ et $\\lvert1\\rangle$ les deux vecteurs de base. On peut les réaliser physiquement de plein de façons : le premier et deuxième niveau d\u0026rsquo;énergie d\u0026rsquo;un atome d\u0026rsquo;hydrogène, le spin haut et le spin bas d\u0026rsquo;une particule de spin $1/2$, la polarisation horizontale et verticale d\u0026rsquo;un photon, etc.\nLa grande différence avec un bit classique est qu\u0026rsquo;entre deux mesures, le qubit existe comme une superposition des deux possibilités : $$\\lvert\\psi\\rangle = \\alpha\\,\\lvert0\\rangle + \\beta\\,\\lvert1\\rangle$$ avec $\\alpha,\\beta\\in\\mathbb{C}$ et $\\lvert\\alpha\\rvert^2 + \\lvert\\beta\\rvert^2 = 1$.\nUne autre différence importante est qu\u0026rsquo;on peut tout à fait mesurer le qubit dans une direction qui ne distingue pas $|0\\rangle$ de $|1\\rangle$.\nOn identifie deux états qui ne diffèrent que par une phase globale étant donné qu\u0026rsquo;ils produisent des prédictions physiques identiques. Cela permet de spécifier un qubit par un poids relatif et une phase relative. En ne considérant que des poids égaux dans la superposition, on peut écrire : $$|\\psi\\rangle = \\frac{|0\\rangle+\\mathrm{e}^{\\mathrm{i}\\theta} |1\\rangle}{\\sqrt{2}}$$\nLes cas particuliers $\\theta=0$ et $\\theta=\\pi$ donnent respectivement les deux états suivants : $$|+\\rangle = \\frac{|0\\rangle + |1\\rangle}{\\sqrt{2}}$$ $$|-\\rangle = \\frac{|0\\rangle - |1\\rangle}{\\sqrt{2}}$$\nIl est possible de réaliser une mesure distinguant $|+\\rangle$ de $|-\\rangle$ en choisissant l’observable $$\\hat X \\;=\\; |+\\rangle\\langle+| \\;-\\; |-\\rangle\\langle-| $$\nContrairement au cas classique, le résultat d\u0026rsquo;une mesure n\u0026rsquo;est pas toujours entièrement déterminé par la mesure elle-même et l\u0026rsquo;état $|\\psi\\rangle$.\nL’information contenue dans un système binaire classique est quantifiée par un bit, soit la quantité nécessaire pour spécifier complètement l’un des deux états possibles. En revanche, pour un état quantique, les coefficients complexes $\\alpha$ et $\\beta$ peuvent varier de façon continue, de sorte qu’il faudrait une quantité infinie d’information classique pour décrire exactement un qubit.\nMatrice densité On définit la matrice densité $\\rho_\\psi$ associée à un état pur $\\lvert\\psi\\rangle$ par : $$ \\rho_\\psi = \\lvert\\psi\\rangle\\langle\\psi\\rvert$$\nSi $\\lvert\\psi\\rangle = \\alpha\\,\\lvert0\\rangle + \\beta\\,\\lvert1\\rangle$, alors $$\\rho_\\psi = \\begin{pmatrix} |\\alpha|^2 \u0026amp; \\alpha^*\\beta\\\\ \\beta^* \\alpha \u0026amp; |\\beta|^2 \\end{pmatrix}$$\nLa condition de normalisation $\\langle\\psi|\\psi\\rangle=1$ entraîne que la trace de la matrice densité vaut un ($\\mathrm{Tr}\\,\\rho_\\psi = 1$).\nFidélité Pour deux états purs $\\lvert\\psi\\rangle$ et $\\lvert\\chi\\rangle$, la fidélité est définie par $F = \\bigl|\\langle\\psi|\\chi\\rangle\\bigr|^2$.\nOu pour deux états dont les matrices de densité sont $\\rho$ et $\\sigma$, $F = \\mathrm{Tr}\\,\\sqrt{\\rho}\\,\\sqrt{\\sigma}$.\nOrthogonalité Deux états $\\lvert\\psi\\rangle$ et $\\lvert\\chi\\rangle$ sont dits orthogonaux si $\\langle\\psi|\\chi\\rangle = 0$. Les deux états ont alors une fidélité nulle.\nÉtat pur et état mixte Un état pur est un vecteur dans l’espace de Hilbert. Toute combinaison linéaire normalisée des vecteurs de base est un état pur : $$|\\psi\\rangle = \\sum_i\\alpha_i|i\\rangle$$ Les $\\alpha_i\\in\\mathbb{C}$ sont appelés poids.\nToute combinaison linéaire d\u0026rsquo;états purs est encore un état pur.\nUn état mixte est une combinaison convexe de matrices densité :\n$$ \\rho = \\sum_i p_i\\,\\rho_i $$\noù chaque $\\rho_i$ représente un état pur, et les $p_i$ sont des probabilités telles que $\\sum_i p_i=1$.\nIl existe de nombreuses décompositions convexes d’un état mixte : par exemple, l’état mixte maximal\n$$ \\rho \\;=\\;\\tfrac12\\bigl(|0\\rangle\\langle0| + |1\\rangle\\langle1|\\bigr) \\;=\\;\\tfrac12\\bigl(|+\\rangle\\langle+| + |-\\rangle\\langle-|\\bigr) \\;=\\;\\tfrac{\\mathbb{I}}2 $$\nUn état mixte ne peut pas s’écrire comme une somme d’états de base à poids complexes.\nLa différence sémantique entre un état « pur » et un état « mixte » tient au fait que l’incertitude pour le premier est une incertitude purement quantique, tandis que pour le second une part d’incertitude classique est introduite. Un état mixte apparaît si l’on prépare un état quantique en fonction du résultat d’un pile ou face classique, en assignant un certain état pour pile et un autre pour face. On peut aussi interpréter cette incertitude classique comme une forme d’erreur expérimentale.\nSphère de Bloch Il existe une représentation géométrique élégante des qubits qui facilite grandement la visualisation des opérations de mesure, d’évolution et de similarité des états dans l’espace de Hilbert.\nDans la représentation par matrice densité d’un état pur, on compte à première vue deux coefficients complexes, soit quatre nombres réels. La condition de normalisation (trace = 1) en élimine un. Il reste donc trois degrés de liberté réels pour un état mixte. Et comme on a vu que deux états purs ne se différentiant que par une phase globale sont en fait identiques, il ne reste que deux degrés de liberté pour un état pur2. Cela permet de représenter les états purs sur la surface d’une sphère et les états mixtes dans son intérieur.\nOn définit le vecteur de Bloch $\\vec{r} = [r_x,,r_y,,r_z]$ associé à un état $\\rho$ par\n$$ \\rho = \\frac12\\bigl(\\mathbb{I} + \\vec{r}\\!\\cdot\\!\\vec{\\sigma}\\bigr) =\\frac12 \\begin{pmatrix} 1 + r_z \u0026amp; r_x - \\mathrm{i}\\,r_y \\\\ r_x + \\mathrm{i}\\,r_y \u0026amp; 1 - r_z \\end{pmatrix} $$\noù $\\boldsymbol{\\sigma} = [\\hat X,\\,\\hat Y,\\,\\hat Z]$ est le triplet de matrices de Pauli.\nGrâce à ce vecteur, tout qubit peut être visualisé comme un point de la sphère de Bloch :\nles états purs correspondent aux points de la surface\u0026nbsp;; les états mixtes se situent à l’intérieur\u0026nbsp;; deux états orthogonaux apparaissent comme des points diamétralement opposés. Par construction, les vecteurs propres des matrices de Pauli occupent trois paires de points antipodaux sur la sphère : $$\\hat X|+\\rangle = |+\\rangle,\\quad \\hat X|-\\rangle = -|-\\rangle$$ $$\\hat Y|{+}\\mathrm{i}\\rangle = |{+}\\mathrm{i}\\rangle=\\frac{|0\\rangle+\\mathrm{i}|1\\rangle}{\\sqrt{2}},\\quad \\hat Y|{-}\\mathrm{i}\\rangle = -|{-}\\mathrm{i}\\rangle=\\frac{|0\\rangle-\\mathrm{i}|1\\rangle}{\\sqrt{2}}$$ $$\\hat Z|0\\rangle = |0\\rangle,\\quad \\hat Z|1\\rangle = -|1\\rangle$$\nChaque paire de vecteurs propres est une base de l\u0026rsquo;espace de Hilbert qui définit un axe orthogonal dans la sphère de Bloch.\nAttention, ça ne veut pas dire que ces trois bases sont orthogonales (ça n\u0026rsquo;aurait pas de sens puisque chacune est complète) ; elles sont mutuellement non biaisées3 (c\u0026rsquo;est ça que traduit géométriquement les axes orthogonaux de la sphère de Bloch).\nPar combinaison convexe, on fait passer les états purs situés sur la surface de la sphère de Bloch vers des états mixtes à l’intérieur de celle‑ci. L’état maximalement mixte, $\\mathbb{I}/2$, se situe au centre de la sphère de Bloch.\nPaire de Qubits Lorsqu’il faut décrire plusieurs systèmes quantiques simultanément, on a recours au produit tensoriel $\\otimes$. Supposons, par exemple, qu’on souhaite modéliser à la fois un qubit situé à La Rochelle et un autre à Paris ; on écrit alors $$ |\\psi\\rangle_{\\text{La Rochelle}}\\;\\otimes\\;|\\phi\\rangle_{\\text{Paris}} $$ où $|\\psi\\rangle$ désigne l’état du premier qubit et $|\\phi\\rangle$ celui du second. Une fois qu’un ordre conventionnel est adopté, il est d’usage de supprimer les indices géographiques (et même de fusionner les deux kets en un seul) : $$ |\\psi\\rangle_{\\text{La Rochelle}}\\otimes|\\phi\\rangle_{\\text{Paris}} \\;=\\; |\\psi\\rangle\\otimes|\\phi\\rangle \\;=\\; |\\psi\\phi\\rangle $$\nCette expression est appelée un état produit ; elle vit dans l’espace de Hilbert tensoriel $H_1\\otimes H_2$. Ainsi, si $|\\psi\\rangle\\in H_1$ possède la base $\\{|L\\rangle,|R\\rangle\\}$ et $|\\phi\\rangle\\in H_2$ la base $\\{|P\\rangle,|S\\rangle\\}$, alors $|\\psi\\rangle\\otimes|\\phi\\rangle$ appartient à $H_1\\otimes H_2$ et la base combinée devient $\\{|LP\\rangle,|LS\\rangle,|RP\\rangle,|RS\\rangle\\}$.\nDe même qu’un qubit unique peut passer par une porte (unitaire), un état produit peut être soumis à l’action d’un opérateur tensoriel $$ \\bigl(\\hat A_{\\text{La Rochelle}}\\otimes\\hat B_{\\text{Paris}}\\bigr)\\, |\\psi\\rangle\\otimes|\\phi\\rangle $$ Si l’on prend $\\hat A=\\mathbb{I}$, on agit uniquement sur le qubit de Paris ; si c’est $\\hat B=\\mathbb{I}$, on agit seulement sur celui de La Rochelle.\nLa base computationnelle $\\{|0\\rangle,|1\\rangle\\}$ d’un qubit se prolonge naturellement en une base de l’espace bipartite : $\\{|00\\rangle,\\;|01\\rangle,\\;|10\\rangle,\\;|11\\rangle\\}$. Un état général de deux qubits est donc une combinaison linéaire de ces quatre kets.\nAu cours de l’évolution, un tel état bipartite peut devenir un état intriqué comme, par exemple, l\u0026rsquo;état de Bell : $$ |{\\rm Bell}\\rangle \\frac{1}{\\sqrt{2}}\\bigl(|00\\rangle+|11\\rangle\\bigr) $$\nOn remarque que l\u0026rsquo;état de Bell ne contient pas les « termes croisés » $|01\\rangle$ et $|10\\rangle$ (leurs poids sont nuls). Il est donc impossible de l’écrire comme produit de deux qubits séparés ! Les états intriqués comme celui‑ci jouent un rôle central dans la plupart des protocoles de l’information quantique.\nNon-clonage quantique Maintenant qu’un espace produit bipartite a été introduit, on peut l’utiliser pour illustrer certains traits particulièrement surprenants de la mécanique quantique.\nSupposons que l’on veuille copier – cloner – un état quantique. Un cloneur universel opère simultanément sur un qubit de données $|\\psi\\rangle$ et sur un qubit ancillaire initialisé dans un état fixe (par exemple l’état « vierge » $|0\\rangle$), de sorte que l’état final soit le produit de deux exemplaires du qubit de données :\n$$U\\,|\\psi\\rangle|0\\rangle \\;=\\; |\\psi\\rangle|\\psi\\rangle$$\net, par le même procédé, pour tout autre état $|\\phi\\rangle$ :\n$$ U\\,|\\phi\\rangle|0\\rangle \\;=\\; |\\phi\\rangle|\\phi\\rangle $$\nEn prenant le produit scalaire des deux relations précédentes, on obtient :\n$$ \\langle\\psi|\\phi\\rangle \\;=\\; \\bigl(\\langle\\psi|\\phi\\rangle\\bigr)^{2} $$\nCette équation n’admet de solution que si les états sont identiques ($\\langle\\psi|\\phi\\rangle = 1$) ou orthogonaux ($\\langle\\psi|\\phi\\rangle = 0$).\nIl en découle qu’un cloneur universel 🐑 capable de copier n’importe quel état est impossible.\nEn d’autres termes, la seule procédure qui conserve les produits scalaires (et donc la structure de l’espace de Hilbert) ne peut cloner, au mieux, que des états orthogonaux. Le point crucial est qu’un processus de clonage universel violerait l’évolution unitaire prescrite par l’Axiome 2 : il ne préserverait pas les produits scalaires et ne pourrait donc être représenté par un opérateur unitaire.\nPour cloner un état, l’appareil devrait accéder à toute l’information qui le caractérise sans le perturber. Les états classiques satisfont cette condition : ils peuvent être entièrement déterminés par une quantité finie d’informations et restent intacts lors d’une mesure, de sorte qu’il suffit de les identifier puis de préparer un second système identique. En revanche, un état quantique ne peut être déterminé qu’au prix d’un nombre infini de mesures et, surtout, le processus de mesure le détruit.\nCette impossibilité fondamentale d’extraire toutes les informations sans perturber le système explique pourquoi un état quantique arbitraire ne peut jamais être cloné. Téléportation quantique La téléportation quantique est un autre phénomène étonnant : il s’accorde parfaitement avec le théorème d’interdiction du clonage, puisqu’on y transfère un état sans jamais en créer une copie supplémentaire.\nLa description du protocole exige un espace de Hilbert tripartite qu\u0026rsquo;on peut voir comme le produit tensoriel de trois qubits : $$ \\mathcal H = \\underbrace{\\mathcal H_1 \\otimes \\mathcal H_2}_{\\text{Alice}} \\otimes \\underbrace{\\mathcal H_3}_{\\text{Bob}} $$\nAlice possède l’état inconnu $|\\psi\\rangle = \\alpha|0\\rangle + \\beta|1\\rangle$ qu’elle veut transmettre à Bob. Aucune ligne « quantique » directe ne relie les deux laboratoires.\nLes amplitudes $\\alpha$, $\\beta$ sont inconnues des deux parties ; un simple message classique ne suffit donc pas.\nAlice et Bob partagent toutefois à l’avance une paire intriquée de Bell $|{\\rm Bell}\\rangle = \\frac{1}{\\sqrt2}\\bigl(|00\\rangle+|11\\rangle\\bigr)$. Le système complet est alors initialement $$|{\\text{teleport}}\\rangle = |\\psi\\rangle\\,|{\\rm Bell}\\rangle = \\frac{1}{\\sqrt2}\\,|\\psi\\rangle\\bigl(|00\\rangle+|11\\rangle\\bigr)$$ On peut le développer pour bien distinguer la partie Alice et la partie Bob : $$ |{\\rm teleport}\\rangle = \\tfrac1{\\sqrt2}\\bigl( \\alpha {\\color{#00AB8E}|00\\rangle} {\\color{#F27200}|0\\rangle} + \\alpha {\\color{#00AB8E}|01\\rangle} {\\color{#F27200}|1\\rangle} + \\beta {\\color{#00AB8E}|10\\rangle} {\\color{#F27200}|0\\rangle} + \\beta {\\color{#00AB8E}|11\\rangle} {\\color{#F27200}|1\\rangle}\\bigr) $$\nAlice effectue maintenant sur ses deux qubits la mesure définie par l’observable $A=\\sum_{i=I,X,Y,Z}\\lambda_i P_i$ dont les quatre issues possibles équiprobables sont associées aux projecteurs suivants : $$ \\begin{aligned} P_{\\mathbb{I}} \u0026amp; =\\frac{1}{2}(|00\\rangle+|11\\rangle)(\\langle 00|+\\langle 11|) \\\\ P_X \u0026amp; =\\frac{1}{2}(|00\\rangle-|11\\rangle)(\\langle 00|-\\langle 11|) \\\\ P_Y \u0026amp; =\\frac{1}{2}(|01\\rangle+|10\\rangle)(\\langle 01|+\\langle 10|) \\\\ P_Z \u0026amp; =\\frac{1}{2}(|01\\rangle-|10\\rangle)(\\langle 01|-\\langle 10|) \\\\ \\end{aligned} $$\nCes projecteurs vérifient $\\sum P_i = \\mathbb{I}$ en accord avec l\u0026rsquo;Axiome 5 et, par la règle de Born (Axiome 3), chaque résultat apparaît avec la probabilité 1/4.\nAprès la mesure, suivant $\\lambda_i$, l’état devient $$|{\\rm teleport^\\prime_\\mathbb{I}}\\rangle = {\\color{#00AB8E} P_\\mathbb{I}}\\otimes {\\color{#F27200}\\mathbb{I}}\\,|{\\rm teleport}\\rangle = {\\color{#00AB8E}|{\\rm Bell}\\rangle}\\,\\bigl({\\color{#F27200}|\\psi\\rangle}\\bigr)$$ $$|{\\rm teleport^\\prime_\\sigma}\\rangle = {\\color{#00AB8E} P_\\sigma}\\otimes{\\color{#F27200}\\mathbb{I}}\\,|{\\rm teleport}\\rangle = {\\color{#00AB8E}|{\\rm Bell^\\prime}\\rangle}\\,\\bigl({\\color{#F27200}\\sigma|\\psi\\rangle}\\bigr)$$\nAvec $\\sigma\\in{\\hat{X},\\hat{Y},\\hat{Z}}$ (opérateurs de Pauli) et où $|{\\text{Bell}^\\prime}\\rangle$ est l\u0026rsquo;état de Bell ou une rotation unitaire de l\u0026rsquo;état de Bell.\nDans 25 % des cas ($P_\\mathbb{I}$), le qubit de Bob est déjà le bon et la téléportation est donc déjà achevée ; sinon il en diffère d’un opérateur de Pauli.\nExemple : supposons que $i=X$ :\n$$ \\begin{aligned} |{\\rm teleport^\\prime_X}\\rangle \u0026amp;= P_X\\otimes\\mathbb{I}\\,|{\\text{teleport}}\\rangle \\\\ \u0026amp;= \\bigl( \\tfrac 12({\\color{#00AB8E}|00\\rangle-|11\\rangle})( {\\color{#00AB8E}\\langle 00| - \\langle 11|})\\otimes {\\color{#F27200}\\mathbb{I}}\\bigr) \\tfrac1{\\sqrt2}\\bigl( \\alpha {\\color{#00AB8E}|00\\rangle} {\\color{#F27200}|0\\rangle} + \\alpha {\\color{#00AB8E}|01\\rangle} {\\color{#F27200}|1\\rangle} + \\beta {\\color{#00AB8E}|10\\rangle} {\\color{#F27200}|0\\rangle} + \\beta {\\color{#00AB8E}|11\\rangle} {\\color{#F27200}|1\\rangle}\\bigr) \\\\ \u0026amp; = \\tfrac 12({\\color{#00AB8E}|00\\rangle-|11\\rangle})\\otimes\\tfrac 1{\\sqrt2}(\\alpha{\\color{#F27200}|0\\rangle}-\\beta{\\color{#F27200}|1\\rangle})\\\\ \u0026amp; = \\tfrac 1{\\sqrt2}({\\color{#00AB8E}|00\\rangle-|11\\rangle}) \\otimes \\tfrac 12 \\hat{Z} (\\alpha{\\color{#F27200}|0\\rangle}+\\beta{\\color{#F27200}|1\\rangle})\\\\ \u0026amp; = \\tfrac 1{\\sqrt2}({\\color{#00AB8E}|00\\rangle-|11\\rangle}) \\otimes \\tfrac 12 {\\color{#F27200}\\hat{Z}|\\psi\\rangle} \\end{aligned} $$\nLa partie Alice est bien un état de Bell et Bob se retrouve avec une transformation de l\u0026rsquo;état à télétransporter.\nAlice téléphone alors à Bob et lui envoie deux bits d\u0026rsquo;information classique pour indiquer son résultat :\n«\u0026nbsp;$00$\u0026nbsp;» pour un succès direct ($P_\\mathbb{I}$)\u0026nbsp;; «\u0026nbsp;$01$\u0026nbsp;», «\u0026nbsp;$10$\u0026nbsp;», «\u0026nbsp;$11$\u0026nbsp;» pour préciser respectivement $\\hat{X}$, $\\hat{Y}$ ou $\\hat{Z}$, l'opérateur de Pauli à utiliser. Rq : dans notre exemple, Alice envoie $11$.\nBob applique alors la correction sur son qubit avec l\u0026rsquo;opérateur de Pauli approprié $\\sigma$ :\n$${\\color{#00AB8E}\\mathbb{I}}\\otimes{\\color{#00AB8E}\\mathbb{I}}\\otimes{\\color{#F27200}\\sigma}\\,|{\\text{teleport}^\\prime_{\\sigma}}\\rangle ={ \\color{#00AB8E}|{\\rm Bell^\\prime}\\rangle}\\,{\\color{#F27200}|\\psi\\rangle}$$\ngrâce à l\u0026rsquo;involution de l\u0026rsquo;opérateur de Pauli $\\sigma^2=\\mathbb{I}$.\nÀ l\u0026rsquo;issue de l\u0026rsquo;algorithme et conformément au paradigme LOCC (opérations locales + communication classique), on a réalisé : $$|\\psi\\rangle\\,|{\\text{Bell}}\\rangle \\;\\longrightarrow\\; |{\\text{Bell}^\\prime}\\rangle\\,|\\psi\\rangle$$\nD\u0026rsquo;une source à l\u0026rsquo;autre, l\u0026rsquo;ordre et le nombre d\u0026rsquo;axiomes varient.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLa normalisation $|\\alpha|^2+|\\beta|^2=1$ contraint de vivre à la surface d\u0026rsquo;une sphère $S^3$ dans $\\mathbb{R}^4$. Multiplier par une phase globale $\\mathrm{e}^{\\mathrm{i}\\gamma}$ fait parcourir un cercle $S_1$ dans cette sphère. Puisqu\u0026rsquo;on identifie tous ces points, on quotiente $S^3$ par $S^1$. Et $S^3/S^1\\simeq S^2$, la sphère ordinaire.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDeux bases $\\{e_i\\}$ et $\\{f_i\\}$ sont mutuellement non biaisées si, pour tout $i,j$ : $\\bigl|\\langle e_i | f_j\\rangle\\bigr|^2 \\;=\\;\\text{cste}$.\nPour des bases orthonormales, on peut démontrer que la constante vaut $\\frac 1 d$ où $d=\\dim\\mathcal H$ grâce à la relation suivante $\\langle e_i|\\Bigl(\\sum_j |f_j\\rangle\\langle f_j|\\Bigr)|e_i\\rangle \\;=\\;\\sum_j \\bigl|\\langle f_j|e_i\\rangle\\bigr|^2 \\;=\\;\\langle e_i|\\mathbb I|e_i\\rangle\\;=\\;1$ où on a utilisé la complétude de la base $\\{f_i\\}$, $\\sum_j |f_j\\rangle\\langle f_j| = \\mathbb I$, et la normalité des éléments $\\{e_i\\}$. Si tous les termes $\\bigl|\\langle f_j|e_i\\rangle\\bigr|^2$ sont en plus égaux (définition de bases non biaisées), chacun vaut forcément $\\frac1d$ (puisqu\u0026rsquo;il y a autant de vecteurs dans chaque base que de dimensions).\nLa conséquence géométrique est que des états mutuellement non biaisés sont équidistances sur la sphère de Bloch puisque les angles entre eux sont identiques.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/algebre/groupes/groupe2/",
	"title": "Représentations",
	"tags": [],
	"description": "",
	"content": " Les représentations Retour sommaire\nLa physique semble soumise aux symétries (jamais les solutions d\u0026rsquo;un problème ne s\u0026rsquo;affranchissent des symétries qui contraignent le système). Essence des symétries, les groupes sont l\u0026rsquo;objet idéal pour décoder ce lien indéfectible.\nMais on ne manipule pas \u0026ldquo;la physique\u0026rdquo;, seulement des objets mathématiques sensés modéliser la réalité sous-jacente. Ces objets sont toujours les solutions d’équations différentielles ou intégrales et vivent donc dans des espaces vectoriels. Le mariage entre physique et espaces vectoriels fut même entièrement consommé à l’avènement de la physique quantique puisqu’ils constituent le cadre fondamental de la théorie.\nCela serait donc sympa que l’on réussisse à balancer nos symétries dans ces espaces vectoriels\u0026hellip; Les représentations sont là pour ça.\nUne représentation, c’est la description d’un groupe dans un espace vectoriel.\nMais maintenant qu’on a un peu de vocabulaire, disons qu\u0026rsquo;une représentation d\u0026rsquo;un groupe est le résultat d’un homomorphisme de ce groupe vers le groupe des opérateurs linéaires sur les espaces vectoriels (espace des états physiques pour ce qui nous intéresse). Ces opérateurs sont incarnés par des matrices dès qu’on a une base.\nRemarque\u0026nbsp;:\non peut vérifier que les matrices carrées d’un ordre donné forment bien des groupes vis-à-vis de la loi de multiplication entre matrices si néanmoins elles ont le bon goût d’être inversibles :\nà toute matrice inversible d’ordre n correspond bien sûr une matrice inverse, elle-même inversible et d’ordre $n$, matrice inversible d’ordre $n$ $\\times$ matrice inversible d’ordre $n$ $=$ matrice inversible d’ordre $n$, la multiplication de matrices est bien associative, la matrice identité d’ordre $n$ sert d’élément neutre. Définition \u0026ldquo;technique\u0026rdquo; d\u0026rsquo;une représentation :\nUne représentation est une application $g \\in G \\stackrel{U}{\\longmapsto} U(g)$ où $U(g)$ est un opérateur linéaire sur un espace vectoriel $V$, tel que\u0026nbsp;: $U\\left(g_1\\right) U\\left(g_1\\right)=U\\left(g_1 \\cdot g_2\\right)$\nEt sur une base orthonormée de l’espace vectoriel (de dimension finie), l’opérateur $U$ est associé à une matrice $D$ :\n$U(g)\\left|e_i\\right\\rangle=\\left|e_j\\right\\rangle D(g)^j_{\\,i}$\nExemple du groupe de symétrie de la molécule d’ammoniac $\\ce{NH3}$\u0026nbsp;: Appelons $\\mathrm{s}_1$, $\\mathrm{s}_2$, $\\mathrm{s}_3$, les orbitales 1s des atomes d’hydrogène et $\\mathrm{s_N}$ l’orbitale 2s de l’atome d’azote. On dénombre 6 éléments de symétrie dans le groupe :\non peut tout laisser à l’identique (élément neutre) , on peut permuter circulairement les orbitales $\\mathrm{s}_i$ des hydrogènes dans un sens ou dans l’autre\u0026nbsp;: $\\mathrm{s}_1 \\rightarrow \\mathrm{s}_2 \\rightarrow \\mathrm{s}_3 \\rightarrow \\mathrm{s}_1$\u0026nbsp;, $\\mathrm{s}_1 \\rightarrow \\mathrm{s}_3 \\rightarrow \\mathrm{s}_2 \\rightarrow \\mathrm{s}_1$, on peut permuter deux à deux les orbitales 1s, ça fait 3 possibilités. On vérifie à nouveau l\u0026rsquo;isomorphisme entre $\\boldsymbol{S_3}$ et $\\boldsymbol{D_3}$ puisque si la molécule de $\\ce{NH3}$ a les symétries de $\\boldsymbol{S_3}$, elle a bien aussi celles du triangle équilatéral, groupe $\\boldsymbol{D_3}$ (mêmes transformations invariantes : l’identité, les 2 rotations $2\\pi/3$ et $-2\\pi/3$, et les 3 réflexions par rapport aux hauteurs).\nL’atome d’azote est, lui, toujours laissé invariant.\nOn peut partir de la table de multiplication de ce groupe (voir chapitre précédent) pour construire certaines représentations.\nReprésentations de dimension 1\u0026nbsp;: La table est respectée par une première représentation de dimension 1 plutôt triviale consistant à associer le nombre 1 à chaque élément du groupe. Une autre représentation de dimension 1 un peu plus intéressante consiste à représenter les rotations par des $1$ et les réflexions par des $-1$.\nUne rotation suivie d’une rotation donne bien une autre rotation ($1\\times 1=1$), de même qu’une réflexion suivi d’une autre réflexion ($(-1)\\times (-1)=1$), alors que les compositions croisées correspondent effectivement toutes à des réflexions ($1\\times(-1)=-1$). En munissant le plan d’un repère orthonormé, on peut aussi écrire les matrices 2×2 de chacune des transformations et obtenir ainsi une représentation de dimension 2\u0026nbsp;: l’identité est alors représentée par\u0026nbsp;: $$ \\left(\\begin{array}{ll} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{array}\\right) $$\nles deux permutations circulaires (ou rotation de $\\pm2\\pi/3$) par\u0026nbsp;: $$ \\left(\\begin{array}{cc} \\cos (2 \\pi / 3) \u0026amp; -\\sin (2 \\pi / 3) \\\\ \\sin (2 \\pi / 3) \u0026amp; \\cos (2 \\pi / 3) \\end{array}\\right), \\left(\\begin{array}{cc} \\cos (4 \\pi / 3) \u0026amp; -\\sin (4 \\pi / 3) \\\\ \\sin (4 \\pi / 3) \u0026amp; \\cos (4 \\pi / 3) \\end{array}\\right) $$\net les 3 permutation deux à deux (réflexions) par\u0026nbsp;: $$ \\left(\\begin{array}{cc} 1 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \\end{array}\\right), \\left(\\begin{array}{cc} \\cos (2 \\pi / 3) \u0026amp; \\sin (2 \\pi / 3) \\\\ \\sin (2 \\pi / 3) \u0026amp; -\\cos (2 \\pi / 3) \\end{array}\\right), \\left(\\begin{array}{cc} \\cos (4 \\pi / 3) \u0026amp; \\sin (4 \\pi / 3) \\\\ \\sin (4 \\pi / 3) \u0026amp; -\\cos (4 \\pi / 3) \\end{array}\\right) $$\nOn aurait aussi pu simplement partir de la base formée des 4 orbitales ($\\mathrm{s_N}$, $\\mathrm{s_1}$, $\\mathrm{s_2}$, $\\mathrm{s_3}$) et regarder ce qu’il advient de chacune. On obtient alors une représentation de dimension 4 de ce groupe de transformations\u0026nbsp;: l’identité s'écrit ainsi\u0026nbsp;:\n$$ \\left(\\begin{array}{llll} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right) $$\nles deux permutations circulaires (ou rotations)\u0026nbsp;: $$ \\left(\\begin{array}{llll} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{array}\\right) , \\left(\\begin{array}{llll} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right) $$\net les 3 permutation deux à deux (réflexions)\u0026nbsp;: $$ \\left(\\begin{array}{llll} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{array}\\right) , \\left(\\begin{array}{llll} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right) , \\left(\\begin{array}{llll} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right) $$\nCet exemple illustre le bazar que le monde des représentations peut vite devenir\u0026hellip;\nOn va voir comment ranger tout ça en regroupant les représentations par classe et surtout en décomposant les représentations d’un groupe en représentations irréductibles, atomes de la théorie.\nRéductibilité et irréductibilité Pour la plupart des groupes susceptibles d’intéresser un physicien, les diverses façon de les représenter sont limitées et peuvent être répertoriées. Cela structure fortement l’espace vectoriel dans lequel joue le système physique.\nAfin de classer les représentations, il faut d’abord être sûr qu’elles sont bien différentes. Pour cela, il faut vérifier qu’elles n’appartiennent pas à la même classe d’équivalence :\nSoit $U(G)$ est une représentation du groupe $G$ sur l’espace vectoriel $V$ et $S$ n’importe quelle opérateur inversible sur $V$.\nAlors la représentation $U^{\\prime}(G)$ telle que $U^{\\prime}(G)=S U(G) S^{-1}$ (les matrices associées sont alors semblables) forme aussi une représentation de $G$ sur $V$, de même dimension. On dit que $U(G)$ et $U^{\\prime}(G)$ sont équivalentes (on note alors $U \\sim U^{\\prime}$).\nEt l’ensemble des représentations équivalentes forme une classe d’équivalence. Il suffit de connaître un élément de chaque classe puisqu’on peut générer tous les autres à partir de celui-ci.\nOn n’a fait là qu’importer la notion de classes des groupes aux représentations.\nPour répertorier les différentes représentation d’un groupe, on se concentre donc sur les représentations non équivalentes.\nEt pour s’assurer que deux représentations sont équivalentes ou non, il faut une grandeur variant d’une classe à l’autre mais pas à l’intérieur d’une classe. On appelle une telle quantité un invariant de similitude (puisqu’il est identique pour deux matrices semblables). La trace en est un (ça lui donne d’ailleurs son nom : indépendante d’un changement de base, la trace caractérise la matrice) !\nMais chez les représentations, le vocabulaire s’enrichit :\nLe caractère $\\chi(G)$ d’un élément de $G$ dans une représentation $U(g)$ est défini comme $\\chi(g)=\\operatorname{Tr} U(g)$. Tous les éléments du groupe d’une même classe ont le même caractère.\nLe caractère caractérise donc une classe.\nLes différentes représentations équivalentes sont autant de doublons à balayer mais une autre redondance pollue aussi l’analyse : une représentation donnée peut être décrite comme la somme de ses sous-parties.\nSupposons que l’on ait deux représentations $U_1(G)$ et $U_2(G)$ dans deux espaces orthogonaux $V_1$ et $V_2$. On peut alors construire une nouvelle représentation dans l’espace somme directe de $V_1$ et $V_2$ : $V_1 \\oplus V_2$. La représentations est alors dite somme directe des représentations : $U(G)=U_1(G) \\oplus U_2(G)$. Chacun des deux sous-espaces reste invariant sous l’action de $U$ par construction (on dit plutôt qu’ils sont laissés stables).\nC’est l’opération inverse qui va nous intéresser : quand une représentation donnée peut être décomposée en sous-représentations laissant stables certains sous-espaces. La représentation est alors dites réductibles.\nPrécisons le vocabulaire :\nUn sous-espace $V_1$ de $V$ est dit stable par l’action de $U(G)$ si pour tout $g\\in G$, et pour tout $x \\in V_1$, $U(g) x \\in V_1$.\nUne représentation $U(G)$ sur $V$ est dite irréductible s’il n’y a pas dans $V$ de sous-espace laissé stable par l’action de $U(G)$.\nSi un tel sous-espace invariant existe et si le sous-espace orthogonal est aussi invariant, alors la représentation est dite complètement réductible.\nExemple de représentation réductible non complètement réductible\u0026nbsp;: c’est le cas des représentations du groupe des translations à une dimension :\n$D(a)=\\left(\\begin{array}{ll} 1 \u0026amp; a \\\\ 0 \u0026amp; 1 \\end{array}\\right)$\nElle laisse invariant tout vecteur $(x,0)$ mais n’a pas de sous-espace supplémentaire invariant (tentons par exemple $(0,1)$ comme sous-espace complémentaire, on se retrouve avec $D(a)(0,1)=\\binom{a}{1}$ qui n\u0026rsquo;appartient pas au sous-espace $\\{(0,1)\\}$ (sauf pour $a=0$).\nReprésentation unitaire Une représentation unitaire $U(g)$ est définie sur un espace vectoriel muni d’un produit scalaire (donnant une norme définie positive). Un tel espace est dit préhilbertien.\nLa représentation unitaire doit respecter $U^{\\dagger} U=1$ (où $U^\\dagger$ est l’opérateur adjoint de $U$). Elle préserve les longueurs, les angles et le produit scalaire, et est donc naturellement associée aux transformations de symétrie (d’où son intérêt).\nTechniquement, une représentation $U(g)$ sur un espace préhilbertien $V$ est unitaire si pour tout $g\\in G$, $\\langle U(g) x \\mid U(g) y\\rangle=\\langle x \\mid y\\rangle$ pour tout $x,y \\in V$, avec $\\langle x \\mid y\\rangle$ désignant le produit scalaire entre les vecteurs $|x\\rangle$ et $|y\\rangle$.\nOn peut retrouver le lien entre représentation unitaire et opération de symétrie en partant d’un état physique quelconque :\nSoit $|\\psi\\rangle$ un «vecteur d’état» d’un système sur un espace vectoriel d’états physiques. Une opération de symétrie transforme $|\\psi\\rangle$ en $|\\psi^\\prime\\rangle$. Les deux ensembles de vecteurs $\\{|\\psi\\rangle\\}$ et $\\{|\\psi^\\prime\\rangle\\}$ doivent fournir des descriptions équivalentes du système physique ce qui implique que l’opérateur de symétrie soit linéaire.\nDe plus, toute observable physique doit rester invariante sous la transformation or ces observable sont toujours exprimées sous la forme de produits scalaires du type $\\langle\\phi \\mid \\psi\\rangle$. Et des transformations linéaires qui préservent le produit scalaire sont induites par des opérateurs unitaires !\nLes représentations unitaires ont une propriété remarquable qui va beaucoup nous occuper :\nSi une représentation unitaire est réductible alors elle est complètement réductible.\nPreuve\u0026nbsp;: Soit $U(G)$ une représentation unitaire réductible sur $G$ et soit $V_1$ un sous-espace stable par l’action de $U(G)$, et $V_2$ le complément orthogonal à $V_1$. Il faut montrer que $V_2$ est lui aussi stable par l’action de $U(G)$.\nQuels que soient $x\\in V_1$ et $y\\in V_2$,\n$$ \\begin{aligned} \\langle x \\mid U(g) y\\rangle \u0026amp; =\\left\\langle U\\left(g^{-1}\\right) x \\mid U\\left(g^{-1}\\right) U(g) y\\right\\rangle \\\\ \u0026amp; =\\left\\langle U\\left(g^{-1}\\right) x \\mid U^{-1}(g) U(g) y\\right\\rangle \\\\ \u0026amp; =\\left\\langle U\\left(g^{-1}\\right) x \\mid y\\right\\rangle=0 \\end{aligned} $$\ncar $U\\left(g^{-1}\\right) x \\in V_1$ comme $U(g)(x)$.\nPar conséquent $U(g)y$ appartient à l’espace orthogonal à $V_1$, c’est-à-dire $V_2$, et ce pour tout $g\\in G$.\nConclusion, l’espace $V_2$ est stable sous l’action de $G$.\nDonc une représentation unitaire pourra toujours s’écrire comme la somme directe de ses représentations irréductibles et non équivalentes (des représentations équivalentes vivent dans le même sous-espace) :\n$$ \\begin{aligned} U(G)\u0026amp;=\\underbrace{U^1(G) \\oplus \\cdots \\oplus U^1(G)}_{n_1\\text{ termes}} \\oplus \\underbrace{U^2(G) \\oplus \\cdots \\oplus U^2(G)}_{n_2\\text{ termes}} \\oplus \\cdots\\\\ \u0026amp;=\\sum_{\\mu \\oplus} n_\\mu U^\\mu(g) \\end{aligned} $$\noù $n_\\mu$ est le nombre de fois que la représentation irréductible $\\mu$ apparaît dans la décomposition.\nTout ce qui va suivre découle de cette décomposition\u0026hellip;\nAvec le bon choix de base, les matrices de la représentation $U(g)$ apparaîtront donc diagonales par bloc.\n$$ D(g)=\\left(\\begin{array}{cccc} D^1(g) \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; D^2(g) \u0026amp; \\cdots \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; D^k(g) \\end{array}\\right) $$\nPour tout $g,g^\\prime \\in G$,\n$$ D(g) D(g^{\\prime})=\\left(\\begin{array}{cccc} D^1(g) D^1(g^{\\prime}) \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; D^2(g) D^2(g^{\\prime}) \u0026amp; \\cdots \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; D^k(g) D^k(g^{\\prime}) \\end{array}\\right) $$\nLe non croisement des termes montre que $D(G)$ ne contient pas de nouvelles informations par rapport à l’ensemble des $D_i(G)$ justifiant que l’on puisse parler de redondance entre une représentation et l’ensemble de ses représentations irréductibles.\nCette décomposition n’est a priori possible que pour des représentations unitaires mais c’est finalement peu restreignant car outre le fait que les groupes de symétrie qui intéressent le physicien sont naturellement associés à des transformations unitaires, toute représentation d’un groupe fini ou compact (éléments variant sur un espace compact) est équivalente à une représentation unitaire.\nEn effet, toute représentation d’un groupe fini sur un espace doté d’un produit scalaire est équivalent à une représentation unitaire.\nPreuve\u0026nbsp;: À partir du produit scalaire $\\langle x \\mid y\\rangle$ défini sur l’espace vectoriel $V$, on en construit un nouveau, toujours sur $V$, en opérant une sorte de moyennage sur l’action du groupe $G$ (cette astuce de passage par la moyenne va beaucoup nous servir) :\n$(x, y) \\equiv \\sum_g\\langle D(g) x \\mid D(g) y\\rangle$\n$(.\\,,.)$ a bien les propriétés d’un produit scalaire : bilinéaire, symétrique, positif, défini.\nPassons maintenant de la base orthonormée $\\{e_i\\}$ adaptée à l’ancien produit scalaire, telle que $\\left\\langle e_i \\mid e_j\\right\\rangle=\\delta_{i j}$ à une base $\\{f_i\\}$ adaptée au nouveau, telle que $\\left(f_i, f_j\\right)=\\delta_{i j}$. Soit $S$ la matrice de changement de base : $f_i=S_i^{\\,j} e_j$.\nOn a ainsi : $(x, y)=\\langle S x \\mid S y\\rangle$ pour tout $x$ et $y$ dans $V$.\nLa représentation $U(g)=S D(g) S^{-1}$, équivalente à $D(g)$, est alors unitaire.\nEn effet :\n$\\begin{aligned} \\langle U(g) x \\mid U(g) y\\rangle \u0026 =\\left\\langle S D(g) S^{-1} x \\mid S D(g) S^{-1} y\\right\\rangle \\\\ \u0026 =\\left(D(g) S^{-1} x, D(g) S^{-1} y\\right) \\\\ \u0026 =\\sum_{g^{\\prime}}\\left\\langle D\\left(g^{\\prime}\\right) D(g) S^{-1} x \\mid D\\left(g^{\\prime}\\right) D(g) S^{-1} y\\right\\rangle \\\\ \u0026 =\\sum_{g^{\\prime \\prime}}\\left\\langle D\\left(g^{\\prime \\prime}\\right) S^{-1} x \\mid D\\left(g^{\\prime \\prime}\\right) S^{-1} y\\right\\rangle \\\\ \u0026 =\\left(S^{-1} x, S^{-1} y\\right) \\\\ \u0026 =\\langle x \\mid y\\rangle \\end{aligned}$ La preuve précédente est généralisable aux groupes compacts en remplaçant la somme dans la moyenne par une intégrale mais ça devient un peu plus technique.\nSavoir qu’une représentation est réductible nous fait une belle jambe tant qu’on ne sait pas distinguer une représentation déjà réduite d’une représentation pouvant l’être. Car si la matrice associée n’a pas le bon goût d’être d’ores et déjà diagonale par bloc, comment fait-on ? Et quand s’arrête-t-on d’essayer de réduire ?\nUne propriété capitale de la théorie des représentations va nous donner les clés pour opérer cette diagonalisation par bloc jusqu’au bout : les représentations irréductibles froment une base orthonormée de l’espace des représentations du groupe.\nConditions d’orthonormalité et de complétude pour les représentations irréductibles On obtient de haute lutte les relations suivantes :\nUne condition d’orthonormalité entre les représentations irréductibles (le «produit scalaire» entre deux représentations irréductibles identiques vaut 1, c’est ce qu’on nomme normalité, et est nul dans les autres cas, c’est l’orthogonalité)\u0026nbsp;: $\\displaystyle \\frac{n_\\mu}{n_G} \\sum_g D_\\mu^{\\dagger}(g)^k_i \\, D^\\nu(g)^j_l=\\delta^\\nu_\\mu \\, \\delta^j_i \\,\\delta^k_l$\nEt une conditions de complétude (le mot a beau sonné vilain, il traduit que le pavage de l’espace des représentations par les morceaux irréductibles est complet)\u0026nbsp;: $\\displaystyle \\sum_{\\mu, l, k} \\frac{n_\\mu}{n_G} D^\\mu(g)^l_k \\, D_\\mu^{\\dagger}\\left(g^{\\prime}\\right)^k_l=\\delta^g_{g^{\\prime}}$\nLe caractère complet est démontré par la très importante ultime relation\u0026nbsp;: $\\displaystyle\\sum_\\mu n_\\mu^2=n_G$\nLes $\\mu$ et $\\nu$ étiquettent les représentations irréductibles non équivalentes de $G$, $n_\\mu$ est la dimension de chacune de ces représentations, et $n_G$ est le nombre d’éléments dans le groupe $G$.\nLes conditions d’orthonormalité sont élevées au rang de Great Orthogonality Theorem (GOT) chez les anglo-saxons.\nOn peut les réécrire en notant à la manière quantique $\\langle g \\mid \\nu, j, l\\rangle=\\sqrt{\\frac{n_\\nu}{n_G}} D^\\nu(g)^j_l$ pour insister sur la nature «vectorielle» de ces relations :\n$$\\displaystyle\\sum_g\\langle\\mu, i, k \\mid g\\rangle\\langle g \\mid \\nu, j, l\\rangle=\\delta^\\nu_\\mu \\,\\delta^j_i \\, \\delta^k_l$$\nCette forme permet d\u0026rsquo;appréhender plus facilement son interprétation géométrique. Il faut s’imaginer un espace vectoriel complexe à $n_G$ dimensions où chaque axe correspond à un élément du groupe.\nChaque $D^\\mu(g)^j_i$ peut donc être vues comme un «vecteur» à $n_G$ composantes (avec $g$ parcourant $G$), tous orthogonaux entre eux. Le premier de ces vecteurs serait par exemple :\n$ \\left(D^1(e)^1_1, D^1\\left(g_1\\right)^1_1, D^1\\left(g_2\\right)^1_1, \\ldots, D^1\\left(g_{n_G}\\right)^1_1\\right) $\nOn peut aussi réécrire à la manière quantique la relation de complétude :\n$$\\sum_{\\mu, l, k}\\langle g \\mid \\mu, l, k\\rangle\\left\\langle\\mu, l, k \\mid g^{\\prime}\\right\\rangle=\\delta^g_{g^{\\prime}}$$\nnote\nOn s’applique en mécanique quantique à vérifier des relations du même type sur les vecteurs de base de l’espace vectoriel des états (c’est d’ailleurs une des motivations pour la notation compacte en bra-ket adoptée ici). Mais au terme «complétude», les quanticiens préfèrent la mieux tournée «relation de fermeture» dont le contenu est le même ; il s’agit de prouver que l’espace ainsi décomposé est complet, c’est-à-dire que tout état peut se décomposer sur les vecteurs de base.\nLa démonstration de l’orthonormalité s’appuie sur le lemme de Schur qu’on va détailler tout de suite (les relations précédentes sont d’ailleurs parfois appelées relations d’orthogonalité de Schur) et la complétude repose sur les représentations régulières qu’on décrira plus loin.\nOpérateur d\u0026rsquo;entrelacement Avant d’arriver au lemme de Schur, il nous faut introduire un nouvel opérateur qui généralise la notion d’homomorphisme d’une représentation à une autre.\nPrenons une représentation $U_1(G)$ d’un groupe $G$ dans un espace vectoriel $V_1$ envoyant un vecteur $v_1$ quelconque vers un vecteur $v^\\prime_1$ pour un certain élément $g$ de $G$ et une représentation $U_2(G)$ du même groupe $G$ dans un espace $V_2$ envoyant un vecteur $v_2$ vers un vecteur $v^\\prime_2$ toujours pour le même élément $g$. On aimerait qu’un homomorphisme $A$ de l’une à l’autre de ces représentations conserve leurs actions en associant parallèlement les vecteurs modifiés pour chaque $g$.\nPour être plus clair : si $A$ envoie $v_1$ sur $v_2$, on voudrait alors du même coup que $v^\\prime_1$ soit envoyé sur $v^\\prime_2$.\nOn aurait ainsi $v_2^{\\prime}=U_2(g) v_2=U_2(g) A v_1=A v_1^{\\prime}=A U_1(g) v_1$.\nFormalisons : Soient $U_1(G) \\in V_1$ et $U_2(G) \\in V_2$ deux représentations. L’opérateur d’entrelacement $A$ (ou homomorphisme de représentations) est une application linéaire de $V_1$ sur $V_2$ tel que $A U_1(g)=U_2(g) A$ pour tout $g\\in G$.\nOn dit que $A$ entrelace $U_1$ et $U_2$ ce qui est plutôt mignon.\nOn dit aussi que $A$ est une application G-équivariante, sobriquet assez parlant bien que moins poétique.\nEt pour les esprits schématiques, l’opérateur d’entrelacement peut aussi être définit tel que le diagramme suivant commute pour tout $g\\in G$, le chemin rouge et le chemin bleu arrivant au même endroit :\nL’ensemble de tous les opérateurs d’entrelacement de $U_1$ à $U_2$ est noté $\\operatorname{Hom}_G\\left(U_1, U_2\\right)$.\nLemme de Schur\u2028Lemme de Schur\u0026nbsp;: Soit $U_1(G)$ et $U_2(G)$ deux représentations irréductible d’un groupe $G$ sur des espaces vectoriels $V_1$ et $V_2$ et $A \\in \\operatorname{Hom}_G\\left(U_1, U_2\\right)$, un opérateur d’entrelacement.\nOn a alors : soit $A=0$, soit $V_1$ et $V_2$ sont isomorphes et les représentations $U_1$ et $U_2$ sont équivalentes.\nPreuve\u0026nbsp;: Si $A=0$, il n’y a rien à démontrer. Supposons alors $A≠0$.\nMontrons que $A$ est surjectif\u0026nbsp;:\ncomme $A≠0$, il existe au moins deux vecteurs $v_2$ et $v_1$ respectivement de $V_2$ et $V_1$ tels que $v_2=A v_1$.\nEt pour tout $g\\in G$, on a\u0026nbsp;: $U_2(g) v_2=U_2(g) A v_1=A\\left(U_1(g) v_1\\right)$ qui appartient à $A V_1$.\nPar conséquent, $A V_1$ est un sous-espace stable non nul de $V_2$ par rapport à $U_2$ et comme $U_2$ est irréductible, on doit avoir $AV_1 = V_2$ . Montrons que $A$ est injectif\u0026nbsp;: soit maintenant $W$ le noyau de $A$. $A$ est une application linéaire, or le noyau d’une application linéaire change un tantinet de ceux côtoyés jusqu’ici puisque l’élément neutre n’est plus l’identité mais le vecteur nul (la composition interne considérée est l’addition). Le noyau étant définit comme l’ensemble aboutissant à l’élément neutre, on a $W=\\left\\{v_1 \\in V_1 ; A v_1=0\\right\\}$.\nPrenons un élément $v_1$ de ce noyau. Alors $A\\left(U_1(g) v_1\\right)=U_2(g) A v_1=U_2(g) 0=0$, donc $U_1(g) v_1$ appartient à $W$ et ce, pour tout $g$. Ce qui implique que $W$ est un sous-espace invariant de $V_1$ pour $U_1$, or comme $U_1$ est irréductible, soit $W=V_1$ mais alors $A=0$, soit $W=\\{0\\}$ (injection). $A$ est donc une application bijective entre $V_1$ et $V_2$, ce qui implique que ces deux espaces soient isomorphes.\nD’autre part, comme le noyau de $A$ est le vecteur nul, $A$ est inversible et de $U_2(g) A v_1=A\\left(U_1(g) v_1\\right)$, on déduit $U_2(g)=A U_1(g) A^{-1}$ pour tout $g\\in G$.\nLes deux représentations sont bien équivalentes.\nOn tire un intéressant corollaire du lemme de Schur :\nSoit $U(G)$ une représentation irréductible et de dimension finie de $G$ dans l’espace vectoriel $V$ complexe. Alors, un homomorphisme de la représentation de $U$ sur elle-même (automorphisme appartenant à $\\operatorname{Hom}_G(U, U)$) est un multiple de la matrice identité $E$.\nPreuve\u0026nbsp;: soit $A$ un opérateur de cet homomorphisme et soit une valeur propre $λ \\in \\mathbb{C}$ de $A$ (comme $A$ est inversible et de dim finie sur un espace vectoriel complexe, il y en a forcément au moins une). $A- λE$ appartient aussi à l’automorphisme puisque $E$ en fait partie et qu’on joue avec des applications linéaires (la loi de composition de groupe, si on peut encore l’appeler ainsi, est justement la combinaison linéaire d’éléments).\nMais par définition d’une valeur propre, $A- λE$ n’est pas inversible donc d’après le lemme, il ne peut s’agir que de l’élément nul (le lemme dit : soit isomorphisme soit zéro).\nRemarque :\nC’est la première fois qu’on s’impose de travailler sur un espace vectoriel complexe. Lui-seul permet d’affirmer qu’un opérateur linéaire sur un espace vectoriel de dimension finie possède une valeur propre. C’est une conséquence de l’existence d’une racine pour tout polynôme de degré ≥ 1 dans $\\mathbb{C}$ (le théorème fondamental de l\u0026rsquo;algèbre dit bien qu\u0026rsquo;un polynôme de degré $n$ a $n$ racines). En particulier, le polynôme caractéristique de l’opérateur a donc une racine.\nEt cela marche aussi pour un espace vectoriel complexe de dimension infinie mais clôt, ce qui permet d’étendre la propriété aux cas qu’on rencontrera dans les chapitres suivants.\nNotons que l’automorphisme d’une représentations commute, par définition, avec tous les éléments de cette représentation.\nPar conséquent, la propriété peut se reformuler ainsi :\nSoit $U(G)$ une représentation irréductible d’un groupe $G$ dans un espace vectoriel $V$.\nUn opérateur $A$ de $V$ commutant avec tous les opérateurs ${U(g), g \\in G}$ est un multiple de l’identité.\nOn utilise cette propriété pour débusquer les opérateurs de Casimir comme $\\overrightarrow{J^2}$ (d’Henrik Casimir, physicien hollandais).\nEt on en déduit aussi (corollaire du corollaire) :\nToute représentation irréductible d’un groupe abélien est de dimension 1.\nPreuve\u0026nbsp;: Soit $U(G)$ une représentation irréductible du groupe abélien $G$. Soit $p$ un élément de $G$. Comme $G$ est abélien, on a $U(p) U(g)=U(g) U(p)$ pour tout $g\\in G$.\nD’après le lemme de Schur, on a alors $U(p)=\\lambda_p E$. Et cela marche pour tout les $p\\in G$. Par conséquent, la représentation $U(G)$ est équivalente à la représentation unidimensionnelle $p \\rightarrow \\lambda_p \\in \\mathbb{C}$ pour tous les $p\\in G$.\nEn physique, ces considérations généralisent un résultat bien connu : des opérateurs qui commutent possèdent un jeu complet de vecteurs propres communs (cf. la recherche d’un «ECOC», ensemble complet d’observables qui commutent, en mécanique quantique).\nPrécisons cela dans le cas de l’Hamiltonien.\nLa dynamique d’un système physique est déterminée par un opérateur appelé l’Hamiltonien $H$ et cet opérateur doit, par définition, rester invariant sous les opérations de symétrie laissant invariant le système physique lui-même.\nMathématiquement, cela revient à dire que $H$ commute avec les opérateurs unitaires de la symétrie considérée. Et par conséquent, dans une représentation irréductible donnée du groupe de symétrie, l’Hamiltonien a pour représentation un multiple de la matrice identité.\nEn d’autres mots, tous les vecteurs de la représentation irréductible sont des vecteurs propres de l’Hamiltonien pour la même valeur propre.\nCette valeur propre de l’Hamiltonien n’est autre que l’énergie du système et donc l’ensemble des états symétriques qu’on obtient (les vecteurs de la représentation irréductible) sont à énergie fixée.\nDémonstration de l\u0026rsquo;orthonormalité On est maintenant armé pour prouver l’orthonormalité des représentations irréductibles :\nOn veut prouver $ \\frac{n_\\mu}{n_G} \\sum_g D_\\mu^{\\dagger}(g)^k_i \\, D^\\nu(g)^j_l=\\delta^\\nu_\\mu \\, \\delta^j_i \\,\\delta^k_l$. Soit $X$ une matrice $n_\\mu \\times n_\\nu$ quelconque à partir de laquelle on forme :\n$M_X=\\sum_g D_\\mu^{\\dagger}(g) X D^\\nu(g)$ où $D_\\mu(g)$ (resp. $D_\\nu(g)$) est une matrice de la représentation irréductible unitaires de $G$ d’ordre $\\mu$ (resp. $\\nu$). L\u0026rsquo;unitarité implique $D_\\mu^{\\dagger}(g)=D_\\mu^{-1}(g)$.\nEn découle :\n$$ \\begin{aligned} D_\\mu^{-1}(p) M_X D^\\nu(p) \u0026 =D_\\mu^{-1}(p)\\left[\\sum_g D_\\mu^{\\dagger}(g) X D^\\nu(g)\\right] D^\\nu(p) \\\\ \u0026 =\\sum_g\\left[D_\\mu^{-1}(p) D_\\mu^{-1}(g)\\right] X\\left[D^\\nu(g) D^\\nu(p)\\right] \\\\ \u0026 =\\sum_{h=p g} D_\\mu^{-1}(h) X D^\\nu(h) \\\\ \u0026 =M_X \\end{aligned} $$ pour tout $p\\in G$.\nD’après le lemme de Schur, soit $\\mu≠\\nu$ (les deux représentations ne sont pas équivalentes) et alors $M_X = 0$, soit $\\mu=\\nu$ et $M_X=c_X E$ avec $c_X$ une constante (car on est dans le cas de l’automorphisme).\nChoisissons $X$ parmi les $n_\\mu \\times n_\\nu$ matrices $X_l^k\\left(k=1, \\cdots, n_\\nu ; l=1, \\cdots, n_\\mu\\right)$ dont les éléments sont définis par : $\\left(X_l^k\\right)^i_j=\\delta_j^k \\delta_l^i$.\nPrenons un exemple pour fixer les idées, avec $n_\\nu=4$ et $n_\\mu=3$, $X^1_2$ s’écrit :\n$X_2^1=\\left(\\begin{array}{llll} 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ \\end{array}\\right)$\nUn seul élément de chaque $X^k_l$ est non nul (et vaut 1), celui se trouvant colonne $k$, ligne $l$.\nOn a alors :\n$\\left(M_l^k\\right)^i_j=\\sum_g D_\\mu^{\\dagger}(g)^i_m\\left(X_l^k\\right)^m_n D^\\nu(g)^n_j=\\sum_g D_\\mu^{\\dagger}(g)^i_l D^\\nu(g)^k_j$ qui doit être nul dans le cas $\\mu≠\\nu$, expliquant le terme $\\delta^\\mu_\\nu$ dans la relation à démontrer.\nEt dans le cas $\\mu=\\nu$, $\\left(M_l^k\\right)^i_j=c_l^k \\,\\delta_j^i$ où les $c_k^l$ sont des constantes.\nOn détermine la valeur de ces constantes en prenant la trace de chacun des membres de l’équation ($i=j$) :\npour le membre de gauche, on obtient $n_\\mu c_l^k$ et pour le membre de droite $\\sum_g\\left[D_\\mu^{\\dagger}(g) D^\\mu(g)\\right]_l^k=n_G \\delta_l^k$.\nFinalement, $c_l^k=\\left(n_G / n_\\mu\\right) \\delta_l^k$.\nRemarque :\nl’orthonormalité entre représentations irréductibles inéquivalentes est insuffisante pour obtenir la relation $\\Sigma_\\mu n_\\mu^2=n_G$.\nEn effet, en considérant les $D^\\mu(g)_j^i$, $g \\in G$ comme un ensemble de vecteurs orthogonaux étiquetés par les $(\\mu,i,j)$, on se retrouve avec $\\Sigma_\\mu n_\\mu^2$ «vecteurs» dans cet ensemble puisque les $(i,j)$ prennent $n_\\mu^2$ valeurs différentes et chacun de ces vecteurs est formé de $n_G$ composantes. Or le nombre de vecteurs mutuellement orthogonaux (donc linéairement indépendants) doit être au mieux égal à la dimension de l’espace vectoriel, ici $n_G$.\nOn a donc seulement $\\sum_\\mu n_\\mu^2 \\leq n_G$.\nCela rend néanmoins déjà possible la quête principale de la théorie des représentations : le recensement de toutes les représentations irréductibles non équivalentes d’un groupe donné.\nBien qu’anticipant la preuve, on sait déjà que l’inégalité est en réalité toujours saturée. Les «vecteurs» étiquetés par $(\\mu,i,j)$ et formés de la collection ordonnée de $n_G$ $D^\\mu(g)_j^i$, forment donc bien un ensemble complet en plus d’être orthogonal.\nApplications de l\u0026rsquo;orthonormalité Voyons maintenant comment utiliser l’orthogonalité pour construire de nouvelles représentations irréductibles.\nExemple\u0026nbsp;: $\\mathrm{C}_2$, groupe le plus simple, a, comme tout les groupes, une représentations évidentes : l’identité $d_1$ définit comme $(e, a) \\xrightarrow{d_1}(1,1)$ (notation signifiant $d_1(e)=1$ et $d_1(a)=1$). La notation réduite prend tout son sens avec la relation d’orthogonalité où les représentations sont vues comme des vecteurs). Si une deuxième représentation non équivalente $d_2$ est aussi regardé comme un vecteur a deux composantes, alors il doit être orthogonal à $(1,1)$. $(1,-1)$ est la seule possibilité à la fois orthogonale et normalisable. Donc le seul candidat pour une seconde représentation irréductible est $(e, a) \\xrightarrow{d_2}(1,-1)$.\nOn ne peut trouver d’autres vecteurs orthogonaux, on a donc l’ensemble des représentation irréductibles de $\\mathrm{C}_2$.\nPartir de représentations simples comme l’identité est une des astuces.\nPour faire progresser l’investigation sur des groupes plus gros, on va avoir recours à une autre astuce liée aux groupes quotients mais cela va nous amener à digresser un peu\u0026hellip;\nLes groupes quotients fournissent de nouvelles représentations et permettent en outre de rendre fidèle une représentation dégénérée.\nReprésentations d’un groupe quotient :\nSi un groupe $G$ a un sous-groupe invariant $H$ non trivial, alors toute représentation du groupe quotient $K=G/H$ est aussi une représentation de $G$, mais cette représentation de $G$ est dégénérée.\nÀ l’inverse, si $U(G)$ est une représentation dégénérée de $G$, alors $G$ contient au moins un sous-groupe invariant $H$ tel que $U(G)$ définisse une représentation fidèle (non dégénérée) du groupe quotient $G/H$.\nExemple\u0026nbsp;: On a déjà vu que $\\mathrm{S_3}$ a un sous-groupe invariant $H=\\{e,(123),(321)\\}$. Le groupe quotient est isomorphe à $\\mathrm{C_2}=\\{e,a\\}$. Or $\\mathrm{C_2} $ a une représentation assez simple $\\{(e, a) \\rightarrow(1,-1)\\}$ (on reviendra plus loin sur ce type de notation et la méthode pour trouver une représentation autrement que par tâtonnement).\nCela induit une représentation unidimensionnelle de $\\mathrm{S_3}$ associant $1$ aux éléments $\\{e,(123),(321)\\}$ et $-1$ à $\\{(12),(23),(31)\\}$. C’est effectivement une des possibilités trouvées dans l’exemple précédent en jouant avec la table de multiplication du groupe. Cette représentation, dégénérée pour $\\mathrm{S_3}$, est bien une représentation fidèle pour $S_3 / H \\simeq C_2$.\nRemarque :\nles représentations dégénérées et la manière de les rendre fidèle (ces choix sémantiques traduisent-ils une croisade morale de certains mathématiciens ?) reviendront sur le devant de la scène quand on parlera des groupes compacts (comme les rotations) et des topologies associées.\nCe nouvel outil peut faciliter grandement la recherche de représentations irréductibles comme on va le voir dans l’exemple suivant.\nExemple\u0026nbsp;: considérons le groupe dihédral $\\mathrm{D_2}$ dont la table de multiplication est :\nLe groupe correspond aux symétries de la figure suivante : Les 4 éléments de symétrie sont l’identité, les deux réflexions $(13)$ et $(24)$ et la rotation d’angle π. Associés deux à deux, ça redonne un des 4. Et comme ils commutent tous, le groupe est abélien. Par conséquent, on s’attend à des représentations unidimensionnelle.\nLa première de ces représentations est la triviale identité\u0026nbsp;: $(e, a, b, c) \\xrightarrow{d_1}(1,1,1,1)$. Maintenant, l’astuce : les éléments $\\{e,a\\}$ forment un sous-groupe invariant et le groupe quotient $\\{\\{e, a\\},\\{b, c\\}\\}$ est isomorphe à $\\mathrm{C_2}$ (il n’y a qu’un groupe à deux éléments) dont on connait déjà les deux représentations irréductibles. Les deux représentations de $\\mathrm{C_2}$ induisent deux représentations dégénérées sur $\\mathrm{D_2}$. La première est la perpétuelle représentation identité, la deuxième associe $-1$ à $b$ et $c$ : $(e, a, b, c) \\xrightarrow{d_2}(1,1,-1,-1)$.\nOn peut ensuite tenir le même raisonnement en partant du sous-groupe invariant $\\{e,b\\}$ qui nous amène la troisième représentation : $(e, a, b, c) \\xrightarrow{d_3}(1,-1,1,-1)$.\nEnfin, le sous-groupe invariant $\\{e,c\\}$ nous donne la quatrième et dernière : $(e, a, b, c) \\xrightarrow{d_4}(1,-1,-1,1)$.\nOn vérifie bien que les 4 «vecteur» sont orthonornaux comme il se doit et qu’aucun autre ne peut exister par la même condition.\nRemarques :\nLa relation $\\Sigma_\\mu n_\\mu^2=n_G$ (que l’on démontrera plus loin) nous assure que pour tout groupe abélien (représentation unidimensionnelle $\\Leftrightarrow n_\\mu=1$), il y a autant de représentation que d’éléments ($n_G$) et les vecteurs formés par les représentations (étiquetés par $\\mu$) forment alors un ensemble orthogonal complet.\nLes représentations de $\\mathrm{C_2}$ et $\\mathrm{D_2}$ débusquées dans les exemples précédents illustrent bien ce point.\nPlus besoin dans ce cas de guillemets au mot vecteur, il ne s’agit plus d’une collection pachydermique de $n_G$ objets de dimension $n_\\mu^2$ mais bien d’une collection ordonnée de $n_G$ nombres qu’on désigne plus volontiers ainsi (ou des tenseurs de rang 1 si on veut fanfaronner), même si techniquement, les gros objets formés par une représentation quelconque peuvent tout autant revendiquer l’appellation. On s’intéressera dans les chapitres suivants à des groupes infinis et par chance, en dépit de quelques ajustements cosmétiques (les intégrales remplaceront les sommes discrètes), la condition d’orthonormalité des représentations irréductibles tient le choc.\nPour le groupe infini le plus simple, abélien et à une dimension, elle se confond avec le théorème de Fourier des fonctions périodiques et aiguise alors notre vision des choses ; la condition d’orthonormalité est en fin de compte une puissante extension du théorème de Fourier. Comme on l’a déjà évoqué, les matrices des représentations dépendent de la base. Pour les représentations 1D des groupes abéliens, pas de soucis, mais chez les groupes plus complexes, les dimensions supplémentaires n’apporteront que tracas et confusion.\nGrâce aux caractères, indépendants de la base, on pourrait maintenir un traitement équivalent aux douillettes représentations 1D quelle que soit la dimension. Transposer les relations d’orthonormalité chez les caractères, bien plus engageants, semble donc une entreprise judicieuse.\nConditions d’orthonormalité et de complétude pour les caractères irréductibles Rappelons à toute fin utile que les caractères d’une représentation $U(G)$ sont les traces des opérateurs $U(g)$. Ils sont indépendant du choix de la base dans l’espace des représentations. Tous les éléments d’un groupe appartenant à une même classe ont par conséquent le même caractère dans une représentation donnée.\nSoit $U^\\mu(G)$ une représentation irréductible de $G$.\nAlors la somme des $U^\\mu(g)$ sur l’ensemble des éléments d’une classe donnée vaut :\n$\\sum_{h \\in \\zeta_i} U^\\mu(h)=\\frac{n_i}{n_\\mu} \\chi_i^\\mu E$\noù $\\zeta_i$ est la classe $i$, $E$ l’opérateur identité, $n_\\mu$ la dimension de la représentation et $n_i$ le nombre d’éléments dans la classe $i$.\nPreuve\u0026nbsp;: Notons $A_i$ le membre de gauche de l’équation. On a alors $U^\\mu(g) A_i U^\\mu(g)^{-1}=A_i$ puisque le produit ne fait que réarranger l’ordre de la sommation (en utilisant le fait que si $h \\in \\zeta_i$, alors $g h g^{-1} \\in \\zeta_i$ pour tout $g\\in G$). Donc $A_i$ commute avec toutes les représentations, ce qui, d’après le lemme de Schur, impose d’être proportionnel à l’opérateur identité : $A_i=c_i E$. On trouve $c_i$ en évaluant la trace des deux membres de l’équation. À gauche, ça donne $n_i \\chi_i^\\mu$, et à droite $c_i n_\\mu$.\nÇa va nous permettre de démontrer les relations d’orthonormalité et de complétude sur le groupe des caractères. Mais d’abord, énonçons-les\u0026hellip;\nLes caractères de représentations irréductibles non équivalentes d’un groupe $G$ satisfont les relations suivantes :\u2028$$ \\sum_i \\frac{n_i}{n_G} \\chi_\\mu^{\\dagger i} \\chi_i^\\nu=\\delta_\\mu^\\nu \\quad \\text { orthonormalité } $$\n$$ \\frac{n_i}{n_G} \\sum_\\mu \\chi_i^\\mu \\chi_\\mu^{\\dagger j}=\\delta_i^j \\quad \\text { complétude } $$\noù par convention $\\chi_\\mu^{\\dagger i}=\\left(\\chi_i^\\mu\\right)^*$\nLes $i$ courent sur les différentes classes du groupe et les $\\mu$ sur les différentes représentations irréductibles non équivalentes. Preuve\u0026nbsp;: On part de la condition d’orthonormalité des représentations irréductibles en imposant $i=k$ et $k=l$ pour obtenir les traces.\nÀ gauche, on obtient :\n$$ \\left(n_\\mu / n_G\\right) \\sum_g \\chi_\\mu^{\\dagger}(g) \\chi^\\nu(g)=\\left(n_\\mu / n_G\\right) \\sum_i n_i \\chi_\\mu^{\\dagger i}(g) \\chi_i^\\nu(g) $$\noù on finit par sommer sur les classes (à l’intérieur desquelles le caractère est invariant) plutôt que sur les éléments du groupe. Et à droite, on obtient $n_\\mu \\delta_\\mu^\\nu$.\nPartons maintenant de la relation de complétude entre représentations irréductibles :\n$$ \\sum_{\\mu, l, k} \\frac{n_\\mu}{n_G} D^\\mu(g)_k^l D_\\mu^{\\dagger}\\left(g^{\\prime}\\right)^k_l=\\delta_{g g^{\\prime}} $$\net sommons les $g$ parmi les éléments de la classe $\\zeta_i$, et les $g^\\prime$ parmi les éléments de la classe $\\zeta_j$ :\n$$ \\sum_{\\mu, l, k} \\frac{n_\\mu}{n_G} \\sum_{g \\in \\zeta_i} D^\\mu(g)_k^l \\sum_{g^{\\prime} \\in \\zeta_j} D_\\mu^{\\dagger}\\left(g^{\\prime}\\right)^k_l=\\sum_{g \\in \\zeta_i} \\sum_{g^{\\prime} \\in \\zeta_j} \\delta_{g g^{\\prime}} $$\nÀ droite, ça nous donne $n_j \\delta_i^j$ et à gauche, on utilise la relation démontrée un peu plus haut $\\sum_{h \\in \\zeta_i} U^\\mu(h)=\\frac{n_i}{n_\\mu} \\chi_i^\\mu E$. On obtient :\n$$ \\sum_{\\mu, l, k} \\frac{n_\\mu}{n_G} \\frac{n_i}{n_\\mu} \\chi_i^\\mu E_k^l \\frac{n_j}{n_\\mu} \\chi_\\mu^{\\dagger j} E_l^k=\\sum_{\\mu, l, k} \\frac{n_i n_j}{n_\\mu n_G} \\chi_i^\\mu \\chi_\\mu^{\\dagger j} E_k^l E_l^k $$\nOr $\\sum_{l, k} E^l_k E^k_l=\\sum_l E_l^l=\\operatorname{Tr} E=n_\\mu$. D\u0026rsquo;où :\n$$ \\sum_{\\mu, l, k} \\frac{n_\\mu}{n_G} \\frac{n_i}{n_\\mu} \\chi_i^\\mu E_k^l \\frac{n_j}{n_\\mu} \\chi_\\mu^{\\dagger j} E_l^k=\\sum_\\mu \\frac{n_i n_j}{n_G} \\chi_i^\\mu \\chi_\\mu^{\\dagger j} $$\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc0/",
	"title": "TQC-0",
	"tags": [],
	"description": "",
	"content": " Théorie quantique des champs \u0026ndash; Partie 0 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell.\nLagrangien et principe de moindre action La détermination de la trajectoire $x(t)$ d\u0026rsquo;une particule entre A et B par intégration de la 2e loi de Newton, $F=m\\ddot{x}$, ne cadre pas avec les enseignements de la mécanique quantique. Car si on peut bien mesurer la particule en A à $t=0$ et en B à $t=\\tau$, un voile impénétrable nous empèche de savoir précisément ce qui s\u0026rsquo;est passé entre les deux. Une équation différentielle ne semble donc pas un point de départ prometteur pour calculer $x(t)$\u0026hellip;\nConcentrons-nous sur les variations de l\u0026rsquo;énergie cinétique $T$ et l\u0026rsquo;énergie potentielle $V$ le long de la trajectoire. L\u0026rsquo;énergie totale $E=T+V$ doit être une constante du mouvement de la particule mais l\u0026rsquo;équilibre entre les deux types d\u0026rsquo;énergie peut varier.\nNotons $\\displaystyle\\bar{T}=\\frac{1}{\\tau}\\int_0^\\tau\\frac{1}{2}m[\\dot{x}(t)]^2 dt$ l\u0026rsquo;énergie cinétique moyenne sur la trajectoire et $\\displaystyle\\bar{V}=\\frac{1}{\\tau}\\int_0^\\tau V[x(t)] dt$, l\u0026rsquo;énergie potentielle moyenne. On a bien sûr $E=\\bar{E}=\\bar{T}+\\bar{V}$.\nMathématiquement, $\\bar{T}$ et $\\bar{V}$ sont des fonctionnelles. Là où des fonctions se nourrissent de nombres pour produire des nombres, les fonctionnelles produisent des nombres à partir de fonctions ($\\bar{T}$ produit un nombre à partir de $\\dot{x}(t)$ et $\\bar{V}$ à partir de $x(t)$).\nLa dérivée fonctionnelle de $F[f(x)]$ nous dit comment le nombre produit par la fonctionnelle varie lorsqu\u0026rsquo;on change légèrement la fonction $f(x)$ qu\u0026rsquo;elle machouille :\n$$ \\frac{\\delta F}{\\delta f(x)}=\\lim_{\\epsilon\\to 0}\\frac{F[f(x')+\\epsilon\\delta(x-x')]-F[f(x')]}{\\epsilon} $$ Dérivées fonctionnelles utilisées dans la suite\u0026nbsp;: $F[f]=\\int_a^b g[f(x)]\\mathrm{d}x$ avec $g'=\\mathrm{d}g/\\mathrm{d}f$ $$ \\begin{aligned} \\frac{\\delta F[f]}{\\delta f(x_0)}\u0026=\\lim_{\\epsilon \\to 0}\\frac{1}{\\epsilon}\\left[\\int g[f(x)+\\epsilon \\delta(x-x_0)]dx-\\int g[f(x)]\\mathrm{d}x\\right]\\\\\\\\ \u0026=\\lim_{\\epsilon \\to 0}\\frac{1}{\\epsilon}\\left[\\int (g[f(x)]+\\epsilon\\delta(x-x_0)g'[f(x)])\\mathrm{d}x - \\int g[f(x)]\\mathrm{d}x\\right]\\\\\\\\ \u0026=\\int \\delta(x-x_0) g'[f(x)] \\mathrm{d}x\\\\\\\\ \u0026= g'[f(x_0)] \\end{aligned} $$ On détermine ainsi que $\\displaystyle \\frac{\\delta \\bar{V}[x]}{\\delta x(t)}=\\frac{1}{\\tau}V\u0026rsquo;[x(t)]$.\n$F[f]=\\int g(f\u0026rsquo;)\\mathrm{d}y$ avec $f\u0026rsquo;=\\mathrm{d}f/\\mathrm{d}y$\n$$\\frac{\\delta F[f]}{\\delta f(x)}=\\lim_{\\epsilon \\to 0}\\frac{1}{\\epsilon}\\left[\\int dy \\,g\\!\\left( \\frac{\\partial}{\\partial y}[f(y)+\\epsilon \\delta(y-x)]\\right) - \\int dy \\,g\\!\\left(\\frac{\\partial f}{\\partial y}\\right)\\right]$$ Par un développement de Taylor au premier ordre, on obtient :\n$$g\\left(\\frac{\\partial}{\\partial y}[f(y)+\\epsilon\\delta(y-x)]\\right)=g(f'+\\epsilon\\delta'(y-x))\\approx g(f')+\\epsilon\\delta'(y-x)\\frac{\\mathrm{d}g(f')}{\\mathrm{d} f'}$$ On aboutit à :\n$$\\frac{\\delta F[f]}{\\delta f(x)}=\\int \\mathrm{d}y \\delta'(y-x)\\frac{\\mathrm{d}g(f')}{\\mathrm{d} f'}$$ On utilise alors une intégration par partie\u0026nbsp;: $$\\int \\mathrm{d}y\\, \\delta'(y-x)\\frac{\\mathrm{d}g(f')}{\\mathrm{d} f'}=\\left[\\delta(y-x)\\frac{\\mathrm{d}g(f')}{\\mathrm{d} f'}\\right]-\\int \\mathrm{d}y \\,\\delta(y-x)\\frac{\\mathrm{d}}{\\mathrm{d} y}\\left(\\frac{\\mathrm{d}g(f')}{\\mathrm{d}f'}\\right)$$ Si $x$ se balade sur l'intervalle d'intégration, le terme entre crochets est de mesure nulle et on obtient finalement\u0026nbsp;: $$\\frac{\\delta F[f]}{\\delta f(x)}=-\\frac{\\mathrm{d}}{\\mathrm{d} x}\\left(\\frac{\\mathrm{d}g(f\u0026rsquo;)}{\\mathrm{d}f\u0026rsquo;}\\right)$$\nOn détermine ainsi que $\\displaystyle \\frac{\\delta \\bar{T}[x]}{\\delta x(t)}=-\\frac{m\\ddot{x}}{\\tau}$.\nD\u0026rsquo;autre part, si on a $F[\\phi]=\\int\\left(\\frac{\\partial\\phi}{\\partial y}\\right)^2\\mathrm{d}y$, on obtient $\\displaystyle \\frac{\\delta F[\\phi]}{\\delta\\phi(x)}=-2\\frac{\\partial^2\\phi}{\\partial x^2}$.\nRésultat qui se généralise à 3 dimensions pour donner un truc plutôt utile :\nSi $\\displaystyle I=\\int(\\nabla\\phi)^2\\mathrm{d}^3 x$, alors $\\displaystyle\\frac{\\delta I}{\\delta\\phi}=-2\\nabla^2\\phi$.\nOn peut maintenant vérifier comment $\\bar{T}$ et $\\bar{V}$ varient lorsque la trajectoire $x(t)$ est un peu modifiée :\n$$ \\frac{\\delta \\bar{V}[x]}{\\delta x(t)}=\\frac{V'[x(t)]}{\\tau}\\qquad \\qquad\\frac{\\delta \\bar{T}[x]}{\\delta x(t)}=-\\frac{m\\ddot{x}}{\\tau} $$ Or d\u0026rsquo;après la 2e loi de Newton, la trajectoire classique correspond à $m\\ddot{x}=-dV/dx$ et d\u0026rsquo;après ce qui précède, cela entraîne que $\\displaystyle \\frac{\\delta \\bar{V}[x]}{\\delta x(t)}=\\frac{\\delta \\bar{T}[x]}{\\delta x(t)}$.\nOn en conclut que pour une légère déviation autour de la trajectoire classique, l\u0026rsquo;énergie potentielle moyenne et l\u0026rsquo;énergie cinétique moyenne vont varier ensemble (généralement augmenter) et de la même valeur !\nCela peut se réécrire $\\displaystyle \\frac{\\delta }{\\delta x(t)}(\\bar{T}[x]- \\bar{V}[x])=0$.\nLa différence entre les deux énergies moyennes est stationnaire près de la trajectoire classique.\nOn décide alors de donner un nom à cette différence entre énergie cinétique et potentielle : le lagrangien $L$.\n$$ L=T-V $$ L\u0026rsquo;intégrale du lagrangien sur le temps définit l\u0026rsquo;action $S$.\n$$ S=\\int_0^\\tau L \\mathrm{d}t $$ Ces deux nouvelles grandeurs vont nous permettre de réécrire de manière synthétique ce qu\u0026rsquo;on a découvert jusque-là.\n$\\displaystyle S=\\int_0^\\tau(T-V)\\mathrm{d}t = \\tau(\\bar{T}[x]-\\bar{V}[x])$, et donc\n$$ \\frac{\\delta S}{\\delta x(t)} = 0 $$ C\u0026rsquo;est le principe de moindre action de Hamilton. Équations d\u0026rsquo;Euler-Lagrange :\nLe lagrangien $L$ peut s\u0026rsquo;écrire comme une fonction à la fois de la position et de la vitesse $L(x(t),\\dot{x}(t))$. Le principe de moindre action donne alors :\n$$ \\frac{\\delta L}{\\delta x(t)}-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\delta L}{\\delta\\dot{x}(t)} = 0 $$ Démonstration\u0026nbsp;: $$ \\begin{aligned} \\frac{\\partial S}{\\partial x(t)} \u0026 = \\int \\mathrm{d}u\\left[\\frac{\\partial L}{\\partial x(u)}\\frac{\\partial x(u)}{\\partial x(t)}+\\frac{\\partial L}{\\partial \\dot{x}(u)}\\frac{\\partial \\dot{x}(u)}{\\partial x(t)}\\right]\\\\\\\\ \u0026 = \\int \\mathrm{d}u\\left[\\frac{\\partial L}{\\partial x(u)}\\delta(u-t)+\\frac{\\partial L}{\\partial \\dot{x}(u)}\\frac{\\mathrm{d}}{\\mathrm{d}t}\\delta(u-t)\\right]\\\\\\\\ \u0026=\\frac{\\partial L}{\\partial x(t)}+\\left[\\delta(u-t)\\frac{\\delta L}{\\delta \\dot{x}(u)}\\right]_{t_i}^{t_f}-\\int \\mathrm{d}u\\,\\delta(u-t)\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\delta L}{\\delta\\dot{x}(u)}\\\\\\\\ \u0026=\\frac{\\delta L}{\\delta x(t)}-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\delta L}{\\delta\\dot{x}(t)} \\end{aligned} $$ On introduit aussi la densité lagrangienne $\\mathcal{L}$ :\n$$ L=\\int \\mathscr{d}x\\,\\mathcal{L} $$ $$ S=\\int \\mathscr{d}t\\,\\mathscr{d}x\\,\\mathcal{L} $$\nPlaçons-nous désormais dans un cadre relativiste : le lagrangien dépend maintenant d\u0026rsquo;une fonction $\\phi(x)$ où $x$ est un point de l\u0026rsquo;espace-temps dont la dérivée est le 4-vecteur $\\partial_\\mu\\phi$.\nL\u0026rsquo;action devient :\n$$S=\\int \\mathrm{d}^4 x\\mathcal{L}(\\phi,\\partial_\\mu\\phi)$$\nLe principe de moindre action fournit maintenant une version quadrivectorielle des équations d\u0026rsquo;Euler-Lagrange :\n$$ \\frac{\\delta S}{\\delta \\phi} = \\frac{\\delta L}{\\delta \\phi}-\\partial_\\mu\\left(\\frac{\\partial\\mathcal{L}}{\\partial(\\partial_\\mu\\phi)}\\right) = 0 $$ Le principe de moindre action tire sa justification de la mécanique quantique où les particules ne sont plus des particules mais des ondes. La particule allant d\u0026rsquo;un point A à un point B en empruntant toutes les trajectoires possibles (jusqu\u0026rsquo;aux plus saugrenues) devient donc une onde affublée d\u0026rsquo;un facteur de phase $\\mathrm{e}^{\\mathrm{i}S/\\hbar}$ où $S$ est l\u0026rsquo;action. Une action stationnaire correspond alors à une phase stationnaire.\nLorsqu\u0026rsquo;on se retrouve à sommer sur toutes les trajectoires possibles, les différents termes vont interférer. L\u0026rsquo;interférence sera destructive dans l\u0026rsquo;immense majorité des cas, là où la phase fluctuera fortement d\u0026rsquo;une trajectoire à l\u0026rsquo;autre. Au contraire, la trajectoire minimisant l\u0026rsquo;action va émerger car toutes les trajectoires voisines auront des phases proches et interféreront constructivement.\nChapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc1/",
	"title": "TQC-1",
	"tags": [],
	"description": "",
	"content": " Théorie quantique des champs \u0026ndash; Partie 1 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell.\nLes oscillateurs harmoniques Une vidéo vieille comme tout sur l\u0026rsquo;oscillateur harmonique quantique qui traite pas mal des points de ce chapitre :\nPremière quantification : les particules se comportent comme des ondes.\nSeconde quantification : les ondes se comportent comme des particules.\nPartons d\u0026rsquo;une masse $m$ accrochée à un ressort de constante de raideur $K$. La quantité de mouvement de la masse est donnée par $p=m\\dot{x}$. L\u0026rsquo;énergie totale $E$ vaut la somme de l\u0026rsquo;énergie cinétique $p^2/2m$ et de l\u0026rsquo;énergie potentielle $\\frac{1}{2}Kx^2$.\nEn mécanique quantique, on remplace $p$ par l\u0026rsquo;opérateur impulsion $-i\\hbar\\partial /\\partial x$ et on obtient alors l\u0026rsquo;équation de Schrödinger d\u0026rsquo;un oscillateur harmonique : $$\\left(-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2}+\\frac{1}{2}K x^2\\right)\\psi=E\\psi$$\nLes solutions sont données par :\n$$\\psi_n(\\xi)=\\frac{1}{\\sqrt{2^n n!}}\\left(\\frac{m\\omega}{\\pi\\hbar}\\right)^{1/4}H_n(\\xi)\\rm{e}^{-\\xi^2/2}$$\noù les $H_n(\\xi)$ sont des polynômes de Hermite et où $\\xi=\\sqrt{m\\omega/\\hbar x}$\nSi ces fonctions propres ressemblent pas mal à des ondes, les valeurs propres $E_n=\\left(n+\\frac{1}{2}\\right)\\hbar\\omega$ (avec $\\omega=\\sqrt{K/m}$) se rangent, elles, plutôt du côté particules. On remarque que pour $n=0$, l\u0026rsquo;énergie n\u0026rsquo;est pas nulle mais vaut $\\hbar\\omega/2$. C\u0026rsquo;est l\u0026rsquo;énergie de point zéro.\nAjouter un quantum d\u0026rsquo;énergie $\\hbar\\omega$ permet de monter d\u0026rsquo;un barreau l\u0026rsquo;échelle des énergies, ce que l\u0026rsquo;on a bien envie de modéliser par l\u0026rsquo;absorption d\u0026rsquo;une particule. On peut formaliser ça élégamment (et sans se salir les mains avec les polynômes de Hermite).\nOn part de l\u0026rsquo;Hamiltonien de l\u0026rsquo;oscillateur harmonique :\n$$\\hat{H}=\\frac{\\hat{p}^2}{2m}+\\frac{1}{2}m\\omega^2\\hat{x}^2$$\noù on a réexprimé la constante de raideur : $K=m\\omega^2$.\nL\u0026rsquo;Hamiltonien semble vouloir être factorisé en $\\frac{1}{2}m\\omega^2\\left(\\hat{x}-\\frac{\\mathrm{i}}{m\\omega}\\hat{p}\\right)\\left(\\hat{x}+\\frac{\\mathrm{i}}{m\\omega}\\hat{p}\\right)$, mais un problème se dresse : $\\hat{x}$ et $\\hat{p}$ ne commutent pas ! En effet, $\\left[\\hat{x},\\hat{p}\\right]\\equiv \\hat{x}\\hat{p}-\\hat{p}\\hat{x}=\\mathrm{i}\\hbar$.\nPar conséquent :\n$$\\frac{1}{2}m\\omega^2\\left(\\hat{x}-\\frac{\\mathrm{i}}{m\\omega}\\hat{p}\\right)\\left(\\hat{x}+\\frac{\\mathrm{i}}{m\\omega}\\hat{p}\\right)=\\frac{1}{2}m\\omega^2\\hat{x}^2+\\frac{\\hat{p}^2}{2m}+\\frac{\\mathrm{i}\\omega}{2}[\\hat{x},\\hat{p}]$$ Plutôt que $\\hat{H}$, on obtient donc $\\hat{H} -\\frac{\\hbar\\omega}{2}$, l'Hamiltonien corrigé de l'énergie de point zéro. Cela ne semble pas un problème indépassable. Les deux opérateurs $\\hat{x}-\\frac{\\mathrm{i}}{m\\omega}\\hat{p}$ et $\\hat{x}+\\frac{\\mathrm{i}}{m\\omega}\\hat{p}$ semblent voués à jouer un rôle important dans cette histoire. Ils sont adjoints l\u0026rsquo;un de l\u0026rsquo;autre (puisque $\\hat{x}$ et $\\hat{p}$ sont hermitiens) ce qui leur interdit d\u0026rsquo;être eux-mêmes hermitiens et donc de correspondre à une quelconque observable.\nAprès un petit toilettage, introduisons les opérateurs d\u0026rsquo;échelle :\n$$\\hat{a}=\\sqrt{\\frac{m\\omega}{2\\hbar}}\\left(\\hat{x}+\\frac{\\mathrm{i}}{m\\omega}\\hat{p}\\right)$$\n$$\\hat{a}^\\dagger=\\sqrt{\\frac{m\\omega}{2\\hbar}}\\left(\\hat{x}-\\frac{\\mathrm{i}}{m\\omega}\\hat{p}\\right)$$\n$$[\\hat{a},\\hat{a}^\\dagger]=1$$\nOn peut inverser les définitions de $\\hat{a}$ et $\\hat{a}^\\dagger$ pour obtenir :\n$$\\hat{x}=\\sqrt{\\frac{\\hbar}{2m\\omega}}\\left(\\hat{a}+\\hat{a}^\\dagger\\right)$$\n$$\\hat{p}=-\\mathrm{i}\\sqrt{\\frac{\\hbar m \\omega}{2}}\\left(\\hat{a}-\\hat{a}^\\dagger\\right)$$\nEt l\u0026rsquo;Hamiltonien devient :\n$$\\hat{H}=\\hbar\\omega\\left(\\hat{a}^\\dagger\\hat{a}+\\frac{1}{2}\\right)$$\nAppelons $|n\\rangle$ un état propre de $\\hat{a}^\\dagger\\hat{a}$ pour une valeur propre $n$. Alors $|n\\rangle$ sera aussi vecteur propre de $\\hat{H}$ mais pour une valeur propre $\\hbar\\omega(n+\\frac{1}{2})$. Si $n$ vaut 0, 1, 2, \u0026hellip;., on aura bien retrouvé les valeurs propres d\u0026rsquo;un oscillateur harmonique !\nMontrons d\u0026rsquo;abord que $n≥0$ :\n$$n=\\langle n|\\hat{a}^\\dagger\\hat{a}|n\\rangle = |\\hat{a}|n\\rangle|^2≥0$$\nMontrons ensuite que $n$ ne prend que des valeurs entières.\nOn commence par définir l\u0026rsquo;opérateur nombre de quantas de vibration $\\hat{n} = \\hat{a}^\\dagger\\hat{a}$. On a alors $\\hat{n}|n\\rangle = n|n\\rangle$.\nNombre de quoi ? $n$ correspond au numéro du barreau d\u0026rsquo;échelle atteint et donc au nombre de quanta d\u0026rsquo;énergie $\\hbar\\omega$ qu\u0026rsquo;il a fallut ajoutés au système dans son état fondamental.\nOn peut réécrire l\u0026rsquo;Hamiltonien $\\hat{H}=\\hbar\\omega\\left(\\hat{n}+\\frac{1}{2}\\right)$. Et donc $\\hat{H}|n\\rangle = \\left(\\hat{n}+\\frac{1}{2}\\right) \\hbar\\omega |n\\rangle$. $|n\\rangle$ est ainsi un raccourci simple pour les vilains $\\psi_n(\\xi)$.\nDe $[\\hat{a},\\hat{a}^\\dagger]=1$, on déduit $\\hat{a}\\hat{a}^\\dagger = 1 +\\hat{a}^\\dagger\\hat{a}=1+\\hat{n}$ et donc $\\hat{n}\\hat{a}^\\dagger|n\\rangle = \\hat{a}^\\dagger\\hat{a}\\hat{a}^\\dagger|n\\rangle=\\hat{a}^\\dagger(1+\\hat{n})|n\\rangle=(n+1)\\hat{a}^\\dagger|n\\rangle$. Par conséquent, $\\hat{a}^\\dagger|n\\rangle$ est un état propre de $\\hat{H}$ pour une valeur propre un rang au-dessus de celle de $|n\\rangle$. $\\hat{a}^\\dagger$ a donc pour effet d\u0026rsquo;ajouter un quantum d\u0026rsquo;énergie ! C\u0026rsquo;est l\u0026rsquo;opérateur de création.\nEn utilisant la relation de commutation $[\\hat{n},\\hat{a}]=\\hat{n}\\hat{a}-\\hat{a}\\hat{n}=\\hat{a}^\\dagger\\hat{a}\\hat{a}-{\\color{#970E53}\\hat{a}\\hat{a}^\\dagger}\\hat{a}=\\hat{a}^\\dagger\\hat{a}\\hat{a}-{\\color{#970E53}(1+\\hat{a}^\\dagger\\hat{a})}\\hat{a}=-\\hat{a}$, on obtient $\\hat{n}\\hat{a}|n\\rangle = (-\\hat{a}+\\hat{a}\\hat{n})|n\\rangle=(n-1)\\hat{a}|n\\rangle$.\n$\\hat{a}|n\\rangle$ est un état propre de $\\hat{H}$ pour une valeur propre un rang en-dessous de celle de $|n\\rangle$. $\\hat{a}$ fait donc descendre d\u0026rsquo;un étage. C\u0026rsquo;est l\u0026rsquo;opérateur d\u0026rsquo;annihilation.\nEn appliquant de manière répétée l\u0026rsquo;opérateur $\\hat{a}$ à $|n\\rangle$, on pourrait finir avec une énergie négative, ce qui semble physiquement blasphématoire. Il faut donc qu\u0026rsquo;il existe un état fondamental $|0\\rangle$ tel que $\\hat{n}|0\\rangle = 0$.\nEt en appliquant maintenant $\\hat{a}^\\dagger$ depuis $|0\\rangle$, on se retrouve bien avec des valeurs de $n$ entières !\nAprès normalisation, on obtient :\n$$\\hat{a}|n\\rangle = \\sqrt{n}|n-1\\rangle$$\n$$\\hat{a}^\\dagger|n\\rangle = \\sqrt{n+1}|n+1\\rangle$$\nPreuve : On a montré que $\\hat{a}|n\\rangle$ est proportionnel à $|n-1\\rangle$ : $\\hat{a}|n\\rangle=k|n-1\\rangle$.\nPrenons la norme de cet état : $|a|n\\rangle|^2 = \\langle n|\\hat{a}^\\dagger\\hat{a}|n\\rangle=|k|^2\\langle n-1|n-1\\rangle=|k|^2$ (vu que les états propres de l\u0026rsquo;oscillateur harmonique sont normalisés).\nMais on peut remarquer aussi que $\\langle n|\\hat{a}^\\dagger\\hat{a}|n\\rangle=\\langle n|\\hat{n}|n\\rangle =n$.\nOn obtient par conséquent $k=\\sqrt{n}$ (cela suppose $k$ réel mais comme les états sont définis à une phase près, on peut toujours choisir la phase afin que $k$ soit bien réel).\nOn a aussi montré que $\\hat{a}^\\dagger=c|n+1\\rangle$ et $|a^\\dagger|n\\rangle|^2 = \\langle n|\\hat{a}\\hat{a}^\\dagger|n\\rangle=|c|^2\\langle n+1|n+1\\rangle=|c|^2$.\nEt comme $\\hat{a}\\hat{a}^\\dagger=1+\\hat{n}$, $\\langle n|\\hat{a}\\hat{a}^\\dagger|n\\rangle=\\langle n|1+\\hat{n}|n\\rangle=n+1$.\nEt donc $c=\\sqrt{n+1}$\nOn vérifie bien que $\\hat{a}|0\\rangle=0$. $|0\\rangle$ est effectivement l\u0026rsquo;état fondamental de l\u0026rsquo;oscillateur harmonique, on ne peut pas aller plus bas.\nEt on retrouve aussi l\u0026rsquo;énergie de point zéro $E_0=\\frac{1}{2}\\hbar\\omega$ :\n$$\\displaystyle\\hat{H}|0\\rangle=\\hbar\\omega\\left(\\hat{n}+\\frac{1}{2}\\right)|0\\rangle=\\frac{1}{2}\\hbar\\omega|0\\rangle$$\nMontons maintenant les barreaux de l\u0026rsquo;échelle en créant à chaque fois ce qui a tout l\u0026rsquo;air d\u0026rsquo;une particule d\u0026rsquo;énergie $\\hbar\\omega$ :\n$\\displaystyle\\hat{a}^\\dagger|0\\rangle=|1\\rangle$,\n$\\displaystyle\\hat{a}^\\dagger|1\\rangle=\\sqrt{2}|2\\rangle \\rightarrow |2\\rangle=\\frac{(\\hat{a}^\\dagger)^2}{\\sqrt{2}}|0\\rangle$,\n$\\displaystyle\\hat{a}^\\dagger|2\\rangle=\\sqrt{3}|3\\rangle \\rightarrow |3\\rangle=\\frac{(\\hat{a}^\\dagger)^3}{\\sqrt{3\\times 2}}|0\\rangle$\u0026hellip;\nEt en généralisant, $\\displaystyle |n\\rangle=\\frac{(\\hat{a}^\\dagger)^n}{\\sqrt{n!}}|0\\rangle$.\nLe problème ondulatoire de départ a spontanément produit des particules !\nSupposons maintenant qu\u0026rsquo;on ait affaire à un troupeau de $N$ oscillateurs harmoniques indépendants, sans couplage. L\u0026rsquo;Hamiltonien devient $\\displaystyle \\hat{H}=\\sum_{k=1}^N\\hat{H}_k$ qu\u0026rsquo;on peut réécrire :\n$$\\displaystyle \\hat{H}=\\sum_{k=1}^N\\hbar\\omega_k\\left(\\hat{a}^\\dagger_k\\hat{a}^{\\phantom{\\dagger}}_k+\\frac{1}{2}\\right)$$\nUn état général du système est donné par $|n_1,n_2,\\cdots,n_N\\rangle$. C\u0026rsquo;est la représentation en nombre d\u0026rsquo;occupation. Et on a :\n$$|n_1,n_2,\\cdots,n_N\\rangle=\\frac{1}{\\sqrt{n_1 !n_2 !\\cdots n_N !}}(\\hat{a}_1^\\dagger)^{n_1}(\\hat{a}_2^\\dagger)^{n_2}\\cdots (\\hat{a}_N^\\dagger)^{n_N}|0,0,\\cdots,0\\rangle$$ Ou de manière plus succincte\u0026nbsp;: $$|{n_k}\\rangle=\\prod_k\\frac{1}{\\sqrt{n_k !}}(\\hat{a}^\\dagger_k)^{n_k}|0\\rangle$$\nCouplons maintenant tous ces petits ressorts.\nL\u0026rsquo;Hamiltonien qui tient compte de l\u0026rsquo;interaction entre les oscillateurs s\u0026rsquo;écrit :\n$$\\hat{H}=\\sum_j\\frac{\\hat{p}_j^2}{2m}+\\frac{1}{2}K(\\hat{x}_{j+1}-\\hat{x}_j)^2$$\nLes excitations de ce système se comportent exactement comme un jeu d\u0026rsquo;oscillateurs indépendants. Par quel miracle ? La transformée de Fourier permet de diagonaliser l\u0026rsquo;Hamiltonien. En effet, si les masses sont bien couplées dans l\u0026rsquo;espace réel, les excitations se découplent dans l\u0026rsquo;espace réciproque.\nEn supposant que les oscillateurs sont séparés d\u0026rsquo;une distance $a$, les transformées de Fourier de $x_j$ et $p_j$ s\u0026rsquo;écrivent :\n$$x_j = \\frac{1}{\\sqrt{N}}\\sum_k \\tilde{x}_k \\mathrm{e}^{\\mathrm{i}kja}\\qquad\\qquad p_j = \\frac{1}{\\sqrt{N}}\\sum_k \\tilde{p}_k \\mathrm{e}^{\\mathrm{i}kja}$$ où $\\tilde{x}_k$ et $\\tilde{p}_k$ sont les nouveaux opérateurs dans l'espace des modes de Fourier. En substituant dans l\u0026rsquo;Hamiltonien, on obtient :\n$$\\hat{H}=\\sum_k\\left[\\frac{1}{2m}\\hat{p}_k\\hat{p}_{-k}+\\frac{1}{2}m\\omega_k^2\\hat{x}_k\\hat{x}_{-k}\\right]$$\noù on a laissé tomber les tildes et où $\\omega_k^2=(4K/m)\\sin^2(ka/2)$.\nAstuce pour gérer des expressions du type :\n$$ \\frac{1}{N} \\sum_j \\sum_{k q} \\tilde{p}_k \\tilde{p}_q \\mathrm{e}^{\\mathrm{i}(k+q) j a} $$\nL\u0026rsquo;idée est de commencer par la somme spatiale en utilisant l\u0026rsquo;identité $\\sum_j \\mathrm{e}^{\\mathrm{i}\\left(k-k^{\\prime}\\right) j a}=N \\delta_{k k^{\\prime}}$. Cela donne :\n$$ \\sum_{k q} \\tilde{p}_k \\tilde{p}_q \\delta_{k,-q} $$\nPuis on utilise le Kronecker sur la somme des moments. Cela fixe $q=-k$, nous laissant avec une somme sur un seul indice :\n$$ \\sum_k \\tilde{p}_k \\tilde{p}_{-k} $$\nChaque mode de Fourier $k$ se comporte donc comme un oscillateur harmonique indépendant.\nOn peut à nouveau introduire des opérateurs de création et d\u0026rsquo;annihilation :\n$$ \\hat{a}_k = \\sqrt{\\frac{m\\omega_k}{2\\hbar}}\\left(\\hat{x}_k+\\frac{\\mathrm{i}}{m\\omega_k}\\hat{p}_k\\right)\\qquad\\displaystyle \\hat{a}^\\dagger_k = \\sqrt{\\frac{m\\omega_k}{2\\hbar}}\\left(\\hat{x}_{-k}-\\frac{\\mathrm{i}}{m\\omega_k}\\hat{p}_{-k}\\right) $$\nÀ partir d\u0026rsquo;eux, on peut isoler $\\hat{x}_k$ et $\\hat{p}_k$ :\n$$ \\hat{x}_k=\\sqrt{\\frac{\\hbar}{2 m \\omega_k}}\\left(\\hat{a}_k+\\hat{a}_{-k}^{\\dagger}\\right) \\qquad \\hat{p}_k= -\\mathrm{i} \\sqrt{\\frac{m \\hbar \\omega_k}{2}}\\left(\\hat{a}_k-\\hat{a}_{-k}^{\\dagger}\\right) $$\nEn réinjectant dans l\u0026rsquo;Hamiltonien, on obtient finalement, après réindexation et utilisation de la relation de commutation $\\left[\\hat{a}_k, \\hat{a}_{k^{\\prime}}^{\\dagger}\\right]=\\delta_{k, k^{\\prime}}$ :\n$$\\hat{H}=\\sum_{k=1}^N\\hbar\\omega_k\\left(\\hat{a}_k^\\dagger\\hat{a}^{\\phantom{\\dagger}}_k+\\frac{1}{2}\\right)$$\nOn appelle ces modes étiquetés par le vecteur d\u0026rsquo;onde $k$ des phonons. Et chacun de ces phonons peut porter des multiples entiers du quantum d\u0026rsquo;énergie $\\hbar\\omega_k$.\nLes paquets d\u0026rsquo;énergie que peut accepter le phonon ressemblent à des particules. Pourquoi alors ne pas considérer les phonons eux-mêmes comme des particules ?\nC\u0026rsquo;est le cœur de la seconde quantification : un problème ondulatoire peut s\u0026rsquo;exprimer comme une collection d\u0026rsquo;oscillateurs et produit donc des particules.\nReprésentation en nombre d’occupation On va se débarrasser des fonctions d\u0026rsquo;ondes, en passant de la représentation $(x,p)$ à un nouveau type de représentation.\nOn place une particule dans une boite de taille $L$ (on prend dans la suite $\\hbar=1$).\nL\u0026rsquo;opérateur impulsion $\\hat{p}$ pour un mouvement dans la direction $x$ est $\\hat{p}=-\\mathrm{i}\\frac{\\partial}{\\partial x}$. Les solutions de l\u0026rsquo;équation de Schrödinger pour la particule dans la boite sont les états propres de l\u0026rsquo;opérateur impulsion, les ondes planes $\\psi(x)=\\frac{1}{\\sqrt{L}}\\mathrm{e}^{\\mathrm{i}px}$. Utiliser des conditions aux limites périodiques ($\\psi(x+L)=\\psi(x)$) entraîne que $\\mathrm{e}^{\\mathrm{i}p(x+L)}={\\mathrm{e}}^{\\mathrm{i}px}$ et implique donc que $\\mathrm{e}^{\\mathrm{i}pL}=1$. Satisfaire la condition nécessite que $pL=2\\pi m$ avec $m$ entier. Cela impose donc une quantification des états d\u0026rsquo;impulsion que peut prendre la particule dans la boite :\n$$p_m = \\frac{2\\pi m}{L}$$\nSi on a plusieurs particules sans interaction dans la boite, l\u0026rsquo;énergie totale est donnée par $\\sum_m n_{p_m}E_{p_m}$ où $n_{p_m}$ est le nombre de particules dans l\u0026rsquo;état $|p_m\\rangle$.\nPlutôt que de noter un état à plusieurs particules identiques de cette façon $|p_1 p_2 p_1 p_3 p_5\\rangle$ (ici pour 5 particules), on va juste lister le nombre de particules dans chacun des états, ce qui donne pour l\u0026rsquo;exemple précédent $|21101\\rangle$. On dit qu\u0026rsquo;on passe alors à une représentation en nombre d\u0026rsquo;occupation qu\u0026rsquo;on a déjà rencontrée dans le chapitre précédent.\nAgir avec l\u0026rsquo;Hamiltonien sur un état dans la représentation en nombre d\u0026rsquo;occupation permet d\u0026rsquo;obtenir l\u0026rsquo;énergie vue un peu plus haut :\n$$\\hat{H}|n_1n_2n_3\\ldots\\rangle=\\left[\\sum_m n_{p_m}E_{p_m}\\right]|n_1n_2n_3\\ldots\\rangle$$ On retrouve une structure en niveaux d'énergie similaire à celle d'un système de $N$ oscillateurs harmoniques indépendants différents (en laissant tomber l'énergie de point zéro, on trouverait effectivement une énergie totale valant $E=\\sum_{k=1}^N n_k \\hbar\\omega_k$). Dans les deux cas, on somme pour chaque mode (chaque niveau d'énergie), le nombre de quanta qu'il contient. Dans le cas d\u0026rsquo;une collection d\u0026rsquo;oscillateurs harmoniques, on a vu dans le chapitre précédent qu\u0026rsquo;on peut se débarrasser de quasiment tous les vecteurs d\u0026rsquo;état (excepté le vide $|0\\rangle$) grâce à l\u0026rsquo;opérateur de création :\n$$|n_1 n_2\\ldots\\rangle = \\prod_k\\frac{1}{(n_k !)^{\\frac{1}{2}}}(\\hat{a}_k^\\dagger)^{n_k}|0\\rangle$$\nOn crée ainsi un état général de plusieurs oscillateurs harmoniques en agissant sur l\u0026rsquo;état du vide. Mais on veut aller plus loin qu\u0026rsquo;une une création de quanta dans des oscillateurs\u0026hellip; L\u0026rsquo;ambition est une création de particules dans des états d\u0026rsquo;impulsion donnés. On veut un opérateur de création $\\hat{a}^\\dagger_{p_m}$ pour faire naître une particule dans l\u0026rsquo;état d\u0026rsquo;impulsion $|p_m\\rangle$. Peut-on juste changer $k$ en $p_m$ pour passer de la création d\u0026rsquo;un quantum dans l\u0026rsquo;oscillateur $k$ à une particule dans l\u0026rsquo;état d\u0026rsquo;impulsion $|p_m\\rangle$ ? Presque\u0026hellip; Juste une petite question de symétrie à régler.\nConsidérons par exemple un système à deux états d\u0026rsquo;impulsion $p_1$ et $p_2$ décrits dans la représentation en nombre d\u0026rsquo;occupation $|n_1n_2\\rangle$. Chacun de ses états est formé en agissant sur l\u0026rsquo;état du vide $|0\\rangle$. Définissons $\\hat{a}_{p_1}^\\dagger|0\\rangle=|10\\rangle$ et $\\hat{a}_{p_2}^\\dagger|0\\rangle=|01\\rangle$ et ajoutons une nouvelle particule dans l\u0026rsquo;état inoccupé : $\\hat{a}_{p_2}^\\dagger\\hat{a}_{p_1}^\\dagger|0\\rangle\\propto|11\\rangle$, $\\hat{a}_{p_1}^\\dagger\\hat{a}_{p_2}^\\dagger|0\\rangle\\propto|11\\rangle$ où la constante de proportionnalité reste à déterminer.\nSuivant qu\u0026rsquo;on ajoute une particule dans l\u0026rsquo;état $p_1$ puis une autre dans l\u0026rsquo;état $p_2$ ou dans l\u0026rsquo;ordre inverse, on doit finir avec le même état $|11\\rangle$, ce qui implique $\\hat{a}_{p_1}^\\dagger\\hat{a}_{p_2}^\\dagger = \\lambda\\hat{a}_{p_2}^\\dagger\\hat{a}_{p_1}^\\dagger$ où $\\lambda$ est une constante.\nDeux possibilités évidentes pour $\\lambda$ : $\\lambda=\\pm 1$. Elles correspondent à des fonctions d\u0026rsquo;onde qui sont soit symétriques, soit antisymétriques lors de l\u0026rsquo;échange des deux particules. Les particules sont appelées bosons dans le cas symétrique et fermions dans le cas antisymétrique. Ces deux cas correspondent à deux relations de commutation possibles entre les opérateurs de création et d\u0026rsquo;annihilation.\nCas 1 : $\\lambda=1$, les bosons\nOn a dans ce cas $\\hat{a}_{p_1}^\\dagger\\hat{a}_{p_2}^\\dagger = \\hat{a}_{p_2}^\\dagger\\hat{a}_{p_1}^\\dagger$ et donc (en étiquetant les opérateurs de manière plus générale),\n$$\\left[\\hat{a}_i^\\dagger,\\hat{a}_j^\\dagger\\right]=0$$\nLes opérateurs de création pour différents états de particule commutent.\nOn a aussi $\\left[\\hat{a}_i,\\hat{a}_j\\right]=0$ et on définit $\\left[\\hat{a}_i,\\hat{a}_j^\\dagger\\right]=\\delta_{ij}$.\nOn obtient donc un formalisme identique à celui des oscillateurs harmoniques et on peut alors construire un état général à plusieurs particules sur le même modèle :\n$$|n_1n_2\\cdots\\rangle=\\prod_m\\frac{1}{(n_{p_m}!)^{\\frac{1}{2}}}(\\hat{a}^\\dagger_{p_m})^{n_{p_m}}|0\\rangle$$\nLes particules ainsi décrites sont des bosons.\nTrois propriétés saillantes :\nOn peut placer tout nombre de particules dans chaque état quantique (on peut les empiler dans un même état d\u0026rsquo;impulsion). Ces états sont symétriques lors de l\u0026rsquo;échange de deux particules. Enfin, l\u0026rsquo;ordre d\u0026rsquo;ajout des particules ne modifie pas l\u0026rsquo;état final obtenu : $\\hat{a}^\\dagger_{p_1}\\hat{a}^\\dagger_{p_2}|0\\rangle=\\hat{a}^\\dagger_{p_2}\\hat{a}^\\dagger_{p_1}|0\\rangle=|1_{p_1}1_{p_2}\\rangle$. Action des opérateurs sur un état général :\n$$ \\begin{aligned} \u0026amp;\\hat{a}_i^\\dagger |n_1\\cdots n_i\\cdots\\rangle = \\sqrt{n_i+1}|n_1\\cdots n_i+1\\cdots\\rangle\\\\ \u0026amp;\\hat{a}_i |n_1\\cdots n_i\\cdots\\rangle = \\sqrt{n_i}|n_1\\cdots n_i-1\\cdots\\rangle \\end{aligned} $$\nCas 2 : $\\lambda=-1$, les fermions\nOn va noter les opérateurs des fermions $\\hat{c}_i^\\dagger$ pour les différentier de ceux des bosons.\nOn obtient $\\left\\{\\hat{c}_i^\\dagger,\\hat{c}_j^\\dagger\\right\\} \\equiv \\hat{c}_i^\\dagger \\hat{c}_j^\\dagger + \\hat{c}_j^\\dagger\\hat{c}_i^\\dagger = 0$ où on a défini l\u0026rsquo;anticommutateur de deux opérateurs.\nLes opérateurs des fermions anticommutent : $\\hat{c}_i^\\dagger\\hat{c}_j^\\dagger+\\hat{c}_j^\\dagger\\hat{c}_i^\\dagger=0$.\nEn prenant $i=j$, on obtient $\\hat{c}_i^\\dagger\\hat{c}_i^\\dagger+\\hat{c}_i^\\dagger\\hat{c}_i^\\dagger=0\\Rightarrow \\hat{c}_i^\\dagger\\hat{c}_i^\\dagger=0$. Essayer de caser deux particules dans le même état d\u0026rsquo;impulsion aboutit à leur annihilation complète.\nC\u0026rsquo;est le principe de Pauli ; chaque état quantique ne peut être occupé que par un et un seul fermion !\nOn a aussi $\\left\\{\\hat{c}_i,\\hat{c}_j\\right\\}=0$ et on définit $\\left\\{\\hat{c}_i,\\hat{c}_j^\\dagger\\right\\}=\\delta_{ij}$ pour pouvoir appliquer ici aussi l\u0026rsquo;analogie avec les oscillateurs harmoniques. Mais gare maintenant à l\u0026rsquo;ordre des opérateurs qui n\u0026rsquo;est plus indifférent !\nAction des opérateurs sur un état général :\n$$ \\begin{aligned} \u0026amp;\\hat{c}_i^\\dagger |n_1\\cdots n_i\\cdots\\rangle = (-1)^{\\sum_i}\\sqrt{1-n_i}|n_1\\cdots n_i+1\\cdots\\rangle\\\\ \u0026amp;\\hat{c}_i |n_1\\cdots n_i\\cdots\\rangle = (-1)^{\\sum_i}\\sqrt{n_i}|n_1\\cdots n_i-1\\cdots\\rangle \\end{aligned} $$\nOù $(-1)^{\\sum_i}=(-1)^{n_1+n_2+\\cdots+n_{i-1}}$. Cela donne un facteur $(-1)$ pour chaque particule placée à gauche de l\u0026rsquo;état étiqueté par $n_i$ dans le vecteur d\u0026rsquo;état.\nPrenons un exemple pour s'aguerrir et vérifier la formule. Pour échanger de place de particules, on va leur faire suivre le processus suivant $|110\\rangle\\rightarrow|101\\rangle\\rightarrow|011\\rangle\\rightarrow|110\\rangle$.\nBouger une particule de place consiste à détruire la particule à un endroit et à la créer à un autre.\nDéplacer une particule de l\u0026rsquo;état 2 vers l\u0026rsquo;état 3 s\u0026rsquo;écrit donc $\\hat{a}^\\dagger_{3}\\hat{a}_2|110\\rangle$. L\u0026rsquo;enchaînement total proposé peut être décrit par l\u0026rsquo;enchaînement $\\hat{a}^\\dagger_{1}\\hat{a}_3\\hat{a}^\\dagger_{2}\\hat{a}_1\\hat{a}^\\dagger_{3}\\hat{a}_2|110\\rangle$. Et le résultat est sensé être $\\pm|110\\rangle$ ($+$ pour des bosons et $-$ pour des fermions).\nVérifions que les relations de commutation donnent les bons résultats. S\u0026rsquo;il s\u0026rsquo;agit de bosons, on peut échanger de place deux opérateurs agissant sur des états différents ($[\\hat{a}_i,\\hat{a}_j]=0$).\n$\\hat{a}^\\dagger_{1}\\hat{a}_3\\hat{a}^\\dagger_{2}\\hat{a}_1\\hat{a}^\\dagger_{3}\\hat{a}_2|110\\rangle=\\hat{a}_3\\hat{a}^\\dagger_{3}\\hat{a}^\\dagger_{1}\\hat{a}_1\\hat{a}^\\dagger_{2}\\hat{a}_2|110\\rangle$\nComme $\\hat{a}^\\dagger_i\\hat{a}_i=\\hat{n}_i$, l\u0026rsquo;opérateur nombre qui compte le nombre de particules dans l\u0026rsquo;état $i$, on obtient :\n$\\hat{a}_3\\hat{a}^\\dagger_{3}\\hat{a}^\\dagger_{1}\\hat{a}_1\\hat{a}^\\dagger_{2}\\hat{a}_2|110\\rangle=\\hat{a}_3\\hat{a}^\\dagger_{3}\\hat{n}_1\\hat{n}_2|110\\rangle =(1)\\times(1)\\times \\hat{a}_3\\hat{a}^\\dagger_{3}|110\\rangle$\nEn utilisant $[\\hat{a}_3,\\hat{a}^\\dagger_3]=1$, on obtient :\n$\\hat{a}_3\\hat{a}^\\dagger_{3}|110\\rangle=(1+\\hat{n}_3)|110\\rangle = |110\\rangle + 0 =|110\\rangle$\nPassons aux fermions. L\u0026rsquo;échange entre deux particules s\u0026rsquo;accompagne maintenant d\u0026rsquo;un changement de signe.\n$\\hat{c}^\\dagger_{1}\\hat{c}_3\\hat{c}^\\dagger_{2}\\hat{c}_1\\hat{c}^\\dagger_{3}\\hat{c}_2|110\\rangle = -\\hat{c}_3\\hat{c}^\\dagger_{3}\\hat{c}^\\dagger_{1}\\hat{c}_1\\hat{c}^\\dagger_{2}\\hat{c}_2|110\\rangle$ car on compte un nombre impair d\u0026rsquo;échanges. Et $-\\hat{c}_3\\hat{c}^\\dagger_{3}\\hat{c}^\\dagger_{1}\\hat{c}_1\\hat{c}^\\dagger_{2}\\hat{c}_2|110\\rangle = -(1-\\hat{n}_3)|110\\rangle = -|110\\rangle$. Youpi !\nOn était jusque-là confiné dans une boite, mais que se passe-t-il si on écarte ses murs infiniment ? On passe alors à la limite du continu. Le symbole de Kronecker $\\delta_{ij}$ des commutateurs devient une fonction delta de Dirac $\\delta^{(3)}(\\boldsymbol{p})$ et les sommes discrètes deviennent des intégrales.\nLe commutateur se transforme donc en :\n$$\\left[\\hat{a}_\\boldsymbol{p},\\hat{a}^\\dagger_\\boldsymbol{q}\\right]=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q})$$\net l\u0026rsquo;Hamiltonien :\n$$\\hat{H}=\\int\\mathrm{d}^3 p\\, E_\\boldsymbol{p}\\hat{a}^\\dagger_\\boldsymbol{p}\\hat{a}_\\boldsymbol{p}$$\nPour montrer que cette nouvelle formulation est correcte, on va retrouver à partir d\u0026rsquo;elle les bonnes fonctions d\u0026rsquo;onde dans l\u0026rsquo;espace des positions.\nPour des états à une seule particule, on a :\n$$\\langle \\boldsymbol{p}|\\boldsymbol{p\u0026rsquo;}\\rangle=\\langle 0|\\hat{a}_\\boldsymbol{p}\\hat{a}^\\dagger_\\boldsymbol{p\u0026rsquo;}|0\\rangle$$\nEt avec les relations de commutation, on obtient :\n$$\\langle \\boldsymbol{p}|\\boldsymbol{p\u0026rsquo;}\\rangle=\\langle 0|\\left[\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{p\u0026rsquo;})\\pm \\hat{a}^\\dagger_\\boldsymbol{p\u0026rsquo;}\\hat{a}_\\boldsymbol{p} \\right]|0\\rangle=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{p\u0026rsquo;})$$\nVérifions que c\u0026rsquo;est la relation attendue en passant en représentation position.\nPar un changement de base, $|\\boldsymbol{x}\\rangle = \\int\\mathrm{d}q\\,\\phi^*_\\boldsymbol{q}(\\boldsymbol{x})|\\boldsymbol{q}\\rangle$\nchangement de base L\u0026rsquo;état $|\\boldsymbol{x}\\rangle$ peut s\u0026rsquo;écrire dans une nouvelle base en utilisant la relation de fermeture $1=\\int\\mathrm{d}^3q\\,|\\boldsymbol{q}\\rangle\\langle\\boldsymbol{q}|$ de telle sorte que $|\\boldsymbol{x}\\rangle = \\int\\mathrm{d}^3q\\,|\\boldsymbol{q}\\rangle\\langle\\boldsymbol{q}|\\boldsymbol{x}\\rangle$. Et comme $\\langle\\boldsymbol{x}|\\boldsymbol{q}\\rangle=\\phi_\\boldsymbol{q}(\\boldsymbol{x})$ et $\\langle\\boldsymbol{q}|\\boldsymbol{x}\\rangle=\\phi^*_\\boldsymbol{q}(\\boldsymbol{x})$, on obtient bien $|\\boldsymbol{x}\\rangle = \\int\\mathrm{d}q\\,\\phi^*_\\boldsymbol{q}(\\boldsymbol{x})|\\boldsymbol{q}\\rangle$.\nCela donne $\\langle \\boldsymbol{x}|\\boldsymbol{p}\\rangle = \\int\\mathrm{d}^3q\\,\\phi_\\boldsymbol{q}(\\boldsymbol{x})\\langle\\boldsymbol{q}|\\boldsymbol{p}\\rangle=\\phi_\\boldsymbol{p}(\\boldsymbol{x})$.\nC\u0026rsquo;est bien ce qui était attendu, mais rien de bien folichon.\nPassons maintenant au cas d\u0026rsquo;un état à deux particules : $\\langle \\boldsymbol{p\u0026rsquo;}\\boldsymbol{q\u0026rsquo;}|\\boldsymbol{q}\\boldsymbol{p}\\rangle=\\langle 0|\\hat{a}_\\boldsymbol{p\u0026rsquo;}\\hat{a}_\\boldsymbol{q\u0026rsquo;}\\hat{a}^\\dagger_\\boldsymbol{q}\\hat{a}^\\dagger_\\boldsymbol{p}|0\\rangle$.\nPar utilisation des relations de commutation, on arrive à $\\langle \\boldsymbol{p\u0026rsquo;}\\boldsymbol{q\u0026rsquo;}|\\boldsymbol{q}\\boldsymbol{p}\\rangle=\\delta^{(3)}(\\boldsymbol{p\u0026rsquo;}-\\boldsymbol{p})\\delta^{(3)}(\\boldsymbol{q\u0026rsquo;}-\\boldsymbol{q})\\pm\\delta^{(3)}(\\boldsymbol{p\u0026rsquo;}-\\boldsymbol{q})\\delta^{(3)}(\\boldsymbol{q\u0026rsquo;}-\\boldsymbol{p})$ avec le signe plus pour les bosons et moins pour les fermions.\nDéterminons à nouveau la fonction d\u0026rsquo;onde dans l\u0026rsquo;espace des positions grâce au changement de base :\n$$|\\boldsymbol{x}\\boldsymbol{y}\\rangle = \\frac{1}{\\sqrt{2!}}\\int \\mathrm{d}^3p\u0026rsquo;\\mathrm{d}^3 q\u0026rsquo; \\phi^*_\\boldsymbol{p\u0026rsquo;}(\\boldsymbol{x})\\phi^*_\\boldsymbol{q\u0026rsquo;}(\\boldsymbol{y})|\\boldsymbol{p\u0026rsquo;}\\boldsymbol{q\u0026rsquo;}\\rangle$$\noù le facteur $\\frac{1}{\\sqrt{2!}}$ compense le double comptage $q\u0026rsquo;p\u0026rsquo;$/$p\u0026rsquo;q\u0026rsquo;$ dû au fait que la somme n\u0026rsquo;est pas restreinte. Cela donne :\n$$ \\frac{1}{\\sqrt{2!}}\\int \\mathrm{d}^3p\u0026rsquo;\\mathrm{d}^3 q\u0026rsquo; \\phi_\\boldsymbol{p\u0026rsquo;}(\\boldsymbol{x})\\phi_\\boldsymbol{q\u0026rsquo;}(\\boldsymbol{y}) \\langle \\boldsymbol{p\u0026rsquo;}\\boldsymbol{q\u0026rsquo;}|\\boldsymbol{p}\\boldsymbol{q}\\rangle = \\frac{1}{\\sqrt{2!}}[\\phi_\\boldsymbol{p}(\\boldsymbol{x})\\phi_\\boldsymbol{q}(\\boldsymbol{y})\\pm \\phi_\\boldsymbol{q}(\\boldsymbol{x})\\phi_\\boldsymbol{p}(\\boldsymbol{y})]$$\nOn retrouve l\u0026rsquo;expression d\u0026rsquo;un état à deux particules !\nCette nouvelle formulation de la mécanique quantique semble donc valide et dans la suite, elle va porter ses fruits.\nSeconde quantification On considère à nouveau ici des particules non relativistes dans une boîte. Cela simplifie pas mal mais cela permet déjà d\u0026rsquo;étudier le comportement d\u0026rsquo;électrons dans un solide.\nParticule dans une boîte de volume $\\mathcal{V}$ Un état $|\\alpha\\rangle$ se décrit en représentation position $\\psi_\\alpha(\\boldsymbol{x})=\\langle\\boldsymbol{x}|\\alpha\\rangle$ et en représentation impulsion $\\tilde{\\psi}_\\alpha(\\boldsymbol{p})=\\langle\\boldsymbol{p}|\\alpha\\rangle$ et comme $\\langle\\boldsymbol{p}|\\alpha\\rangle:\\int\\mathrm{d}^3\\langle\\boldsymbol{p}|\\boldsymbol{x}\\rangle\\langle\\boldsymbol{x}|\\alpha\\rangle$, on déduit $\\langle\\boldsymbol{p}|\\boldsymbol{x}\\rangle=\\frac{1}{\\sqrt{\\mathcal{V}}}\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}$ (par identification avec la transformée de Fourier $\\tilde{\\psi}_\\alpha(\\boldsymbol{p})=\\frac{1}{\\sqrt{\\mathcal{V}}}\\int\\mathrm{d}^3 x\\,\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}\\psi_\\alpha(\\boldsymbol{x})$).\nLa transformée de Fourier inverse est discrète puisque les valeurs de $\\boldsymbol{p}$ le sont : $\\psi_\\alpha(\\boldsymbol{x})=\\frac{1}{\\sqrt{\\mathcal{V}}}\\sum_\\boldsymbol{p}\\mathrm{e}^{\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}\\tilde{\\psi}_\\alpha(\\boldsymbol{p})$.\nEt on a aussi $\\int\\mathrm{d}^3 x\\,\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}} = \\mathcal{V}\\delta_{\\boldsymbol{p},0}$ et $\\frac{1}{\\mathcal{V}}\\sum_\\boldsymbol{p}\\mathrm{e}^{\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}=\\delta^{(3)}(\\boldsymbol{x})$.\nOn sait d\u0026rsquo;ores et déjà créer une particule d\u0026rsquo;impulsion $\\boldsymbol{p}$ à partir de l\u0026rsquo;état du vide en appliquant l\u0026rsquo;opérateur de création : $\\hat{a}^\\dagger_\\boldsymbol{p}|0\\rangle$. On obtient ainsi une particule complètement localisée dans l\u0026rsquo;espace des impulsions et donc s\u0026rsquo;étendant sur tout l\u0026rsquo;espace des coordonnées. Supposons maintenant que l\u0026rsquo;on veuille créer une particule localisée dans l\u0026rsquo;espace des coordonnées. Dans ce but, on construit des nouveaux opérateurs de création et d\u0026rsquo;annihilation appelés opérateur de champ par transformée de Fourier discrète des opérateurs $\\hat{a}^\\dagger_\\boldsymbol{p}$ et $\\hat{a}_\\boldsymbol{p}$.\n$$\\hat{\\psi}^\\dagger(\\boldsymbol{x}) = \\frac{1}{\\sqrt{\\mathcal{V}}}\\sum_\\boldsymbol{p}\\hat{a}^\\dagger_\\boldsymbol{p}\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}$$\n$$\\hat{\\psi}(\\boldsymbol{x}) = \\frac{1}{\\sqrt{\\mathcal{V}}}\\sum_\\boldsymbol{p}\\hat{a}_\\boldsymbol{p}\\mathrm{e}^{\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}$$\nCes opérateurs de champ peuvent créer et annihiler une particule en une position $\\boldsymbol{x}$.\nPour s\u0026rsquo;en convaincre, explorons l\u0026rsquo;état $|\\Psi\\rangle = \\hat{\\psi}^\\dagger(\\boldsymbol{x})|0\\rangle$. On a d\u0026rsquo;une part $\\sum_\\boldsymbol{q}\\hat{n}_\\boldsymbol{q}|\\Psi\\rangle = |\\Psi\\rangle$, ce qui signifie que $|\\Psi\\rangle$ est un état propre de l\u0026rsquo;opérateur nombre d\u0026rsquo;occupation pour la valeur propre 1 et donc qu\u0026rsquo;une seule particule a été créée. D\u0026rsquo;autre part $\\langle\\boldsymbol{y}|\\Psi\\rangle=\\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y})$, ce qui prouve que la particule créée est bien localisée en $\\boldsymbol{x}$.\n$$ \\begin{aligned} \\langle\\boldsymbol{y}|\\Psi\\rangle = \\langle\\boldsymbol{y}|\\psi^\\dagger(\\boldsymbol{x})|0\\rangle \u0026amp;= \\frac{1}{\\sqrt{\\mathcal{V}}}\\sum_\\boldsymbol{p}\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}\\langle\\boldsymbol{y}|\\boldsymbol{p}\\rangle\\\\ \u0026amp;=\\frac{1}{\\sqrt{\\mathcal{V}}}\\sum_\\boldsymbol{p}\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot(\\boldsymbol{x}-\\boldsymbol{y})}\\\\ \u0026amp;=\\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) \\end{aligned} $$\nLes opérateurs de champ satisfont les relations de commutation suivantes :\npour des bosons : $\\left[\\hat{\\psi}(\\boldsymbol{x}),\\hat{\\psi}^\\dagger(\\boldsymbol{y})\\right]=\\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y})$, $\\left[\\hat{\\psi}^\\dagger(\\boldsymbol{x}),\\hat{\\psi}^\\dagger(\\boldsymbol{y})\\right]=0$ et $\\left[\\hat{\\psi}(\\boldsymbol{x}),\\hat{\\psi}(\\boldsymbol{y})\\right]=0$\net pour des fermions : $\\left\\{\\hat{\\psi}(\\boldsymbol{x}),\\hat{\\psi}^\\dagger(\\boldsymbol{y})\\right\\}=\\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y})$, $\\left\\{\\hat{\\psi}^\\dagger(\\boldsymbol{x}),\\hat{\\psi}^\\dagger(\\boldsymbol{y})\\right\\}=0$ et $\\left\\{\\hat{\\psi}(\\boldsymbol{x}),\\hat{\\psi}(\\boldsymbol{y})\\right\\}=0$\nC\u0026rsquo;est le moment : on va maintenant apprendre à upgrader les opérateurs issus de la première quantification en opérateurs de seconde quantification !\nRappelons-nous d\u0026rsquo;abord qu\u0026rsquo;un opérateur $\\hat{\\mathcal{A}}$ associe à l\u0026rsquo;état quantique $|\\psi\\rangle$, élément de l\u0026rsquo;espace de Hilbert, un nouvel état quantique $\\hat{\\mathcal{A}}|\\psi\\rangle$.\nOn va supposer ici que $\\hat{\\mathcal{A}}$ n\u0026rsquo;agit que sur une seule particule à la fois (contrairement à un opérateur potentiel d\u0026rsquo;interaction par exemple). En utilisant doublement la relation de fermeture et en partant de l\u0026rsquo;égalité triviale $\\hat{\\mathcal{A}}=\\hat{\\mathcal{A}}$, on peut obtenir la décomposition de $\\hat{\\mathcal{A}}$ sur une base donnée de l\u0026rsquo;espace de Hilbert :\n$$\\hat{\\mathcal{A}}=\\sum_{\\alpha,\\beta}|\\alpha\\rangle\\langle\\alpha|\\hat{\\mathcal{A}}|\\beta\\rangle\\langle\\beta|=\\sum_{\\alpha,\\beta}\\mathcal{A}_{\\alpha\\beta}|\\alpha\\rangle\\langle\\beta|$$\noù les $\\mathcal{A}_{\\alpha\\beta}=\\langle\\alpha|\\hat{\\mathcal{A}}|\\beta\\rangle$ sont les éléments de matrice de la décomposition de l\u0026rsquo;opérateur, ils donnent l\u0026rsquo;amplitude de la transition d\u0026rsquo;un état $|\\beta\\rangle$ vers un état $|\\alpha\\rangle$.\nL\u0026rsquo;idée va être de passer de cette transition entre états à un transfert de particules (mort de l\u0026rsquo;une suivie de la naissance d\u0026rsquo;une autre).\nL\u0026rsquo;espace de Hilbert permet de décrire l\u0026rsquo;état d\u0026rsquo;une particule. L\u0026rsquo;espace de Fock rend possible, lui, la description d\u0026rsquo;états à $N$ particules (l\u0026rsquo;espace de Fock $\\mathcal{F}$ inclut tous les états à $N$-particules pour toutes les valeurs de $N$ : $\\mathcal{F}=\\bigoplus_{N=0}^\\infty \\mathcal{F}_N=\\mathcal{F}_0\\oplus\\mathcal{F}_1\\oplus\\cdots$ où $\\mathcal{F}_0=\\{|0\\rangle\\}$ est un ensemble ne contenant que l\u0026rsquo;état du vide et $\\mathcal{F}_1$ est l\u0026rsquo;espace de Hilbert. Les sous-espaces $\\mathcal{F}_{N≥2}$ doivent être symétrisés pour des bosons et antisymétrisés pour des fermions (donc $\\mathcal{F}_2$ n\u0026rsquo;est pas seulement $\\mathcal{F}_1\\otimes\\mathcal{F}_1$ mais seulement sa partie symétrique ou antisymétrique). Les opérateurs de création permettent de passer d\u0026rsquo;un élément de $\\mathcal{F}_N$ à un élément de $\\mathcal{F}_{N+1}$ et les opérateurs d\u0026rsquo;annihilation font l\u0026rsquo;inverse.\nLa version seconde quantification $\\hat{A}$ de l\u0026rsquo;opérateur $\\hat{\\mathcal{A}}$ qui correspond à l\u0026rsquo;upgrade d\u0026rsquo;un opérateur à une particule agissant sur l\u0026rsquo;espace de Hilbert vers un opérateur multi-particules agissant sur l\u0026rsquo;espace de Fock est simplement donnée par :\n$$\\hat{A} = \\sum_{\\alpha\\beta}\\mathcal{A}_{\\alpha\\beta}\\hat{a}^\\dagger_\\alpha\\hat{a}_\\beta$$\nLes éléments de matrice $\\mathcal{A}_{\\alpha\\beta}$ sont les mêmes qu\u0026rsquo;au-dessus et ils continuent à représenter l\u0026rsquo;ensemble des transitions possibles d\u0026rsquo;une particule seule. Mais $\\hat{A}$ opère bien maintenant sur un état à plusieurs particules. On commence par utiliser $\\hat{a}_\\beta$ pour retirer une particule dans l\u0026rsquo;état $|\\beta\\rangle$, on multiplie par l\u0026rsquo;élément de matrice $\\mathcal{A}_{\\alpha\\beta}$ donnant l\u0026rsquo;amplitude de la transition vers le nouvel état, et enfin on utilise $\\hat{a}^\\dagger_\\alpha$ pour placer la particules dans l\u0026rsquo;état final $|\\alpha\\rangle$.\nDans les exemples suivants, on va construire pas à pas un Hamiltonien simple d\u0026rsquo;une particule unique soumise à un potentiel en version seconde quantification. L\u0026rsquo;idée est de se familiariser avec le formalisme et constater qu\u0026rsquo;il nous redonne bien les résultats qu\u0026rsquo;on attend.\nLa relation de fermeture $\\hat{1}=\\sum_\\alpha|\\alpha\\rangle\\langle\\alpha|$ est l\u0026rsquo;exemple le plus simple de $\\hat{\\mathcal{A}} = \\sum_{\\alpha\\beta}\\mathcal{A}_{\\alpha\\beta}\\hat{a}^\\dagger_\\alpha\\hat{a}_\\beta$ avec $\\mathcal{A}_{\\alpha\\beta}=\\delta_{\\alpha\\beta}$.\nSa promotion à l\u0026rsquo;étage de la seconde quantification est simplement $\\hat{n}=\\sum_\\alpha\\hat{a}^\\dagger_\\alpha\\hat{a}_\\alpha$, l\u0026rsquo;opérateur nombre qui compte un pour chaque particule dans l\u0026rsquo;état sur lequel il agit.\nL\u0026rsquo;opérateur impulsion usuel peut s\u0026rsquo;écrire $\\hat{\\mathcal{A}}=\\hat{\\boldsymbol{p}}=\\sum_\\boldsymbol{p}\\boldsymbol{p}|\\boldsymbol{p}\\rangle\\langle\\boldsymbol{p}|$ qui est promu automatiquement en $\\hat{\\boldsymbol{p}}=\\sum_\\boldsymbol{p}\\boldsymbol{p}\\,\\hat{a}^\\dagger_\\boldsymbol{p}\\hat{a}_\\boldsymbol{p}=\\sum_\\boldsymbol{p}\\boldsymbol{p}\\,\\hat{n}_\\boldsymbol{p}$.\nDe même, une fonction de l\u0026rsquo;opérateur impulsion $\\hat{\\mathcal{A}}=f(\\boldsymbol{p})$ devient $\\hat{A}=\\sum_\\boldsymbol{p}f(\\boldsymbol{p})\\hat{a}^\\dagger_\\boldsymbol{p}\\hat{a}_\\boldsymbol{p}=\\sum_\\boldsymbol{p}f(\\boldsymbol{p})\\hat{n}_\\boldsymbol{p}$.\nUn exemple particulier d\u0026rsquo;une telle fonction : l\u0026rsquo;Hamiltonien d\u0026rsquo;une particule libre $\\frac{\\boldsymbol{p}^2}{2m}$.\nL\u0026rsquo;Hamiltonien d\u0026rsquo;une particule libre devient ainsi en seconde quantification :\n$\\displaystyle\\hat{H}=\\sum_\\boldsymbol{p}\\frac{\\boldsymbol{p}^2}{2m}\\hat{n} _\\boldsymbol{p}$\n$\\hat{H}$ est diagonal dans la base des états nombre d\u0026rsquo;occupation puisqu\u0026rsquo;ils sont des états propres de $\\hat{n}_\\boldsymbol{p}$. Et de fait, pour diagonaliser tout hamiltonien (ce qui revient à trouver les énergies des états propres), on n\u0026rsquo;a qu\u0026rsquo;à l\u0026rsquo;exprimer en termes de nombres d\u0026rsquo;opérateurs. Donc ici, on dit finalement que l\u0026rsquo;énergie totale du système est donnée par la somme des énergies $\\frac{\\boldsymbol{p}^2}{2m}$ de toutes les particules. Ça paraît sensé.\nEt si l\u0026rsquo;opérateur est une fonction de $\\hat{\\boldsymbol{x}}$ plutôt que $\\hat{\\boldsymbol{p}}$ ?\nLa formule de promotion reste valable si les états $|\\alpha\\rangle$ et $|\\beta\\rangle$ sont des états de position. Il suffit de remplacer les opérateurs de création et d\u0026rsquo;annihilation par les opérateurs de champ, et la somme sur les impulsions par une intégrale sur l\u0026rsquo;espace.\nOn peut ainsi écrire l\u0026rsquo;opérateur $\\hat{V}$ en version seconde-quantification :\n$\\displaystyle \\hat{V}=\\int\\mathrm{d}^3 x\\,\\hat{\\phi}^\\dagger(\\boldsymbol{x})V(\\boldsymbol{x})\\hat{\\phi}(\\boldsymbol{x})$\nEt si on préfère repasser dans l\u0026rsquo;espace des impulsions :\n$\\displaystyle \\hat{V}=\\frac{1}{\\mathcal{V}}\\int\\mathrm{d}^3 x\\,\\sum_{\\boldsymbol{p}_1,\\boldsymbol{p}_2} \\hat{a}^\\dagger_{\\boldsymbol{p}_1}\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}_1\\cdot\\boldsymbol{x} }V(\\hat{\\boldsymbol{x}})\\hat{a}_{\\boldsymbol{p}_2}\\mathrm{e}^{\\mathrm{i}\\boldsymbol{p}_2\\cdot\\boldsymbol{x} }$\nEn introduisant la transformée de Fourier de $V(\\boldsymbol{x})$, $\\tilde{V}_{\\boldsymbol{p}}=\\frac{1}{\\mathcal{V}}\\int\\mathrm{d}^3 x\\,V(\\boldsymbol{x})\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x} }$, la formule s\u0026rsquo;éclaircit :\n$\\displaystyle \\hat{V}=\\sum_{\\boldsymbol{p}_1,\\boldsymbol{p}_2}\\tilde{V}_{\\boldsymbol{p}_1-\\boldsymbol{p}_2}\\hat{a}^\\dagger_{\\boldsymbol{p}_1}\\hat{a}_{\\boldsymbol{p}_2}$\nSchématisation du processus : une particule arrive avec l\u0026rsquo;impulsion $\\boldsymbol{p}_2$, interagit avec un champ de potentiel (représenté par $\\tilde{V}_{\\boldsymbol{p}_1-\\boldsymbol{p}_2}$) puis repart avec l\u0026rsquo;impulsion $\\boldsymbol{p}_1$.\nRq : l\u0026rsquo;opérateur $\\hat{V}$ n\u0026rsquo;est pas diagonal puisque les opérateurs $\\hat{a}$ créent et annihilent des particules avec des impulsions différentes.\nÉtudions l\u0026rsquo;influence du potentiel sur les états et valeurs propres de l\u0026rsquo;Hamiltonien suivant :\n$\\displaystyle\\hat{H} = E_0 \\sum_{\\boldsymbol{p}} \\hat{d}_{\\boldsymbol{p}}^\\dagger \\hat{d}_{\\boldsymbol{p}} - \\frac{V}{2} \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2} \\hat{d}_{\\boldsymbol{p}_1}^\\dagger \\hat{d}_{\\boldsymbol{p}_2}$\nOn limite le système à 3 niveaux d\u0026rsquo;énergie, ce qui permet d\u0026rsquo;exprimer les états dans une base $|n_{\\boldsymbol{p}_1}n_{\\boldsymbol{p}_2}n_{\\boldsymbol{p}_3}\\rangle$.\nCommençons par débrancher le potentiel ($V=0$). Une particule placée dans ce système ne peut être que dans un des trois états suivants : $|100\\rangle$, $|010\\rangle$ ou $|001\\rangle$.\nRebranchons le potentiel et voyons comment il agit sur chacun des trois états :\n$\\displaystyle\\frac{V}{2} \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2} \\hat{d}_{\\boldsymbol{p}_1}^\\dagger \\hat{d}_{\\boldsymbol{p}_2} |100\\rangle = \\frac{V}{2} \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2} \\hat{d}_{\\boldsymbol{p}_1}^\\dagger \\hat{d}_{\\boldsymbol{p}_2} |010\\rangle = \\frac{V}{2} \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2} \\hat{d}_{\\boldsymbol{p}_1}^\\dagger \\hat{d}_{\\boldsymbol{p}_2} |001\\rangle = \\frac{V}{2} \\left( |100\\rangle + |010\\rangle + |001\\rangle \\right)$\nUne forme matricielle permet de synthétiser l\u0026rsquo;information :\n$\\displaystyle H = \\left[E_0 \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} - \\frac{V}{2} \\begin{pmatrix} 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \\end{pmatrix}\\right]$\nLes valeurs propres de cette équation sont $\\varepsilon=E_0,E_0,E_0-\\frac{3V}{2}$. L\u0026rsquo;état fondamental du système $|\\Omega\\rangle$ a donc l\u0026rsquo;énergie $E_0-\\frac{3V}{2}$ et l\u0026rsquo;état propre correspondant s\u0026rsquo;écrit $|\\Omega\\rangle = \\frac{1}{\\sqrt{3}} \\left( |100\\rangle + |010\\rangle + |001\\rangle \\right)$.\nL\u0026rsquo;état fondamental est donc une superposition 1:1:1 des trois états d\u0026rsquo;impulsion desquels on est parti.\nQuel est l\u0026rsquo;équivalent de la densité de probabilité d\u0026rsquo;une fonction d\u0026rsquo;onde dans le langage de la seconde quantification ? La densité de particules en un point $\\boldsymbol{x}$ !\nL\u0026rsquo;opérateur densité $\\hat{\\rho}(\\boldsymbol{x})$ est donné par : $$ \\begin{aligned} \\hat{\\rho}(\\boldsymbol{x}) \u0026amp;= \\hat{\\psi}^\\dagger(\\boldsymbol{x})\\hat{\\psi}(\\boldsymbol{x})\\\\ \u0026amp;=\\frac{1}{\\mathcal{V}} \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2} \\mathrm{e}^{-\\mathrm{i} (\\boldsymbol{p}_1 - \\boldsymbol{p}_2) \\cdot \\boldsymbol{x}} \\hat{a}^\\dagger_{\\boldsymbol{p}_1} \\hat{a}_{\\boldsymbol{p}_2} \\end{aligned} $$\nOn vérifie que le terme de densité de particule est adapté puisqu\u0026rsquo;en intégrant sur tout l\u0026rsquo;espace, on retrouve le nombre de particules : $\\int \\mathrm{d}^3x\\hat{\\rho}(\\boldsymbol{x})= \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2} \\delta_{\\boldsymbol{p}_2,\\boldsymbol{p}_1} \\hat{a}^\\dagger_{\\boldsymbol{p}_1} \\hat{a}_{\\boldsymbol{p}_2} = \\sum_\\boldsymbol{p} \\hat{n}_\\boldsymbol{p}$\nCet opérateur permet de réécrire l\u0026rsquo;opérateur énergie potentielle d\u0026rsquo;une particule unique soumise à un potentiel extérieur comme :\n$\\displaystyle \\hat{V} = \\int \\mathrm{d}^3 x \\, V(\\boldsymbol{x}) \\hat{\\rho}(\\boldsymbol{x})$.\nLe cadre non-relativiste dans lequel on se place jusqu\u0026rsquo;ici est limité mais il permet déjà de jouer avec des modèles de matière condensée. Finissons donc le chapitre en appliquant notre nouveau formalisme à des électrons se déplaçant sur un réseau d\u0026rsquo;atomes.\nModèle des liaisons fortes pour l\u0026rsquo;énergie cinétique Travaillons dans une base où $\\hat{c}^\\dagger_i$ crée un électron sur un site du réseau étiqueté $i$. Comme l\u0026rsquo;énergie cinétique d\u0026rsquo;une particule est d\u0026rsquo;autant plus grande que celle-ci est spatialement confinée (dans le cas d\u0026rsquo;un puits infini de largeur $L$, l\u0026rsquo;énergie cinétique vaut $E_n = \\frac{1}{2m}\\left( \\frac{n\\pi}{L}\\right)^2$), on va considérer qu\u0026rsquo;un saut d\u0026rsquo;un site $j$ vers un site $i$ permet d\u0026rsquo;économiser l\u0026rsquo;énergie cinétique $t_{ij}$ (ce terme dépend d\u0026rsquo;une façon ou d\u0026rsquo;une autre du recouvrement entre les orbitales atomiques du réseau). L\u0026rsquo;Hamiltonien somme tous les sauts possibles :\n$$\\hat{H} = \\sum_{ij}(-t_{ij})\\hat{c}^\\dagger_i\\hat{c}_j$$\nChaque terme de la somme correspond à un processus où une particule est annihilée au site $j$ puis recréée au site $i$, modélisant un saut et sauvant ainsi l\u0026rsquo;énergie $t_{ij}$.\nOn va d\u0026rsquo;abord considérer le cas le plus simple où $t_{ij}=t$ pour des voisins immédiats et $t_{ij}=0$ sinon. L\u0026rsquo;Hamiltonien devient :\n$$\\hat{H} = -t\\sum_{i\\tau}\\hat{c}^\\dagger_i\\hat{c}_{i+\\tau}$$\noù la somme sur $\\tau$ se fait sur les plus proches voisins.\nLa combinaison bilinéaire $\\hat{c}^\\dagger_i\\hat{c}_j$ rend l\u0026rsquo;Hamiltonien non diagonal. Pour le diagonaliser, on passe à nouveau en impulsion via les transformées de Fourier :\n$$ \\hat{H} = \\sum_{\\boldsymbol{k}} E_{\\boldsymbol{k}}\\hat{c}^\\dagger_{\\boldsymbol{k}} \\hat{c}_{\\boldsymbol{k}} $$\noù la relation de dispersion est donnée par $E_{\\boldsymbol{k}}=- \\sum_{\\tau} t \\mathrm{e}^{\\mathrm{i} \\boldsymbol{k} \\cdot \\boldsymbol{r}_{\\tau}}$.\nPour un réseau carré où $\\tau$ parcourt les vecteurs $(a,0)$, $(-a,0)$, $(0,a)$ et $(0,-a)$, on obtient $E_{\\boldsymbol{k}} = -2t \\left( \\cos(k_x a) + \\cos(k_y a) \\right)$\nOn a tracé ci-dessus les contours du profil énergétique dans l\u0026rsquo;espace réciproque (l\u0026rsquo;énergie est constante sur une ligne). Pour $t\u0026gt;0$, l\u0026rsquo;énergie est minimale au centre $(k_x,k_y)=(0,0)$, ce qui correspond à un électron délocalisé, occupant tout le réseau et donc sans mouvement.\nPotentiel à deux particules Ajoutons un deuxième électron capable d\u0026rsquo;interagir avec le premier.\nPour la promotion seconde quantification de $\\mathcal{A}_{\\alpha\\beta\\gamma\\delta}=\\langle\\alpha,\\beta|\\hat{A}|\\gamma,\\delta\\rangle$ on peut tenter intuitivement :\n$$Â = \\sum_{\\alpha\\beta\\gamma\\delta} A_{\\alpha\\beta\\gamma\\delta} \\hat{a}^\\dagger_\\alpha \\hat{a}^\\dagger_\\beta \\hat{a}_\\gamma \\hat{a}_\\delta$$\nL\u0026rsquo;opérateurs à deux particules typique est le potentiel $\\hat{V}$. Et il sera le plus souvent fonction des coordonnées spatiales. Son expression impliquant les opérateurs de champ est alors :\n$$\\hat{V} = \\frac{1}{2} \\int \\mathrm{d}^3 x\\, \\mathrm{d}^3 y \\, \\hat{\\psi}^\\dagger(\\boldsymbol{x}) \\hat{\\psi}^\\dagger(\\boldsymbol{y}) V(\\boldsymbol{x}, \\boldsymbol{y}) \\hat{\\psi}(\\boldsymbol{y}) \\hat{\\psi}(\\boldsymbol{x})$$\nLe $\\frac{1}{2}$ compense le double comptage des interactions. L\u0026rsquo;ordre des opérateurs n\u0026rsquo;est pas anodin ! Celui utilisé est appelé ordre normal et sa vertu principale est d\u0026rsquo;assurer que l\u0026rsquo;opérateur $\\hat{V}$ ait une valeur moyenne prédite (espérance quantique) nulle pour l\u0026rsquo;état de vide ($\\langle 0|\\hat{V}|0\\rangle=0$).\nOn restreint le potentiel à la forme $V(\\boldsymbol{x}-\\boldsymbol{y})$ ne dépendant que de la séparation relative des particules (cela va garantir la conservation de l\u0026rsquo;impulsion dans l\u0026rsquo;interaction) et on décompose en modes d\u0026rsquo;impulsion les opérateurs champ :\n$$ \\hat{V} = \\frac{1}{2\\mathcal{V}^2} \\int \\mathrm{d}^3x \\,\\mathrm{d}^3 y \\!\\!\\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2 \\boldsymbol{p}_3 \\boldsymbol{p}_4} \\mathrm{e}^{\\mathrm{i}(-\\boldsymbol{p}_1 \\cdot \\boldsymbol{x} - \\boldsymbol{p}_2 \\cdot \\boldsymbol{y} + \\boldsymbol{p}_3 \\cdot \\boldsymbol{y} + \\boldsymbol{p}_4 \\cdot \\boldsymbol{x})} \\hat{a}^\\dagger_{\\boldsymbol{p}_1} \\hat{a}^\\dagger_{\\boldsymbol{p}_2} V(\\boldsymbol{x} - \\boldsymbol{y}) \\hat{a}_{\\boldsymbol{p}_3} \\hat{a}_{\\boldsymbol{p}_4} $$\nChangement de variables et jeu sur les indices aboutissent à :\n$$\\hat{V} = \\frac{1}{2} \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2 \\boldsymbol{q}} \\tilde{V}_{\\boldsymbol{q}} \\,\\hat{a}^\\dagger_{\\boldsymbol{p}_1 + \\boldsymbol{q}} \\hat{a}^\\dagger_{\\boldsymbol{p}_2 - \\boldsymbol{q}} \\hat{a}_{\\boldsymbol{p}_2} \\hat{a}_{\\boldsymbol{p}_1}$$\noù $\\tilde{V}_\\boldsymbol{q}$ est la transformée de Fourier du potentiel.\nLa formule peut s\u0026rsquo;interpréter comme une diffusion dans l\u0026rsquo;espace des impulsions qui se représente conventionnellement par un dessin appelé diagramme de Feynman.\nUne particule arrive avec une impulsion $\\boldsymbol{p}_2$, émet une particule porteuse de force d\u0026rsquo;impulsion $\\boldsymbol{q}$, réduisant ainsi son impulsion finale à $\\boldsymbol{p}_2-\\boldsymbol{q}$. La particule porteuse de force est absorbée par une autre particule dont l\u0026rsquo;impulsion passe alors de $\\boldsymbol{p}_1$ à $\\boldsymbol{p}_1+\\boldsymbol{q}$. On constate que l\u0026rsquo;impulsion est conservée aux points de croisement du diagramme.\nPour réussir à évaluer l\u0026rsquo;énergie facilement, l\u0026rsquo;idée est de transformer toutes les combinaisons d\u0026rsquo;opérateurs en opérateurs nombre. Mais on se heurte à la non diagonalité du potentiel (les problèmes impliquant une énergie potentielle ne peuvent généralement pas être résolus de manière exacte). Malgré cela, le potentiel à deux particules peut nous mener à des richesses physiques insoupçonnées (magnétisme, superfluidité, supraconductivité,\u0026hellip;).\nModèle de Hubbard Le modèle de Hubbard est central en matière condensée. Il permet de capturer les implications physiques de la compétition entre énergie cinétique (qui favorise la délocalisation des électrons) et énergie potentielle (qui favorise leur localisation).\nL\u0026rsquo;Hamiltonien du modèle de Hubbard est construit à partir de nos deux ingrédients précédents : l\u0026rsquo;Hamiltonien du modèle de liaisons fortes pour modéliser l\u0026rsquo;énergie cinétique et le potentiel à deux particules pour modéliser les interactions electron-electron :\n$$\\hat{H} = \\sum_{ij} \\left(-t_{ij} \\hat{c}^\\dagger_i \\hat{c}_j\\right) + \\frac{1}{2} \\sum_{ijkl} \\hat{c}^\\dagger_i \\hat{c}^\\dagger_j V_{ijkl} \\hat{c}_k \\hat{c}_l$$\nEt comme les électrons possèdent un spin $\\sigma$ qui peut pointer soit vers le haut ($|\\uparrow\\rangle$), soit vers le bas ($|\\downarrow\\rangle$), l\u0026rsquo;Hamiltonien devient :\n$$\\hat{H} = \\sum_{ij\\sigma} (-t_{ij}) \\hat{c}^\\dagger_{i\\sigma} \\hat{c}_{j\\sigma} + \\frac{1}{2} \\sum_{ijkl\\sigma\\sigma\u0026rsquo;} \\hat{c}^\\dagger_{i\\sigma} \\hat{c}^\\dagger_{j\\sigma\u0026rsquo;} V_{ijkl} \\hat{c}_{k\\sigma\u0026rsquo;} \\hat{c}_{l\\sigma}$$\nOn suppose ici que les spins ne peuvent pas basculer. Comme l\u0026rsquo;interaction entre électrons vient de l\u0026rsquo;interaction de Coulomb, elle est indépendante de l\u0026rsquo;orientation du spin. Mais pour simplifier, on supposera que l\u0026rsquo;interaction coulombienne n\u0026rsquo;est notable que lorsque les électrons sont sur le même site. Les électrons interagissent donc via une énergie potentielle constante $U=V_{iiii}$. Mais comme le principe de Pauli impose que deux électrons sur le même site aient des spins opposés, l\u0026rsquo;Hamiltonien devient :\n$$\\hat{H} = \\sum_{ij\\sigma} (-t_{ij}) \\hat{c}^\\dagger_{i\\sigma} \\hat{c}_{j\\sigma} + U \\sum_i \\hat{n}_{i\\uparrow} \\hat{n}_{i\\downarrow}$$\nBien que simple d\u0026rsquo;allure, obtenir les états propres est souvent une tâche complexe et ceux-ci sont généralement fortement corrélés.\nSimplifions à l\u0026rsquo;extrême avec un réseau de seulement deux sites, et un seul électron avec un spin haut $|\\uparrow\\rangle$. L\u0026rsquo;électron peut être soit sur le premier site, ce que l\u0026rsquo;on note $|\\uparrow,0\\rangle$ (spin haut sur le site 1, rien sur le site 2), soit sur le second $|0,\\uparrow\\rangle$ (rien sur le site 1, spin haut sur le site 2).\nUn état général est une combinaison de ces deux états $|\\psi\\rangle = a|\\uparrow,0\\rangle+b|0,\\uparrow\\rangle$ et l\u0026rsquo;Hamiltonien de Hubbard peut s\u0026rsquo;écrire dans cette base :\n$ \\displaystyle \\hat{H} = \\begin{pmatrix} 0 \u0026amp; -t \\\\ -t \u0026amp; 0 \\end{pmatrix} $\nEn diagonalisant, on obtient un état fondamental $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}(|\\uparrow,0\\rangle+|0,\\uparrow\\rangle )$ pour une énergie $E=-t$, et un état excité $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}(|\\uparrow,0\\rangle-|0,\\uparrow\\rangle )$ pour une énergie $E=t$.\nAjoutons un électron dans le système. S\u0026rsquo;il a le même spin, la solution est simplement $|\\uparrow,\\uparrow\\rangle$ et l\u0026rsquo;énergie associée est $E=0$ (pas d\u0026rsquo;interaction possible puisque les deux électrons ne peuvent pas occuper le même site et ils ne peuvent pas sauter d\u0026rsquo;un site à l\u0026rsquo;autre, ils sont bloqués).\nMontrons-le par le calcul\u0026nbsp;: Pour la partie potentiel, on obtient bien 0 puisque $U (\\hat{n}_{1\\uparrow} \\hat{n}_{1\\downarrow}+\\hat{n}_{2\\uparrow} \\hat{n}_{2\\downarrow})|\\uparrow,\\uparrow\\rangle=U(1\\times 0+1\\times 0)|\\uparrow,\\uparrow\\rangle = 0$ (puisque l'opérateur qui compte les spins bas trouve toujours zéro).\nEt pour la partie cinétique, on ne garde que les termes de spin haut de l'Hamiltonien\u0026nbsp;: $-t (\\hat{c}^\\dagger_{1\\uparrow} \\hat{c}_{2\\uparrow}+\\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow})$. Et on réécrit $|\\uparrow,\\uparrow\\rangle$ comme $\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}|0\\rangle$.\n$-t (\\hat{c}^\\dagger_{1\\uparrow} \\hat{c}_{2\\uparrow}+\\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow})\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}|0\\rangle=-t \\hat{c}^\\dagger_{1\\uparrow} \\hat{c}_{2\\uparrow}\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}|0\\rangle-t\\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow}\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}|0\\rangle=t \\hat{c}^\\dagger_{1\\uparrow} \\hat{c}_{2\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}\\hat{c}^\\dagger_{1\\uparrow}|0\\rangle-t\\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow}\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}|0\\rangle=t \\hat{c}^\\dagger_{1\\uparrow}(1-\\hat{n}_{2\\uparrow})\\hat{c}^\\dagger_{1\\uparrow}|0\\rangle-t\\hat{c}^\\dagger_{2\\uparrow}(1-\\hat{n}_{1\\uparrow})\\hat{c}^\\dagger_{2\\uparrow}|0\\rangle = t \\cancel{\\hat{c}^\\dagger_{1\\uparrow} \\hat{c}^\\dagger_{1\\uparrow} }|0\\rangle - t\\hat{c}^\\dagger_{1\\uparrow}\\cancel{\\hat{n}_{2\\uparrow}|\\uparrow,0\\rangle}- t\\cancel{\\hat{c}^\\dagger_{2\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}}|0\\rangle+t\\hat{c}^\\dagger_{2\\uparrow}\\cancel{\\hat{n}_{1\\uparrow}|0,\\uparrow\\rangle}=0$ S\u0026rsquo;ils sont de spins opposés, une base possible est $\\{|\\uparrow\\downarrow,0\\rangle,|\\uparrow,\\downarrow\\rangle,|\\downarrow,\\uparrow\\rangle,|0,\\uparrow\\downarrow\\rangle\\}$. Et dans cette base, l\u0026rsquo;Hamiltonien de Hubbard s\u0026rsquo;écrit :\n$ \\displaystyle \\hat{H} = \\begin{pmatrix} U \u0026amp; -t \u0026amp; t \u0026amp; 0 \\\\ -t \u0026amp; 0 \u0026amp; 0 \u0026amp; -t \\\\ t \u0026amp; 0 \u0026amp; 0 \u0026amp; t \\\\ 0 \u0026amp; -t \u0026amp; t \u0026amp; U \\end{pmatrix} $\nDétaillons juste l'action de l'Hamiltonien sur $|\\uparrow\\downarrow,0\\rangle$ pour s'en convaincre\u0026nbsp;: Terme potentiel : $U \\hat{n}_{1\\uparrow}\\hat{n}_{1\\downarrow}|\\uparrow\\downarrow,0\\rangle = U\\times 1\\times 1=U$ et $U \\hat{n}_{2\\uparrow}\\hat{n}_{2\\downarrow}|\\uparrow\\downarrow,0\\rangle = U\\times 0\\times 0=0$ (puisque l\u0026rsquo;opérateur $\\hat{n}$ ne fait que compter le nombres de particules dans un site donné).\nTerme cinétique : comme $i$ et $j$ doivent être différents, on a 4 termes dans la somme $(-t) \\hat{c}^\\dagger_{1\\uparrow} \\hat{c}_{2\\uparrow}+(-t) \\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow}+(-t) \\hat{c}^\\dagger_{1\\downarrow} \\hat{c}_{2\\downarrow}+(-t) \\hat{c}^\\dagger_{2\\downarrow} \\hat{c}_{1\\downarrow}$.\nSeuls les deux termes contenant un opérateur d\u0026rsquo;annihilation sur le site 1 ne donneront pas un résultat nul. Il nous reste $-t(\\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow} +\\hat{c}^\\dagger_{2\\downarrow} \\hat{c}_{1\\downarrow})$.\nAppliquons cette somme d\u0026rsquo;opérateurs à $|\\uparrow\\downarrow,0\\rangle$ qu\u0026rsquo;on peut aussi écrire $\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{1\\downarrow}|0\\rangle$. Le premier terme nous donne\n$-t\\hat{c}^\\dagger_{2\\uparrow} \\hat{c}\\_{1\\uparrow} \\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{1\\downarrow}|0\\rangle=-t\\hat{c}^\\dagger_{2\\uparrow} (1-\\hat{n}\\_{1\\uparrow})\\hat{c}^\\dagger_{1\\downarrow}|0\\rangle=-t\\hat{c}^\\dagger_{2\\uparrow}\\hat{c}^\\dagger_{1\\downarrow}|0\\rangle+t\\hat{c}^\\dagger_{2\\uparrow} \\cancel{\\hat{n}\\_{1\\uparrow}|\\downarrow,0\\rangle}=-t\\hat{c}^\\dagger_{2\\uparrow}|\\downarrow,0\\rangle=t|\\downarrow,\\uparrow\\rangle$ Le dernier changement de signe venant du fait qu'il y ait déjà un fermion à gauche de celui créé dans l'état 2 ($(-1)^{n_1+n_2+\\cdots+n_{i-1}}=(-1)^1=-1$).\nEt le deuxième terme donne $-t\\hat{c}^\\dagger_{2\\downarrow} \\hat{c}\\_{1\\downarrow}\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{1\\downarrow}|0\\rangle=t\\hat{c}^\\dagger_{2\\downarrow} \\hat{c}\\_{1\\downarrow}\\hat{c}^\\dagger_{1\\downarrow}\\hat{c}^\\dagger_{1\\uparrow}|0\\rangle=t\\hat{c}^\\dagger_{2\\downarrow} (1-\\hat{n}\\_{1\\downarrow})\\hat{c}^\\dagger_{1\\uparrow}|0\\rangle=t\\hat{c}^\\dagger_{2\\downarrow}\\hat{c}^\\dagger_{1\\uparrow}|0\\rangle-t\\hat{c}^\\dagger_{2\\downarrow}\\cancel{\\hat{n}\\_{1\\downarrow}|\\uparrow,0\\rangle}=-t|\\uparrow,\\downarrow\\rangle$ La matrice peut se diagonaliser et l\u0026rsquo;état fondamental s\u0026rsquo;écrit $|\\psi\\rangle = N(|\\uparrow\\downarrow,0\\rangle+W|\\uparrow,\\downarrow\\rangle-W|\\downarrow,\\uparrow\\rangle+|0,\\uparrow\\downarrow\\rangle)$ où $N$ est une constante de normalisation et $W=\\frac{U}{4t}+\\frac{1}{4t}\\sqrt{U^2+16t^2}$ pour une énergie $E=\\frac{U}{2}-\\frac{1}{2}\\sqrt{U^2+16t^2}$. C\u0026rsquo;est déjà un résultat étonament complexe pour un système si simple (deux électrons sur deux sites)\u0026hellip;\nOn a découvert jusqu\u0026rsquo;ici comment des systèmes variés peuvent se réduire à un simple jeu d\u0026rsquo;oscillateurs harmoniques, chacun décrivant un certain mode normal du système. Et chacun de ces modes normaux peut-être vu comme un état d\u0026rsquo;impulsion pour des particules identiques sans interaction. Le nombre de particules correspond alors au nombre d\u0026rsquo;excitations quantifiées du mode. Et on a défini des opérateurs qui peuvent créer et annihiler ces particules.\nLes modèles considérés se restreignaient à des réseaux discrets adaptés à la physique de la matière condensée. Dans le prochain chapitre, on va généraliser au continu.\nChapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc3/",
	"title": "TQC-3",
	"tags": [],
	"description": "",
	"content": " Théorie quantique des champs \u0026ndash; Partie 3 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Très souvent une simple traduction.\nRetour sommaire\nÉvolution temporelle Deux représentations de la mécanique quantique s\u0026rsquo;opposent quant à la description de l\u0026rsquo;évolution temporelle d\u0026rsquo;un système : la représentation de Schrödinger et la représentation d\u0026rsquo;Heisenberg.\nReprésentation de Schrödinger Dans la représentation de Schrödinger, ce sont les fonctions d\u0026rsquo;onde qui dépendent du temps et leur évolution est déterminée par l\u0026rsquo;équation de Schrödinger :\n$$ \\mathrm{i} \\frac{\\partial \\psi(\\boldsymbol{x}, t)}{\\partial t}=\\hat{H} \\psi(\\boldsymbol{x}, t) $$\nOn accède aux variables dynamiques comme la position et l\u0026rsquo;impulsion via des opérateurs ($\\hat{\\boldsymbol{x}}=\\boldsymbol{x}$, $\\hat{\\boldsymbol{p}}=-\\mathrm{i} \\nabla$) qui agissent sur la fonction d\u0026rsquo;onde.\nOn est très loin de la mécanique classique où les variables dynamiques ($\\boldsymbol{p}(t)$, $\\boldsymbol{x}(t)$,\u0026hellip;) dépendent du temps et sont donc décrites par des équations du mouvement. Mais comme aucun \u0026ldquo;opérateur temps\u0026rdquo; n\u0026rsquo;existe en mécanique quantique, il faut se contenter d\u0026rsquo;un paramètre temporel $t$ que la représentation de Schrödinger confine donc entièrement dans la fonction d\u0026rsquo;onde.\nPas d\u0026rsquo;opérateur donnant le temps mais quand même un opérateur d\u0026rsquo;évolution $\\hat{U}(t_2,t_1)$ permettant de faire évoluer une particule de $t_1$ à $t_2$.\n$$ \\psi\\left(t_2\\right)=\\hat{U}\\left(t_2, t_1\\right) \\psi\\left(t_1\\right) $$\nL\u0026rsquo;opérateur d\u0026rsquo;évolution a les propriétés suivantes :\n$\\hat{U}\\left(t_1, t_1\\right)=1$\nRien ne se passe si les instants sont les mêmes. $\\hat{U}\\left(t_3, t_2\\right) \\hat{U}\\left(t_2, t_1\\right)=\\hat{U}\\left(t_3, t_1\\right)$\nCette relation de composition montre qu'on peut construire une translation temporelle quelconque en multipliant entre elles une multitude de translations temporelles minuscules. $\\displaystyle \\mathrm{i} \\frac{\\mathrm{~d}}{\\mathrm{~d} t_2} \\hat{U}\\left(t_2, t_1\\right)=\\hat{H} \\hat{U}\\left(t_2, t_1\\right)$\nL'opérateur d'évolution lui-même obéit à l'équation de Schrödinger. Pour le prouver, on part de $\\psi\\left(t_2\\right)=\\hat{U}\\left(t_2, t_1\\right) \\psi\\left(t_1\\right)$ et on dérive par rapport à $t_2$ :\n$$ \\frac{\\mathrm{d} \\psi\\left(t_2\\right)}{\\mathrm{d} t_2}=\\frac{\\mathrm{d} \\hat{U}\\left(t_2, t_1\\right)}{\\mathrm{d} t_2} \\psi\\left(t_1\\right) $$\nEt en utilisant $\\mathrm{i} \\frac{\\partial \\psi(\\boldsymbol{x}, t)}{\\partial t}=\\hat{H} \\psi(\\boldsymbol{x}, t)$, on obtient ;\n$$ \\mathrm{i} \\frac{\\mathrm{~d} \\psi\\left(t_2\\right)}{\\mathrm{d} t_2}=\\hat{H} \\psi\\left(t_2\\right)=\\hat{H} \\hat{U}\\left(t_2, t_1\\right) \\psi\\left(t_1\\right) $$\n$\\hat{U}\\left(t_1, t_2\\right)=\\hat{U}^{-1}\\left(t_2, t_1\\right)$\nOn prenant l'inverse de l'opérateur d'évolution, on remonte le temps. $\\hat{U}^{\\dagger}\\left(t_2, t_1\\right) \\hat{U}\\left(t_2, t_1\\right)=1,$\nL'opérateur d'évolution est unitaire. Pour $t_2=t_1$, c\u0026rsquo;est trivial.\nPour $t_2\\neq t_1$, montrons que cette composition d\u0026rsquo;opérateurs est constante (et cette constante vaut 1 par normalisation) :\n$$ \\begin{aligned} \\frac{\\mathrm{d}}{\\mathrm{~d} t_2}\\left[\\hat{U}^{\\dagger}\\left(t_2, t_1\\right) \\hat{U}\\left(t_2, t_1\\right)\\right] \u0026amp; =\\frac{\\mathrm{d} \\hat{U}^{\\dagger}}{\\mathrm{d} t_2} U+U^{\\dagger} \\frac{\\mathrm{d} \\hat{U}}{\\mathrm{~d} t_2} \\\\ \u0026amp; =-\\frac{\\hat{U}^{\\dagger} \\hat{H} \\hat{U}}{\\mathrm{i}}+\\frac{\\hat{U}^{\\dagger} \\hat{H} \\hat{U}}{\\mathrm{i}}\\\\ \u0026amp;=0 \\end{aligned} $$\noù on a utilisé $\\frac{\\mathrm{d} \\hat{U}}{\\mathrm{~d} t}=\\frac{\\hat{H} \\hat{U}}{\\mathrm{i}}$ et $\\frac{\\mathrm{d} \\hat{U}^{\\dagger}}{\\mathrm{d} t}=-\\frac{\\hat{U}^{\\dagger} \\hat{H}}{\\mathrm{i}}$.\nUne conséquence directe est que $\\hat{U}^\\dagger(t_2,t_1)=\\hat{U}^{-1}(t_2,t_1)$ ; l\u0026rsquo;opérateur adjoint est l\u0026rsquo;opérateur inverse.\nLa propriété 3 permet d\u0026rsquo;exprimer explicitement $\\hat{U}(t_2,t_1)$ :\n$$ \\hat{U}\\left(t_2, t_1\\right)=\\mathrm{e}^{-\\mathrm{i} \\hat{H}\\left(t_2-t_1\\right)} $$\nCe type d\u0026rsquo;expression exponentiel cache le développement :\n$$ \\mathrm{e}^{\\hat{A}}=1+\\hat{A}+\\frac{1}{2!} \\hat{A} \\hat{A}+\\frac{1}{3!} \\hat{A} \\hat{A} \\hat{A}+\\ldots $$\nReprésentation d\u0026rsquo;Heisenberg Plutôt que de placer la dépendance temporelle dans la fonction d\u0026rsquo;onde, dans la représentation d\u0026rsquo;Heisenberg, ce sont les opérateurs qui la prennent en charge.\nPour passer d\u0026rsquo;une représentation à l\u0026rsquo;autre, commençons par écrire la valeur moyenne de l\u0026rsquo;opérateur $O$ dans l\u0026rsquo;état $\\psi(t)$ :\n$$ \\langle\\hat{O}(t)\\rangle=\\langle\\psi(t)| \\hat{O}|\\psi(t)\\rangle $$\nToutes les représentations doivent s\u0026rsquo;accorder sur ce résultat.\nUtilisons maintenant l\u0026rsquo;opérateur d\u0026rsquo;évolution pour ne plus avoir à s\u0026rsquo;occuper que de l\u0026rsquo;état à l\u0026rsquo;instant initial $\\psi(0)$ :\n$$ \\psi(t)=\\hat{U}(t, 0) \\psi(0)=\\mathrm{e}^{-\\mathrm{i} \\hat{H} t} \\psi(0) $$\nEt en replaçant $\\psi(t)$ dans l\u0026rsquo;expression de la valeur moyenne :\n$$ \\langle\\psi(t)| \\hat{O}|\\psi(t)\\rangle=\\langle\\psi(0)| \\hat{U}^{\\dagger}(t, 0) \\hat{O} \\hat{U}(t, 0)|\\psi(0)\\rangle $$\nDans la représentation de Schrödinger, on considère les opérateurs comme indépendants du temps ($\\hat{O}_{\\mathrm{S}} \\equiv \\hat{O}$), contrairement aux états ($\\left|\\psi_{\\mathrm{S}}(t)\\right\\rangle \\equiv \\hat{U}(t, 0)|\\psi(0)\\rangle$) de telle sorte que :\n$$ \\langle\\psi(0)| \\hat{U}^{\\dagger}(t, 0)[\\hat{O}] \\hat{U}(t, 0)|\\psi(0)\\rangle=\\left\\langle\\psi_{\\mathrm{S}}(t)\\right| \\hat{O}_{\\mathrm{S}}\\left|\\psi_{\\mathrm{S}}(t)\\right\\rangle $$\nMais en étendant un peu les crochets, on obtient une nouvelle façon de voir les choses où les opérateurs deviennent des objets dynamiques :\n$$ \\langle\\psi(0)|\\left[\\hat{U}^{\\dagger}(t, 0) \\hat{O} \\hat{U}(t, 0)\\right]|\\psi(0)\\rangle=\\left\\langle\\psi_{\\mathrm{H}}\\right| \\hat{O}_{\\mathrm{H}}(t)\\left|\\psi_{\\mathrm{H}}\\right\\rangle $$\nOn obtient ainsi la représentation d\u0026rsquo;Heisenberg où les états sont indépendants du temps ($\\psi_{\\mathrm{H}} \\equiv \\psi(0)$), contrairement aux opérateurs :\n$$ \\hat{O}_{\\mathrm{H}}(t) \\equiv \\hat{U}^{\\dagger}(t, 0) \\hat{O}_{\\mathrm{S}} \\hat{U}(t, 0) $$\nOn retrouve une situation similaire à la mécanique classique avec ses variables dynamiques.\nEt pour expliciter la dépendance temporelle de $\\hat{O}_H(t)$, on différentie l\u0026rsquo;équation précédente et on utilise la propriété 3 de l\u0026rsquo;opérateur d\u0026rsquo;évolution :\n$$ \\frac{\\mathrm{d} \\hat{O}_{\\mathrm{H}}(t)}{\\mathrm{d} t}=\\frac{\\mathrm{d} \\hat{U}^{\\dagger}}{\\mathrm{d} t} \\hat{O}_{\\mathrm{S}} \\hat{U}+\\hat{U}^{\\dagger} \\hat{O}_{\\mathrm{S}} \\frac{\\mathrm{~d} \\hat{U}}{\\mathrm{~d} t}=\\frac{1}{\\mathrm{i}}\\left(-\\hat{U}^{\\dagger} \\hat{H} \\hat{O}_{\\mathrm{S}} \\hat{U}+\\hat{U}^{\\dagger} \\hat{O}_{\\mathrm{S}} \\hat{H} \\hat{U}\\right) $$\nEn utilisant la définition de $\\hat{O}_H(t)$ et le fait que $\\hat{U}$ et $\\hat{H}$ commutent (puisque $\\hat{U}$ s\u0026rsquo;exprime à partir de $\\hat{H}$), on obtient l\u0026rsquo;équation du mouvement d\u0026rsquo;Heisenberg :\n$$ \\frac{\\mathrm{d} \\hat{O}_{\\mathrm{H}}(t)}{\\mathrm{d} t}=\\frac{1}{\\mathrm{i} \\hbar}\\left[\\hat{O}_{\\mathrm{H}}(t), \\hat{H}\\right] $$\nLes limites de la description du monde par la mécanique quantique La mécanique quantique \u0026ldquo;classique\u0026rdquo; est une description formidable du monde à petite échelle. Mais c\u0026rsquo;est fondamentalement une description à une particule. Tant qu\u0026rsquo;on n\u0026rsquo;est pas embêté par la relativité, ça suffit. Mais les deux résultats suivants témoignent de l\u0026rsquo;incapacité de la mécanique quantique à une particule à s\u0026rsquo;accommoder d\u0026rsquo;un cadre relativiste.\nPremier coup dur : la probabilité de trouver la particule en dehors de son cône de lumière n\u0026rsquo;est pas nulle. En effet, $\\langle\\boldsymbol{x}| \\mathrm{e}^{-\\mathrm{i} \\hat{H} t}|\\boldsymbol{x}=0\\rangle$ donne un résultat proportionnel à $\\mathrm{e}^{-m|x|}$ pour un intervalle de type espace (tel que $|x|\u0026gt;t$). C\u0026rsquo;est certes infime pour des grands $|\\boldsymbol{x}|$, mais cela reste difficilement acceptable\u0026hellip;\nDeuxième coup porté : imaginons que l\u0026rsquo;on cloisonne la particule entre deux murs (des barrières de potentiel) séparés d\u0026rsquo;une distance bien inférieure à sa longueur d\u0026rsquo;onde de Compton $\\lambda=h m / c$ ($1/m$ en unités naturelles). La minuscule incertitude sur la position ($\\Delta x \\ll \\lambda$) entraîne une incertitude sur l\u0026rsquo;impulsion telle ($\\Delta p \\gg 1/\\lambda=m$) que l\u0026rsquo;énergie de la particule devient suffisante pour faire jaillir des paires de particules-antiparticules. La boite devrait alors contenir en moyenne plus d\u0026rsquo;une particule, situation que la mécanique quantique \u0026ldquo;classique\u0026rdquo; n\u0026rsquo;est pas câblée pour décrire.\nCes objets définis localement (en un point $x$ de l\u0026rsquo;espace-temps) que sont les champs et plus précisément les champs d\u0026rsquo;opérateurs $\\hat{\\phi}(x)$ vont tirer la quantique de ce mauvais pas. Et puisqu\u0026rsquo;il s\u0026rsquo;agit de placer au premier plan des opérateurs dynamiques, la représentation d\u0026rsquo;Heisenberg va se trouver être la représentation idoine.\nPour rendre un champ d\u0026rsquo;opérateurs compatible avec la relativité, il suffit de s\u0026rsquo;assurer que des opérateurs éloignés d\u0026rsquo;un intervalle de type espace commutent ($[\\hat{\\phi}(x), \\hat{\\phi}(y)]=0$ si $(x-y)^2\u0026lt;0$). Cela empêche que le résultat de l\u0026rsquo;une des mesures puisse influer causalement le résultat de l\u0026rsquo;autre.\nTransformations continues Translations dans l\u0026rsquo;espace-temps Pour translater une particule dans l\u0026rsquo;espace, il nous faut un opérateur $\\hat{U}$ qui transforme un état localisé en $\\boldsymbol{x}$ en un état localisé en $\\boldsymbol{x}+\\boldsymbol{a}$ :\n$$ \\hat{U}(\\boldsymbol{a})|\\boldsymbol{x}\\rangle=|\\boldsymbol{x}+\\boldsymbol{a}\\rangle $$\nOn suppose ici qu\u0026rsquo;on a transporté la particule à sa nouvelle place ; on parle alors de point de vue actif.\nFaisons maintenant agir l\u0026rsquo;opérateur sur une fonction d\u0026rsquo;onde $\\psi(x)$ :\n$$ \\hat{U}(\\boldsymbol{a})\\psi(\\boldsymbol{x})=\\psi(\\boldsymbol{x}-\\boldsymbol{a}) $$\nSoit $|\\psi\\rangle$ l\u0026rsquo;état décrit par la fonction d\u0026rsquo;onde $\\psi(\\boldsymbol{x})=\\langle\\psi|\\boldsymbol{x}\\rangle$ et soit $|\\phi\\rangle=\\hat{U}(\\boldsymbol{a})|\\psi\\rangle$, l\u0026rsquo;état translaté.\n$$ \\begin{aligned} \\phi(\\boldsymbol{x}) \u0026amp; =\\langle \\boldsymbol{x} | \\phi\\rangle\\\\ \u0026amp;=\\langle \\boldsymbol{x} | \\hat{U}(\\boldsymbol{a})|\\psi\\rangle\\\\ \u0026amp;=\\int d \\boldsymbol{x}^{\\prime} \\langle \\boldsymbol{x} | \\hat{U}(\\boldsymbol{a}) | \\boldsymbol{x}^{\\prime}\\rangle \\langle \\boldsymbol{x}^{\\prime} | \\psi \\rangle\\\\ \u0026amp;=\\int d \\boldsymbol{x}^{\\prime} \\langle \\boldsymbol{x} | \\boldsymbol{x}^{\\prime}+\\boldsymbol{a} \\rangle \\langle \\boldsymbol{x}^{\\prime} | \\psi\\rangle \\\\ \u0026amp; =\\int d \\boldsymbol{x}^{\\prime} \\delta (\\boldsymbol{x}-\\boldsymbol{x}^{\\prime}-\\boldsymbol{a}) \\psi (\\boldsymbol{x}^{\\prime})\\\\ \u0026amp;=\\psi(\\boldsymbol{x}-\\boldsymbol{a}) \\end{aligned} $$\nEn réécrivant ça $\\phi(\\boldsymbol{x}+\\boldsymbol{a})=\\psi(\\boldsymbol{x})$, on obtient un moyen simple de se souvenir du signe :\nla valeur de la nouvelle fonction d\u0026rsquo;onde au nouveau point est égale à la valeur de l\u0026rsquo;ancienne fonction d\u0026rsquo;onde à l\u0026rsquo;ancien point.\nPropriétés de l\u0026rsquo;opérateur de translation :\n$\\hat{U}(\\boldsymbol{0})=1$\nPrésence d'un élément neutre. $\\hat{U}(\\boldsymbol{a})\\hat{U}(\\boldsymbol{b})=\\hat{U}(\\boldsymbol{a}+\\boldsymbol{b})$\nLoi de composition. $\\hat{U}(\\boldsymbol{a})^{-1}=\\hat{U}(-\\boldsymbol{a})$\nL'opérateur de translation admet un inverse. $\\hat{U}(\\boldsymbol{a})^{-1}=\\hat{U}(\\boldsymbol{a})^\\dagger$\nL'opérateur de translation est unitaire. Ces propriétés nous montrent que ces transformations forment un groupe et comme chaque élément dépend d\u0026rsquo;un paramètre continu ($\\boldsymbol{a}$) entraînant que le groupe possède un nombre infini d\u0026rsquo;éléments, il s\u0026rsquo;agit d\u0026rsquo;un groupe de Lie.\nUn groupe est un ensemble $G$ muni d\u0026rsquo;une loi de composition interne $\\bullet$ associative admettant un élément neutre et, pour chaque élément de l\u0026rsquo;ensemble, un élément symétrique :\n$\\forall a, b \\in G, a \\bullet b \\in G$ (loi de composition interne assurant la fermeture du groupe) $\\forall a, b, c \\in G, a \\bullet(b \\bullet c)=(a \\bullet b) \\bullet c$ (associativité) $\\exists e \\in G$ tel que $\\forall a \\in G, a \\bullet e=e \\bullet a=a$ (élément neutre) $\\forall a \\in G, \\exists a^{-1} \\in G$ tel que $a \\bullet a^{-1}=a^{-1} \\bullet a=e$ (inverse) note\nDes vieilles notes de lecture d\u0026rsquo;un autre chouette livre pour en savoir un peu plus sur les groupes discrets.\nL\u0026rsquo;opérateur de translation peut aussi être vu comme la transformation d\u0026rsquo;un opérateur :\n$$ \\hat{U}^{\\dagger}(\\boldsymbol{a}) \\hat{\\boldsymbol{x}} \\hat{U}(\\boldsymbol{a})=(\\hat{\\boldsymbol{x}}+\\boldsymbol{a}) . $$\nOn a en effet\u0026nbsp;: $$ \\begin{aligned} \\hat{U}(\\boldsymbol{a})|\\boldsymbol{x}\\rangle \u0026amp; =|\\boldsymbol{x}+\\boldsymbol{a}\\rangle \\\\ \\hat{\\boldsymbol{x}} U(\\boldsymbol{a})|\\boldsymbol{x}\\rangle \u0026amp; =\\hat{\\boldsymbol{x}}|\\boldsymbol{x}+\\boldsymbol{a}\\rangle=(\\boldsymbol{x}+\\boldsymbol{a})|\\boldsymbol{x}+\\boldsymbol{a}\\rangle \\\\ U^{\\dagger}(\\boldsymbol{a}) \\hat{\\boldsymbol{x}} U(\\boldsymbol{a})|\\boldsymbol{x}\\rangle \u0026amp; =(\\boldsymbol{x}+\\boldsymbol{a}) U^{\\dagger}(\\boldsymbol{a})|\\boldsymbol{x}+\\boldsymbol{a}\\rangle=(\\boldsymbol{x}+\\boldsymbol{a})|\\boldsymbol{x}\\rangle \\end{aligned} $$\nImaginons une observable représentée par l\u0026rsquo;opérateur $\\hat{O}$. Si une translation n\u0026rsquo;a pas d\u0026rsquo;effet sur la propriété mesurée par cet opérateur, on peut écrire :\n$$ \\langle\\psi(\\boldsymbol{x})| \\hat{O}|\\psi(\\boldsymbol{x})\\rangle=\\langle\\psi(\\boldsymbol{x})| \\hat{U}^{-1}(\\boldsymbol{a}) \\hat{O} \\hat{U}(\\boldsymbol{a})|\\psi(\\boldsymbol{x})\\rangle $$\nOn dit alors que l\u0026rsquo;opérateur est un invariant et la condition pour qu\u0026rsquo;un opérateur $\\hat{O}$ soit un invariant est donc :\n$$ \\hat{U}^{-1}(\\boldsymbol{a}) \\hat{O} \\hat{U}(\\boldsymbol{a})=\\hat{O} $$\nCe qui devient en faisant agir $\\hat{U}$ à gauche de chaque membre de l\u0026rsquo;égalité :\n$$ [\\hat{O},\\hat{U}]=0 $$\nL\u0026rsquo;invariance par rapport à une transformation implique la commutation des opérateurs.\nEssayons maintenant d\u0026rsquo;obtenir une formule explicite pour l\u0026rsquo;opérateur de translation.\n$$ \\psi(x-\\delta a)=\\psi(x)-\\frac{\\mathrm{d} \\psi(x)}{\\mathrm{d} x} \\delta a+\\ldots $$\nCe qu\u0026rsquo;on peut réécrire au premier ordre, en se rappelant que $\\hat{p}=-\\mathrm{i} \\frac{\\mathrm{~d}}{\\mathrm{~d} x}$ :\n$$ \\psi(x-\\delta a)=(1-\\mathrm{i} \\hat{p} \\delta a) \\psi(x) $$\nOn dit que l\u0026rsquo;opérateur $\\hat{p}$ est le générateur des translations spatiales. Pour opérer une translation d\u0026rsquo;une distance $a$, on peut translater de $\\delta a$ un grand nombre $N$ de fois :\n$$ \\begin{aligned} \\psi(x-a) \u0026amp; =\\lim _{N \\rightarrow \\infty}(1- \\mathrm{i} \\hat{p} \\delta a)^N \\psi(x) \\\\ \u0026amp; =\\mathrm{e}^{-\\mathrm{i} \\hat{p} a} \\psi(x) \\end{aligned} $$\nPar identification :\n$$ \\hat{U}(\\boldsymbol{a})=\\mathrm{e}^{-\\mathrm{i} \\hat{\\boldsymbol{p}} \\cdot \\boldsymbol{a}} $$\nAction de l\u0026rsquo;opérateur de translation sur un état d\u0026rsquo;impulsion $|\\boldsymbol{q}\\rangle :$\n$$ \\begin{aligned} \\hat{U}(\\boldsymbol{a})|\\boldsymbol{q}\\rangle \u0026amp; =\\mathrm{e}^{-\\mathrm{i} \\hat{p} \\cdot \\boldsymbol{a}}|\\boldsymbol{q}\\rangle \\\\ \u0026amp; =\\mathrm{e}^{-\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{a}}|\\boldsymbol{q}\\rangle \\end{aligned} $$\nProjeté sur l\u0026rsquo;axe des coordonnées, on obtient bien une fonction d\u0026rsquo;onde translatée :\n$$ \\langle\\boldsymbol{x}| \\hat{U}(\\boldsymbol{a})|\\boldsymbol{q}\\rangle=\\langle\\boldsymbol{x} | \\boldsymbol{q}\\rangle \\mathrm{e}^{-\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{a}}=\\frac{1}{\\sqrt{\\mathcal{V}}} \\mathrm{e}^{\\mathrm{i} \\boldsymbol{q} \\cdot(\\boldsymbol{x}-\\boldsymbol{a})} $$\nL\u0026rsquo;opérateur d\u0026rsquo;évolution de la section précédente peut aussi être vu comme un opérateur de translation temporelle.\nOn peut le reconstruire sur le modèle des translations spatiales en partant d\u0026rsquo;une petite variation $\\delta t_a$ dans la fonction d\u0026rsquo;onde :\n$$ \\psi\\left(t-\\delta t_a\\right)=\\psi(t)-\\frac{\\mathrm{d} \\psi(t)}{\\mathrm{d} t} \\delta t_a+\\ldots $$\nEt comme $\\hat{H}=\\mathrm{i} \\frac{\\mathrm{~d}}{\\mathrm{~d} t}$, on obtient :\n$$ \\psi\\left(t-\\delta t_a\\right)=\\left(1+\\mathrm{i} \\hat{H} \\delta t_a\\right) \\psi(t) $$\nCe qui donne finalement :\n$$ \\hat{U}(t_a)=\\mathrm{e}^{\\mathrm{i}\\hat{H}t_a} $$\nOn peut dès lors combiner les deux translations en un seul opérateur de translation spatio-temporelle en définissant l\u0026rsquo;opérateur quadri-impulsion $\\hat{p}=(\\hat{H}, \\hat{\\boldsymbol{p}})$ :\n$$ \\hat{U}(a)=\\mathrm{e}^{\\mathrm{i} \\hat{p} \\cdot a}=\\mathrm{e}^{\\mathrm{i} \\hat{H} t_a-\\mathrm{i} \\hat{\\boldsymbol{p}} \\cdot \\boldsymbol{a}} $$\nRotations Une matrice de rotation $\\mathbf{R}(\\boldsymbol{\\theta})$ (où $\\boldsymbol{\\theta}$ est le vecteur dont l\u0026rsquo;axe est celui de la rotation et la norme est donnée par l\u0026rsquo;angle) agit sur une grandeur vectorielle comme l\u0026rsquo;impulsion : $\\boldsymbol{p}^{\\prime}=\\mathbf{R}(\\boldsymbol{\\theta}) \\boldsymbol{p}$. L\u0026rsquo;opérateur vectoriel associé peut se définir comme :\n$$ \\left|\\boldsymbol{p}^{\\prime}\\right\\rangle=\\hat{U}(\\boldsymbol{\\theta})|\\boldsymbol{p}\\rangle=|\\mathbf{R}(\\boldsymbol{\\theta}) \\boldsymbol{p}\\rangle $$\nL\u0026rsquo;opérateur possède à nouveau les propriétés clés attendues pour une telle transformation :\nunitarité\u0026nbsp;: $\\hat{U}^{\\dagger}(\\boldsymbol{\\theta}) \\hat{U}(\\boldsymbol{\\theta})=1$ présence d'un élément neutre\u0026nbsp;: $\\hat{U}(0)=1$ loi de composition interne\u0026nbsp;: $\\hat{U}(\\boldsymbol{\\theta}\\_1)\\hat{U}(\\boldsymbol{\\theta}\\_2)=\\hat{U}(\\boldsymbol{\\theta}\\_{12})$ où $\\mathbf{R}(\\boldsymbol{\\theta}\\_{12})=\\mathbf{R}(\\boldsymbol{\\theta}\\_1) \\mathbf{R}(\\boldsymbol{\\theta}\\_2)$ Preuve de l'unitarité\u0026nbsp;: $$ \\begin{aligned} \\hat{U}(\\boldsymbol{\\theta}) \\hat{U}^{\\dagger}(\\boldsymbol{\\theta}) \u0026amp; =\\hat{U}(\\boldsymbol{\\theta})\\left(\\int \\mathrm{d}^3 p|\\boldsymbol{p}\\rangle\\langle\\boldsymbol{p}|\\right) \\hat{U}^{\\dagger}(\\boldsymbol{\\theta}) \\\\ \u0026amp; =\\int \\mathrm{d}^3 p|\\mathbf{R}(\\boldsymbol{\\theta}) \\boldsymbol{p}\\rangle\\langle\\mathbf{R}(\\boldsymbol{\\theta}) \\boldsymbol{p}|\\\\ \u0026amp;=\\int \\mathrm{d}^3 \\boldsymbol{p}^{\\prime}|\\boldsymbol{p}^{\\prime}\\rangle\\langle\\boldsymbol{p}^{\\prime}|\\\\ \u0026amp;=1 \\end{aligned} $$\nPuisque $\\boldsymbol{p}^{\\prime}=\\mathbf{R}(\\boldsymbol{\\theta}) \\boldsymbol{p}$ et $\\mathrm{d}^3 p^{\\prime}=\\mathrm{d}^3 p$ car $\\operatorname{det} \\mathbf{R}(\\boldsymbol{\\theta})=1$ par conservation de l\u0026rsquo;orientation (et donc le jacobien vaut 1).\nCes opérateurs forment un nouveau groupe de Lie appelé le groupe des rotations.\nTranslations dans l\u0026rsquo;espace-temps, rotations et boosts de Lorentz et leurs combinaisons sont tous des éléments du groupe de Poincaré et peuvent tous être représentés par des opérateurs unitaires.\nLa rotation, non plus d\u0026rsquo;un état, mais d\u0026rsquo;un opérateur, est donnée par :\n$$ \\hat{U}^{\\dagger}(\\boldsymbol{\\theta}) \\,\\hat{\\boldsymbol{p}} \\,\\hat{U}(\\boldsymbol{\\theta})=\\mathbf{R}(\\boldsymbol{\\theta}) \\hat{\\boldsymbol{p}} $$\nCherchons là encore à exprimer explicitement l\u0026rsquo;opérateur en partant d\u0026rsquo;une petite rotation selon l\u0026rsquo;axe des $z$ d\u0026rsquo;une fonction d\u0026rsquo;onde :\n$$ \\psi\\left(\\theta^z-\\delta \\theta^z\\right)=\\psi\\left(\\theta^z\\right)-\\frac{\\mathrm{d} \\psi\\left(\\theta^z\\right)}{\\mathrm{d} \\theta^z} \\delta \\theta^z+\\ldots $$\nC\u0026rsquo;est maintenant l\u0026rsquo;opérateur moment angulaire qui va jouer le rôle de générateur de la transformation. Et dans notre cas, on utilise le fait que $\\hat{J}^z=-\\mathrm{i} \\frac{\\mathrm{~d}}{\\mathrm{~d} \\theta^z}$ :\n$$ \\psi\\left(\\theta-\\delta \\theta^z\\right)=\\left(1-\\mathrm{i} \\hat{J}^z \\delta \\theta^z\\right) \\psi\\left(\\theta^z\\right) $$\nEt finalement, en répétant $N\\rightarrow\\infty$ fois l\u0026rsquo;opération, l\u0026rsquo;opérateur s\u0026rsquo;écrit $\\hat{U}(\\theta^z)=\\mathrm{e}^{-\\mathrm{i} \\hat{J}^z \\theta^z}$. Et en généralisant à une rotation quelconque :\n$$ \\hat{U}(\\boldsymbol{\\theta})=\\mathrm{e}^{-\\mathrm{i} \\hat{\\boldsymbol{J}} \\cdot \\boldsymbol{\\theta}} $$\nReprésentation des transformations Transformer un champ est un poil plus compliqué puisqu\u0026rsquo;il faut à la fois bouger le champ au nouvel endroit et transformer aussi l\u0026rsquo;objet que le champ produit. Or cet objet peut être de nature différente : scalaire, vectorielle ou spinorielle. Il faut donc pouvoir adapter une même transformation à ces différents objets et c\u0026rsquo;est la théorie des représentations qui permet cela.\nUne représentation d\u0026rsquo;un groupe, notée $D$, est obtenue en associant chaque élément $g_i$ du groupe $G$ à un opérateur linéaire continu qui agit sur un espace vectoriel. Cette association doit préserver la règle de composition : si $g_1\\bullet g_2=g_3$, alors $D(g_1)D(g_2)=D(g_3)$.\nEn pratique, il s\u0026rsquo;agit de représenter les éléments du groupe par des matrices.\nExemple des rotations\u0026nbsp;: Toute rotation $\\boldsymbol{R}(\\boldsymbol{\\theta})$ peut être représentée par une matrice $D(\\boldsymbol{\\theta})$ qui prend une forme très similaire à l\u0026rsquo;opérateur :\n$$ D(\\boldsymbol{\\theta})=\\mathrm{e}^{-\\mathrm{i} \\boldsymbol{J} \\cdot \\boldsymbol{\\theta}} $$\n$\\boldsymbol{J}$ est une matrice carrée et une représentation de l\u0026rsquo;opérateur $\\hat{\\boldsymbol{J}}$.\nOn peut isoler $J^i$ dans l\u0026rsquo;équation précédente :\n$$ J^i=-\\left.\\frac{1}{\\mathrm{i}} \\frac{\\partial D\\left(\\theta^i\\right)}{\\partial \\theta^i}\\right|_{\\theta^i=0} $$\nOn note souvent les représentations des rotations d\u0026rsquo;un champ caractérisé par un nombre quantique de moment cinétique $j$, $D^{(j)}(\\boldsymbol{\\theta})$.\nConsidérons des rotations autour de l\u0026rsquo;axe des $z$.\nUne représentation triviale est $D(\\theta^z)=1$, et donc $J^z=-\\left.\\frac{1}{\\mathrm{i}} \\frac{\\partial D\\left(\\theta^z\\right)}{\\partial \\theta^z}\\right|\\_{\\theta^z=0}=0$. Et par extension, $J^x=J^y=0$. C'est une représentation appropriée pour un champ scalaire puisqu'un scalaire ne peut avoir de moment cinétique. On a donc obtenu la représentation des rotations d'un champ scalaire\u0026nbsp;: $D^{(0}(\\boldsymbol{\\theta})=1$. Pour un champ de spineurs (décrivant des particules de spin $\\frac{1}{2}$), la représentation d'une rotation selon l'axe des $z$ peut s'écrire\u0026nbsp;: $$ D^{1/2}\\left(\\theta^z\\right)=\\left(\\begin{array}{cc} \\mathrm{e}^{-\\mathrm{i} \\theta^z / 2} \u0026amp; 0 \\\\ 0 \u0026amp; \\mathrm{e}^{\\mathrm{i} \\theta^z / 2} \\end{array}\\right) $$\nEt donc\n$$ J^z=-\\left.\\frac{1}{\\mathrm{i}} \\frac{\\partial D^{1/2}\\left(\\theta^z\\right)}{\\partial \\theta^z}\\right|_{\\theta^z=0}=\\frac{1}{2}\\left(\\begin{array}{cc} 1 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \\end{array}\\right) $$\nPour un champ vectoriel, la représentation matricielle des rotations est plus familière. Pour une rotation selon l'axe $z$, on a\u0026nbsp;: $$ D^{(1)}(\\theta^z)=\\mathbf{R}\\left(\\theta^z\\right)=\\left(\\begin{array}{cccc} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\cos \\theta^z \u0026amp; -\\sin \\theta^z \u0026amp; 0 \\\\ 0 \u0026amp; \\sin \\theta^z \u0026amp; \\cos \\theta^z \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right) $$\nCe qui donne :\n$$ J^z=-\\left.\\frac{1}{\\mathrm{i}} \\frac{\\partial \\mathbf{R}\\left(\\theta^z\\right)}{\\partial \\theta^z}\\right|_{\\theta^z=0}=\\mathrm{i}\\left(\\begin{array}{cccc} 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right) $$\nLa formule générale pour les éléments de matrice de $D^{(j)}(\\boldsymbol{\\theta})$ est donnée par :\n$$ \\begin{aligned} {\\left[D^{(j)}(\\boldsymbol{\\theta})\\right]_{m, m^{\\prime}} } \u0026amp; =\\left\\langle j m^{\\prime}\\right| \\hat{U}(\\boldsymbol{\\theta})|j m\\rangle \\\\ \u0026amp; =\\left\\langle j m^{\\prime}\\right| \\mathrm{e}^{-\\mathrm{i} \\hat{\\boldsymbol{J}} \\cdot \\boldsymbol{\\theta}}|j m\\rangle \\end{aligned} $$\nOn peut ensuite les relations standards de l\u0026rsquo;opérateur moment cinétique :\n$$ \\begin{gathered} \\hat{J}^z|j \\, m\\rangle=m|j \\, m\\rangle, \\\\ \\hat{J}^{ \\pm}|j \\, m\\rangle=\\sqrt{(j \\mp m)(j+1 \\pm m)}|j \\,m \\pm 1\\rangle, \\\\ \\hat{J}^{ \\pm}=\\hat{J}^x \\pm \\mathrm{i} \\hat{J}^y . \\end{gathered} $$\nLe point remarquable de toutes ces représentations est qu\u0026rsquo;elles partagent la même structure algébrique sous-jacente de l\u0026rsquo;opérateur rotation. Cette algèbre est appelé algèbre de Lie et peut apparaître dès qu\u0026rsquo;on a un groupe continu.\nLa continuité entraîne en effet qu\u0026rsquo;il existe des éléments du groupe arbitrairement proches de l\u0026rsquo;identité pour lesquels on peut écrire $g(\\boldsymbol{\\alpha})=1+\\mathrm{i} \\alpha^i T^i+O(\\alpha^2)$ où les $T^i$ sont les générateurs du groupe. L\u0026rsquo;algèbre de Lie s\u0026rsquo;exprime alors par le commutateur $\\left[T^i, T^j\\right]=\\mathrm{i} f^{i j k} T^k$ où les $f^{ijk}$ sont appelées constantes de structure.\nPour les rotations $T^i = J^i$ et $f^{ijk}=\\varepsilon^{ijk}$.\nTransformations d\u0026rsquo;un champ quantique Commençons par translater un champ scalaire.\nSi on translate à la fois un état et un opérateur du même vecteur $\\boldsymbol{a}$, alors rien ne devrait changer :\n$$ \\langle\\boldsymbol{y}| \\hat{\\phi}(\\boldsymbol{x})|\\boldsymbol{y}\\rangle=\\langle\\boldsymbol{y}+\\boldsymbol{a}| \\hat{\\phi}(\\boldsymbol{x}+\\boldsymbol{a})|\\boldsymbol{y}+\\boldsymbol{a}\\rangle $$\nOr $|\\boldsymbol{y}+\\boldsymbol{a}\\rangle=\\hat{U}(\\boldsymbol{a})|\\boldsymbol{y}\\rangle$, donc $\\hat{\\phi}(\\boldsymbol{x}) = \\hat{U}^\\dagger (\\boldsymbol{a}) \\hat{\\phi}(\\boldsymbol{x}+\\boldsymbol{a}) \\hat{U}(\\boldsymbol{a})$ et par conséquent :\n$$ \\hat{U}^{\\dagger}(\\boldsymbol{a}) \\hat{\\phi}(\\boldsymbol{x}) \\hat{U}(\\boldsymbol{a})=\\hat{\\phi}(\\boldsymbol{x}-\\boldsymbol{a}) $$\nEn passant aux rotations, on doit se rappeler que transformer le champ agit à la fois sur le point sur lequel le champ agit et aussi sur la polarisation du champ. Et cette modification de la polarisation du champ se fait via la représentation appropriée de l\u0026rsquo;opérateur rotation (pour un champ scalaire, par exemple, $D(\\boldsymbol{\\theta})=1$).\n$$ \\hat{U}^{\\dagger}(\\boldsymbol{\\theta}) \\hat{\\boldsymbol{\\Phi}}(x) \\hat{U}(\\boldsymbol{\\theta})=D(\\boldsymbol{\\theta}) \\hat{\\boldsymbol{\\Phi}}\\left(\\mathbf{R}^{-1}(\\boldsymbol{\\theta}) x\\right) $$\nRemarque : dans le dessin, $D(\\boldsymbol{\\theta}) $ s\u0026rsquo;occupe de tourner la flèche rouge à sa nouvelle place.\nTransformations de Lorentz Après les translations et les rotations cherchons à exprimer les boosts de Lorentz.\nUne transformation de Lorentz selon l\u0026rsquo;axe $x$ est donnée par $x^{\\prime \\mu}=\\boldsymbol{\\Lambda}\\left(\\beta^1\\right)_{\\;\\nu}^\\mu x^\\nu$ où :\n$$ \\boldsymbol{\\Lambda}\\left(\\beta^1\\right)=\\left(\\begin{array}{cccc} \\gamma^1 \u0026amp; \\beta^1 \\gamma^1 \u0026amp; 0 \u0026amp; 0 \\\\ \\beta^1 \\gamma^1 \u0026amp; \\gamma^1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right) $$\nCette transformation connecte deux référentiels inertiels en mouvement relatif avec une vitesse $v=c\\beta^1$ selon $x$. En introduisant la rapidité $\\phi^i$ définie par $\\tanh \\phi^i=\\beta^i$, la matrice devient :\n$$ \\boldsymbol{\\Lambda}\\left(\\phi^1\\right)=\\left(\\begin{array}{cccc} \\cosh \\phi^1 \u0026amp; \\sinh \\phi^1 \u0026amp; 0 \u0026amp; 0 \\\\ \\sinh \\phi^1 \u0026amp; \\cosh \\phi^1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right) $$\nIntroduisons un opérateur pour cette transformation dans l\u0026rsquo;espace de Hilbert :\n$$ \\hat{U}(\\boldsymbol{\\phi})|\\boldsymbol{p}\\rangle=|\\boldsymbol{\\Lambda}(\\boldsymbol{\\phi}) \\boldsymbol{p}\\rangle $$\nEt sur le modèle des rotations, donnons une forme générale matricielle pour les représentations de la transformation de Lorentz :\n$$ D(\\phi)=\\mathrm{e}^{\\mathrm{i} \\boldsymbol{K} \\cdot \\boldsymbol{\\phi}} $$\nLes générateurs de la transformation sont donnés par :\n$$ K^i=\\left.\\frac{1}{\\mathrm{i}} \\frac{\\partial D\\left(\\phi^i\\right)}{\\partial \\phi^i}\\right|_{\\phi^i=0} $$\nUn champ quantique se transforme sous l\u0026rsquo;action d\u0026rsquo;un boost comme :\n$$ \\hat{U}^{\\dagger}(\\boldsymbol{\\phi}) \\hat{\\boldsymbol{\\Phi}}(x) \\hat{U}(\\boldsymbol{\\phi})=D(\\boldsymbol{\\phi}) \\hat{\\boldsymbol{\\Phi}}\\left(\\boldsymbol{\\Lambda}^{-1}(\\boldsymbol{\\phi}) x\\right) $$\nEnfin, en jouant avec un jeu de générateurs, on obtient la surprenante relation de commutation suivante :\n$$ \\left[K^1, K^2\\right]=K^1 K^2-K^2 K^1=-\\mathrm{i} J^3 $$\nLa différence entre un boost selon les $x$ suivi d\u0026rsquo;un boost selon les $y$ et un boost selon les $y$ suivi d\u0026rsquo;un boost selon les $x$ est une rotation autour des $z$ !\nMathématiquement, cela implique que l\u0026rsquo;algèbre de Lie des transformations de Lorentz n\u0026rsquo;est pas fermée et que ses générateurs ne forment pas un groupe.\nEn généralisant, les relations de commutations s\u0026rsquo;écrivent :\n$$ \\left[J^i, K^j\\right]=\\mathrm{i} \\varepsilon^{i j k} K^k $$\nPris ensemble, les boosts et les rotations forment une algèbre de Lie fermée et c\u0026rsquo;est ce groupe élargi qu\u0026rsquo;on appelle le groupe de Lorentz. Et de fait, une transformation générale de Lorentz s\u0026rsquo;écrit :\n$$ D(\\boldsymbol{\\theta}, \\boldsymbol{\\phi})=\\mathrm{e}^{-\\mathrm{i}(\\boldsymbol{J} \\cdot \\boldsymbol{\\theta}-\\boldsymbol{K} \\cdot \\boldsymbol{\\phi})} $$\nnote\nLes transformations $L$ du groupe de Lorentz sont l\u0026rsquo;ensemble des transformations linéaires de $\\mathbb{R}^{1,3}$ qui préservent la forme bilinéaire de Lorentz (ou pseudo-norme de Minkowski) : $(L(x),L(y))=(x,y)$\nEt en ajoutant les translations à la fête, on obtient le groupe de Poincaré.\nSymétrie Invariance et conservation Une quantité est invariante lorsqu\u0026rsquo;elle garde la même valeur après une transformation. On dit alors qu\u0026rsquo;elle présente une certaine symétrie. Un cylindre par exemple est invariant par rotation autour de son axe et on dit alors qu\u0026rsquo;il possède une symétrie de rotation autour de cet axe.\nUne quantité est conservée lorsqu\u0026rsquo;elle garde la même valeur avant et après un évènement. Dans une collision entre particules par exemple, la quadri-impulsion est conservée dans un référentiel donné.\nCe sont ces deux notions différentes que le théorème de Noether lie entre elles en stipulant qu\u0026rsquo;une invariance conduit à une loi de conservation.\nParamétrons par une quantité $\\lambda$ les variations du champ $\\phi(x^\\mu)$ soumis à une transformation continue.\nPour une transformation infinitésimale, on va noter :\n$$ D \\phi=\\left.\\frac{\\partial \\phi}{\\partial \\lambda}\\right|_{\\lambda=0} $$\nde telle façon que le changement infinitésimal $\\delta\\phi$ du champ induit par $\\delta\\lambda$ puisse se noter :\n$$ \\delta \\phi=D \\phi \\delta \\lambda $$\nPour une translation d\u0026rsquo;un quadrivecteur $a^\\mu$, par exemple, on peut écrire :\n$$ \\phi\\left(x^\\mu\\right) \\rightarrow \\phi\\left(x^\\mu+\\lambda a^\\mu\\right) $$\nEt en posant $y^\\mu=x^\\mu+\\lambda a^\\mu$, on obtient :\n$$ \\frac{\\partial \\phi}{\\partial \\lambda}=\\frac{\\partial \\phi}{\\partial y^\\mu} \\frac{\\partial y^\\mu}{\\partial \\lambda}=\\frac{\\partial \\phi}{\\partial y^\\mu} a^\\mu $$\nEt en prenant la limite $\\lambda\\rightarrow 0$, on a finalement :\n$$ D \\phi=a^\\mu \\partial_\\mu \\phi $$\nThéorème de Noether Considérons une petite variation du champ $\\phi(x)\\rightarrow\\phi(x)+\\delta\\phi(x)$ où $\\delta\\phi(x) = 0$ sur les bords de la région d\u0026rsquo;espace-temps considérée (ce sont les conditions aux limites de Dirichlet imposant un champ fixe sur les bords permettant d\u0026rsquo;avoir un principe variationnel bien défini).\nLa petite variation de la densité lagrangienne $\\mathcal{L}$ s\u0026rsquo;écrit :\n$$ \\delta \\mathcal{L}=\\frac{\\partial \\mathcal{L}}{\\partial \\phi} \\delta \\phi+\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_\\mu \\phi\\right)} \\delta\\left(\\partial_\\mu \\phi\\right) $$\nPosons :\n$$ \\Pi^\\mu(x)=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_\\mu \\phi\\right)} $$\n$\\Pi^\\mu(x)$ est la densité d\u0026rsquo;impulsion.\nC\u0026rsquo;est une généralisation quadrivectorielle du moment conjugué $\\pi(x)=\\delta \\mathcal{L} / \\delta \\dot{\\phi}$. Et d\u0026rsquo;ailleurs, le moment conjugué est la composante temporelle de la densité d\u0026rsquo;impulsion : $\\Pi^0(x)=\\pi(x)$.\nLa variation de la densité lagrangienne peut maintenant s\u0026rsquo;écrire :\n$$ \\delta \\mathcal{L}=\\frac{\\partial \\mathcal{L}}{\\partial \\phi} \\delta \\phi+\\Pi^\\mu \\delta\\left(\\partial_\\mu \\phi\\right) $$\nEn utilisant $\\delta\\left(\\partial_\\mu \\phi\\right)=\\partial_\\mu(\\delta \\phi)$ et $\\partial_\\mu\\left(\\Pi^\\mu \\delta \\phi\\right)=\\Pi^\\mu \\partial_\\mu(\\delta \\phi)+\\left(\\partial_\\mu \\Pi^\\mu\\right) \\delta \\phi$, on obtient :\n$$ \\delta \\mathcal{L}=\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\phi}-\\partial_\\mu \\Pi^\\mu\\right) \\delta \\phi+\\partial_\\mu\\left(\\Pi^\\mu \\delta \\phi\\right) $$\nSupposons que l\u0026rsquo;action n\u0026rsquo;est pas modifiée par la transformation :\n$$ \\begin{aligned} \\delta S \u0026amp;= \\int \\mathrm{d}^4 x\\, \\delta \\mathcal{L}\\\\ \u0026amp;=\\int \\mathrm{d}^4 x\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\phi}-\\partial_\\mu \\Pi^\\mu\\right) \\delta \\phi+\\cancel{\\int \\mathrm{d}^4 x\\, \\partial_\\mu\\left(\\Pi^\\mu \\delta \\phi\\right)}\\\\ \u0026amp;=0 \\end{aligned} $$\nPourquoi $\\int \\mathrm{d}^4 x\\, \\partial_\\mu\\left(\\Pi^\\mu \\delta \\phi\\right)=0$ ?\nLe théorème de la divergence (Gauss-Ostrogradski) nous dit que la variation d\u0026rsquo;une quantité dans un volume vaut le flux de cette quantité à travers les parois du volume :\n$$ \\int_\\mathcal{V} \\mathrm{d}^4 x\\, \\partial_\\mu\\left(\\Pi^\\mu \\delta \\phi\\right) = \\int_\\mathcal{\\partial V} \\mathrm{d}\\mathcal{A} \\ n_\\mu \\left(\\Pi^\\mu \\delta \\phi\\right) $$\noù $n_\\mu$ est le vecteur normal à la surface $\\mathcal{A}$ qui délimite le volume d\u0026rsquo;intégration.\nOr, comme stipulée plus haut, les variations du champ $\\delta\\phi$ s\u0026rsquo;évanouissent sur la frontière.\nPar conséquent, on obtient :\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\phi}=\\partial_\\mu \\Pi^\\mu $$\nC\u0026rsquo;est l\u0026rsquo;équation d\u0026rsquo;Euler-Lagrange.\nPartons maintenant du fait que le champ respecte l\u0026rsquo;équation du mouvement :\n$$ \\delta\\mathcal{L}=\\cancel{\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\phi}-\\partial_\\mu \\Pi^\\mu\\right) \\delta \\phi}+\\partial_\\mu\\left(\\Pi^\\mu \\delta \\phi\\right) $$\nEn utilisant $\\delta \\phi=D \\phi \\delta \\lambda$, on obtient :\n$$ \\delta \\mathcal{L}=\\partial_\\mu\\left(\\Pi^\\mu D \\phi\\right) \\delta \\lambda $$\nComme la transformation est sensée être une symétrie du système, son action doit restée inchangée. Cela implique une densité lagrangienne inchangée à la quadri-divergence d\u0026rsquo;une fonction $W^\\mu(x)$ près :\n$$ \\delta \\mathcal{L}=\\left(\\partial_\\mu W^\\mu\\right) \\delta \\lambda $$\nEn effet, l\u0026rsquo;intégration sur l\u0026rsquo;espace d\u0026rsquo;une divergence totale va donner une constante. L\u0026rsquo;action sera donc la même à une constante près, ce qui sera sans effet sur ses variations (de même qu\u0026rsquo;en mécanique des particules, le lagrangien est défini à une dérivée totale du temps près).\nFinalement, une action stationnaire implique :\n$$ \\partial_\\mu\\left(\\Pi^\\mu D \\phi-W^\\mu\\right)=0 $$\nOu encore $\\partial_\\mu J_{\\mathrm{N}}^\\mu=0$ où\n$$ J_{\\mathrm{N}}^\\mu(x)=\\Pi^\\mu(x) D \\phi(x)-W^\\mu(x) $$\nest le courant de Noether.\nLe courant de Noether est donc conservé localement.\nThéorème de Noether :\nSi une transformation de symétrie continue $\\phi\\rightarrow\\phi+D\\phi$ ne change $\\mathcal{L}$ que par l\u0026rsquo;addition d\u0026rsquo;une quadridivergence ($D\\mathcal{L}=\\partial_\\mu W^\\mu$) pour un $\\phi$ arbitraire, alors cela implique l\u0026rsquo;existence d\u0026rsquo;un courant $J_{\\mathrm{N}}^\\mu(x)=\\Pi^\\mu(x) D \\phi(x)-W^\\mu(x)$.\nSi $\\phi$ obéit aux équations du mouvement, alors le courant est conservé : $\\partial_\\mu J_{\\mathrm{N}}^\\mu=0$.\nLes champs conservés donnent naissance à des charges conservées $Q_{\\mathrm{N}}=\\int J_{\\mathrm{N}}^\\mu \\,\\mathrm{d} \\mathcal{A}_\\mu$ aussi appelées charges de Noether.\nEn effet, si on fixe le temps, la \u0026ldquo;surface\u0026rdquo; d\u0026rsquo;intégration devient le volume tridimensionnel et donc : $Q_{\\mathrm{N}}=\\int \\mathrm{d}^3 x J_{\\mathrm{N}}^0$ où $J^0_N$ est la composante temporelle (normale à la surface).\nEt comme $\\partial_\\mu J^\\mu_N=0$,\n$$ \\int \\mathrm{d}^3 x\\left(\\partial_\\mu J_{\\mathrm{N}}^\\mu\\right)=\\int \\mathrm{d}^3 x\\left(\\partial_0 J_{\\mathrm{N}}^0+\\partial_k J_{\\mathrm{N}}^k\\right)=0 $$\nEn utilisant le théorème de la divergence, le second terme devient :\n$$ \\int \\mathrm{d}^3 x \\,\\partial_k J_{\\mathrm{N}}^k=\\int \\mathrm{d} \\mathcal{A}_k J_{\\mathrm{N}}^k $$\nEt il disparaît si le volume est assez grand.\nOn a donc finalement :\n$$ \\int \\mathrm{d}^3 x\\, \\partial_0 J_{\\mathrm{N}}^0=\\frac{\\mathrm{d} Q_{\\mathrm{N}} }{ \\mathrm{d} t} = 0 $$\nÇa correspond bien à une charge conservée.\nRecette pour trouver des charges conservées à partir du théorème de Noether :\ndéterminer $D \\phi=\\left.\\frac{\\partial \\phi}{\\partial \\lambda}\\right|_{\\lambda \\rightarrow 0}$ déterminer $\\Pi^\\mu(x)=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_\\mu \\phi\\right)}$ déterminer $\\partial_\\mu W^\\mu=D \\mathcal{L}$ écrire $J_{\\mathrm{N}}^\\mu=D \\phi \\Pi^\\mu-W^\\mu$ déterminer $Q_{\\mathrm{N}}=\\int \\mathrm{d}^3 x J_{\\mathrm{N}}^0$ Application : translations de l\u0026rsquo;espace-temps Supposons une translation de l\u0026rsquo;espace-temps $x^{\\prime \\mu}=x^\\mu+a^\\mu$ qui nous donne $D \\phi=a^\\mu \\partial_\\mu \\phi$. On a aussi $D \\mathcal{L}=a^\\mu \\partial_\\mu \\mathcal{L}=\\partial_\\mu\\left(a^\\mu \\mathcal{L}\\right)$.\nOn reconnait alors que $D \\mathcal{L}=\\partial_\\mu W^\\mu$ avec $W^\\mu=a^\\mu \\mathcal{L}$. Le courant conservé s\u0026rsquo;en déduit :\n$$ \\begin{aligned} J_{\\mathrm{N}}^\\mu \u0026amp; =\\Pi^\\mu D \\phi-W^\\mu \\\\ \u0026amp; =\\Pi^\\mu a^\\nu \\partial_\\nu \\phi-a^\\mu \\mathcal{L} \\\\ \u0026amp; =a^\\nu\\left[\\Pi^\\mu \\partial_\\nu \\phi-\\delta_\\nu^\\mu \\mathcal{L}\\right] \\\\ \u0026amp; =a_\\nu T^{\\mu \\nu} \\end{aligned} $$\noù $T^{\\mu \\nu}=\\Pi^\\mu \\partial^\\nu \\phi-g^{\\mu \\nu} \\mathcal{L}$ est le tenseur énergie-impulsion.\nLa charge conservée correspondante peut s\u0026rsquo;écrire :\n$$ P^\\alpha=\\int \\mathrm{d}^3 x \\,T^{0 \\alpha} $$\nLa composante temporelle de cette charge est :\n$$ P^0=\\int \\mathrm{d}^3 x \\,T^{00}=\\int \\mathrm{d}^3 x[\\pi(x) \\dot{\\phi}(x)-\\mathcal{L}(x)]=\\int \\mathrm{d}^3 x\\, \\mathcal{H} $$\nOn reconnaît l\u0026rsquo;énergie du champ (on a utilisé $g^{00} = 1$).\nEt les composantes spatiales nous donnent :\n$$ P^k=\\int \\mathrm{d}^3 x\\, T^{0 k}=\\int \\mathrm{d}^3 x\\, \\pi(x) \\partial^k \\phi(x) $$\nOn reconnaît là l\u0026rsquo;impulsion du champ (on a utilisé $g^{0k}=0$).\nUn champ symétrique par rapport aux translations voit son énergie et son impulsion conservées.\nnote\nPetit article de Quantamagazine sur le théorème de Noether.\nSymétries internes Pour des champs plus complexes que des champs scalaires réels, en plus des symétries de l\u0026rsquo;espace-temps, il faudra se préoccuper de la façon dont les symétries affecte les champ eux-mêmes. On verra par exemple un peu plus loin que la symétrie $U(1)$ d\u0026rsquo;un champ scalaire complexe entraîne la conservation du nombre de particules !\nChapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc4/",
	"title": "TQC-4",
	"tags": [],
	"description": "",
	"content": " Théorie quantique des champs \u0026ndash; Partie 4 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Très souvent une simple traduction.\nRetour sommaire\nLa recette de la quantification canonique des champs La théorie quantique des champs nous permet de décrire un monde où des particules indistinguables peuvent être créées ou détruites lors d\u0026rsquo;interactions (entre elles ou avec des entités extérieures), un monde foncièrement non linéaire donc.\nL\u0026rsquo;idée est de considérer les particules comme de simples excitations de champs quantiques obtenus en quantifiant leurs alter ego classiques.\nPour obtenir une théorie quantique des champs à partir d\u0026rsquo;une théorie des champs classique, on suit la méthode suivante :\nÉtape 1\u0026nbsp;: écrire la densité lagrangienne classique en termes de champs. C'est la partie \"créative\", le reste est algorithmisé. Étape 2\u0026nbsp;: calculer la densité d'impulsion et déterminer la densité hamiltonienne en termes de champs. Étape 3\u0026nbsp;: considérer les champs et la densité d'impulsion comme des opérateurs et leur appliquer les relations de commutation pour les rendre quantique. Étape 4\u0026nbsp;: décomposer les champs en termes d'opérateurs de création et d'annihilation. Étape 5\u0026nbsp;: respecter l'ordre normal pour se débarrasser des infinis. Déroulons la recette sur des exemples pour voir la quantification en action.\nQuantification canonique d\u0026rsquo;un champ scalaire réel Application de la recette Étape 1 : Le Lagrangien :\n$$ \\mathcal{L}=\\frac{1}{2}\\left[\\partial_\\mu \\phi(x)\\right]^2-\\frac{1}{2} m^2[\\phi(x)]^2 $$\nÉtape 2 : La densité d\u0026rsquo;impulsion :\n$$ \\Pi^\\mu(x)=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_\\mu \\phi(x)\\right)} $$\nIci, cela donne $\\Pi^\\mu(x)=\\partial^\\mu \\phi(x)$ avec pour composante temporelle $\\Pi^0(x)=\\pi(x)=\\partial^0 \\phi(x)$ (on utilise la métrique $(+- - -)$).\nOn peut maintenant écrire l\u0026rsquo;Hamiltonien :\n$$ \\begin{aligned} \\mathcal{H}\u0026amp;=\\Pi^0(x) \\partial_0 \\phi(x)-\\mathcal{L}\\\\ \u0026amp;=\\partial^0 \\phi(x) \\partial_0 \\phi(x)-\\mathcal{L} \\end{aligned} $$\nOn obtient :\n$$ \\mathcal{H}=\\frac{1}{2}\\left[\\partial_0 \\phi(x)\\right]^2+\\frac{1}{2}[\\nabla \\phi(x)]^2+\\frac{1}{2} m^2[\\phi(x)]^2 $$\nCet Hamiltonien est très mignon. On y retrouve la somme d\u0026rsquo;une énergie cinétique (coût d\u0026rsquo;une variation temporelle du champ) et d\u0026rsquo;une énergie potentielle, elle-même décomposée en deux termes : un terme de gradient (coût d\u0026rsquo;une variation spatiale du champ) et un terme de masse (coût d\u0026rsquo;avoir un champ plutôt que rien).\nÉtape 3 : On promeut les champs en opérateurs : $\\phi(x) \\rightarrow \\hat{\\phi}(x)$ et $\\Pi^0(x) \\rightarrow \\hat{\\Pi}^0(x)$.\nEt pour les rendre quantiques, on leur impose des relations de commutation. En mécanique quantique à une particule, on a $[\\hat{x}, \\hat{p}]=\\mathrm{i} \\hbar$. Par analogie, on définit le commutateur à temps égaux pour les opérateurs champ :\n$$ \\left[\\hat{\\phi}(t, \\boldsymbol{x}), \\hat{\\Pi}^0(t, \\boldsymbol{y})\\right]=\\mathrm{i} \\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) $$\nSi les instants sont différents, les champs commutent, et on a aussi $[\\hat{\\phi}(x), \\hat{\\phi}(y)]=\\left[\\hat{\\Pi}^0(x), \\hat{\\Pi}^0(y)\\right]=0$.\nExprimée à partir de ces champs, la densité hamiltonienne $\\mathcal{H}$ se mue en opérateur $\\hat{\\mathcal{H}}$ agissant sur les vecteurs d\u0026rsquo;état.\nMais on ne sait pas encore comment un opérateur comme $\\hat{\\phi}(x)$ agit sur un état nombre d\u0026rsquo;occupation $|n_1n_2n_3\\ldots\\rangle$. Par contre, on sait comment les opérateurs de création et d\u0026rsquo;annihilation, eux, agissent sur ces vecteurs\u0026hellip;\nÉtape 4 : Décomposons les opérateurs de champ en termes d\u0026rsquo;opérateurs de création et d\u0026rsquo;annihilation.\nRetour au premier chapitre, au moment d\u0026rsquo;évoquer les oscillateurs couplés : en combinant la décomposition en modes de Fourier de $x_j$, $x_j=\\frac{1}{\\sqrt{N}} \\sum_k \\tilde{x}_k \\mathrm{e}^{\\mathrm{i}kj a}$ et l\u0026rsquo;écriture de l\u0026rsquo;opérateur correspondant à un de ces modes en fonction des opérateurs de création et d\u0026rsquo;annihilation, $\\hat{\\tilde{x}}_k=\\sqrt{\\frac{\\hbar}{2 m \\omega_k}}\\left(\\hat{a}_k+\\hat{a}_{-k}^{\\dagger}\\right)$, on obtient :\n$$ \\hat{x}_j=\\left(\\frac{\\hbar}{m}\\right)^{\\frac{1}{2}} \\sum_k \\frac{1}{\\left(2 \\omega_k N\\right)^{\\frac{1}{2}}}\\left[\\hat{a}_k \\mathrm{e}^{\\mathrm{i} k j a}+\\hat{a}_k^{\\dagger} \\mathrm{e}^{-\\mathrm{i} k j a}\\right] $$\nPar analogie, on peut écrire la version continue d\u0026rsquo;un opérateur champ :\n$$ \\hat{\\phi}(\\boldsymbol{x})=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}\\left(\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{\\mathrm{i} \\boldsymbol{p} \\cdot \\boldsymbol{x}}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{-\\mathrm{i} \\boldsymbol{p} \\cdot \\boldsymbol{x}}\\right) $$\noù on est passé de $\\boldsymbol{k}$ à $\\boldsymbol{p}$ pour les moments, et de $\\omega_{\\boldsymbol{k}}$ à $E_\\boldsymbol{p}=\\left(\\boldsymbol{p}^2+m^2\\right)^{\\frac{1}{2}}$ pour l\u0026rsquo;énergie.\nEt comme avant, la relation de commutation entre les opérateurs de création et d\u0026rsquo;annihilation est : $\\left[\\hat{a}_{\\boldsymbol{p}}, \\hat{a}_{\\boldsymbol{q}}^{\\dagger}\\right]=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q})$.\nnote\nL\u0026rsquo;ensemble des quadri-impulsions $p$ satisfaisant la relation de dispersion relativiste $p^2=m^2$ forme ce qu\u0026rsquo;on appelle la \u0026ldquo;coquille de masse\u0026rdquo; (masse shell). C\u0026rsquo;est l\u0026rsquo;équivalent dans l\u0026rsquo;espace de Minkowkski de la sphère dans l\u0026rsquo;espace euclidien et elle forme un hyperboloïde de révolution.\nOn obtient la mesure invariante de Lorentz $ \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}$ en restreignant la mesure naturelle de Lebesgue $ \\frac{\\mathrm{d}^4 p}{(2 \\pi)^{4}}$ à la coquille de masse.\nIl faudrait maintenant ajouter une dépendance temporelle à notre champ, i.e. le rendre dynamique. Appliquons la méthode d\u0026rsquo;Heisenberg :\n$$ \\hat{\\phi}(x)=\\hat{\\phi}(t, \\boldsymbol{x})=\\hat{U}^{\\dagger}(t, 0) \\hat{\\phi}(\\boldsymbol{x}) \\hat{U}(t, 0)=\\mathrm{e}^{\\mathrm{i} \\hat{H} t} \\hat{\\phi}(\\boldsymbol{x}) \\mathrm{e}^{-\\mathrm{i} \\hat{H} t} $$\nSeuls les opérateurs de création et annihilation sont affectés par l\u0026rsquo;opérateur d\u0026rsquo;évolution $\\hat{U}(t, 0)=\\mathrm{e}^{-\\mathrm{i} \\hat{H} t}$ :\n$$ \\begin{aligned} \\hat{U}^{\\dagger}(t, 0) \\hat{a}_{\\boldsymbol{p}} \\hat{U}(t, 0)\u0026amp;=\\mathrm{e}^{-\\mathrm{i} E_p t} \\hat{a}_{\\boldsymbol{p}}\\\\ \\hat{U}^{\\dagger}(t, 0) \\hat{a}^\\dagger_{\\boldsymbol{p}} \\hat{U}(t, 0)\u0026amp;=\\mathrm{e}^{\\mathrm{i} E_p t} \\hat{a}^\\dagger_{\\boldsymbol{p}} \\end{aligned} $$\nPreuve\u0026nbsp;: Convainquons-nous sur un cas simplifié :\n$$ \\begin{aligned} \u0026amp; \\mathrm{e}^{\\mathrm{i} \\hat{H} t} \\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{x}} \\mathrm{e}^{-\\mathrm{i} \\hat{H} t}\\left|n_{\\boldsymbol{p}} n_{\\boldsymbol{q}} n_{\\boldsymbol{r}}\\right\\rangle \\\\ = \u0026amp; \\mathrm{e}^{\\mathrm{i} \\hat{H} t} \\hat{a}_{\\boldsymbol{q}}\\left|n_{\\boldsymbol{p}} n_{\\boldsymbol{q}} n_{\\boldsymbol{r}}\\right\\rangle \\mathrm{e}^{\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{x}} \\mathrm{e}^{-\\mathrm{i}\\left(n_{\\boldsymbol{p}} E_{\\boldsymbol{p}}+n_{\\boldsymbol{q}} E_{\\boldsymbol{q}}+n_{\\boldsymbol{r}} E_{\\boldsymbol{r}}\\right) t} \\\\ = \u0026amp; \\sqrt{n_{\\boldsymbol{q}}} \\mathrm{e}^{\\mathrm{i} \\hat{H} t}\\left|n_{\\boldsymbol{p}}\\left(n_{\\boldsymbol{q}}-1\\right) n_{\\boldsymbol{r}}\\right\\rangle \\mathrm{e}^{\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{x}} \\mathrm{e}^{-\\mathrm{i}\\left(n_{\\boldsymbol{p}} E_{\\boldsymbol{p}}+n_{\\boldsymbol{q}} E_{\\boldsymbol{q}}+n_{\\boldsymbol{r}} E_{\\boldsymbol{r}}\\right) t} \\\\ = \u0026amp; \\sqrt{n_{\\boldsymbol{q}}}\\left|n_{\\boldsymbol{p}}\\left(n_{\\boldsymbol{q}}-1\\right) n_{\\boldsymbol{r}}\\right\\rangle \\mathrm{e}^{\\mathrm{i}\\left(n_{\\boldsymbol{p}} E_{\\boldsymbol{p}}+\\left(n_{\\boldsymbol{q}}-1\\right) E_{\\boldsymbol{q}}+n_{\\boldsymbol{r}} E_{\\boldsymbol{r}}\\right) t} \\mathrm{e}^{\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{x}} \\mathrm{e}^{-\\mathrm{i}\\left(n_{\\boldsymbol{p}} E_{\\boldsymbol{p}}+n_{\\boldsymbol{q}} E_{\\boldsymbol{q}}+n_{\\boldsymbol{r}} E_{\\boldsymbol{r}}\\right) t} \\\\ = \u0026amp; \\sqrt{n_{\\boldsymbol{q}}}\\left|n_{\\boldsymbol{p}}\\left(n_{\\boldsymbol{q}}-1\\right) n_{\\boldsymbol{r}}\\right\\rangle \\mathrm{e}^{-\\mathrm{i} E_{\\boldsymbol{q}} t} \\mathrm{e}^{\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{x}} . \\end{aligned} $$\nAvec $\\hat{a}_\\boldsymbol{q}$ seul, on aurait obtenu $\\sqrt{n_{\\boldsymbol{q}}}\\left|n_{\\boldsymbol{p}}\\left(n_{\\boldsymbol{q}}-1\\right) n_{\\boldsymbol{r}}\\right\\rangle$. Rendre l\u0026rsquo;opérateur dynamique a pour effet de multiplier le résultat par un facteur $\\mathrm{e}^{\\mathrm{i}E_\\boldsymbol{q}t}$.\nOn obtient au bout du compte :\n$$ \\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{-\\mathrm{i}\\left(E_{\\boldsymbol{q}} t-\\boldsymbol{q} \\cdot \\boldsymbol{x}\\right)}=\\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{-\\mathrm{i} q \\cdot x} $$\nAu final, la décomposition en modes du champ scalaire est donnée par :\n$$ \\hat{\\phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}\\left(\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\navec $E_{\\boldsymbol{p}}=+\\left(\\boldsymbol{p}^2+m^2\\right)^{\\frac{1}{2}}$\nLa décomposition du champ position nous offre en prime celle du champ impulsion puisque $\\Pi^\\mu(x)=\\partial^\\mu \\phi(x)$.\nCela va permettre de valider à posteriori les facteurs de normalisation\u0026hellip;\nL\u0026rsquo;intensité d\u0026rsquo;impulsion est donnée par :\n$$ \\hat{\\Pi}^\\mu(x)=\\partial^\\mu \\hat{\\phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}\\left(2 E_\\boldsymbol{p}\\right)^{\\frac{1}{2}}}\\left(-\\mathrm{i} p^\\mu\\right)\\left(\\hat{a}_\\boldsymbol{p} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}-\\hat{a}_\\boldsymbol{p}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nEt sa composante temporelle vaut :\n$$ \\hat{\\Pi}^0(x) =\\partial^0 \\hat{\\phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_\\boldsymbol{p}\\right)^{\\frac{1}{2}}}\\left(-\\mathrm{i}E_{\\boldsymbol{p}}\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\mathrm{i}E_{\\boldsymbol{p}}\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nLe commutateur donne alors :\n$$ \\begin{aligned} \\left[\\hat{\\phi}(x),\\hat{\\Pi}^0(x)\\right] \u0026amp;= \\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{\\mathrm{d}^3 q}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(4 E_{\\boldsymbol{p}}E_{\\boldsymbol{q}}\\right)^\\frac{1}{2}}\\left[\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}, -\\mathrm{i}E_{\\boldsymbol{q}}\\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{-\\mathrm{i} q \\cdot y}+\\mathrm{i}E_{\\boldsymbol{q}}\\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} q \\cdot y}\\right]\\\\ \u0026amp;=\\int\\frac{\\mathrm{d}^3 p\\; \\mathrm{d}^3 q}{2(2 \\pi)^{3}\\sqrt{ E_{\\boldsymbol{p}}E_{\\boldsymbol{q}}}}\\left(\\mathrm{i}E_{\\boldsymbol{q}}[\\hat{a}_{\\boldsymbol{p}},\\hat{a}^\\dagger_{\\boldsymbol{q}}]\\mathrm{e}^{-\\mathrm{i} p \\cdot x + \\mathrm{i} q \\cdot y} - \\mathrm{i}E_{\\boldsymbol{q}}[\\hat{a}^\\dagger_{\\boldsymbol{p}},\\hat{a}_{\\boldsymbol{q}}]\\mathrm{e}^{\\mathrm{i} p \\cdot x - \\mathrm{i} q \\cdot y}\\right) \\end{aligned} $$\nSéparons composantes spatiales et temporelles (en prenant des temps égaux pour les deux champs) et utilisons$\\left[\\hat{a}_{\\boldsymbol{p}}, \\hat{a}_{\\boldsymbol{q}}^{\\dagger}\\right]=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q})$ :\n$$ \\begin{aligned} \\left[\\hat{\\phi}(\\boldsymbol{x},t),\\hat{\\Pi}^0(\\boldsymbol{y},t)\\right] \u0026amp;=i\\int\\frac{\\mathrm{d}^3 p\\; \\mathrm{d}^3 q}{2(2 \\pi)^{3}\\sqrt{ E_{\\boldsymbol{p}}E_{\\boldsymbol{q}}}}E_{\\boldsymbol{q}}\\,\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q}) \\mathrm{e}^{\\mathrm{i}t(E_\\boldsymbol{q}-E_\\boldsymbol{q})+\\mathrm{i} \\boldsymbol{p} \\cdot \\boldsymbol{x} - \\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{y}} - i\\int\\frac{\\mathrm{d}^3 p\\; \\mathrm{d}^3 q}{2(2 \\pi)^{3}\\sqrt{ E_{\\boldsymbol{p}}E_{\\boldsymbol{q}}}}E_{\\boldsymbol{q}}(-\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q})) \\mathrm{e}^{\\mathrm{i}t(E_\\boldsymbol{p}-E_\\boldsymbol{q})-\\mathrm{i} \\boldsymbol{p} \\cdot \\boldsymbol{x} + \\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{y}}\\\\ \u0026amp;=\\frac{i}{2}\\int\\frac{\\mathrm{d}^3 p } {(2 \\pi)^{3}}\\mathrm{e}^{\\mathrm{i} \\boldsymbol{p} \\cdot (\\boldsymbol{x}-\\boldsymbol{y})} +\\frac{i}{2}\\int\\frac{\\mathrm{d}^3 p } {(2 \\pi)^{3}}\\mathrm{e}^{\\mathrm{i} \\boldsymbol{p} \\cdot (\\boldsymbol{y}-\\boldsymbol{x})}\\\\ \u0026amp;=i\\delta(\\boldsymbol{x}-\\boldsymbol{y}) \\end{aligned} $$\nLes facteurs de normalisation dans la formule de décomposition du champ $\\hat{\\phi}(x)$ permettent donc de retrouver la relation de commutation à temps égaux attendue !\nÉnergie infinie ? Au tour de l\u0026rsquo;Hamiltonien de subir la quantification. Il va suffire d\u0026rsquo;y substituer la décomposition en modes de $\\hat{\\phi}(x)$.\nL\u0026rsquo;Hamiltonien est donné par l\u0026rsquo;intégrale sur le volume de la densité hamiltonienne :\n$$ \\hat{H}=\\int \\mathrm{d}^3 x \\frac{1}{2}\\left\\{\\left[\\partial_0 \\hat{\\phi}(x)\\right]^2+[\\boldsymbol{\\nabla} \\hat{\\phi}(x)]^2+m^2[\\hat{\\phi}(x)]^2\\right\\} $$\nRenotons la densité d\u0026rsquo;impulsion :\n$$ \\hat{\\Pi}_\\mu(x)=\\partial_\\mu \\hat{\\phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}\\left(2 E_\\boldsymbol{p}\\right)^{\\frac{1}{2}}}\\left(-\\mathrm{i} p_\\mu\\right)\\left(\\hat{a}_\\boldsymbol{p} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}-\\hat{a}_\\boldsymbol{p}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nEt sa composante temporelle vaut :\n$$ \\partial_0 \\hat{\\phi}(x)=\\int \\frac{d^3 p}{(2 \\pi)^{\\frac{3}{2}}\\left(2 E_\\boldsymbol{p}\\right)^{\\frac{1}{2}}}\\left(-\\mathrm{i} E_\\boldsymbol{p}\\right)\\left(\\hat{a}_\\boldsymbol{p} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}-\\hat{a}_\\boldsymbol{p}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nÉcrivons maintenant sa composante spatiale :\n$$ \\boldsymbol{\\nabla} \\hat{\\phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}(\\mathrm{i} \\boldsymbol{p})\\left(\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}-\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nOn a ainsi tous les ingrédients pour calculer l\u0026rsquo;Hamiltonien :\n$$ \\hat{H}= \\frac{1}{2} \\int^{} \\frac{\\mathrm{~d}^3 x \\mathrm{~d}^3 p \\mathrm{~d}^3 q}{(2 \\pi)^3\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}\\left(2 E_{\\boldsymbol{q}}\\right)^{\\frac{1}{2}}} \\left[ (-E_{\\boldsymbol{p}} E_{\\boldsymbol{q}}-\\boldsymbol{p} \\cdot \\boldsymbol{q})[\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}-\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}] \\times [\\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{-\\mathrm{i} q \\cdot x}-\\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} q \\cdot x}]+m^2 [\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}] [\\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{-\\mathrm{i} q \\cdot x}+\\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} q \\cdot x}] \\right] $$\nOn commence par l\u0026rsquo;intégrations sur les $x$ en utilisant $\\int \\mathrm{d}^3 x \\mathrm{e}^{\\mathrm{i} \\boldsymbol{p} \\cdot \\boldsymbol{x}}=(2 \\pi)^3 \\delta^{(3)}(\\boldsymbol{p})$ :\n$$ \\hat{H}= \\frac{1}{2} \\int \\frac{\\mathrm{~d}^3 p \\mathrm{~d}^3 q}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}\\left(2 E_{\\boldsymbol{q}}\\right)^{\\frac{1}{2}}} \\times\\left[\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q}) (E_{\\boldsymbol{p}} E_{\\boldsymbol{q}}+\\boldsymbol{p} \\cdot \\boldsymbol{q}+m^2) [\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{\\mathrm{i}(E_{\\boldsymbol{p}}-E_{\\boldsymbol{q}}) t}+\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\mathrm{e}^{-\\mathrm{i}(E_{\\boldsymbol{p}}-E_{\\boldsymbol{q}}) t}] +\\delta^{(3)}(\\boldsymbol{p}+\\boldsymbol{q})(-E_{\\boldsymbol{p}} E_{\\boldsymbol{q}}-\\boldsymbol{p} \\cdot \\boldsymbol{q}+m^2)[\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\mathrm{e}^{\\mathrm{i}(E_{\\boldsymbol{p}}+E_{\\boldsymbol{q}}) t}+\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{-\\mathrm{i}(E_{\\boldsymbol{p}}+E_{\\boldsymbol{q}}) t}]\\right] $$\nEnsuite, l\u0026rsquo;intégrale sur les $q$ permet de se débarrasser des distributions de Dirac :\n$$ \\hat{H}= \\frac{1}{2} \\int \\mathrm{~d}^3 p \\frac{1}{2 E_{\\boldsymbol{p}}}\\left[(E_{\\boldsymbol{p}}^2+\\boldsymbol{p}^2+m^2) (\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}+\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{p}}^{\\dagger}) + (-E_{\\boldsymbol{p}}^2+\\boldsymbol{p}^2+m^2) (\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{-\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{2 \\mathrm{i} E_{\\boldsymbol{p}} t}+\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{-\\boldsymbol{p}} \\mathrm{e}^{-2 \\mathrm{i} E_{\\boldsymbol{p}} t})\\right] $$\nEt puisque $E_{\\boldsymbol{p}}^2=\\boldsymbol{p}^2+m^2$, cela se simplifie en :\n$$ \\hat{H}=\\frac{1}{2} \\int \\mathrm{~d}^3 p\\, E_{\\boldsymbol{p}}\\left(\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{p}}^{\\dagger}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}\\right) $$\nOn termine en utilisant $\\left[\\hat{a}_{\\boldsymbol{p}}, \\hat{a}_{\\boldsymbol{q}}^{\\dagger}\\right]=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q})$.\nOn obtient :\n$$ \\hat{H}=\\int \\mathrm{d}^3 p \\,E_{\\boldsymbol{p}}\\left(\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}+\\frac{1}{2} \\delta^{(3)}(0)\\right) $$\nLe terme $\\langle0|\\hat{H}|0\\rangle = \\frac{1}{2} \\int\\!\\! \\mathrm{~d}^3 p \\, \\delta^3(0)$ donne une énergie infinie pour le vide 😱 Mais ce n\u0026rsquo;est pas si alarmant si on se convainc qu\u0026rsquo;en pratique, seul le mesurable nous intéresse. Or on ne mesure que des différences d\u0026rsquo;énergie, et ces différences auront évidemment le bon goût de faire disparaître les infinis (en annulant les vilains $\\frac{1}{2} \\delta^{(3)}(0)$). L\u0026rsquo;infini obtenu ne correspondrait finalement qu\u0026rsquo;à une mauvaise définition de l\u0026rsquo;énergie du niveau zéro.\nMalgré tout, ces infinis qui traînent partout, ça fait désordre. En ordonnant savamment les opérateurs, on va pouvoir les glisser discrètement sous le tapis.\nnote\nLe terme constant devient par contre un gros (!) problème lorsqu\u0026rsquo;on essaye de réconcilier théorie quantique des champs et relativité générale où ce ne sont plus les différences d\u0026rsquo;énergie qui importent mais directement la densité d\u0026rsquo;énergie-impulsion.\nC\u0026rsquo;est le \u0026ldquo;problème de la constante cosmologique\u0026quot; : la densité d\u0026rsquo;énergie du vide prévue par la TQC est $10^{120}$ 😵‍💫 ordres de grandeur trop grands par rapport à la valeur mesurée (facile la pire prédiction jamais faite en physique)\u0026hellip;\nOrdre normal L\u0026rsquo;ordre normal consiste simplement à placer tous les opérateurs de création à gauche.\nC\u0026rsquo;est sans douleur pour les champs de Bose, mais pour ceux de Fermi, on doit multiplier par un terme $(-1)^P$ où $P$ est le nombre de permutations nécessaires pour obtenir l\u0026rsquo;ordre normal.\nExemples :\n$\\color{#D41876 }N\\left[\\color{#000 }\\hat{a} \\hat{a}^{\\dagger}\\color{#D41876 }\\right]\\color{#000 }=\\hat{a}^{\\dagger} \\hat{a}$, $\\color{#D41876 }N\\left[\\color{#000 }\\hat{a}^{\\dagger} \\hat{a}\\color{#D41876 }\\right]\\color{#000 }=\\hat{a}^{\\dagger} \\hat{a}$, $\\color{#D41876 }N\\left[\\color{#000 }\\hat{a}^{\\dagger} \\hat{a} \\hat{a} \\hat{a}^{\\dagger} \\hat{a}^{\\dagger}\\color{#D41876 }\\right]\\color{#000 }=\\hat{a}^{\\dagger} \\hat{a}^{\\dagger} \\hat{a}^{\\dagger} \\hat{a} \\hat{a}$, $\\color{#D41876 }N[\\color{#000 }\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\hat{a}_{\\boldsymbol{r}}\\color{#D41876 }]\\color{#000 }=\\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{r}}$, $\\color{#D41876 }N[\\color{#000 }\\hat{c}_{\\boldsymbol{p}} \\hat{c}_{\\boldsymbol{q}}^{\\dagger} \\hat{c}_r\\color{#D41876 }]\\color{#000 }=-\\hat{c}_{\\boldsymbol{q}}^{\\dagger} \\hat{c}_{\\boldsymbol{p}} \\hat{c}_r$.\nÉtape 5 : On arrive finalement au bout du programme en mettant dans l\u0026rsquo;ordre normal les opérateurs dans l\u0026rsquo;Hamiltonien :\n$$ \\begin{aligned} \\color{#D41876 }N[\\color{#000 }\\hat{H}\\color{#D41876 }] \\color{#000 }\u0026amp; =\\frac{1}{2} \\int \\mathrm{d}^3 p E_{\\boldsymbol{p}} \\,\\color{#D41876 }N\\left[\\color{#000 }\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{p}}^{\\dagger}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}\\color{#D41876 }\\right] \\\\ \u0026amp; =\\frac{1}{2} \\int \\mathrm{d}^3 p E_{\\boldsymbol{p}} 2 \\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}} \\end{aligned} $$\nD\u0026rsquo;où :\n$$ \\color{#D41876 }N[\\color{#000 }\\hat{H}\\color{#D41876 }] \\color{#000 }=\\int \\mathrm{d}^3 p E_{\\boldsymbol{p}} \\hat{n}_{\\boldsymbol{p}} $$\n$\\hat{n}_{\\boldsymbol{p}}=\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}$ est l\u0026rsquo;opérateur nombre. $\\hat{n}_{\\boldsymbol{p}}|\\boldsymbol{p}\\rangle$ nous dit combien il y a d\u0026rsquo;excitations dans l\u0026rsquo;état à impulsion $\\boldsymbol{p}$.\nLe niveau zéro (le vide) a maintenant une énergie bien mieux définie :\n$$ \\langle0|N[\\hat{H}]|0\\rangle = 0 $$\nOn retrouve le même Hamiltonien que pour des particules indépendantes ! Les états d\u0026rsquo;excitation de l\u0026rsquo;équation d\u0026rsquo;onde peuvent être vues comme des particules possédant une impulsion quantifiée. Ce sont des bosons avec un spin $S=0$.\nSignification de la décomposition en modes Généralisons un poil la décomposition en modes de l\u0026rsquo;opérateur champ en changeant $\\hat{a}_{\\boldsymbol{p}}^{\\dagger}$ en $\\hat{b}_{\\boldsymbol{p}}^{\\dagger}$ :\n$$ \\hat{\\phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}\\left(\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nPour coller à l\u0026rsquo;interprétation de Feynman des énergies négatives\n$$ \\phi(x) = \\sum_\\boldsymbol{p}\\left[ \\begin{array}{c} \\text{annihilation d\u0026rsquo;une particule} \\\\ \\text{incidente d\u0026rsquo;énergie positive }E_\\boldsymbol{p} \\\\ \\end{array} \\right] + \\sum_\\boldsymbol{p}\\left[ \\begin{array}{c} \\text{création d\u0026rsquo;une antiparticule} \\\\ \\text{sortante d\u0026rsquo;énergie positive }E_\\boldsymbol{p}\\\\ \\end{array} \\right]$$\nil faut que $\\hat{a}_{\\boldsymbol{p}}$ annihile les particules et $\\hat{a}_{\\boldsymbol{p}}^{\\dagger}$ les crée, alors que $\\hat{b}_{\\boldsymbol{p}}$ doit annihiler les antiparticules et $\\hat{b}_{\\boldsymbol{p}}^{\\dagger}$ les créer. Et l\u0026rsquo;énergie des particules et antiparticules vaut $E_{\\boldsymbol{p}}=+\\left(\\boldsymbol{p}^2+m^2\\right)^{\\frac{1}{2}}$.\nDans le cas du champ scalaire, chaque particule est sa propre antiparticule. Et donc $\\hat{b}_{\\boldsymbol{p}}^{\\dagger}=\\hat{a}_{\\boldsymbol{p}}^{\\dagger}$.\nRegardons ce qu\u0026rsquo;il advient lorsqu\u0026rsquo;on fait agir l\u0026rsquo;opérateur champ sur le vide.\nComme $\\hat{a}_{\\boldsymbol{p}}^{\\dagger}|0\\rangle=|\\boldsymbol{p}\\rangle$, on a :\n$$ \\hat{\\phi}(x)|0\\rangle=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}} \\mathrm{e}^{\\mathrm{i} p \\cdot x}|\\boldsymbol{p}\\rangle $$\nOn obtient une superposition de modes sortants. Cherchons l\u0026rsquo;amplitude correspondant à un de ces états $\\langle q|=(2 \\pi)^{\\frac{3}{2}}\\left(2 E_{\\boldsymbol{q}}\\right)^{\\frac{1}{2}}\\langle\\boldsymbol{q}|$ correctement normalisé :\n$$ (2 \\pi)^{\\frac{3}{2}}\\left(2 E_{\\boldsymbol{q}}\\right)^{\\frac{1}{2}}\\langle\\boldsymbol{q}| \\hat{\\phi}(x)|0\\rangle=\\int \\mathrm{d}^3 p\\, \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\langle\\boldsymbol{q} \\mid \\boldsymbol{p}\\rangle=\\int \\mathrm{d}^3 p\\, \\mathrm{e}^{\\mathrm{i} p \\cdot x} \\delta^{(3)}(\\boldsymbol{q}-\\boldsymbol{p})=\\mathrm{e}^{\\mathrm{i}\\left(E_{\\boldsymbol{q}} t-\\boldsymbol{q} \\cdot \\boldsymbol{x}\\right)}=\\mathrm{e}^{\\mathrm{i} q \\cdot x} $$\n$\\mathrm{e}^{\\mathrm{i} q \\cdot x}$ est ainsi l\u0026rsquo;amplitude dans le $q$e mode pour une particule scalaire créée au point $x$ de l\u0026rsquo;espace-temps.\nLa recette de quantification canonique ne fonctionne que sur des Lagrangiens de théories sans interaction (des champs libres) car la possibilité de leur diagonalisation (la décomposition en modes d\u0026rsquo;impulsion est bien, de fait, une diagonalisation) repose sur l\u0026rsquo;écriture du Lagrangien en termes quadratiques des champs et de leurs dérivées et les couplages viennent mettre le bazar dans ces jolies écritures.\nQuantification canonique d\u0026rsquo;un champ scalaire complexe Application de la recette Étape 1 : Le Lagrangien d\u0026rsquo;un champ scalaire complexe possède deux composantes :\n$$ \\mathcal{L}= \\frac{1}{2}\\left[\\partial_\\mu \\phi_1(x)\\right]^2-\\frac{1}{2} m^2\\left[\\phi_1(x)\\right]^2 +\\frac{1}{2}\\left[\\partial_\\mu \\phi_2(x)\\right]^2-\\frac{1}{2} m^2\\left[\\phi_2(x)\\right]^2 $$\nEn posant\n$$ \\psi=\\frac{1}{\\sqrt{2}}\\left[\\phi_1(x)+\\mathrm{i} \\phi_2(x)\\right] \\qquad \\psi^{\\dagger}=\\frac{1}{\\sqrt{2}}\\left[\\phi_1(x)-\\mathrm{i} \\phi_2(x)\\right] $$\non obtient quasiment le Lagrangien du champ scalaire réel, mais sans le facteur $\\frac{1}{2}$ :\n$$ \\mathcal{L}=\\partial^\\mu \\psi^{\\dagger}(x) \\partial_\\mu \\psi(x)-m^2 \\psi^{\\dagger}(x) \\psi(x) $$\nÉtape 2 : Chaque composante $\\sigma$ du champ ($\\sigma=\\psi$ ou $\\psi^\\dagger$) a une densité d\u0026rsquo;impulsion différente :\n$$ \\Pi_{\\sigma=\\psi}^0=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_0 \\psi\\right)}=\\partial^0 \\psi^{\\dagger} \\qquad \\Pi_{\\sigma=\\psi^{\\dagger}}^0=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_0 \\psi^{\\dagger}\\right)}=\\partial^0 \\psi $$\nDéterminons l\u0026rsquo;Hamiltonien :\n$$ \\mathcal{H} =\\sum_\\sigma \\Pi_\\sigma^0(x) \\partial_0 \\psi^\\sigma(x)-\\mathcal{L} $$\nOn obtient après simplification :\n$$ \\mathcal{H}=\\partial_0 \\psi^{\\dagger}(x) \\partial_0 \\psi(x)+\\nabla \\psi^{\\dagger}(x) \\cdot \\nabla \\psi(x)+m^2 \\psi^{\\dagger}(x) \\psi(x) $$\nÉtape 3 : On promeut les champs au rang d\u0026rsquo;opérateurs quantiques et on impose les relations de commutation à temps égaux :\n$$ \\left[\\hat{\\psi}(t, \\boldsymbol{x}), \\hat{\\Pi}_\\psi^0(t, \\boldsymbol{y})\\right]=\\left[\\hat{\\psi}^{\\dagger}(t, \\boldsymbol{x}), \\hat{\\Pi}_{\\psi^{\\dagger}}^0(t, \\boldsymbol{y})\\right]=\\mathrm{i} \\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) $$\nTous les autres commutateurs sont nuls.\nÉtape 4 : On décompose les champs en modes :\n$$ \\begin{aligned} \\hat{\\psi}(x) \u0026amp; =\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}\\left(\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) \\\\ \\hat{\\psi}^{\\dagger}(x) \u0026amp; =\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}\\left(\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}+\\hat{b}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}\\right) \\end{aligned} $$\navec $E_\\boldsymbol{p}=+\\left(\\boldsymbol{p}^2+m^2\\right)^{\\frac{1}{2}}$. Les opérateurs $\\hat{a}_{\\boldsymbol{p}}$ et $\\hat{b}_{\\boldsymbol{p}}$ annihilent deux types différents de particules. Ils satisfont les relations de commutation $\\left[\\hat{a}_{\\boldsymbol{p}}, \\hat{a}_{\\boldsymbol{q}}^{\\dagger}\\right]=\\left[\\hat{b}_{\\boldsymbol{p}}, \\hat{b}_{\\boldsymbol{q}}^{\\dagger}\\right]=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q})$ et toute autre combinaison est nulle.\nÉtape 5 : On substitue les relations de commutation dans l\u0026rsquo;Hamiltonien et on ordonne les opérateurs :\n$$ \\begin{aligned} N[\\hat{H}] \u0026amp; =\\int \\mathrm{d}^3 p E_{\\boldsymbol{p}}\\left(\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}+\\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\hat{b}_{\\boldsymbol{p}}\\right) \\\\ \u0026amp; =\\int \\mathrm{d}^3 p E_{\\boldsymbol{p}}\\left(\\hat{n}_{\\boldsymbol{p}}^{(a)}+\\hat{n}_{\\boldsymbol{p}}^{(b)}\\right), \\end{aligned} $$\noù $\\hat{n}_{\\boldsymbol{p}}^{(a)}$ compte les particules $a$ avec une impulsion $\\boldsymbol{p}$ et $\\hat{n}_{\\boldsymbol{p}}^{(b)}$ compte les particules $b$ avec une impulsion $\\boldsymbol{p}$.\nLes particules $a$ et $b$ ayant la même énergie $E_\\boldsymbol{p}$, on les interprète respectivement comme des particules et antiparticules.\n$\\hat{\\psi}(x)$ s\u0026rsquo;interprète alors comme une somme sur les impulsions d\u0026rsquo;opérateurs qui annihilent des particules ($\\hat{a}_{\\boldsymbol{p}}$) et d\u0026rsquo;opérateurs qui créent des antiparticules ($\\hat{b}^\\dagger_{\\boldsymbol{p}}$).\nCourant de Noether associé Le champ scalaire complexe possède une symétrie interne $U(1)$ puisque les transformations globales suivantes n\u0026rsquo;ont pas d\u0026rsquo;effet sur le Lagrangien :\n$$ \\psi \\rightarrow \\mathrm{e}^{\\mathrm{i} \\alpha} \\psi, \\quad \\psi^{\\dagger} \\rightarrow \\mathrm{e}^{-\\mathrm{i} \\alpha} \\psi^{\\dagger} $$\nPour déterminer le courant associé grâce au théorème de Noether, on passe à des transformations infinitésimales :\n$$ \\begin{array}{cc} \\psi \\rightarrow \\psi+\\mathrm{i} \\psi \\delta \\alpha, \u0026amp; D \\psi=+\\mathrm{i} \\psi, \\\\ \\psi^{\\dagger} \\rightarrow \\psi^{\\dagger}-\\mathrm{i} \\psi^{\\dagger} \\delta \\alpha, \u0026amp; D \\psi^{\\dagger}=-\\mathrm{i} \\psi^{\\dagger} \\end{array} $$\n$$ \\mathcal{L} \\rightarrow \\partial^\\mu( \\psi^{\\dagger}-\\mathrm{i} \\psi^{\\dagger} \\delta \\alpha)\\partial_\\mu(\\psi+\\mathrm{i} \\psi \\delta \\alpha) - m^2( \\psi^{\\dagger}-\\mathrm{i} \\psi^{\\dagger} \\delta \\alpha)(\\psi+\\mathrm{i} \\psi \\delta \\alpha) $$\ndonne $\\mathcal{L} \\rightarrow \\mathcal{L} + \\delta a^2\\mathcal{L}$ et on a donc bien $D\\mathcal{L}=0$.\nEt comme $D\\mathcal L=\\partial_\\mu W^\\mu$, cela implique que $W^\\mu = 0$ (c\u0026rsquo;est le cas pour toutes les symétries internes !).\nLe courant de Noether est alors donné par $J_{\\mathrm{N}}^\\mu=\\sum_\\sigma \\Pi_\\sigma^\\mu D \\sigma$ (avec $\\sigma=\\psi$ ou $\\psi^\\dagger$) :\n$$ \\begin{aligned} J_{\\mathrm{N}}^\\mu \u0026amp; =\\sum_\\sigma \\Pi_\\sigma^\\mu D \\sigma=\\Pi_\\psi^\\mu D \\psi+\\Pi_{\\psi^{\\dagger}}^\\mu D \\psi^{\\dagger} \\\\ \u0026amp; =\\mathrm{i}\\left[\\left(\\partial^\\mu \\psi^{\\dagger}\\right) \\psi-\\left(\\partial^\\mu \\psi\\right) \\psi^{\\dagger}\\right] \\end{aligned} $$\nLe courant devient opérateur en utilisant les opérateurs champ $\\hat{\\psi}$ et $\\hat{\\psi}^\\dagger$.\nL\u0026rsquo;opérateur charge conservée est donnée par :\n$$ \\hat{Q}_{\\mathrm{N}}=\\int \\mathrm{d}^3 x\\, \\hat{J}_{\\mathrm{N}}^0=\\int \\mathrm{d}^3 x\\,\\mathrm{i}\\left[\\left(\\partial^0 \\hat{\\psi}^{\\dagger}\\right) \\hat{\\psi}-\\left(\\partial^0 \\hat{\\psi}\\right) \\hat{\\psi}^{\\dagger}\\right] $$\nAprès injection des décompositions en modes des champs, on obtient :\n$$ \\hat{Q}_{\\mathrm{N}}=\\frac{1}{2} \\int \\mathrm{~d}^3 p\\left(-\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}+\\hat{b}_{\\boldsymbol{p}} \\hat{b}_{\\boldsymbol{p}}^{\\dagger}-\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{p}}^{\\dagger}+\\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\hat{b}_{\\boldsymbol{p}}\\right) $$\nOn ordonne ce petit monde :\n$$ \\begin{aligned} N\\left[\\hat{Q}_{\\mathrm{N}}\\right]\u0026amp;=\\int \\mathrm{d}^3 p\\left(\\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\hat{b}_{\\boldsymbol{p}}-\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}\\right)\\\\ \u0026amp;=\\int \\mathrm{d}^3 p\\left(\\hat{n}_{\\boldsymbol{p}}^{(b)}-\\hat{n}_{\\boldsymbol{p}}^{(a)}\\right) \\end{aligned} $$\nLa charge conservée est donc la différence entre le nombre d\u0026rsquo;antiparticules et le nombre de particules ! Chacun des deux types de particules (alias excitations du champ) porte une charge de Noether de signe opposé. Pour avoir conservation de la charge globale, il faut bien que la différence entre chaque type reste constante. L\u0026rsquo;existence même des antiparticules est donc impliquée par cette nécessité de conserver la charge.\nSi $J^\\mu_N$ est conservé, il en est de même de $-J^\\mu_N$ et donc le choix du type de particules portant une charge de Noether positive est arbitraire. Par convention le courant nombre de particules $\\hat{J}^\\mu_{Nc}$ (\u0026ldquo;c\u0026rdquo; pour conventionnel) est défini positivement pour les particules et négativement pour les antiparticules. Cela amène à poser $\\hat{J}_{\\mathrm{Nc}}^\\mu=-N\\left[\\hat{J}_{\\mathrm{N}}^\\mu\\right]$, $\\hat{Q}_{\\mathrm{Nc}}=-N\\left[\\hat{Q}_{\\mathrm{N}}\\right]$, pour aboutir à :\n$$ \\hat{Q}_{\\mathrm{Nc}}=\\int \\mathrm{d}^3 p\\left(\\hat{n}_{\\boldsymbol{p}}^{(a)}-\\hat{n}_{\\boldsymbol{p}}^{(b)}\\right) $$\nLimite non-relativiste Dans le domaine non-relativiste, les énergies d\u0026rsquo;excitation des particules sont infimes comparées à l\u0026rsquo;énergie de masse : $E=mc^2+\\varepsilon$ où $\\varepsilon\\ll mc^2$.\nUne stratégie possible pour trouver la limite non-relativiste d\u0026rsquo;une théorie est de factoriser le \u0026ldquo;gros\u0026rdquo; terme :\n$$ \\phi(\\boldsymbol{x}, t) \\rightarrow \\Psi(\\boldsymbol{x}, t) \\mathrm{e}^{-\\mathrm{i} m c^2 t / \\hbar} $$\nRegardons ce que ça donne avec l\u0026rsquo;équation de Klein-Gordon :\n$$ \\left(\\hbar^2 \\frac{\\partial^2}{\\partial t^2}-\\hbar^2 c^2 \\nabla^2+m^2 c^4\\right) \\Psi(\\boldsymbol{x}, t) \\mathrm{e}^{-\\mathrm{i} m c^2 t / \\hbar}=0 $$\nLe premier terme donne :\n$$ \\hbar^2 \\frac{\\partial^2}{\\partial t^2} \\Psi(\\boldsymbol{x}, t) \\mathrm{e}^{-\\mathrm{i} m c^2 t / \\hbar}=\\hbar^2\\left(\\frac{\\partial^2 \\Psi}{\\partial t^2}-\\frac{2 \\mathrm{i} m c^2}{\\hbar} \\frac{\\partial \\Psi}{\\partial t}-\\frac{m^2 c^4}{\\hbar^2} \\Psi\\right) \\mathrm{e}^{-\\mathrm{i} m c^2 t / \\hbar} $$\nEn remplaçant dans l\u0026rsquo;équation, le dernier terme de la dérivée seconde se télescope avec le terme de masse :\n$$ \\hbar^2 \\frac{\\partial^2 \\Psi}{\\partial t^2}-2 \\mathrm{i} m c^2 \\hbar \\frac{\\partial \\Psi}{\\partial t}-\\hbar^2 c^2 \\nabla^2 \\Psi=0 $$\nOn peut négliger le premier terme puisqu\u0026rsquo;il ne contient pas de facteur $c^2$ et on obtient :\n$$ \\mathrm{i} \\hbar \\frac{\\partial}{\\partial t} \\Psi(\\boldsymbol{x}, t)=-\\frac{\\hbar^2}{2 m} \\nabla^2 \\Psi(\\boldsymbol{x}, t) $$\nOn a retrouvé l\u0026rsquo;équation de Schrödinger d\u0026rsquo;une particule libre !\nPour arriver à la limite non-relativiste du champ scalaire complexe, on prend (en unités naturelles) $\\psi=\\frac{1}{\\sqrt{2 m}} \\mathrm{e}^{-\\mathrm{i} m t} \\Psi$ (où $1/\\sqrt{2m}$ est un facteur de normalisation).\nOn utilise maintenant le Lagrangien avec interaction\n$$ \\mathcal{L}=\\partial^\\mu \\psi^{\\dagger}(x) \\partial_\\mu \\psi(x)-m^2 \\psi^{\\dagger}(x) \\psi(x)-\\lambda\\left[\\psi^{\\dagger}(x) \\psi(x)\\right]^2 $$\nOn obtient :\n$$ \\partial_0 \\psi^{\\dagger} \\partial_0 \\psi=\\frac{1}{2 m}\\left[\\partial_0 \\Psi^{\\dagger} \\partial_0 \\Psi+\\mathrm{i} m\\left(\\Psi^{\\dagger} \\partial_0 \\Psi-\\left(\\partial_0 \\Psi^{\\dagger}\\right) \\Psi\\right)+m^2 \\Psi^{\\dagger} \\Psi\\right] $$\nLe premier terme, en $1/m$, est négligeable par rapport aux autres et le troisième se télescope avec le terme de masse dans le Lagrangien. La partie dynamique de la théorie est donc confinée dans le deuxième terme.\nEn décomposant en ondes planes $\\mathrm{e}^{-\\mathrm{i}p\\cdot x}$, la dérivée temporelle de $\\Psi$ apporte un facteur $-\\mathrm{i}E_\\boldsymbol{p}$ et celle de $\\Psi^\\dagger$ un facteur $\\mathrm{i}E_\\boldsymbol{p}$ et donc $(\\Psi^{\\dagger} \\partial_0 \\Psi-\\Psi \\partial_0 \\Psi^{\\dagger})$ peut être remplacé par $2 \\Psi^{\\dagger} \\partial_0 \\Psi$. On aurait aussi pu remplacer par $-2 \\Psi^{\\dagger} \\partial_0 \\Psi$ si on avait choisi une décomposition en modes $\\mathrm{e}^{\\mathrm{i}p\\cdot x}$, mais on préfère favoriser la matière par rapport à l\u0026rsquo;antimatière.\nOn obtient au final :\n$$ \\mathcal{L}=\\mathrm{i} \\Psi^{\\dagger}(x) \\partial_0 \\Psi(x)-\\frac{1}{2 m} \\boldsymbol{\\nabla} \\Psi^{\\dagger}(x) \\cdot \\nabla \\Psi(x)-\\frac{g}{2}\\left[\\Psi^{\\dagger}(x) \\Psi(x)\\right]^2 $$\navec $g=\\lambda/2m^2$ L\u0026rsquo;asymétrie (entre matière et antimatière) qu\u0026rsquo;on a injecté dans le Lagrangien lui a fait perdre sa belle covariance relativiste.\nPassons à la quantification canonique :\nÉtape 1 : la densité lagrangienne avec un potentiel extérieur est donné par :\n$$ \\mathcal{L}=\\mathrm{i} \\Psi^{\\dagger}(x) \\partial_0 \\Psi(x)-\\frac{1}{2 m} \\nabla \\Psi^{\\dagger}(x) \\cdot \\nabla \\Psi(x)-V(x) \\Psi^{\\dagger}(x) \\Psi(x) $$\nLes équations d\u0026rsquo;Euler-Lagrange redonne logiquement l\u0026rsquo;équation de Schrödinger et, pour $V(x)=0$, la relation de dispersion $E_\\boldsymbol{p}=\\frac{\\boldsymbol{p}^2}{2 m}$. Comme il n\u0026rsquo;y a plus que des énergies positives, on n\u0026rsquo;aura pas besoin des fréquences négatives dans la décomposition en modes.\nÉtape 2 : on calcule les densités d\u0026rsquo;impulsion :\n$$ \\Pi_{\\Psi}^0=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_0 \\Psi\\right)}=\\mathrm{i} \\Psi^{\\dagger} \\qquad \\Pi_{\\Psi^{\\dagger}}^0=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_0 \\Psi^{\\dagger}\\right)}=0 $$\nL\u0026rsquo;absence de moment conjugué au champ $\\Psi^\\dagger$ découle de notre choix de favoriser la matière. On peut maintenant calculer la densité hamiltonienne :\n$$ \\begin{aligned} \\mathcal{H} \u0026amp; =\\Pi_{\\Psi}^0 \\partial_0 \\Psi-\\mathcal{L} \\\\ \u0026amp; =\\frac{1}{2 m} \\nabla \\Psi^{\\dagger}(x) \\cdot \\nabla \\Psi(x)+V(x) \\Psi^{\\dagger}(x) \\Psi(x) \\end{aligned} $$\nUne densité à la Schrödinger\u0026hellip;\nÉtape 3 : les commutateurs à temps égaux entre les positions et les impulsions s\u0026rsquo;écrivent :\n$$ \\begin{aligned} {\\left[\\hat{\\Psi}(t, \\boldsymbol{x}), \\hat{\\Pi}_{\\Psi}^0(t, \\boldsymbol{y})\\right] } \u0026amp; =\\mathrm{i} \\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) \\\\ {\\left[\\hat{\\Psi}(t, \\boldsymbol{x}), \\hat{\\Psi}^{\\dagger}(t, \\boldsymbol{y})\\right] } \u0026amp; =\\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) \\end{aligned} $$\nÉtape 4 : une décomposition en modes avec des fréquences positives et négatives ne respecterait pas la relation de commutation ci-dessus. La décomposition idoine est :\n$$ \\hat{\\Psi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x} $$\navec $E_{\\boldsymbol{p}}=\\frac{\\boldsymbol{p}^2}{2 m}$.\nÉtape 5 : on substitue la décomposition en modes dans l\u0026rsquo;Hamiltonien :\n$$ \\hat{H}=\\int \\mathrm{d}^3 p\\left(\\frac{\\boldsymbol{p}^2}{2 m} \\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}\\right)+\\int \\frac{\\mathrm{d}^3 x \\mathrm{~d}^3 p \\mathrm{~d}^3 q}{(2 \\pi)^3}\\left(V(t, \\boldsymbol{x}) \\mathrm{e}^{\\mathrm{i}\\left(E_{\\boldsymbol{p}}-E_{\\boldsymbol{q}}\\right) t} \\mathrm{e}^{-\\mathrm{i}(\\boldsymbol{p}-\\boldsymbol{q}) \\cdot \\boldsymbol{x}} \\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{q}}\\right) $$\nLa partie dépendante du temps doit garantir la conservation de l\u0026rsquo;énergie, ce qui force le potentiel à s\u0026rsquo;écrire $V(t, \\boldsymbol{x})=\\mathrm{e}^{-\\mathrm{i}\\left(E_{\\boldsymbol{p}}-E_{\\boldsymbol{q}}\\right) t} V(\\boldsymbol{x})$. L\u0026rsquo;Hamiltonien devient alors :\n$$ \\hat{H}=\\int \\mathrm{d}^3 p\\left(\\frac{\\boldsymbol{p}^2}{2 m} \\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}\\right)+\\int \\mathrm{d}^3 p \\mathrm{~d}^3 q\\left(\\tilde{V}(\\boldsymbol{p}-\\boldsymbol{q}) \\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{q}}\\right) $$\navec $\\tilde{V}(\\boldsymbol{p}-\\boldsymbol{q})=\\int \\mathrm{d}^3 x \\frac{1}{(2 \\pi)^3} V(\\boldsymbol{x}) \\mathrm{e}^{-\\mathrm{i}(\\boldsymbol{p}-\\boldsymbol{q}) \\cdot \\boldsymbol{x}}$.\nOn retrouve un Hamiltonien très ressemblant à celui prédit pour des systèmes discrets. Et on constate que la partie potentielle de l\u0026rsquo;Hamiltonien n\u0026rsquo;est pas diagonale (comme prédit, le couplage empèche la diagonalisation).\nComme le champ scalaire complexe non-relativiste garde sa symétrie $U(1)$, on peut l\u0026rsquo;envisager d\u0026rsquo;une autre façon en l\u0026rsquo;écrivant en termes d\u0026rsquo;amplitude et de phase :\n$$ \\Psi(x)=\\sqrt{\\rho(x)} \\mathrm{e}^{\\mathrm{i} \\theta(x)} $$\nUne transformation de $U(1)$ correspond maintenant à $\\theta\\rightarrow\\theta+\\alpha$.\nOn est ainsi passé des deux champs $\\phi_1(x)$ et $\\phi_2(x)$ aux deux nouveaux champs $\\rho(x)$ et $\\theta(x)$.\nÉtape 1 : on substitue ces nouveaux champs dans le Lagrangien :\n$$ \\mathcal{L}=\\frac{\\mathrm{i}}{2} \\partial_0 \\rho-\\rho \\partial_0 \\theta-\\frac{1}{2 m}\\left[\\frac{1}{4 \\rho}(\\boldsymbol{\\nabla} \\rho)^2+\\rho(\\boldsymbol{\\nabla} \\theta)^2\\right]-\\frac{g}{2} \\rho^2 $$\nOn va maintenant éteindre les interactions ($g=0$).\nÉtape 2 : on détermine les densités d\u0026rsquo;impulsion :\n$$ \\begin{aligned} \\Pi_\\rho^0(x) \u0026amp; =\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_0 \\rho(x)\\right)}=\\frac{\\mathrm{i}}{2} \\\\ \\Pi_\\theta^0(x) \u0026amp; =\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_0 \\theta(x)\\right)}=-\\rho(x) \\end{aligned} $$\nÉtape 3 : on impose les relations de commutation :\n$$ \\left[\\hat{\\theta}(\\boldsymbol{x}, t), \\hat{\\Pi}_\\theta^0(\\boldsymbol{y}, t)\\right]=-[\\hat{\\theta}(\\boldsymbol{x}, t), \\hat{\\rho}(\\boldsymbol{y}, t)]=\\mathrm{i} \\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) $$\nQue nous dit le théorème de Noether ?\n$D\\theta = \\left.\\frac{\\partial \\theta}{\\partial \\alpha}\\right|_{\\alpha\\rightarrow 0}=1$.\n$D\\mathcal{L}=0$ puisque $\\alpha$ est une constante (on regarde une transformation globale et non locale $\\alpha(\\cancel{t,\\boldsymbol{x}})$). Et donc $W^\\mu =0$.\n$\\Pi^0_\\theta=\\frac{\\partial\\mathcal{L}}{\\partial(\\partial_0\\theta)}=-\\rho$ et $\\Pi^i_\\theta=\\frac{\\partial\\mathcal{L}}{\\partial(\\partial_i\\theta)}=\\frac{\\rho}{m}\\partial^i\\theta$.\nDétail $\\Pi^i_\\theta=\\frac{\\partial\\mathcal{L}}{\\partial(\\partial_i\\theta)}=\\frac{-\\rho}{2m}\\frac{\\partial(\\boldsymbol{\\nabla}\\theta)^2}{\\partial(\\partial_i\\theta)}=\\frac{-\\rho}{2m}\\frac{\\partial(\\eta^{kl}\\partial_k\\theta\\partial_l\\theta)}{\\partial(\\partial_i\\theta)}=\\frac{-\\rho}{2m}(\\eta^{kl}\\delta^i_k\\partial_l\\theta+\\eta^{kl}\\delta^i_l\\partial_k\\theta)=\\frac{-\\rho}{2m}(-\\delta^{kl}\\delta^i_k\\partial_l\\theta-\\delta^{kl}\\delta^i_l\\partial_k\\theta)=\\frac{\\rho}{m}\\partial^i\\theta$ avec la signature $(+,-,-,-)$ On en déduit $J^0_\\mathrm{N}=\\Pi^0_\\theta D\\theta = -\\rho(x)$ et $\\boldsymbol{J}_\\mathrm{N}=\\Pi^i_\\theta D\\theta = -\\frac{\\rho}{m}\\boldsymbol{\\nabla}\\theta$\nEt enfin $Q_{\\mathrm{Nc}}=\\int \\mathrm{d}^3 x \\rho(x)$ et $\\boldsymbol{J}_{\\mathrm{Nc}}=\\frac{\\rho}{m} \\boldsymbol{\\nabla} \\theta$.\nLa composante temporelle du courant conservé est $\\rho(x)$. On définit alors le nombre total de particules comme $\\hat{N}(t)=\\int\\mathrm{d}^3x\\,\\hat{\\rho}(\\boldsymbol{x},t)$ et en intégrant la relation de commutation, on obtient :\n$$ [\\hat{N}(t), \\hat{\\theta}(\\boldsymbol{x}, t)]=\\mathrm{i} $$\nC\u0026rsquo;est la relation d\u0026rsquo;incertitude nombre-phase. Elle nous dit que pour les systèmes cohérents en matière condensée, l\u0026rsquo;opérateur nombre d\u0026rsquo;excitations du champ est conjugué à sa phase.\nQuantification canonique d\u0026rsquo;un champ à plusieurs composantes Symétries internes Il est tentant de considérer que des particules se ressemblant, comme un neutron et un proton, sont en fait une seule et même particule possédant un curseur interne permettant de passer d\u0026rsquo;une forme à l\u0026rsquo;autre. L\u0026rsquo;invariance du Lagrangien par rapport à ce curseur est alors décrite par une symétrie interne (sans rapport évident avec les symétries de l\u0026rsquo;espace-temps).\nOn a déjà croisé une symétrie interne avec la symétrie $U(1)$ du champ scalaire complexe. L\u0026rsquo;isospin est un autre exemple. C\u0026rsquo;est lui, le curseur permettant de passer d\u0026rsquo;un proton à un neutron.\nLe proton et le neutron ont tous les deux un isospin $I=\\frac{1}{2}$ avec, comme pour le spin conventionnel, deux valeurs propres possibles de l\u0026rsquo;opérateur $\\hat{I}_z$ : $I_z=1/2$ pour le proton et $I_z=-1/2$ pour le neutron. Par analogie avec le moment cinétique, on peut assembler proton et neutron dans un objet à deux composantes $\\binom{p}{n}$, le doublet d\u0026rsquo;isospin. Et on opère une rotation de ce doublet avec les mêmes matrices permettant de tourner les spins 1/2. Remarquons enfin que cette symétrie n\u0026rsquo;est qu\u0026rsquo;approximative car on n\u0026rsquo;a pas strictement $m_p=m_n$.\nPour explorer l\u0026rsquo;idée de ces symétries internes, prenons trois particules scalaires $t$, $d$ et $h$ arrangées au sein d\u0026rsquo;un vecteur $(t,d,h)^t$ possédant une symétrie $SO(3)$. Tourner le curseur interne de 90° selon l\u0026rsquo;axe $h$ transforme ainsi une particule $t$, $(1,0,0)^t$, en une particule $d$, $(0,1,0)^t$. Toute superposition de particules obtenue en tournant le curseur est aussi valide que l\u0026rsquo;une des 3 particules originelles.\nComme les particules sont des excitations du champ, on doit pouvoir étudier l\u0026rsquo;isospin en théorie des champs. Pour cela, on arrange les champs responsables de ces particules en un vecteur $(\\phi_1,\\phi_2,\\phi_3)^t$. Soumettre ce vecteur à une rotation interne en tournant le curseur revient à pouvoir transformer $\\phi_1$ en $\\phi_2$ en tournant autour de l\u0026rsquo;axe correspondant à $\\phi_3$. Et si le Lagrangien décrivant la théorie est invariant par rapport à ces rotations internes, le théorème de Noether va nous offrir une loi de conservation concernant les charges de ces champs.\nDéroulons la mécanique de la quantification canonique :\nÉtape 1 : posons $ \\boldsymbol{\\Phi}(x)=\\left(\\begin{array}{l} \\phi_1(x) \\\\ \\phi_2(x) \\\\ \\phi_3(x) \\end{array}\\right) $\nLe Lagrangien libre pour cette théorie peut s\u0026rsquo;écrire :\n$$ \\mathcal{L}=\\frac{1}{2}\\left(\\partial^\\mu \\boldsymbol{\\Phi}\\right) \\cdot\\left(\\partial_\\mu \\boldsymbol{\\Phi}\\right)-\\frac{m^2}{2} \\boldsymbol{\\Phi} \\cdot \\boldsymbol{\\Phi} $$\nqui n\u0026rsquo;est que la contraction de la somme des Lagrangiens de chaque champ :\n$$ \\mathcal{L}=\\frac{1}{2}\\left[\\left(\\partial_\\mu \\phi_1\\right)^2-m^2 \\phi_1^2+\\left(\\partial_\\mu \\phi_2\\right)^2-m^2 \\phi_2^2+\\left(\\partial_\\mu \\phi_3\\right)^2-m^2 \\phi_3^2\\right] $$\nNotons bien que $\\boldsymbol{\\Phi}(x)$ n\u0026rsquo;est pas un champ vectoriel évoluant dans l\u0026rsquo;espace de Minkowski comme $x^\\mu$ ou $p^\\mu$.\nDe fait, le produit scalaire n\u0026rsquo;est pas défini en utilisant la métrique ($g_{\\mu\\nu}A^\\mu A^\\nu$), mais par $\\boldsymbol{\\Phi} \\cdot \\boldsymbol{\\Phi}=\\phi_1 \\phi_1+\\phi_2 \\phi_2+\\phi_3 \\phi_3$. Et de même $\\partial^\\mu \\boldsymbol{\\Phi} \\cdot \\partial_\\mu \\boldsymbol{\\Phi}=\\partial^\\mu \\phi_1 \\partial_\\mu \\phi_1+\\partial^\\mu \\phi_2 \\partial_\\mu \\phi_2+\\partial^\\mu \\phi_3 \\partial_\\mu \\phi_3$.\nL\u0026rsquo;indice $\\alpha$ dans $\\Phi_\\alpha$ n\u0026rsquo;est donc pas un indice tensoriel, il n\u0026rsquo;y a pas de différence entre $\\Phi^\\alpha$ et $\\Phi_\\alpha$.\nNotre exemple possèdant une symétrie $SO(3)$, on peut transformer le vecteur $\\boldsymbol{\\Phi}$ en $\\boldsymbol{\\Phi\u0026rsquo;}$ en utilisant une matrice de rotation 3d sans modifier le Lagrangien :\n$$ \\left(\\begin{array}{c} \\phi_1^{\\prime} \\\\ \\phi_2^{\\prime} \\\\ \\phi_3^{\\prime} \\end{array}\\right)=\\left(\\begin{array}{ccc} \\cos \\theta \u0026amp; -\\sin \\theta \u0026amp; 0 \\\\ \\sin \\theta \u0026amp; \\cos \\theta \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right)\\left(\\begin{array}{l} \\phi_1 \\\\ \\phi_2 \\\\ \\phi_3 \\end{array}\\right) $$\nÉtape 2 : l\u0026rsquo;Hamiltonien s\u0026rsquo;écrit :\n$$ \\hat{\\mathcal{H}}=\\sum_\\alpha\\left[\\frac{1}{2}\\left(\\partial_0 \\hat{\\phi}_\\alpha\\right)^2+\\frac{1}{2}\\left(\\nabla \\hat{\\phi}_\\alpha\\right)^2+\\frac{1}{2} m^2 \\hat{\\phi}_\\alpha^2\\right] $$\nÉtape 3 : les relations de commutation à temps égaux sont données par :\n$$ \\left[\\hat{\\Phi}_\\alpha(t, \\boldsymbol{x}), \\hat{\\Pi}_\\beta^0(t, \\boldsymbol{y})\\right]=\\mathrm{i} \\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) \\delta_{\\alpha \\beta} $$\nLe Kronecker fait en sorte que les valeurs non nulles correspondent bien au commutateur entre une composante de $\\boldsymbol{\\Phi}$ et la même composante de sa densité d\u0026rsquo;impulsion.\nÉtape 4 : la décomposition en modes donne :\n$$ \\boldsymbol{\\Phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}\\left(\\begin{array}{l} \\hat{a}_{\\boldsymbol{p} 1} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p} 1}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x} \\\\ \\hat{a}_{\\boldsymbol{p} 2} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x} \\\\ \\hat{a}_{\\boldsymbol{p} 3} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p} 3}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x} \\end{array}\\right) $$\noù les $\\hat{a}_{\\boldsymbol{p} \\alpha}$ sont les opérateurs d\u0026rsquo;annihilation pour le champ $\\alpha$ avec les commutateurs $\\left[\\hat{a}_{\\boldsymbol{p} \\alpha}, \\hat{a}_{\\boldsymbol{q} \\beta}^{\\dagger}\\right]=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q}) \\delta_{\\alpha \\beta}$.\nCompactons l\u0026rsquo;expression :\n$$ \\boldsymbol{\\Phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}} \\sum_{\\alpha=1}^3 \\boldsymbol{h}_\\alpha\\left(\\hat{a}_{\\boldsymbol{p} \\alpha} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p} \\alpha}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\noù $\\boldsymbol{h}_1=\\left(\\begin{array}{l}1 \\\\0 \\\\0\\end{array}\\right)$, $\\boldsymbol{h}_2=\\left(\\begin{array}{l}0 \\\\1 \\\\0\\end{array}\\right)$ et $\\boldsymbol{h}_3=\\left(\\begin{array}{l}0 \\\\0 \\\\1\\end{array}\\right)$ nous renseignent sur la polarisation du champ dans l\u0026rsquo;espace interne.\nÉtapes 5 : en substituant dans l\u0026rsquo;Hamiltonien et après réordonancement, on obtient :\n$$ \\hat{H}=\\int \\mathrm{d}^3 p\\, E_{\\boldsymbol{p}} \\sum_{\\alpha=1}^3 \\hat{a}_{\\boldsymbol{p} \\alpha}^{\\dagger} \\hat{a}_{\\boldsymbol{p} \\alpha} $$\nMoralité, on somme maintenant à la fois sur toutes les impulsions et sur toutes les polarisations.\nPenchons-nous enfin sur les charges de Noether conservées en partant de la symétrie du Lagrangien par rapport aux transformations $\\boldsymbol{\\Phi} \\rightarrow \\boldsymbol{\\Phi} - \\boldsymbol{\\theta} \\times \\boldsymbol{\\Phi}$.\nPrenons l\u0026rsquo;exemple d\u0026rsquo;une rotation autour de l\u0026rsquo;axe $\\phi_3$ : on a $\\Phi^a \\rightarrow \\Phi^a-\\varepsilon^{a 3 c} \\theta^3 \\Phi^c$ et $D^3\\phi^1 = \\phi^2$, $D^3\\phi^2 = -\\phi^1$, $D^3\\phi^3 = 0$, où on appelle la symétrie de rotation autour de l\u0026rsquo;axe $b$, $D^b\\phi^a$.\nPour toute symétrie interne, $D\\mathcal{L}=0$ (par définition, elles ne modifient pas le Lagrangien) et donc $W^\\mu = 0$. Pour les rotations autour de l\u0026rsquo;axe 3, le courant de Noether $J_{\\mathrm{N}}^{3 \\mu}$ est :\n$$ J_{\\mathrm{N}}^{3 \\mu}=\\Pi^{a \\mu} D^3 \\Phi^a=\\left(\\partial^\\mu \\phi^1\\right) \\phi^2-\\left(\\partial^\\mu \\phi^2\\right) \\phi^1 $$\nEn injectant les décompositions en modes et après inversion de signes et ordre normal, on obtient :\n$$ \\hat{Q}_{\\mathrm{Nc}}^3=-\\mathrm{i} \\int \\mathrm{~d}^3 p\\left(\\hat{a}_{1 \\boldsymbol{p}}^{\\dagger} \\hat{a}_{2 p}-\\hat{a}_{2 \\boldsymbol{p}}^{\\dagger} \\hat{a}_{1 \\boldsymbol{p}}\\right) $$\nOn obtient des formules similaires à partir des autres axes et on peut généraliser avec :\n$$ \\boldsymbol{Q}_{\\mathrm{Nc}}=\\int \\mathrm{d}^3 x\\left(\\boldsymbol{\\Phi} \\times \\partial_0 \\boldsymbol{\\Phi}\\right) $$\n$$ \\hat{Q}_{\\mathrm{N} c}^a=-\\mathrm{i} \\int \\mathrm{~d}^3 p\\, \\varepsilon^{a b c} \\hat{a}_{b \\boldsymbol{p}}^{\\dagger} \\hat{a}_{c \\boldsymbol{p}} $$\nCette charge conservée est l\u0026rsquo;isospin !\nQuantification canonique d\u0026rsquo;un champ massif de spin 1 Prenons le Lagrangien de l\u0026rsquo;électromagnétisme $-\\frac{1}{4} F_{\\mu \\nu} F^{\\mu \\nu}$ (où $F_{\\mu \\nu}=\\partial_\\mu A_\\nu-\\partial_\\nu A_\\mu$) et ajoutons-lui un terme de masse à la Klein-Gordon :\n$$ \\mathcal{L}=-\\frac{1}{4} F_{\\mu \\nu} F^{\\mu \\nu}+\\frac{1}{2} m^2 A_\\mu A^\\mu $$\nCe Lagrangien décrit un champ dont les excitations sont des bosons vectoriels de spin 1.\nLes équations du mouvement de ce champ sont obtenues grâce aux équations d\u0026rsquo;Euler-Lagrange :\n$$ \\partial_\\mu F^{\\mu \\nu}+m^2 A^\\nu=0 $$\nC\u0026rsquo;est l\u0026rsquo;équation de Proca.\nPar symétrie, $\\partial_\\mu \\partial_\\nu F^{\\mu \\nu}=0$. On obtient donc $m^2 \\partial_\\nu A^\\nu=0$ et comme $m≠0$, le champ décrit est à divergence nulle : $\\partial_\\mu A^\\mu=0$. Et le champ pouvant à nouveau se décomposer en ondes planes, on peut aussi écrire $p_\\mu A^\\mu = 0$. Cela fournit une contrainte sur les composantes du champ en en liant l\u0026rsquo;une aux trois autres. Au final, le champ aura donc 3 degrés de liberté de polarisation au lieu de 4.\nComme dans le cas du champ $\\boldsymbol{\\Phi}$ avec la symétrie $SO(3)$, on va avoir besoin, pour la décomposition en modes, d\u0026rsquo;opérateurs de création et d\u0026rsquo;annihilation séparés pour chaque polarisation. Mais en plus, chacun de ces opérateurs devra être multiplié par un vecteur polarisation $\\epsilon^\\mu_\\lambda(p)$ qui vit dans l\u0026rsquo;espace de Minkowski et dont les composantes dépendent de l\u0026rsquo;impulsion de la particule considérée.\n$$ \\begin{aligned} \\hat{A}^\\mu(x)= \\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_\\boldsymbol{p}\\right)^{\\frac{1}{2}}}\\left[\\left(\\begin{array}{c} \\epsilon_1^0(p) \\\\ \\epsilon_1^1(p) \\\\ \\epsilon_1^2(p) \\\\ \\epsilon_1^3(p) \\end{array}\\right) \\hat{a}_{\\boldsymbol{p} 1} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\left(\\begin{array}{c} \\epsilon_1^{0 *}(p) \\\\ \\epsilon_1^{1 *}(p) \\\\ \\epsilon_1^{2 *}(p) \\\\ \\epsilon_1^{3 *}(p) \\end{array}\\right) \\hat{a}_{\\boldsymbol{p} 1}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x} +\\left(\\begin{array}{c} \\epsilon_2^0(p) \\\\ \\epsilon_2^1(p) \\\\ \\epsilon_2^2(p) \\\\ \\epsilon_2^3(p) \\end{array}\\right) \\hat{a}_{\\boldsymbol{p} 2} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\left(\\begin{array}{c} \\epsilon_2^{0 *}(p) \\\\ \\epsilon_2^{1 *}(p) \\\\ \\epsilon_2^{2 *}(p) \\\\ \\epsilon_2^{3 *}(p) \\end{array}\\right) \\hat{a}_{\\boldsymbol{p} 2}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x} +\\left(\\begin{array}{c} \\epsilon_3^0(p) \\\\ \\epsilon_3^1(p) \\\\ \\epsilon_3^2(p) \\\\ \\epsilon_3^3(p) \\end{array}\\right) \\hat{a}_{\\boldsymbol{p} 3} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\left(\\begin{array}{c} \\epsilon_3^{0 *}(p) \\\\ \\epsilon_3^{1 *}(p) \\\\ \\epsilon_3^{2 *}(p) \\\\ \\epsilon_3^{4 *}(p) \\end{array}\\right) \\hat{a}_{\\boldsymbol{p} 3}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right] \\end{aligned} $$\nEn plus compact, cela donne :\n$$ \\hat{A}^\\mu(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}} \\sum_{\\lambda=1}^3\\left(\\epsilon_\\lambda^\\mu(p) \\hat{a}_{\\boldsymbol{p} \\lambda} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\epsilon_\\lambda^{\\mu *}(p) \\hat{a}_{\\boldsymbol{p} \\lambda}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nLa condition $p_\\mu A^\\mu=0$ devient $p_\\mu \\epsilon_\\lambda^\\mu(p)=0$ montrant bien que les vecteurs polarisation dépendent de l\u0026rsquo;impulsion. C\u0026rsquo;est leur rôle de rendre le champ $A^\\mu$ perpendiculaire à $p^\\mu$.\nÀ quoi doivent ressembler les vecteurs polarisation ? Comme tout quadrivecteur de l\u0026rsquo;espace de Minkowski, ils se transforment selon les transformations de Lorentz. On peut donc partir d\u0026rsquo;une particule dans son référentiel propre, puis généraliser en lui faisant subir un boost.\nConsidérons donc une particule dans son référentiel propre avec une impulsion $p^\\mu=(m,0,0,0)^t$. On veut $p^\\mu \\epsilon_{\\lambda \\mu}(p)=0$ pour tout $\\lambda$, c\u0026rsquo;est à dire que l\u0026rsquo;on veut des vecteurs polarisation normaux à $p^\\mu$ dans ce référentiel. Un choix simple de vecteurs linéairement indépendants consiste à prendre :\n$$ \\epsilon_1(m, 0)=\\left(\\begin{array}{l} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array}\\right), \\epsilon_2(m, 0)=\\left(\\begin{array}{l} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right), \\epsilon_3(m, 0)=\\left(\\begin{array}{l} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{array}\\right) $$\nPour obtenir $\\epsilon_\\lambda(p)$ pour un référentiel arbitraire, on applique la transformation de Lorentz $\\boldsymbol{\\Lambda}(p)$. Par exemple, pour une particule avec une impulsion $p_z=|\\boldsymbol{p}|$ dans la direction $z$ (avec donc $p^\\mu=\\left(E_{\\boldsymbol{p}}, 0,0,|\\boldsymbol{p}|\\right)^t$), on applique le boost donné par la matrice :\n$$ \\Lambda^\\mu{ }_\\nu(p)=\\frac{1}{m}\\left(\\begin{array}{cccc} E_{\\boldsymbol{p}} \u0026amp; 0 \u0026amp; 0 \u0026amp; |\\boldsymbol{p}| \\\\ 0 \u0026amp; m \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; m \u0026amp; 0 \\\\ |\\boldsymbol{p}| \u0026amp; 0 \u0026amp; 0 \u0026amp; E_{\\boldsymbol{p}} \\end{array}\\right) $$\nEt on obtient les vecteurs polarisations :\n$$ \\epsilon_1\\left(E_{\\boldsymbol{p}}, 0,0,|\\boldsymbol{p}|\\right)=\\left(\\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array}\\right), \\epsilon_2\\left(E_{\\boldsymbol{p}}, 0,0,|\\boldsymbol{p}|\\right)=\\left(\\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right), \\epsilon_3\\left(E_{\\boldsymbol{p}}, 0,0,|\\boldsymbol{p}|\\right)=\\left(\\begin{array}{c} |\\boldsymbol{p}| / m \\\\ 0 \\\\ 0 \\\\ E_{\\boldsymbol{p}} / m \\end{array}\\right) $$\nLa forme diagonalisée de l\u0026rsquo;Hamiltonien est pour sa part :\n$$ \\hat{H}=\\int \\mathrm{d}^3 p E_{\\boldsymbol{p}} \\sum_{\\lambda=1}^3 \\hat{a}_{\\boldsymbol{p} \\lambda}^{\\dagger} \\hat{a}_{\\boldsymbol{p} \\lambda} $$\nÉnergie de toutes les particules dans toutes les polarisations.\nChapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/tqcd/",
	"title": "TQC-4",
	"tags": [],
	"description": "",
	"content": " Théorie quantique des champs note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Très souvent une simple traduction.\nRetour sommaire\nPartie 4 : Évolution temporelle Chapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc5/",
	"title": "TQC-5",
	"tags": [],
	"description": "",
	"content": " Théorie quantique des champs \u0026ndash; Partie 5 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Très souvent une simple traduction.\nRetour sommaire\nChamps de jauge et théorie de jauge Une invariance de jauge trahit moins une symétrie du système qu\u0026rsquo;une redondance dans sa description (différentes configurations du champ aboutissent à des observables identiques). Cette redondance nous laisse une certaine latitude quant au choix de la meilleure formulation, c\u0026rsquo;est le choix de jauge. Une transformation d\u0026rsquo;une description à une autre est appelée transformation de jauge et l\u0026rsquo;invariance sous-jacente est l\u0026rsquo;invariance de jauge.\nL\u0026rsquo;invariance de jauge n\u0026rsquo;est pas une symétrie dans le sens où il n\u0026rsquo;est pas question ici de curseurs internes permettant de passer d\u0026rsquo;une particule à une autre. C\u0026rsquo;est plutôt l\u0026rsquo;affirmation de notre incapacité à trouver une description unique du système. On peut citer comme exemple l\u0026rsquo;indétermination de l\u0026rsquo;origine des potentiels électriques (le choix d\u0026rsquo;associer 0 V à la terre est arbitraire), celle de l\u0026rsquo;origine des phases en mécanique quantique (on peut passer de $\\psi(x)$ à $\\psi(x)\\mathrm{e}^{\\mathrm{i}\\alpha}$ sans changer la physique), et, en électromagnétisme, la liberté sur le choix de $A$ laissé par la transformation $A \\rightarrow A+\\nabla \\chi$ (où $\\chi(\\boldsymbol{x})$ est une fonction de la position) qui est sans effet sur $\\boldsymbol{B}=\\boldsymbol{\\nabla} \\times \\boldsymbol{A}$. On verra que ces trois indéterminations sont en fait liées entre elles. On constate aussi que dans les trois cas, seules des variations de la grandeur indéterminée peuvent faire l\u0026rsquo;objet d\u0026rsquo;observations.\nPour avoir des définitions propres, on doit lever toute ambiguïté en fixant des jauges.\nIl peut s\u0026rsquo;agir de choix de jauges globaux, identiques en tout point, ou de choix locaux, susceptibles de varier d\u0026rsquo;un point à l\u0026rsquo;autre.\nPartons du Lagrangien du champ scalaire complexe :\n$$ \\mathcal{L}=\\left(\\partial^\\mu \\psi\\right)^{\\dagger}\\left(\\partial_\\mu \\psi\\right)-m^2 \\psi^{\\dagger} \\psi $$\nComme on l\u0026rsquo;a vu, la symétrie $U(1)$ se traduit par la non variation du Lagrangien (et par extension des équations du mouvement) lors de la transformation $\\psi(x) \\rightarrow \\psi(x) \\mathrm{e}^{\\mathrm{i} \\alpha}$. Il s\u0026rsquo;agit là d\u0026rsquo;une transformation globale puisqu\u0026rsquo;elle change le champ d\u0026rsquo;une même valeur en tout point de l\u0026rsquo;espace-temps. La théorie est donc dite invariante par transformation $U(1)$ globale.\nQue se passerait-il si on imposait une invariance locale par rapport à la phase ? Il faudrait que la transformation $\\psi(x) \\rightarrow \\psi(x) \\mathrm{e}^{\\mathrm{i} \\alpha(x)}$ (où $\\alpha(x)$ peut maintenant différer d\u0026rsquo;un point à l\u0026rsquo;autre) soit sans effet sur les équations du mouvement. Cela semble une demande un peu extrême, mais elles se révèle surprenemment féconde : l\u0026rsquo;électromagnétisme en découle !\nOn n\u0026rsquo;est pas embêté par le terme de masse : $m^2\\psi^\\dagger\\psi\\rightarrow m^2\\psi^\\dagger \\mathrm{e}^{-\\mathrm{i} \\alpha(x)} \\mathrm{e}^{\\mathrm{i} \\alpha(x)} \\psi = m^2\\psi^\\dagger\\psi $. Par contre, le terme contenant les dérivées pose problème puisque la dérivée agit maintenant sur $\\alpha(x)$ :\n$$ \\begin{aligned} \\partial_\\mu \\psi(x) \u0026amp; \\rightarrow \\partial_\\mu \\psi(x) \\mathrm{e}^{\\mathrm{i} \\alpha(x)} \\\\ \u0026amp; =\\mathrm{e}^{\\mathrm{i} \\alpha(x)} \\partial_\\mu \\psi(x)+\\psi(x) \\mathrm{e}^{\\mathrm{i} \\alpha(x)} \\mathrm{i} \\partial_\\mu \\alpha(x) \\\\ \u0026amp; =\\mathrm{e}^{\\mathrm{i} \\alpha(x)}\\left[\\partial_\\mu+\\mathrm{i} \\partial_\\mu \\alpha(x)\\right] \\psi(x) \\end{aligned} $$\nEt de même, on a $\\partial^\\mu \\psi^{\\dagger}(x) \\rightarrow \\mathrm{e}^{-\\mathrm{i} \\alpha(x)}\\left[\\partial^\\mu-\\mathrm{i} \\partial^\\mu \\alpha(x)\\right] \\psi^{\\dagger}(x)$.\nLe premier terme du Lagrangien est donc tout chamboulé :\n$$ \\left(\\partial^\\mu \\psi^{\\dagger}\\right)\\left(\\partial_\\mu \\psi\\right)-\\mathrm{i}\\left(\\partial^\\mu \\alpha\\right) \\psi^{\\dagger}\\left(\\partial_\\mu \\psi\\right)+\\mathrm{i}\\left(\\partial^\\mu \\psi^{\\dagger}\\right)\\left(\\partial_\\mu \\alpha\\right) \\psi+\\left(\\partial^\\mu \\alpha\\right)\\left(\\partial_\\mu \\alpha\\right) \\psi^{\\dagger} \\psi $$\nFaire dépendre $\\alpha$ de la position a logiquement retiré sa symétrie $U(1)$ à la théorie qui n\u0026rsquo;est donc pas invariante sous une transformation $U(1)$ locale. Mais peut-on restaurer cette symétrie ?\nOui, en ajoutant un nouveau champ $A^\\mu(x)$ dont la mission sera d\u0026rsquo;annuler les variations de la phase d\u0026rsquo;un point à l\u0026rsquo;autre. On greffe ce champ à la dérivée pour créer une sorte de \u0026ldquo;super dérivée\u0026rdquo; : la dérivée covariante $D_\\mu$.\n$$ D_\\mu=\\partial_\\mu+\\mathrm{i} q A_\\mu(x) $$\nLa dérivée covariante peut réparer la symétrie $U(1)$ si le nouveau champ $A_\\mu$ se transforme comme :\n$$ A_\\mu \\rightarrow A_\\mu-\\frac{1}{q} \\partial_\\mu \\alpha(x) $$\n$q$ est le paramètre de couplage, il nous informe sur la force de l\u0026rsquo;interaction entre $A_\\mu$ et les autres champs.\nSi $\\psi(x) \\rightarrow \\psi(x) \\mathrm{e}^{\\mathrm{i} \\alpha(x)}$, alors $\\partial_\\mu \\psi \\rightarrow\\left(\\partial_\\mu \\psi\\right) \\mathrm{e}^{\\mathrm{i} \\alpha}+\\mathrm{i}\\left(\\partial_\\mu \\alpha\\right) \\psi$ et donc\n$$ \\begin{aligned} D_\\mu \\psi=\\left(\\partial_\\mu+\\mathrm{i} q A_\\mu\\right) \\psi \u0026amp; \\rightarrow\\left(\\partial_\\mu \\psi\\right) \\mathrm{e}^{\\mathrm{i} \\alpha}+\\mathrm{i}\\left(\\partial_\\mu \\alpha\\right) \\psi+\\mathrm{i} q A_\\mu \\psi \\mathrm{e}^{\\mathrm{i} \\alpha}-\\mathrm{i}\\left(\\partial_\\mu \\alpha\\right) \\psi \\\\ \u0026amp; =D_\\mu\\left(\\psi \\mathrm{e}^{\\mathrm{i} \\alpha}\\right) \\end{aligned} $$\nLe Lagrangien entier devient invariant si on remplace les dérivées ordinaires par des dérivées covariantes :\n$$ \\mathcal{L}=\\left(D^\\mu \\psi\\right)^{\\dagger}\\left(D_\\mu \\psi\\right)-m^2 \\psi^{\\dagger} \\psi $$\nPour imposer une symétrie $U(1)$ locale, la théorie se doit alors d\u0026rsquo;être invariante par rapport à deux jeux de transformations en parallèle :\n$$ \\begin{aligned} \\psi(x) \u0026amp; \\rightarrow \\psi(x) \\mathrm{e}^{\\mathrm{i} \\alpha(x)} \\\\ A_\\mu(x) \u0026amp; \\rightarrow A_\\mu(x)-\\frac{1}{q} \\partial_\\mu \\alpha(x) \\end{aligned} $$\nUne théorie où un champ $A^\\mu(x)$ est introduit pour permettre une invariance par rapport à une transformation locale est appelée théorie de jauge. Le champ $A^\\mu(x)$ est appelé champ de jauge.\nLe champ de jauge, introduit pour satisfaire notre envie soudaine d\u0026rsquo;invariance locale, peut-il s\u0026rsquo;avérer suffisamment réel jusqu\u0026rsquo;à avoir sa propre dynamique ?\nThéorie de jauge la plus simple : l\u0026rsquo;électromagnétisme Une théorie dont le Lagrangien contient des termes décrivant $A^\\mu(x)$ se doit d\u0026rsquo;être invariante sous des transformations du type $A_\\mu(x) \\rightarrow A_\\mu(x)-\\frac{1}{q} \\partial_\\mu \\alpha(x)$. L\u0026rsquo;électromagnétisme est justement un exemple d\u0026rsquo;une telle théorie avec son champ vectoriel $A^\\mu(x)=(V(x), \\boldsymbol{A}(x))$ formant le Lagrangien :\n$$ \\mathcal{L}=-\\frac{1}{4}\\left(\\partial_\\mu A_\\nu-\\partial_\\nu A_\\mu\\right)\\left(\\partial^\\mu A^\\nu-\\partial^\\nu A^\\mu\\right)-J_{\\mathrm{em}}^\\mu A^\\mu $$\nLes équations du mouvement qu\u0026rsquo;on en déduit ne sont autres que les deux équations de Maxwell inhomogènes :\n$$ \\partial^2 A^\\nu-\\partial^\\nu\\left(\\partial_\\mu A^\\mu\\right)=J_{\\mathrm{em}}^\\nu $$\nNi le Lagrangien, ni les équations du mouvement ne sont modifiés par la transformation $A_\\mu(x) \\rightarrow A_\\mu(x)-\\partial_\\mu \\chi(x)$ qui se décompose en :\n$$ \\begin{aligned} V \u0026amp; \\rightarrow V-\\partial_0 \\chi \\\\ \\boldsymbol{A} \u0026amp; \\rightarrow \\boldsymbol{A}+\\boldsymbol{\\nabla} \\chi \\end{aligned} $$\nC\u0026rsquo;est bien ce qu\u0026rsquo;on nomme en électromagnétisme l\u0026rsquo;invariance de jauge (si $A_\\mu$ décrit correctement le champ électromagnétique dans une certaine situation, alors $A_\\mu-\\partial_\\mu \\chi$ aussi). Et on en déduit que l\u0026rsquo;électromagnétisme est une théorie de jauge puisqu\u0026rsquo;en choisissant de redéfinir $\\chi(x)$ comme $\\alpha(x)/q$, on retrouve bien la définition vue plus haut.\nComment choisir $\\chi(x)$ ? Il est commun d\u0026rsquo;en passer par la jauge de Lorenz (sans \u0026ldquo;t\u0026rdquo;) :\n$$ \\partial_\\mu A^\\mu(x)=0 $$\n$A_\\mu$ se transforme en $A_\\mu^{\\prime}=A_\\mu-\\partial_\\mu \\chi$ et la jauge de Lorenz impose $\\partial^\\mu A_\\mu^{\\prime}=\\partial^\\mu A_\\mu-\\partial^\\mu \\partial_\\mu \\chi=0$. Pour la respecter, il faut donc poser $\\partial^2 \\chi=\\partial^\\mu A_\\mu$.\nGrâce à la jauge de Lorenz et en l\u0026rsquo;absence de courant $J_{\\mathrm{em}}^\\mu$, on obtient l\u0026rsquo;équation d\u0026rsquo;un champ libre sans masse. En effet l\u0026rsquo;équation du mouvement $\\partial^2 A^\\nu-\\partial^\\nu\\left(\\partial_\\mu A^\\mu\\right)=J_{\\mathrm{em}}^\\nu$ devient $\\partial^2 A^{\\prime \\nu}-\\partial^\\nu\\left(\\partial_\\mu A^{\\prime \\mu}\\right)=\\partial^2 A^{\\prime \\nu}=0$ dont les solutions sont des ondes planes de la forme $A^\\mu=\\epsilon^\\mu(p) \\mathrm{e}^{-\\mathrm{i} p \\cdot x}$ avec $E_{\\boldsymbol{p}}=|\\boldsymbol{p}|$. La jauge de Lorenz fait donc ressembler l\u0026rsquo;électromagnétisme à une théorie de champ vectoriel.\nOn avait déjà rencontré la condition de Lorenz dans le cas du champ massif de spin 1 mais elle n\u0026rsquo;avait alors rien d\u0026rsquo;un choix ; on l\u0026rsquo;obtenait en prenant la divergence de l\u0026rsquo;équation de Proca\u0026hellip; Mais dans tous les cas, la condition réduit le nombre de composantes indépendantes de $A\u0026rsquo;^\\mu$ de quatre à trois.\nCela ne rend toujours pas $A^{\\prime \\mu}$ unique ici puisqu\u0026rsquo;on peut continuer à transformer le champ $A_\\mu^\\prime \\rightarrow A_\\mu^{\\prime \\prime}=A_\\mu^{\\prime}-\\partial_\\mu \\xi$ tant que $\\partial^2 \\xi = 0$ ($A^{\\prime \\mu}$ et $A^{\\prime \\prime \\mu}$ respectent tous deux la condition de Lorenz). Pour rendre $A^{\\prime \\prime \\mu}$ unique, on choisit en plus de fixer $\\partial_0 \\xi=A_0^{\\prime}$, ce qui implique $A_0^{\\prime \\prime}=0$.\nAvec ce choix, la condition de Lorenz implique finalement la jauge de Coulomb :\n$$ \\boldsymbol{\\nabla} \\cdot \\boldsymbol{A}^{\\prime \\prime}=0 $$\nLe nombre de degrés de liberté du champ est encore réduit d\u0026rsquo;un cran.\nLa physique impose finalement au champ $A^\\mu$ de n\u0026rsquo;avoir que deux composantes indépendantes !\nLes équations du mouvement sous la jauge de Lorenz donnent $\\partial^2A^\\mu = 0$. Avec la condition $A^0 = 0$, cela implique des ondes planes de la forme $\\boldsymbol{A}=\\boldsymbol{\\epsilon} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}$.\nLa jauge de Coulomb $\\boldsymbol{\\nabla} \\cdot \\boldsymbol{A}=0$ impose alors $\\boldsymbol{p} \\cdot \\boldsymbol{A}=\\boldsymbol{p} \\cdot \\boldsymbol{\\epsilon}=0$ qui nous dit que la direction de propagation de l\u0026rsquo;onde est perpendiculaire à la polarisation ; l\u0026rsquo;onde est transverse !\nEn supposant une propagation selon l\u0026rsquo;axe $z$ avec une impulsion $q^\\mu=(|\\boldsymbol{q}|, 0,0,|\\boldsymbol{q}|)$, on peut par exemple se donner une polarisation linéaire :\n$$ \\boldsymbol{\\epsilon}_1(q)=\\left(\\begin{array}{l} 1 \\\\ 0 \\\\ 0 \\end{array}\\right), \\quad \\boldsymbol{\\epsilon}_2(q)=\\left(\\begin{array}{l} 0 \\\\ 1 \\\\ 0 \\end{array}\\right) $$\nou encore une polarisation circulaire avec :\n$$ \\epsilon_{\\mathrm{R}}^*(q)=-\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{l} 1 \\\\ \\mathrm{i} \\\\ 0 \\end{array}\\right), \\quad \\epsilon_{\\mathrm{L}}^*(q)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{c} 1 \\\\ -\\mathrm{i} \\\\ 0 \\end{array}\\right) $$\nPour observer les effets du champ électromagnétique, il faut le coupler à un champ de matière. La recette la plus simple consiste à remplacer les dérivées ordinaires par les dérivées covariantes dans le Lagrangien. On nomme ce procédé couplage minimal.\nConsidérons un champ scalaire complexe en présence d\u0026rsquo;un champ électromagnétique. Si les champs sont indépendants, le Lagrangien total s\u0026rsquo;écrit comme la somme des Lagrangiens de chacune des théories :\n$$ \\mathcal{L}=\\left(\\partial^\\mu \\psi\\right)^{\\dagger}\\left(\\partial_\\mu \\psi\\right)-m^2 \\psi^{\\dagger} \\psi-\\frac{1}{4} F_{\\mu \\nu} F^{\\mu \\nu} $$\nOn obtient un couplage entre les champs en passant de $\\partial$ à $D$ :\n$$ \\begin{aligned} \\mathcal{L}\u0026amp;= \\left(D^\\mu \\psi\\right)^{\\dagger}\\left(D_\\mu \\psi\\right)-m^2 \\psi^{\\dagger} \\psi-\\frac{1}{4} F_{\\mu \\nu} F^{\\mu \\nu} \\\\ \u0026amp;= \\left(\\partial^\\mu \\psi^{\\dagger}-\\mathrm{i} q A^\\mu \\psi^{\\dagger}\\right)\\left(\\partial_\\mu \\psi+\\mathrm{i} q A_\\mu \\psi\\right)-m^2 \\psi^{\\dagger} \\psi-\\frac{1}{4} F_{\\mu \\nu} F^{\\mu \\nu} \\\\ \u0026amp;= \\partial^\\mu \\psi^{\\dagger} \\partial_\\mu \\psi-m^2 \\psi^{\\dagger} \\psi-\\frac{1}{4} F_{\\mu \\nu} F^{\\mu \\nu} + {\\color{#D41876}\\left(-\\mathrm{i} q A^\\mu \\psi^{\\dagger}\\left(\\partial_\\mu \\psi\\right)+\\mathrm{i} q\\left(\\partial^\\mu \\psi^{\\dagger}\\right) A_\\mu \\psi+q^2 \\psi^{\\dagger} \\psi A^\\mu A_\\mu\\right) } \\end{aligned} $$\nLe couplage entre le champ $A^\\mu$ et les champs $\\psi$ et $\\psi^\\dagger$ est contenu dans le dernier terme et l\u0026rsquo;importance du couplage est fixée par $q$, la charge électromagnétique.\nOn appelle principe de jauge la notion selon laquelle un champ de jauge introduit pour assurer une symétrie locale dicte la forme du couplage, c\u0026rsquo;est-à-dire des interactions, dans la théorie.\nQuantification canonique du champ électromagnétique Le terme de masse du champ massif de spin 1 vu précédemment lui ôtait toute possibilité d\u0026rsquo;invariance de jauge alors que la nature non massive du champ de jauge lui confère cette invariance et lui retire une composante.\nOn obtient in fine :\n$$ \\hat{A}^\\mu(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}} \\sum_{\\lambda=1}^2\\left(\\epsilon_\\lambda^\\mu(p) \\hat{a}_{\\boldsymbol{p} \\lambda} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\epsilon_\\lambda^{\\mu *}(p) \\hat{a}_{\\boldsymbol{p} \\lambda}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\navec $E_p=|\\boldsymbol{p}|$.\nEt l\u0026rsquo;Hamiltonien est donné par :\n$$ \\hat{H}=\\int \\mathrm{d}^3 p \\sum_{\\lambda=1}^2 E_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{p} \\lambda}^{\\dagger} \\hat{a}_{\\boldsymbol{p} \\lambda} $$\nLes excitations du champ électromagnétique sont des photons qu\u0026rsquo;on peut observer dans deux états de polarisation transverses.\nCes particules ont un spin $S=1$ et on en trouve deux types : $\\hat{a}_{\\boldsymbol{p} 1}^{\\dagger}|0\\rangle$ et $\\hat{a}_{\\boldsymbol{p} 2}^{\\dagger}|0\\rangle.$\nConsidérons un photon se propageant selon la direction $z$ avec l\u0026rsquo;impulsion $q^\\mu=(|\\boldsymbol{q}|, 0,0,|\\boldsymbol{q}|)$. Dans une base de polarisation circulaire, on peut écrire $\\epsilon_{\\lambda=\\mathrm{R}}^*(q)=-\\frac{1}{\\sqrt{2}}(0,1, \\mathrm{i}, 0)$ (correspondant à $S_z = 1$) et $\\epsilon_{\\lambda=\\mathrm{L}}^*(q)=\\frac{1}{\\sqrt{2}}(0,1,-\\mathrm{i}, 0)$ (correspondant à $S_z = -1$). Il n\u0026rsquo;y a pas de photon avec $S^z = 0$ puisque cela correspondrait à une polarisation longitudinale interdite $\\epsilon_{\\lambda=3}^*(p)=(0,0,0,1).$\nSymétries discrètes On a rencontré jusque-là des symétries portées par des transformations continues (translations, rotations) représentées par des groupes continus (groupes de Lie). Mais on peut aussi rencontrer des symétries correspondant à des transformations discrètes représentées cette fois-ci par des groupes finis.\nConjugaison de charge On appelle conjugaison de charge la transformation qui change une particule en son antiparticule. C\u0026rsquo;est l\u0026rsquo;opérateur $\\text { C }$ qui se charge de cette prouesse. Il ne permute pas seulement la charge d\u0026rsquo;une particule, mais aussi son nombre leptonique, son hypercharge et tout autre \u0026ldquo;nombre de charge\u0026rdquo;. On a alors :\n$$ \\mathrm{C}|p\\rangle=|\\bar{p}\\rangle $$\nLa charge d\u0026rsquo;une particule $p$ de charge $q$ est mesurée par un opérateur $\\hat{Q}$ : $\\hat{Q}|p\\rangle=q|p\\rangle$. Par contre : $\\hat{Q}|\\bar{p}\\rangle=-q|\\bar{p}\\rangle$. On peut donc écrire $\\mathrm{C} \\hat{Q}|p\\rangle=q \\mathrm{C}|p\\rangle=q|\\bar{p}\\rangle$, mais $\\hat{Q} \\mathrm{C}|p\\rangle=\\hat{Q}|\\bar{p}\\rangle=-q|\\bar{p}\\rangle$, ce qui implique que $\\hat{Q} \\mathrm{C}=-\\mathrm{C} \\hat{Q}$, ou de manière équivalente :\n$$ \\mathrm{C}^{-1} \\hat{Q} \\mathrm{C}=-\\hat{Q} $$\nL\u0026rsquo;échange entre particule et antiparticule impose :\n$$ \\mathrm{C}^{-1} \\hat{a}_{\\boldsymbol{p}} \\mathrm{C}=\\hat{b}_{\\boldsymbol{p}} \\quad\\quad \\mathrm{C}^{-1} \\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{C}=\\hat{a}_{\\boldsymbol{p}}^{\\dagger} $$\nEt comme un champ scalaire $\\hat{\\psi}(x)$ peut s\u0026rsquo;écrire $\\hat{\\psi}(x)=\\int_{\\boldsymbol{p}}\\left(\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right)$, on doit avoir $\\mathrm{C}^{-1} \\hat{\\psi} \\mathrm{C}=\\hat{\\psi}^{\\dagger}$ ($\\hat{\\psi}^{\\dagger}=\\int_{\\boldsymbol{p}}\\left(\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}+\\hat{b}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}\\right)$).\nComme $\\mathrm{C}^2=I$, les valeurs propre de $\\mathrm{C}$ ne peuvent être que $\\pm 1$. La plupart des particules ne sont pas des états propres de $\\mathrm{C}$ puisque si elles l\u0026rsquo;étaient, on aurait $\\mathrm{C}|p\\rangle=|\\bar{p}\\rangle= \\pm|p\\rangle$, ce qui impliquerait que $|\\bar{p}\\rangle$ est le même état que $|p\\rangle$ et donc que la particule est sa propre antiparticule. C\u0026rsquo;est vrai pour les particules sans charge quantique.\nLe photon $\\gamma$, lui, est un état propre de $\\mathrm{C}$ avec la valeur propre $-1$, puisqu\u0026rsquo;en changeant totues les particules en leurs antiparticules, le champ électromagnétique est renversé ($A^\\mu \\rightarrow-A^\\mu$). C\u0026rsquo;est aussi le cas pour le pion neutre $\\pi^0$, mais avec la valeur propre $+1$. Cela explique pourquoi la réaction $\\pi^0 \\rightarrow \\gamma+\\gamma$ est autorisée alors que $\\pi^0 \\rightarrow \\gamma+\\gamma+\\gamma$ est impossible.\nParité Une symétrie miroir inverse la direction de l\u0026rsquo;axe perpendiculaire au miroir et conserve les autres. Si on fait suivre cette transformation d\u0026rsquo;une rotation à 180° autour de l\u0026rsquo;axe perpendiculaire au miroir, on aura inversé toutes les directions spatiales, opérant ainsi une inversion spatiale ($\\boldsymbol{x}\\rightarrow-\\boldsymbol{x}$). Cette transformation, appelée parité, est prise en charge par l\u0026rsquo;opérateur $\\mathrm{P}$. L\u0026rsquo;opérateur position va donc anticommuter avec l\u0026rsquo;opérateur parité :\n$$ \\hat{\\boldsymbol{x}} \\mathrm{P}=-\\mathrm{P} \\hat{\\boldsymbol{x}} $$\nOu de manière équivalente :\n$$ \\mathrm{P}^{-1} \\hat{\\boldsymbol{x}} \\mathrm{P}=-\\hat{\\boldsymbol{x}} $$\nL\u0026rsquo;effet sur les coordonnées de l\u0026rsquo;impulsion est le même ($p \\rightarrow-\\boldsymbol{p}$) et donc :\n$$ \\mathrm{P}^{-1} \\hat{\\boldsymbol{p}} \\mathrm{P}=-\\hat{\\boldsymbol{p}} $$\nL\u0026rsquo;opérateur $\\mathrm{P}$ est hermitien et son propre inverse ($\\mathrm{P}^2=I$), donc $\\mathrm{P}$ est aussi un opérateur unitaire.\nL\u0026rsquo;opérateur de parité est sans effet sur les scalaires mais renverse les vecteurs. Mais il existe une classe spéciale de scalaire et de vecteurs (même si pas réellement des scalaires et des vecteurs mais plutôt des ojets composites) pour lesquels ce n\u0026rsquo;est pas vrai : les pseudoscalaires formés par un produit mixte et les pseudovecteurs (aussi appelés vecteurs axiaux) formés par un produit vectoriel entre vecteurs ordinaires (aussi appelés vecteurs polaires).\nLe champ électrique $\\boldsymbol{E}$ agit comme un vecteur ordianaire (polaire) alors que le champ magnitique et le moment cinétique $\\boldsymbol{L}$ sont des pesudovecteurs (vecteurs axiaux).\n$$ \\begin{aligned} \\mathrm{P}(\\text { scalaire })\u0026amp;=\\text { scalaire }\\\\ \\mathrm{P}(\\text { pseudoscalaire })\u0026amp;=-\\text { pseudoscalaire }\\\\ \\mathrm{P}(\\textbf{ vecteur })\u0026amp;=- \\textbf{ vecteur }\\\\ \\mathrm{P}(\\textbf{ pseudovecteur })\u0026amp;= \\textbf{pseudovecteur } \\end{aligned} $$\nComme $\\mathrm{P}^2 = I$, sous la multiplication, le groupe ${I, P}$ est isomorphe à $\\mathbb{Z}_2$, le groupe cyclique d\u0026rsquo;ordre 2.\n$$ \\begin{array}{c|cc} \u0026amp; I \u0026amp; \\mathrm{P} \\\\ \\hline I \u0026amp; I \u0026amp; \\mathrm{P} \\\\ \\mathrm{P} \u0026amp; \\mathrm{P} \u0026amp; I \\end{array} $$\nComme pour $\\mathrm{C}$, cela signifie que les valeurs propres de $\\mathrm{P}$ sont $\\pm1$. Les scalaires et pseudovecteurs ont une valeur de parité de $+1$ alors que pseudoscalaires et vecteurs ont une parité de $-1$.\nLe photon étant une excitation d\u0026rsquo;un champ vectoriel non massique, il possède une parité intrinsèque de $-1$. Le pion est, lui, décrit par un champ pseudoscalaire et a donc aussi une parité de $-1$. On verra plus tard que la parité d\u0026rsquo;un fermion est opposée à celle de son antiparticule.\nLa complication avec l\u0026rsquo;opérateur de parité est qu\u0026rsquo;il agit à la fois sur les coordonnées du champ et sur la nature même du champ.\nPrenons le cas d\u0026rsquo;un champ scalaire. La parité va faire en sorte que $\\phi(t, \\boldsymbol{x}) \\rightarrow \\phi(t,-\\boldsymbol{x})$. Étudions maintenant son action sur les opérateurs de création et d\u0026rsquo;annihilation :\n$$ \\mathrm{P}^{-1} \\hat{\\phi}(t, \\boldsymbol{x}) \\mathrm{P}=\\hat{\\phi}(t,-\\boldsymbol{x})=\\int_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i}(E t+\\boldsymbol{p} \\cdot \\boldsymbol{x})}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i}(E t+\\boldsymbol{p} \\cdot \\boldsymbol{x})} $$\nC\u0026rsquo;est possible si $\\mathrm{P}^{-1} \\hat{a}_{\\boldsymbol{p}} \\mathrm{P}=\\hat{a}_{-{\\boldsymbol{p}}}$ et $\\mathrm{P}^{-1} \\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{P}=\\hat{a}_{-\\boldsymbol{p}}^{\\dagger}$. L\u0026rsquo;opérateur de parité renverse simplement les impulsions pour les opérateurs de création et d\u0026rsquo;annihilation.\nRenversement du temps L\u0026rsquo;opérateur renversement du temps $\\mathrm{T}$ transforme un champ scalaire $\\phi(t, \\boldsymbol{x})$ en $\\phi(-t, \\boldsymbol{x})$. Elle laisse donc le vecteur position tranquille :\n$$ \\mathrm{T}^{-1} \\hat{\\boldsymbol{x}} \\mathrm{~T}=\\hat{\\boldsymbol{x}} $$\nMais elle renverse l\u0026rsquo;impulsion :\n$$ \\mathrm{T}^{-1} \\hat{\\boldsymbol{p}} \\mathrm{~T}=-\\hat{\\boldsymbol{p}} $$\nLa seule possibilité pour préserver la relation de commutation $\\left[\\hat{x}, \\hat{p}_x\\right]=\\mathrm{i}$ est que $\\mathrm{T}$ soit antiunitaire puisqu\u0026rsquo;il faut que $\\mathrm{T}^{-1} \\mathrm{i} \\mathrm{~T}=-\\mathrm{i}$.\nPour un opérateur antiunitaire, $\\mathrm{T}^2=-I$. On peut noter aussi que $\\mathrm{T}^{-1} \\mathrm{i} \\mathrm{~T}=-\\mathrm{i}$ équivaut à $\\mathrm{i} T=-\\mathrm{Ti}$ ce qui signifie que $\\mathrm{i}$ anticommute avec $\\mathrm{T}$.\nL\u0026rsquo;opérateur antiunitaire archétypal est $\\mathrm{K}$, l\u0026rsquo;opérateur de conjugaison complexe. Et on peut construire un opérateur antiunitaire général par le produit d\u0026rsquo;un opérateur unitaire $\\mathrm{U}$ et de $\\mathrm{K}$. Écrivons ainsi $\\mathrm{T}=\\mathrm{UK}$, qui équivaut (en multipliant les deux membres à droite par $\\mathrm{K}$) à $\\mathrm{U}=\\mathrm{T} \\mathrm{K}$.\nPour des particules sans spin, on peut choisir $\\mathrm{U}=I$ et $\\mathrm{U}=\\mathrm{K}$. Ce n\u0026rsquo;est pas surprenant si on regarde l\u0026rsquo;effet de la conjugaison complexe sur l\u0026rsquo;équation de Schrödinger :\n$$ \\begin{gathered} \\hat{H} \\psi=\\mathrm{i} \\frac{\\partial \\psi}{\\partial t} \\\\ \\hat{H} \\psi^*=-\\mathrm{i} \\frac{\\partial \\psi^*}{\\partial t}=\\mathrm{i} \\frac{\\partial \\psi^*}{\\partial(-t)} \\end{gathered} $$\nLa combinaison d\u0026rsquo;un renversement du temps et d\u0026rsquo;une conjugaison complexe laisse invariante l\u0026rsquo;équation de Schrödinger. Ça semble bien montrer que dans ce cas, $\\mathrm{K}$ et $\\mathrm{T}$ sont une seule et même transformation.\nPour des particules avec spin, les choses se compliquent puisque le moment cinétique est renversé lorsqu\u0026rsquo;on change le sens d\u0026rsquo;écoulement du temps. Et donc l\u0026rsquo;action de $\\mathrm{T}$ sur l\u0026rsquo;opérateur de spin $\\hat{\\boldsymbol{S}}$ s\u0026rsquo;écrit :\n$$ \\mathrm{T}^{-1} \\hat{\\boldsymbol{S}} \\mathrm{~T}=-\\hat{\\boldsymbol{S}} $$\nEn se rappelant que seul la matrice de Pauli $\\sigma_y$ a des composantes complexes, l\u0026rsquo;action de l\u0026rsquo;opérateur de conjugaison complexe sur les opérateurs de spin est plus tordue :\n$$ \\mathrm{K}^{-1} \\hat{S}_x \\mathrm{~K}=\\hat{S}_x, \\quad \\mathrm{~K}^{-1} \\hat{S}_y \\mathrm{~K}=-\\hat{S}_y, \\quad \\mathrm{~K}^{-1} \\hat{S}_z \\mathrm{~K}=\\hat{S}_z $$\nUne forme appropriée pour $\\mathrm{U}$ serait donc $\\mathrm{U}=\\exp \\left(-\\mathrm{i} \\pi \\hat{S}_y\\right)$ correspondant à une rotation de π autour de la direction $y$ de telle sorte qu\u0026rsquo;en combinant $\\mathrm{U}$ et $\\mathrm{K}$, on renverse bien les trois composantes du spin. On a ainsi :\n$$ \\mathrm{T}=\\exp \\left(-\\mathrm{i} \\pi \\hat{S}_y\\right) \\mathrm{K} $$\nOn obtient alors :\n$$ \\mathrm{T}^2=\\mathrm{U}\\mathrm{K}\\mathrm{U}\\mathrm{K}=\\exp \\left(- \\mathrm{i} \\pi \\hat{S}_y\\right)\\exp \\left(+\\mathrm{i} \\pi (-\\hat{S}_y)\\right)=\\exp \\left(- 2\\mathrm{i} \\pi \\hat{S}_y\\right)=(-1)^{2S} $$\nPour un électron unique, on a $S=\\frac{1}{2}$ et donc $\\mathrm{T}^2=-1$. Cela reste le cas si on a un nombre impair d\u0026rsquo;électrons, mais si le nombre est pair, alors $\\mathrm{T}^2=1$.\nPlaçons-nous dans le cas où le nombre d\u0026rsquo;électrons est impair et supposons que l\u0026rsquo;Hamiltonien $\\mathcal{H}$ du système est invariant par rapport à une inversion temporelle ($\\mathcal{H}$ commute avec $\\mathrm{T}$). Les états $|\\psi\\rangle$ et $\\mathrm{T}|\\psi\\rangle$ ont alors la même énergie. Mais correspondent-ils au même état ? S\u0026rsquo;ils l\u0026rsquo;étaient, on aurait $\\mathrm{T}|\\psi\\rangle=\\alpha|\\psi\\rangle$ où $\\alpha$ est un nombre complexe. Mais alors, $\\mathrm{T}^2|\\psi\\rangle=\\mathrm{T} \\alpha|\\psi\\rangle=\\alpha^* \\mathrm{~T}|\\psi\\rangle=|\\alpha|^2|\\psi\\rangle$ et comme $\\mathrm{T}^2=-1$, on aboutit à une contradiction $|\\alpha|^2=-1$. Conclusion, $|\\psi\\rangle$ et $\\mathrm{T}|\\psi\\rangle$ sont linéairement indépendants et sont appelés doublets de Kramers. On vient ainsi de déduire que les niveaux d\u0026rsquo;énergie d\u0026rsquo;un système temporellement symétrique avec un nombre impair d\u0026rsquo;électrons sont $n$-fois dégénérés avec un $n$ pair. C\u0026rsquo;est le théorème de Kramers. Pour séparer ces paires, il faut introduire une perturbation qui brise la symétrie temporelle, comme un champ magnétique.\nCombinaisons de transformations discrètes En renversant à la fois le temps $t$ avec $\\mathrm{T}$ et les coordonnées spatiales $\\boldsymbol{x}$ avec $\\mathrm{P}$, on obtient un renversement complet de l\u0026rsquo;espace-temps $x$. Sur un champ scalaire, on obtient :\n$$ (\\mathrm{PT})^{-1} \\hat{\\phi}(x)(\\mathrm{PT})=\\hat{\\phi}(-x) $$\nCette opération laisse les opérateurs de création et d\u0026rsquo;annihilation inchangés puisque l\u0026rsquo;impulsion est retournée une fois par l\u0026rsquo;opération de parité et une nouvelle fois par le renversement du temps.\n$$ (\\mathrm{PT})^{-1} \\hat{a}_{\\boldsymbol{p}}(\\mathrm{PT})=\\hat{a}_{\\boldsymbol{p}} \\quad(\\mathrm{PT})^{-1} \\hat{a}_{\\boldsymbol{p}}^{\\dagger}(\\mathrm{PT})=\\hat{a}_{\\boldsymbol{p}}^{\\dagger} $$\nLe seul effet sur la décomposition en modes est alors de changer le signe de $\\mathrm{i}$ dans l\u0026rsquo;exponentielle. $\\mathrm{P}\\mathrm{T}$ agit donc comme un opérateur de conjugaison complexe.\nLes symétries liées à $\\mathrm{C}$, $\\mathrm{P}$ et $\\mathrm{T}$ sont chacune conservées dans la plupart des processus à plusieurs particules, mais pas tous. $\\mathrm{P}$ est par exemple \u0026ldquo;violée\u0026rdquo; en interaction faible.\nThéorème $\\mathrm{CPT}$ :\nSi le Lagrangien d\u0026rsquo;une théorie est invariant de Lorentz, local, hermitien et normalement ordonné, alors la théorie possède la symétrie $\\textrm{CPT}$ ; renverser à la fois l\u0026rsquo;espace-temps et les particules en antiparticules doit laisser la théorie invariante.\nLa preuve consiste à montrer que $(\\mathrm{CPT})^{-1} \\mathcal{L}(x)(\\mathrm{CPT})=\\mathcal{L}(-x)$ et ainsi d\u0026rsquo;en déduire que $\\mathrm{CPT}$ commute avec l\u0026rsquo;Hamiltonien et est donc une symétrie. Jusqu\u0026rsquo;ici, la symétrie $\\mathrm{CPT}$ a résisté à tous les tests.\nCombinaisons de transformations discrètes et continues $SO(3)$, le groupe orthogonal spécial, est le groupe des rotations à 3 dimensions représentées par des matrices $3\\times 3$ orthogonales et de déterminant $+1$ (spéciales). Ces rotations qui respectent l\u0026rsquo;orientation (c\u0026rsquo;est ce qu\u0026rsquo;assure le déterminant de $+1$) sont dites propres.\nLa transformation de parité peut être représentée par la matrice $\\text{diag}(-1,-1,-1)$ :\n$$ \\left(\\begin{array}{ccc} -1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -1 \\end{array}\\right) $$\nLà, le déterminant est clairement $-1$. En combinant avec $SO(3)$, c\u0026rsquo;est-à-dire en s\u0026rsquo;autorisant les rotations impropres (ne conservant pas l\u0026rsquo;orientation), on obtient le groupe $O(3)$ de toutes les matrices orthogonales $3\\times 3$.\nL\u0026rsquo;othogonalité implique $R^TR = I$ et en prenant le déterminant $\\operatorname{det} \\mathbf{R} \\times \\operatorname{det} \\mathbf{R}^{\\mathrm{T}}=1$. Et comme $\\operatorname{det} \\mathbf{R}=\\operatorname{det} \\mathbf{R}^{\\mathrm{T}}$, on obtient $(\\operatorname{det} \\mathbf{R})^2=1$. D\u0026rsquo;où les deux possibilités $\\operatorname{det} \\mathbf{R}= \\pm 1$.\nLe groupe $O(3)$ est composé de deux ensembles disjoints liés l\u0026rsquo;un à l\u0026rsquo;autre par une parité. Seul l\u0026rsquo;ensemble spécial correspond à un groupe indépendant car lui seul possède l\u0026rsquo;identité.\nPour obtenir $\\text{diag}(-1,-1,-1)$, l\u0026rsquo;opération de parité, on peut faire le produit d\u0026rsquo;une réflexion par un miroir dans le plan $x-y$, représentée par $\\text{diag}(1,1,-1)$ par une rotation de $\\pi$ autour de l\u0026rsquo;axe $z$, représentée par $\\text{diag}(-1,-1,1)$. En tant que produit entre une rotation impropre et une rotation propre, l\u0026rsquo;opération de parité est une rotation impropre.\n$SO(3)$ est un groupe connexe dans le sens où on peut se promener continument d\u0026rsquo;un élément à l\u0026rsquo;autre. Au contraire, $O(3)$ consiste en l\u0026rsquo;union de deux ensembles disjoints ; celui des éléments de déterminant $+1$ et celui des déterminants $-1$.\nOn obtient quelque chose de similaire avec le groupe de Lorentz (souvent appelé $O(3,1)$, pour distinguer les 3 directions spatiales de la direction temporelle) contenant toutes les rotations, réflexions et boosts de Lorentz. Ce groupe consiste en 4 composants séparés topologiquement car en plus de $\\mathrm{P}$, on doit considérer $\\mathrm{T}$.\nDans une représentation à 4 dimensions, $\\mathrm{P}=\\operatorname{diag}(1,-1,-1,-1)$ et $\\mathrm{T}=\\operatorname{diag}(-1,1,1,1)$. Le sous-groupe du groupe de Lorentz qui ne renverse ni les coordonnées spatiales ni temporelles est appelé sous-groupe propre (conserve l\u0026rsquo;orientation spatiale) orthochrone (conserve l\u0026rsquo;orientation du temps) de Lorentz $SO^+(1,3)$. Ce sous-groupe connexe est une des quatre composantes du groupe de Lorentz. On accède aux autres composantes à partir de $SO^+(1,3)$ :\npar action de $\\mathrm{P}$, par action de $\\mathrm{T}$, par action de $\\mathrm{PT}$. Revenons enfin sur $SO(3)$ et sa topologie. Une rotation est caractérisée par un axe et un angle. Par conséquent, tous les points dans une boule de rayon $\\pi$ peuvent représenter une rotation (l\u0026rsquo;axe est donné par le vecteur entre le centre de la sphère et le point choisi et l\u0026rsquo;angle est donné par la norme de ce vecteur). Dans cette représentation, deux points antipodaux correspondent à le même rotation (une rotation de $\\pi$ autour d\u0026rsquo;un axe est équivalente à une rotation de $-\\pi$ autour de l\u0026rsquo;axe inverse). La topologie de $SO(3)$ est donc celle d\u0026rsquo;une boule dont les points antipodaux de la surface sont identifiés entre eux (on peut se téléporter d\u0026rsquo;un point à l\u0026rsquo;autre).\nCela signifie que l\u0026rsquo;espace topologique de $SO(3)$ est connexe mais pas simplement connexe. En effet, dans un espace simplement connexe, tout lacet (chemin continu fermé) doit pouvoir se réduire continument à un point. Or ici, le lacet allant d\u0026rsquo;un pôle à l\u0026rsquo;autre (ce chemin est bien un lacet puisque ses extrémités correspondent à un seul et même point) n\u0026rsquo;est pas déformable en un point puisque tout mouvement d\u0026rsquo;une extrémité s\u0026rsquo;accompagne d\u0026rsquo;un mouvement opposé de l\u0026rsquo;autre extrémité pour rester antipodal. Par contre, en faisant un deuxième tour d\u0026rsquo;un pôle à l\u0026rsquo;autre, on peut maintenant faire disparaître le lacet comme le montre le dessin ci-dessus. Cela montre que les rotations de $4\\pi$ sont continument déformables en un point alors que les rotations de $2\\pi$ ne le sont pas. La \u0026ldquo;ceinture de Dirac\u0026rdquo; ou les \u0026ldquo;assiettes de Feynman\u0026rdquo; tentent d\u0026rsquo;illustrer expérimentalement ce phénomène.\nOn peut faire correspondre les rotations 3D aux éléments d\u0026rsquo;un autre groupe : $SU(2)$, le groupe spécial unitaire représenté par des matrices $2\\times 2$ de déterminant 1. Les éléments de $SU(2)$ permettent de faire tourner les spineurs.\nUne matrice de rotation peut en effet s\u0026rsquo;écrire $\\mathbf{R}(\\hat{\\boldsymbol{n}}, \\theta)$ avec :\n$$ \\mathbf{R}(\\hat{\\boldsymbol{n}}, \\theta)=\\exp \\left(-\\mathrm{i} \\frac{\\theta}{2} \\boldsymbol{\\sigma} \\cdot \\boldsymbol{n}\\right)=I \\cos \\frac{\\theta}{2}-\\mathrm{i} \\sin \\frac{\\theta}{2} \\boldsymbol{\\sigma} \\cdot \\boldsymbol{n} $$\noù $\\boldsymbol{\\sigma}=\\left(\\sigma_x, \\sigma_y, \\sigma_z\\right)$ sont les matrices de Pauli et $I$ la matrice identité. On remarque alors que :\n$$ \\mathbf{R}(\\hat{\\boldsymbol{n}}, 0)=I\\quad,\\quad \\mathbf{R}(\\hat{\\boldsymbol{n}}, 2 \\pi)=-I \\quad,\\quad \\mathbf{R}(\\hat{\\boldsymbol{n}}, 4 \\pi)=I $$\nOn dit que $SU(2)$ est un double recouvrement de $SO(3)$. Prenons l\u0026rsquo;identité par exemple : dans $SO(3)$, l\u0026rsquo;absence de rotation est représentée par $\\operatorname{diag}(1,1,1)$ et dans $SU(2)$, à la fois par $\\operatorname{diag}(1,1)$ et $\\operatorname{diag}(-1,-1)$.\nUn spineur peut s\u0026rsquo;écrire comme une entité à deux composantes $\\binom{a}{b}$ où $a$ et $b$ sont des nombres complexes tels que $|a|^2+|b|^2=1$. En écrivant $a=x_0+\\mathrm{i} x_1$ et $b=x_2+\\mathrm{i} x_3$ où les $x_i$ sont des nombres réels, la condition $|a|^2+|b|^2=1$ devient $x_0^2+x_1^2+x_2^2+x_3^2=1$ et donc $SU(2)$ est isomorphe à $S^3$, la 3-sphère, ce qui montre que $SU(2)$ est simplement connexe, contrairement à $SO(3)$. $SO(3)$ est finalement un groupe quotient : $S O(3) \\cong S U(2) / \\mathbb{Z}_2$.\nOn peut généraliser ces arguments à la composante connexe du groupe de Lorentz : $S O(1,3) \\cong S L(2, \\mathbb{C}) / \\mathbb{Z}_2$ où $S L(2, \\mathbb{C})$ est le groupe des matrices $2\\times 2$ complexes de déterminant unité.\nChapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc6/",
	"title": "TQC-5",
	"tags": [],
	"description": "",
	"content": " Théorie quantique des champs \u0026ndash; Partie 6 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Très souvent une simple traduction.\nRetour sommaire\nPropagateurs et fonctions de Green Chapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/electromag/arcs/",
	"title": "Arcs électriques",
	"tags": [],
	"description": "",
	"content": " Arcs électriques Le champ disruptif d\u0026rsquo;un isolant, ou plutôt sa rigidité diélectrique, désigne la valeur de champ électrique maximale que le milieu peut supporter avant le déclenchement d’un arc électrique, ou claquage.\nPour l\u0026rsquo;air, la valeur fréquemment admise est de $\\pu{36 kV/cm}$ dans l\u0026rsquo;air sec et tombe à $\\pu{10 kV/cm}$ pour un air saturé en humidité. Cela signifie qu\u0026rsquo;il faut, dans l\u0026rsquo;air sec, une tension d\u0026rsquo;au moins $\\pu{36 kV}$ entre deux électrodes séparées d\u0026rsquo;un centimètre pour qu\u0026rsquo;une étincelle se crée entre elles.\nComment retrouver cet ordre de grandeur à partir de considérations physiques simples ?\nUn arc électrique est un conduit d\u0026rsquo;air ionisé, du plasma (le 4e état de la matière). Son origine ?\nUn champ électrique suffisamment costaud.\nImaginons qu\u0026rsquo;un électron soit arraché à une molécule d\u0026rsquo;air. Il est alors accéléré par le champ électrique ambiant et gagne ainsi de l\u0026rsquo;énergie cinétique. Si au moment de rencontrer une nouvelle molécule, l\u0026rsquo;électron a atteint une énergie suffisante pour la ioniser, on peut se retrouver avec une cascade d\u0026rsquo;ionisations successives !\nLe secret est donc d\u0026rsquo;avoir un champ suffisant pour donner à un électron une énergie cinétique de l\u0026rsquo;ordre de grandeur de l\u0026rsquo;énergie de ionisation d\u0026rsquo;une molécule (autour de la dizaine d\u0026rsquo;électronvolts) sur une distance correspondant à son libre parcours moyen dans l\u0026rsquo;air.\nFaisons le point sur le libre parcours moyen dans un gaz de particules identiques de rayon $d$ et de densité $n$ :\nLa section efficace de collision $\\sigma$ vaut $\\pi d^2$.\nLa valeur de la vitesse relative entre deux particules vaut\u0026nbsp;: $$ \\begin{aligned} v_{rel}\u0026amp;=\\sqrt{\\vec{v}_{rel}\\cdot \\vec{v}_{rel}}\\\\ \u0026amp;=\\sqrt{\\left(\\vec{v}_2-\\vec{v}_1\\right)\\cdot\\left(\\vec{v}_2-\\vec{v}_1\\right)}\\\\ \u0026amp;=\\sqrt{\\vec{v}_2\\cdot\\vec{v}_2+\\vec{v}_1\\cdot\\vec{v}_1-2\\vec{v}_2\\cdot\\vec{v}_1} \\end{aligned} $$\nPour obtenir la vitesse relative moyenne, on va supposer que les vitesses des particules se répartissent aléatoirement selon une certaine distribution de probabilité.\n$$ \\begin{aligned} \\overline{v_{rel}}\u0026amp;=\\sqrt{\\overline{\\vec{v}_2\\cdot\\vec{v}_2}+\\overline{\\vec{v}_1\\cdot\\vec{v}_1}-2\\,{\\cancel{\\overline{\\vec{v}_2\\cdot\\vec{v}_1}}}}\\\\ \u0026amp;= \\sqrt{\\overline{\\vec{v}_2^2}+\\overline{\\vec{v}_1^2}}\\\\ \u0026amp;= \\sqrt{2}\\overline{v} \\end{aligned} $$\nOn suppose en effet que les vitesses des particules 1 et 2 ne sont pas corrélées (${\\vec{v}_2\\cdot\\vec{v}_1}=0$) et que leurs valeurs moyennes sont les mêmes ($\\overline{\\vec{v}_2^2}=\\overline{\\vec{v}_2^2}=\\overline{v}^2$).\nLe nombre de collisions d\u0026rsquo;une particule pendant un laps de temps $\\Delta t$ peut être estimé comme le nombre moyen de fois que le centre de masse d\u0026rsquo;une particule se trouve dans le volume balayé par la section efficace $\\sigma$ pendant $\\Delta t$, c\u0026rsquo;est-à-dire sur une distance $\\overline{v_{rel}}\\Delta t$.\nLe volume en question vaut $V=\\sigma \\,\\overline{v_{rel}}\\,\\Delta t$.\nEt le nombre de particules rencontrées vaut donc $nV=n\\,\\sigma\\, \\overline{v_{rel}}\\,\\Delta t= \\sqrt{2}\\,n\\,\\sigma\\, \\overline{v}\\,\\Delta t= \\sqrt{2} \\,n\\,\\pi \\,d^2 \\,\\overline{v}\\,\\Delta t$.\nLe libre parcours moyen $\\lambda$ va alors correspondre à la distance parcourue pendant $\\Delta t$ divisée par le nombre de collisions ayant eu lieu pendant ce laps de temps.\nD\u0026rsquo;où :\n$$ \\begin{aligned} \\lambda \u0026amp;= \\frac{\\cancel{\\overline{v}\\Delta t}}{\\sqrt{2}n\\sigma {\\cancel{\\overline{v}\\Delta t}}}\\\\ \u0026amp;=\\frac{1}{\\sqrt{2}n\\sigma}\\\\ \u0026amp;=\\frac{1}{\\sqrt{2}n\\pi d^2} \\end{aligned} $$\nLe libre parcours moyen ne dépend donc que de la taille des particules et de leur densité, pas de leur vitesse relative !\nPour un gaz à pression $P$ et température $T$, on peut utiliser en première approximation la relation des gaz parfaits pour obtenir la densité de particules :\n$n=P/k_B T$.\nEn remplaçant dans $\\lambda$, on obtient :\n$$ \\lambda = \\frac{k_B T}{\\sqrt{2}P\\pi d^2} $$\nUne molécule de diazote a un diamètre $d=\\pu{0,37 nm}$ et pour une pression $P=\\pu{1 bar}$ et une température $T = \\pu{293 K}$, on obtient un libre parcours moyen $\\lambda\\approx \\pu{66 nm}$.\nOn peut remarquer que c\u0026rsquo;est près de 200 fois plus grand que la distance moyenne entre particules donnée par $1/n^{\\frac{1}{3}}=\\left(\\frac{k_B T}{P}\\right)^{\\frac{1}{3}}\\approx \\pu{3,4 nm}$ !\nPour vérifier, on simule un gaz idéal dans un cube de 100 nm de côté avec des conditions aux limites périodiques contenant environ 24 000 molécules considérées comme des sphères dures de diamètre 0,37 nm et dont la vitesse est tirée dans la distribution de Maxwell-Boltzmann pour une température de 300 K. On suit la trajectoire d\u0026rsquo;une molécule particulière tracée en rouge. Après 30 collisions, la moyenne des libres parcours est de 68 nm.\nPour un électron, on peut reprendre la formule du libre parcours moyen en modifiant seulement un peu la section efficace puisque $\\sigma=\\pi\\left(r_\\text{molécule}+r_\\text{électron}\\right)^2\\approx \\pi \\left(r_\\text{molécule}\\right)^2 = \\pi d^2 /4$.\nEt donc $\\lambda= \\frac{2\\sqrt{2}k_B T}{P\\pi d^2}\\approx 0,27 \\text{ μm}$.\nNotre électron devant récupérer une énergie d\u0026rsquo;environ $\\pu{20 eV}$ sur la distance $\\lambda$ pour réussir à ioniser la prochaine molécule de diazote rencontrée, le champ doit donc être de $\\pu{20 V}/\\lambda$, soit à peu près $\\pu{20 V}$ pour $0,27 \\text{ μm}$ $\\rightarrow$ $0,74\\text{ MV/cm.}$ C\u0026rsquo;est plus d\u0026rsquo;un ordre de grandeur au-dessus de ce qu\u0026rsquo;on aurait aimé obtenir 😢.\nUne des raisons possibles de cet écart est notre trop grand optimisme quant à l\u0026rsquo;efficacité des collisions ; il est en effet fort peu probable qu\u0026rsquo;un électron parvienne à ioniser systématiquement chaque molécule qu\u0026rsquo;il rencontre, en particulier avec une énergie à peine suffisante.\nCela revient au final à surestimer la section efficace de collision. Penchons-nous alors sur la littérature pour trouver une valeur plus réaliste\u0026hellip;\nLe graphique suivant rapporte un tas de sections efficaces différentes pour des collisions électron-diazote donnant lieu à des rotations, des vibrations, ou (et c\u0026rsquo;est ce qui nous intéresse ici) des ionisations :\nY. Itikawa et al. Cross Sections for Collisions of Electrons and Photons with Nitrogen Molecules, Journal of Physical and Chemical Reference Data 15, 985 (1986) D\u0026rsquo;après le graphique, l\u0026rsquo;énergie minimale que doit atteindre l\u0026rsquo;électron pour obtenir la ionisation du diazote semble être un peu supérieure à $\\pu{20 eV}$, et la section efficace associée vaut approximativement $\\pu{4e-17 cm2}$.\nCela nous donne un libre parcours moyen de $\\frac{1}{\\sqrt{2}n\\sigma}=7\\text{ μm}$, largement supérieur à notre prédiction initiale !\nLe champ électrique correspondant vaut, roulement de tambour\u0026hellip; $20\\text{ eV}/7 \\text{ μm}\\approx\\pu{30 kV/cm}$. Youpi !\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/arithmetique/",
	"title": "Arithmétique",
	"tags": [],
	"description": "",
	"content": " Arithmétique et théorie des nombres : Infinis dénombrables et indénombrables Des entiers aux transcendants en passant par l\u0026rsquo;hypothèse du continu ($\\aleph_0 = 2^{\\aleph_1}$) :\nNumération \u0016De nombreux systèmes de numération à bases entières ont été utilisés par différents peuples et à différentes époques.\nPar exemple :\nsystème binaire (base 2) utilisé dans des langues d\u0026rsquo;Amérique du Sud et d\u0026rsquo;Océanie, et utilisé de nos jours en informatique.\nsystème quinaire (base 5) dont il reste des traces jusqu\u0026rsquo;au xxe siècle dans des langues africaines, mais aussi, partiellement, dans les notations tchouvache, suzhou, romaine et maya. Le nom des chiffres 6, 7, 8 et 9 dans de nombreuses langues témoignent de ce système quinaire: ils se disent 5+1, 5+2, 5+3 et 5+4 en wolof (langue de la famille nigéro-congolaise), en khmer (langue austro-asiatique), en nahuatl (langue uto-aztèque), et, dans de nombreuses langues austronésiennes telles qu\u0026rsquo;en lote ou en ngadha (sous forme partielle). La base quinaire apparait parfois comme base auxiliaire ou sous-base de la base décimale, comme dans le système romain, ou de la base vigésimale.\nsystème duodécimal (base 12), déjà utilisé par les Sumériens et Assyro-babyloniens pour des mesures de longueur et de temps. On le retrouve dans un certain nombre de monnaies et d\u0026rsquo;unités de compte courantes en Europe au Moyen Âge, notamment dans le système impérial d\u0026rsquo;unités (il faut 12 pouces pour faire un pied), et dans le commerce. Il sert encore, par exemple, pour compter les mois, les heures, les fleurs, les huîtres et les œufs.\nsystème hexadécimal (base 16), très couramment utilisé en électronique ainsi qu\u0026rsquo;en informatique. Son intérêt réside dans les conversions triviales avec la base 2, tout en permettant une écriture plus compacte des nombres.\nsystème vigésimal (ou vicésimal, base 20) existe au Bhoutan en langue dzongkha, et était en usage chez les Aztèques vers 1200 et, quoiqu\u0026rsquo;irrégulier, pour la numération maya. Il était aussi présent en vieux français, ce qui explique l\u0026rsquo;usage du mot quatre-vingts pour le nombre 80, ou encore le nom de l\u0026rsquo;hôpital des Quinze-Vingts, qui pouvait accueillir 300 patients.\nnote\nChiffres de Kaktovik :\nToutes les langues eskimo-aléoutes d\u0026rsquo;Alaska et du Canada utilisent un système vigésimal pour compter. Les chiffres arabes, qui ont été conçus pour un système décimal, sont inadéquats pour l\u0026rsquo;iñupiaq et les autres langues inuites. Pour remédier à ce problème, des élèves d\u0026rsquo;une école de Kaktovik, en Alaska, ont inventé un système à base 20 en 1994, qui s\u0026rsquo;est répandu parmi les Iñupiat en Alaska et a été envisagé au Canada.\nsystème sexagésimal (base 60) était utilisé pour la numération babylonienne et en Mésopotamie vers 3300 av. J.-C., ainsi que par les Indiens et les Arabes en trigonométrie. Il sert encore actuellement dans la mesure du temps et certaines mesures des angles. Les chiffres et nombres mésopotamiens de 1 à 59 : Source : Wikipedia\nNombres algébriques Un nombre algébrique est un nombre réel ou complexe solution d\u0026rsquo;une équation polynomiale à coefficients dans le corps $\\mathbb{Q}$ des rationnels.\nTraçons l\u0026rsquo;ensemble des racines des $2^{21}$ (≈ 2 millions) polytnômes de degré 20 possible si chacun des coefficients vaut soit 1, soit -1.\nExemple d\u0026rsquo;un de ces polynômes : $-x^{20}+x^{19}+x^{18}+x^{17}-x^{16}-x^{15}-x^{14}-x^{13}+x^{12}-x^{11}+x^{10}-x^{9}+x^{8}+x^{7}+x^{6}-x^{5}-x^{4}-x^{3}-x^{2}+x+1$\nUn polynôme de degré 20 a 20 racines complexes. On se retrouve donc avec 40 millions de points (dont beaucoups sont aux mêmes endroits)\u0026hellip; Et cela donne ça :\nSont tracés ci-dessous les racines pour des degrés croissants de ces polynômes à coefficients unitaires.\nnote\nCette page pour en savoir plus. Ça parle même de dragons !\nEn représentant dans le plan complexe les racines d\u0026rsquo;un polynôme de degré 2 dont on fait varier chacun des trois coefficients entre -100 et 100, on obtient la jolie figure suivante (ici dans une zone centrée sur zéro et de rayon 1,7) :\nFractions continues Algorithme d\u0026rsquo;Euclide Une écriture récursive de l\u0026rsquo;algo en Python :\ndef pgcd(a,b): if b == 0: return a else: return pgcd(b,a%b) Un des plus anciens algorithmes connus, très simple dans sa formulation mais qui recèle des surpises comme l\u0026rsquo;apparition du nombre d\u0026rsquo;or dans la détermination de sa complexité temporelle (nombre d\u0026rsquo;étapes de calcul en fonction des entiers a et b ).\nTriplets pythagoriciens Divisibilité et nombres premiers Complexes et quaternions note\nJolie série de vidéos sur les complexes.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/electromag/",
	"title": "Électromagnétisme",
	"tags": [],
	"description": "",
	"content": " Électromagnétisme Rayonnement Source : Magnetism, Radiation, and Relativity Supplementary notes for a calculus-based introductory physics course Daniel V. Schroeder 🌐\nFormule de Larmor et Diffusion Rayleigh sans douleur jusqu\u0026rsquo;au bleu du ciel et au rougeoiement du soleil couchant.\nTrois animations montrant le champ rayonné (densité et lignes) pour diverses accélérations (rebond, arrêt brutal, osccillation).\n▶︎\u0026nbsp;Code Python pour cette dernière vidéo import numpy as np import matplotlib.pyplot as plt from scipy.optimize import fsolve import os x0 = 0.2 def beta_function(t): return np.array([0.,-x0*c*np.sin(np.pi*t),0]) def beta_dot_function(t): return np.array([0,-x0*c*np.pi*np.cos(np.pi*t),0]) c = 1 def r_0(t): return np.array([float(0.),float(x0*c/np.pi*np.cos(np.pi*t)),0]) def determinetprime(r,r_0,t): \"\"\" r_0(t_prime) est la fonction donnant la position au cours du temps r est le point où on cherche la valeur du champ \"\"\" def func(t_prime): return t - t_prime - np.linalg.norm(r - r_0(t_prime)) / c t_prime_initial_guess = t - 1 t_prime_solution = fsolve(func, t_prime_initial_guess) return t_prime_solution[0] def champ_vectorized(x_grid, y_grid, r_0_func, beta_function, beta_dot_function, t): q = 1 E_field = np.zeros(x_grid.shape + (3,)) # Initialisation d'un tableau 3D pour le champ électrique for i in range(x_grid.shape[0]): for j in range(x_grid.shape[1]): r = np.array([x_grid[i, j], y_grid[i, j], 0]) tprime = determinetprime(r, r_0_func, t) beta = beta_function(tprime) beta_dot = beta_dot_function(tprime) r0 = r_0_func(tprime) R = r - r0 R_norm = np.linalg.norm(R) R_hat = R / R_norm gamma = 1 / np.sqrt(1 - np.dot(beta, beta)) term1_numerator = R_hat - beta term1_denominator = gamma**2 * R_norm**2 * (1 - np.dot(R_hat, beta))**3 term1 = term1_numerator / term1_denominator term2_numerator = np.cross(R_hat, np.cross(R_hat - beta, beta_dot)) term2_denominator = c * R_norm * (1 - np.dot(R_hat, beta))**3 term2 = term2_numerator / term2_denominator E = q * (term1 + term2) E_field[i, j, :] = E return E_field def calculate_field_line_point(r0, beta, n_hat, t, t_prime): gamma = 1 / np.sqrt(1 - np.dot(beta, beta)) # Facteur de Lorentz if np.linalg.norm(beta) != 0: beta_hat = beta/np.linalg.norm(beta) else: beta_hat = np.array([0.0,0.0,0.0]) term_inside_brackets = beta + ((1/gamma - 1) * np.dot(n_hat, beta_hat) * beta_hat + n_hat) / (gamma * (1 + np.dot(n_hat, beta))) return r0 + c * (t - t_prime) * term_inside_brackets # Fonction pour tracer les lignes de champ à partir d'une position donnée à l'instant t def plot_field_lines(r0_func, beta_func, t, num_lines=24, num_points=500, step=0.05): for i in range(num_lines): n_hat = np.array([np.cos(i/num_lines*2*np.pi),np.sin(i/num_lines*2*np.pi),0]) line_points = [] t_prime = t # Initialisation de t' à la valeur de t pour le début de la ligne de champ for _ in range(num_points): r0 = r0_func(t_prime) beta = beta_func(t_prime) point = calculate_field_line_point(r0, beta, n_hat, t, t_prime) line_points.append(point) t_prime -= step # Décrémentation de t' pour suivre la ligne de champ vers le passé line_points = np.array(line_points) plt.plot(line_points[:, 0], line_points[:, 1], 'r-') # Trace la ligne en rouge # Set up grid for field calculation x_range = np.linspace(-12, 12, 1200) y_range = np.linspace(-6, 6, 600) x_grid, y_grid = np.meshgrid(x_range, y_range) # Créer le répertoire pour les images si nécessaire output_dir = \"animation\" if not os.path.exists(output_dir): os.makedirs(output_dir) # Définir la plage de temps et le nombre d'images time_values = np.linspace(0, 2, num=51) # 401 images pour t de 0 à 4 for i, t in enumerate(time_values): plt.figure(figsize=(15, 10), dpi=150) plot_field_lines(r_0, beta_function, t) E_field_grid = champ_vectorized(x_grid, y_grid, r_0, beta_function, beta_dot_function, t) E_field_magnitude = np.linalg.norm(E_field_grid, axis=2) plt.imshow(np.log10(E_field_magnitude), extent=(x_range.min(), x_range.max(), y_range.min(), y_range.max()), cmap='Spectral_r', aspect='equal') plt.axis('off') # Construire le nom de fichier avec un remplissage de zéros filename = f\"image{str(i).zfill(4)}.png\" filepath = os.path.join(output_dir, filename) # Sauvegarder l'image plt.savefig(filepath, bbox_inches='tight', pad_inches=0, transparent=True) # Fermer la figure pour libérer de la mémoire plt.close() Ce code a été obtenu à partir de cet article et de cette page de forum. Arcs électriques Où on essaye de retrouver l\u0026rsquo;ordre de grandeur de la valeur du champ électrique qui rend l\u0026rsquo;air conducteur.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/info/",
	"title": "Informatique",
	"tags": [],
	"description": "",
	"content": " Un peu d\u0026rsquo;informatique Cours d\u0026rsquo;informatique pour prépas TSI1 et TSI2 avec TP corrigés\nBase de Python\nProjets et défis\nAlgorithmique Généralités et quelques détails\nOrdinateur Fonctionnement simplifié d\u0026rsquo;un ordinateur\nIntelligence Artificielle Histoire, exemples et dangers\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/optique/",
	"title": "Optique",
	"tags": [],
	"description": "",
	"content": " Optique Arcs-en-ciel La physique de l\u0026rsquo;arc-en-ciel avec des simulations en VPython.\nLentilles Physique et maths des lentilles avec simulations VPython.\nPolarisation "
},
{
	"uri": "https://sciencesilencieuse.github.io/info/ordinateur/",
	"title": "Ordinateur",
	"tags": [],
	"description": "",
	"content": " Dans la bête Ordinateur Petite série de vidéo sur le fonctionnement simplifié d\u0026rsquo;un ordinateur.\nnote\nSource : \u0026ldquo;But how do it know?\u0026rdquo; de J. Clark Scott\nPortes et logique Article de Fredkin et Toffoli de 1981 sur la possibilité d\u0026rsquo;un ordinateur réversible démontrée par l\u0026rsquo;implémentation de portes à base de boules de billard !\nPlus récemment, une équipe japonaise a remplacé les boules de billard par des crabes\u0026hellip;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/meca/paraboles/",
	"title": "Paraboles",
	"tags": [],
	"description": "",
	"content": " Chutes libres et paraboles "
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique2/",
	"title": "Propositions - Sémantique",
	"tags": [],
	"description": "",
	"content": " info\nNotes de lecture du livre La logique pas à pas de Jacques Duparc que je paraphrase allégrement.\nCalcul des propositions Syntaxe Sémantique Preuve Sémantique Modèle La sémantique concerne l\u0026rsquo;interprétation des formules. Le calcul propositionnel est très frustre à cet égard. Une variable propositionnelle vaut soit 1, soit 0, c\u0026rsquo;est-à-dire qu\u0026rsquo;elle est soit vraie, soit fausse. Et il en est de même pour une formule dont l\u0026rsquo;interprétation se déduit de celles des variables et des règles qu\u0026rsquo;impliquent les connecteurs.\nÀ partir de l\u0026rsquo;ensemble des variables $VAR$, on peut construire une fonction $\\delta$ qui associe une valeur de vérité à certains éléments de $VAR$ :\n$\\delta :\\;VAR\\rightarrow\\set{0,1}$\n$\\delta$ est une distribution de valeurs de vérité.\n$\\delta$ permet de définir un modèle $\\mathcal{M}$ du calcul propositionnel.\nImaginons qu\u0026rsquo;un ensemble de formules ne possèdent que deux variables $P$ et $Q$. On a alors 4 modèles différents possibles :\n$\\mathcal{M}_1$ pour lequel la distribution de valeur de vérité $\\delta_1$ est définie par : $\\delta_1(P) = 0$ et $\\delta_1(Q)=0$ $\\mathcal{M}_2$ pour lequel la distribution de valeur de vérité $\\delta_2$ est définie par : $\\delta_2(P) = 0$ et $\\delta_2(Q)=1$ $\\mathcal{M}_3$ pour lequel la distribution de valeur de vérité $\\delta_3$ est définie par : $\\delta_3(P) = 1$ et $\\delta_3(Q)=0$ $\\mathcal{M}_4$ pour lequel la distribution de valeur de vérité $\\delta_4$ est définie par : $\\delta_4(P) = 1$ et $\\delta_4(Q)=1$ Pour un modèle $\\mathcal{M}$, si $\\delta(P)=1$, on note $\\mathcal{M}\\models P$. Cela signifie que $P$ est vraie dans le modèle $\\mathcal{M}$.\nÀ linverse, si $\\delta(P)=0$, on écrira $\\mathcal{M}\\not\\models P$.\nSi $\\mathcal{M}\\not\\models P$ alors $\\mathcal{M}\\models \\neg P$.\nExemple :\nsi $\\mathcal{M}\\models P,\\neg Q,R$, cela signifie que dans le modèle $\\mathcal{M}$, $P$ et $R$ sont vraies, mais $Q$ est fausse.\nÉvaluation des formules Pour évaluer des formules, il faut pouvoir étendre la fonction de distribution de valeurs de vérité des seules variables propositionnelles à toutes les formules de $\\mathcal{F}$. $\\delta$ devient alors $\\delta_\\mathcal{F}$ et on construit $\\delta_\\mathcal{F}$ à partir de $\\delta$ en utilisant les règles suivantes :\n$\\delta_\\mathcal{F}(\\phi) = \\delta(\\phi)$ si $\\phi$ est une variable propositionnelle. $\\delta_\\mathcal{F}(\\neg\\phi) = 1- \\delta_\\mathcal{F}(\\phi)$ (non $\\phi$ est vraie seulement si $\\phi$ est fausse). $\\delta_\\mathcal{F}(\\phi \\lor\\psi) = \\max(\\delta_\\mathcal{F}(\\phi) ,\\delta_\\mathcal{F}(\\psi) ) $ ($\\phi \\lor\\psi$ est toujours vraie sauf lorsque les deux sous-formules $\\phi$ et $\\psi$ sont toutes deux fausses). $\\delta_\\mathcal{F}(\\phi \\land \\psi) = \\delta_\\mathcal{F}(\\phi) \\cdot \\delta_\\mathcal{F}(\\psi)$ ($\\phi \\land\\psi$ est toujours fausse sauf lorsque les deux sous-formules $\\phi$ et $\\psi$ sont toutes deux vraies). $\\delta_\\mathcal{F}(\\phi \\rightarrow \\psi) = \\max(1- \\delta_\\mathcal{F}(\\phi) ,\\delta_\\mathcal{F}(\\psi) )$ ($\\phi \\rightarrow \\psi$ est toujours vraie sauf quand à la fois $\\phi$ est vraie et $\\psi$ est fausse). $\\delta_\\mathcal{F}(\\phi \\leftrightarrow \\psi) = \\max(\\delta_\\mathcal{F}(\\phi) \\cdot\\delta_\\mathcal{F}(\\psi), (1-\\delta_\\mathcal{F}(\\phi) )\\cdot(1-\\delta_\\mathcal{F}(\\psi) ) )$ ($\\phi \\leftrightarrow \\psi$ est vraie lorsque $\\phi$ et $\\psi$ ont la même valeur de vérité). Avec nos petits coloriages, on peut maintenant s\u0026rsquo;atteler à la valeur de vérité d\u0026rsquo;une formule complexe dans un modèle donné.\nSoit le modèle $\\mathcal{M}$ défini par $\\mathcal{M}\\models P,\\neg Q, R$ et la formule $\\phi$ qu\u0026rsquo;on s\u0026rsquo;est amusé à linéariser à la fin du chapitre sur la syntaxe. On va partir des feuilles et remonter jusqu\u0026rsquo;à la racine de l\u0026rsquo;arbre. Comme la racine est fausse, la formule $\\phi$ est fausse dans ce modèle : $\\mathcal{M}\\not\\models\\phi$.\nLa formule n\u0026rsquo;est d\u0026rsquo;ailleurs vraie que dans un seul des 8 modèles possibles. Si la hauteur $n$ de la formule est grande, le coloriage devient vite impraticable car trop long. En imaginant que tous les opérateurs sont binaires et que chaque branche est de longueur $n$, cela nous demanderait le coloriage de $0+2+2^2+\\ldots+2^{n-1}=2^n-1$ nouveaux nœuds. Avec une formule de hauteur 31, ce qui ne semble pas délirant, on pourrait se retrouver à devoir colorier plus d\u0026rsquo;un milliard de nœuds\u0026hellip;\nFormules logiquement équivalentes Une formule $\\phi$ est satisfaite dans un modèle $\\mathcal{M}$ si elle est vraie dans ce modèle. La distribution de valeur de vérité $\\delta$ qui caractérise $\\mathcal{M}$ vaut alors 1.\n$\\mathcal{M}\\models\\phi$ signifie que $\\delta_\\mathcal{F}(\\phi) = 1$ et $\\mathcal{M}\\not\\models\\phi$ signifie que $\\delta_\\mathcal{F}(\\phi) = 0$.\nDeux formules $\\phi$ et $\\psi$ sont deux formules équivalentes ($\\phi\\equiv\\psi$) ssi elles sont satisfaites dans les mêmes modèles (mêmes valeurs de vérité dans les mêmes modèles).\ninfo\n$\\equiv$ est une équivalence sémantique alors que $\\leftrightarrow$ est une équivalence syntaxique, parfois appelée équivalence matérielle.\n$\\phi\\equiv\\psi$ ssi (pour tout modèle $\\mathcal{M}$, $\\mathcal{M}\\models \\phi\\leftrightarrow \\psi$).\nPreuve :\nSi les deux formules sont équivalentes dans un modèle, alors $\\delta_\\mathcal{F}(\\phi)=\\delta_\\mathcal{F}(\\psi)$ dans ce modèle. Et par conséquent $\\delta_\\mathcal{F}(\\phi\\leftrightarrow\\psi)=1$. Inversement, si la formule $\\phi\\leftrightarrow\\psi$ est vraie dans tout modèle, les valeurs que prennent $\\phi$ et $\\psi$ dans ces modèles sont les mêmes. La relation est\u0026nbsp;:\nréflexive : $\\phi\\equiv\\phi$ symétrique : $\\phi\\equiv\\psi$ ssi $\\psi\\equiv\\phi$ transitive : Si ($\\phi\\equiv\\psi$ et $\\psi \\equiv\\theta$) alors $\\psi\\equiv\\theta$ Donc il s'agit bien d'une relation d'équivalence.\nThéorie et conséquence sémantique Une théorie $\\mathcal{T}$ du calcul des propositions est un ensemble de formules : $\\mathcal{T} \\subseteq \\mathcal{F}$.\n$\\mathcal{T}$ est satisfaite dans le modèle $\\mathcal{M}$ ($\\mathcal{M}$ est un modèle de $\\mathcal{T}$), noté $\\mathcal{M}\\models\\mathcal{T}$, si pour toute formule $\\phi$ de $\\mathcal{T}$, $\\mathcal{M}\\models\\phi$. $\\mathcal{T}$ est satisfaisable ou consistante s'il existe au moins un modèle $\\mathcal{M}$ tel que $\\mathcal{M}\\models\\mathcal{T}$. $\\mathcal{T}$ est inconsistante si $\\mathcal{T}$ n'est pas satisfaisable. Une théorie est inconsistante lorsqu\u0026rsquo;elle se contredit. Elle dit alors quelque chose et son contraire, ce qui ne peut être vérifié dans aucun modèle.\nComparons maintenant deux théories $\\mathcal{T}$ et $\\mathcal{T\u0026rsquo;}$ :\n$\\mathcal{T'}$ est une conséquence sémantique de $\\mathcal{T}$, noté $\\mathcal{T} \\models \\mathcal{T'}$ si tout modèle satisfaisant $\\mathcal{T}$ satisfait aussi $\\mathcal{T'}$. $\\mathcal{T}$ et $\\mathcal{T'}$ sont deux théories équivalentes, noté $\\mathcal{T} \\equiv \\mathcal{T'}$, si elles ont exactement les mêmes modèles. Plus une théorie raconte de choses, moins elle possède de modèles. Et si elle raconte trop de choses, elle finit par devenir inconsistante car plus aucun modèle ne peut la satisfaire.\nOn écrit $\\mathcal{T}\\models\\phi$ si la théorie $\\mathcal{T\u0026rsquo;}$ se réduit au singleton $\\set{\\phi}$.\nEt $\\models\\phi$ est un raccourci pour $\\emptyset \\models \\phi$ ce qui signifie que $\\phi$ est une conséquence logique de la théorie vide. Or la théorie vide ne dit rien et est donc satisfaite dans tous les modèles. Cela revient donc à dire que $\\phi$ est vraie dans tous les modèles. $\\phi$ est appelée une tautologie.\n$\\mathcal{T}\\models\\mathcal{T\u0026rsquo;}$ ssi l\u0026rsquo;ensemble des modèles de $\\mathcal{T}$ est contenu dans l\u0026rsquo;ensemble des modèles de $\\mathcal{T\u0026rsquo;}$.\nEn effet, $\\mathcal{T\u0026rsquo;}$ est une conséquence sémantique de $\\mathcal{T}$ si elle est vraie partout où $\\mathcal{T}$ est vraie.\nDeux théories équivalentes ont exactement les mêmes modèles ($\\mathcal{T}\\equiv \\mathcal{T\u0026rsquo;}$ correspond à avoir à la fois $\\mathcal{T}\\models \\mathcal{T\u0026rsquo;}$ et $\\mathcal{T\u0026rsquo;}\\models \\mathcal{T}$), ce qui revient à dire qu\u0026rsquo;il n\u0026rsquo;existe pas de modèle pouvant les discriminer. Elles ne sont pas nécessairement égales (pas nécessairement le même ensemble de formules), mais elles sont égales sur le plan sémantique puisqu\u0026rsquo;elles signifient la même chose.\ninfo\nLa notion de conséquence sémantique est la version sémantique de la notion de déduction. Dire qu\u0026rsquo;une formule est une conséquence sémantique d\u0026rsquo;une théorie, c\u0026rsquo;est affirmer que partout où la théorie est satisfaite (c.-à-d. quand les hypothèses sont vraies), la formule l\u0026rsquo;est également. La formule découle donc de la théorie.\nSi $\\mathcal{T}$ est satisfaisable et $\\mathcal{T'}\\subseteq\\mathcal{T}$, alors $\\mathcal{T'}$ est également satisfaisable.\nEn restreignant une théorie, on conserve la non contradiction.\nLe contraire n'est évidemment pas vrai. Si on étend une théorie (en ajoutant de nouvelles formules), on diminue le nombre de modèles qui la satisfait. Et on accroît ce nombre de modèles en enlevant des formules à la théorie, pour atteindre à la limite tous les modèles possibles lorsque la théorie devient vide. Si $\\mathcal{T'}$ est inconsistante et $\\mathcal{T'}\\subseteq\\mathcal{T}$, alors $\\mathcal{T}$ est également inconsistante.\nÉtendre une théorie inconsistante ne peut pas la rendre satisfaisable. Si $\\mathcal{T'}\\models\\phi$ et $\\mathcal{T'}\\subseteq\\mathcal{T}$, alors $\\mathcal{T}\\models\\phi$. Si $\\phi$ est la conséquence d'une théorie, alors elle est la conséquence de toute théorie qui étend celle-ci (puisque le nombre de modèles satisfaisant cette théorie étendue s'est réduit et qu'on ne peut donc pas y trouver un nouveau modèle où $\\phi$ deviendrait fausse). $\\mathcal{T}$ est inconsistante ssi $\\mathcal{T}\\models\\phi\\land\\neg\\phi$. $\\mathcal{T}$ est inconsistante ssi pour toute formule $\\phi$, $\\mathcal{T}\\models\\phi$. En effet, la plus grande théorie possible ($\\mathcal{F}$) contient toute formule $\\phi$ et sa négation. $\\mathcal{T}\\models\\phi$ ssi $\\mathcal{T}\\cup\\set{\\neg\\phi}$ est inconsistante.\nEn effet, dire que $\\phi$ est conséquence sémantique de $\\mathcal{T}$, c'est dire que $\\phi$ est vraie dans tous les modèles de $\\mathcal{T}$ et donc que $\\neg\\phi$ est fausse dans tous les modèles de $\\mathcal{T}$. Impossible donc pour un modèle de satisfaire à la fois $\\mathcal{T}$ et $\\neg\\phi$.\nÀ l'inverse, si $\\mathcal{T}\\cup\\set{\\neg\\phi}$ est inconsistante, alors $\\neg\\phi$ est fausse dans tous les modèles de $\\mathcal{T}$ et donc $\\phi$ est vraie dans tous les modèles de $\\mathcal{T}$. D'où $\\mathcal{T}\\models\\phi$. $\\mathcal{T}\\cup\\set{\\phi}\\models\\psi$ ssi $\\mathcal{T}\\models\\phi\\rightarrow\\psi$.\nLa seule possibilité pour que $\\phi\\rightarrow\\psi$ soit fausse est si à la fois $\\phi$ est vraie et $\\psi$ est fausse. Or si $\\mathcal{T}\\cup\\set{\\phi}\\models\\psi$, cela signifie que si $\\phi$ est vraie dans un modèle, alors $\\psi$ est nécessairement vraie dans ce même modèle.\nÀ l'inverse, si $\\phi\\rightarrow\\psi$ est conséquence de $\\mathcal{T}$, alors $\\psi$ est vraie dans tous les modèles de $\\mathcal{T}\\cup\\set{\\phi}$ puisque $\\phi\\rightarrow\\psi$ est vraie dans tous les modèles de $\\mathcal{T}$.\nEn d'autres termes, déduire $\\phi\\rightarrow\\psi$ de nos hypothèses $\\mathcal{T}$, c'est la même chose que déduire $\\psi$ en faisant l'hypothèse supplémentaire qu'on a $\\phi$. Substitution de sous formules équivalentes Dans une formule, substituer une sous-formule par une autre revient à retirer toutes la partie de l\u0026rsquo;arbre qui descend d\u0026rsquo;un nœud et la remplacer par un autre sous-arbre.\nSi la sous-formule \u0026ldquo;greffée\u0026rdquo; est équivalente à la sous-formule retirée, alors la nouvelle formule est équivalente à l\u0026rsquo;originale.\nL\u0026rsquo;intérêt des substitutions est de simplifier la formule pour rendre son interprétation plus facile.\nUne simplification forte (bien qu\u0026rsquo;elle agrandisse l\u0026rsquo;arbre) consiste à restreindre le nombre de symboles utilisés.\nOn peut ainsi toujours transformer une formule $\\phi$ par une formule $\\phi_{\\neg,\\lor,\\land}\\equiv\\phi$ avec tous ses connecteurs logiques dans $\\set{\\neg,\\lor,\\land}$.\nIl suffit en effet de substituer les nœuds $\\rightarrow$ et $\\leftrightarrow$ :\nOn peut aller encore plus loin en transformant une formule $\\phi$ par une formule $\\phi_{\\neg,\\lor}\\equiv\\phi$ avec tous ses connecteurs logiques dans $\\set{\\neg,\\lor}$.\nOu bien en transformant une formule $\\phi$ par une formule $\\phi_{\\neg,\\land}\\equiv\\phi$ avec tous ses connecteurs logiques dans $\\set{\\neg,\\land}$.\nOn utilise pour cela les deux substitutions suivantes :\nDans l\u0026rsquo;exemple suivant, on détermine une formule équivalente avec $\\neg$ et $\\lor$ comme seuls connecteurs logiques (et on élimine les doubles négations dans la dernière étape.\nJeux d\u0026rsquo;évaluation Ernst Zermelo a démontré que les jeux tour à tour à deux joueurs finis, à information parfaite (pas comme la bataille navale), sans hasard, et sans match nul, sont tous déterminés.\nCela signifie qu\u0026rsquo;un des deux joueurs a une stratégie gagnante.\nPour pouver ce résultat, on va représenter toutes les configurations possibles dans le jeu par un arbre dont les nœuds sont étiquetés par un des deux joueurs (0 ou 1) et les arêtes sont ses coups possibles depuis cette position. Les feuilles de l\u0026rsquo;arbre correspondent à des positions gagnantes, soit pour 0, soit pour 1. Puis on va décrire un algorithme permettant de déterminer lequel des deux joueurs a une stratégie gagnante sur une position donnée.\nOn commence par colorier en rouge les feuilles gagnantes pour 0 et en vert les feuilles gagnantes pour 1.\nEnsuite on regarde les prédécesseurs de ces feuilles :\ns\u0026rsquo;il s\u0026rsquo;agit d\u0026rsquo;une position de 0, alors on le colorie en rouge si au moins un des successeurs est rouge car alors 0 peut y aller et gagner. on le colorie en vert si tous ses successeurs sont verts (car 0 n\u0026rsquo;a alors pas d\u0026rsquo;autre choix que d\u0026rsquo;aller sur une position gagnante de 1 et lui donner la victoire). s\u0026rsquo;il s\u0026rsquo;agit d\u0026rsquo;une position de 1, alors on inverse le raisonnement : on le colorie en vert si au moins un des successeurs est vert car alors 1 peut y aller et gagner. on le colorie en rouge si tous ses successeurs sont rouges (car 1 n\u0026rsquo;a alors pas d\u0026rsquo;autre choix que d\u0026rsquo;aller sur une position gagnante de 0 et lui donner la victoire). On répète ensuite la procédure avec les prédécesseurs des nœuds que l\u0026rsquo;on vient de colorer jusqu\u0026rsquo;à remonter à la racine.\nComme on finira fatalement par colorier ainsi chaque nœud de l\u0026rsquo;arbre, on prouve qu\u0026rsquo;il existe au moins une stratégie gagnante pour chacun des sous-arbres et pour le jeu complet.\nUn petit exemple :\nÉtant donné un modèle $\\mathcal{M}$ et une formule $\\phi \\in \\mathcal{F}$, la valeur que prend la formule dans le modèle peut être obtenue grâce à un jeu inventé par Jaakko Hintikka, noté $\\mathbb{E}v(\\mathcal{M},\\phi)$ et appelé jeu d\u0026rsquo;évaluation.\n$\\phi$ doit être écrite avec seulement les connecteurs logiques $\\neg$, $\\land$ et $\\lor$ (ce qui, comme on l\u0026rsquo;a vu, est toujours possible).\nLes deux joueurs s\u0026rsquo;appellent le Vérificateur (V) dont le but est de prouver que la formule est satisfaite dans le modèle ($\\mathcal{M}\\models\\phi$) et le Falsificateur (F) qui doit montrer que la formule est fausse ($\\mathcal{M}\\not\\models\\phi$).\nL\u0026rsquo;arbre du jeu correspond à l\u0026rsquo;arbre de la formule. Si un nœud est une disjonction $\\lor$, c\u0026rsquo;est au tour de V et si c\u0026rsquo;est une conjonction $\\land$, c\u0026rsquo;est au tour de F. Si le nœud est une négation $\\neg$, les deux joueurs échangent leur rôle. De plus, les propositions vraies correspondent à des positions gagnantes pour V et les propositions fausses sont gagnantes pour F.\nGrâce à ces règles, on obtient le théorème suivant :\n$\\mathcal{M}\\models\\phi$ ssi V a une stratégie gagnante dans $\\mathbb{E}v(\\mathcal{M},\\phi)$. $\\mathcal{M}\\models\\neg\\phi$ ssi F a une stratégie gagnante dans $\\mathbb{E}v(\\mathcal{M},\\phi)$. Prouvons-le par induction sur la hauteur de la formule $\\phi$ :\nhauteur $0$\u0026nbsp;: correspond à une formule $\\phi$ réduite à une variable propositionnelle. On vérifie alors tout de suite 1. et 2.. hauteur $n+1$\u0026nbsp;: si $\\phi=\\neg\\psi$, alors l'hypothèse d'induction opère sur $\\psi$.\nLe jeu d'évaluation impliquant $\\psi$ est le même que celui impliquant $\\phi$ à un changement de rôle près. Si $\\psi$ est vraie, alors V a une stratégie gagnante dans $\\mathbb{E}v(\\mathcal{M},\\psi)$ (par hypothèse d'induction) et donc F a une stratégie gagnante dans $\\mathbb{E}v(\\mathcal{M},\\phi)$ (par la règle de changement de rôle). Une $\\phi$ fausse donne bien une stratégie gagnante pour F. Et inversement, une $\\phi$ vraie donnera bien une stratégie gagnante pour V. si $\\phi=\\psi_1\\land\\psi_2$, alors l'hypothèse d'induction opère sur $\\psi_1$ et $\\psi_2$.\nPuisqu'on est sur un $\\land$, les règles du jeu stipulent que c'est au tour de F de jouer.\n$\\phi$ vraie implique $\\psi_1$ et $\\psi_2$ vraie, ce qui donne une stratégie gagnante pour V puisque les deux branches sont gagnantes. Et à l'inverse, si $\\phi$ est fausse, cela implique qu'au moins une des sous-formules soit fausse et donc que F dispose d'au moins une branche avec une stratégie gagnante ce qui assure que le nœud $\\land$ est gagnant pour F. si $\\phi=\\psi_1\\lor\\psi_2$, alors l'hypothèse d'induction opère sur $\\psi_1$ et $\\psi_2$.\nC'est au tour de V de jouer d'après les règles. Il n'a besoin que d'une branche gagnante pour rendre le nœud victorieux. Or si $\\phi$ est vraie, c'est bien le cas\u0026nbsp;: au moins une des sous-formules est vraie et donc au moins une branche victorieuse s'ouvre à V.\nEt une $\\phi$ fausse implique que les deux sous-formules sont fausses aussi et donc victorieuses pour F, ce qui donne le nœud $\\lor$ à F. Exemple : considérons la formule $\\bar{\\phi}=\\neg (( \\neg( \\neg P \\land (( Q\\land ( R \\land \\neg P )) \\lor ( Q \\land \\neg P ))) \\land ( P \\land ( R \\lor \\neg R )) \\lor ( ( R \\land Q ) ))$ dont l\u0026rsquo;arbre est représenté ci-dessous. Et on se donne le modèle $\\mathcal{M}\\models P,Q,\\neg R$\nOn commence par colorier les feuilles conformément au modèle, puis on remplace chaque occurrence de $\\lor$, $\\land$, $\\neg$ respectivement par les symboles V, F et ⇔.\nEnsuite on s\u0026rsquo;occupe des changements de rôle : si la branche connectant un nœud à la racine contient un nombre impair d\u0026rsquo;inversions, on intervertit l\u0026rsquo;étiquette du nœud, et sinon on la laisse telle quelle.\nOn peut maintenant retirer les symboles d\u0026rsquo;inversion et chercher lequel de V ou F a une stratégie gagnante en remontant depuis les feuilles.\nVictoire au Falsificateur F ! D\u0026rsquo;après le théorème, on a donc $\\mathcal{M}\\not\\models\\phi$.\nLes trois stratégies gagnantes possibles pour le Falsificateur sont les suivantes :\nTable de vérité Une table de vérité est un tableau regroupant de manière synthétique tous les modèles d\u0026rsquo;une fomrule donnée.\nSi la formule possède $n$ variables propositionnelles, le tableau contient $2^n$ lignes, une ligne par modèle.\nExemple :\nConstruisons la table de vérité de $\\phi := \\left(\\left(P\\land\\neg Q\\right)\\rightarrow\\left(\\neg\\left( P\\lor R\\right) \\leftrightarrow\\neg Q\\right)\\right)$\nTautologies et contradictions Comme on l'a vu plus haut, $\\phi$ est une tautologie si elle est vraie dans tous les modèles, ce qu'on note $\\models \\phi$. On écrit alors aussi : $\\phi \\equiv \\top$\nEt rappelons le lien étroit entre la notion de tautologie et celle d\u0026rsquo;équivalence $\\leftrightarrow$ :\n$\\phi\\equiv \\psi$ ssi $\\phi\\leftrightarrow\\psi\\equiv\\top$ $\\phi$ est une contradiction ssi $\\phi$ est une tautologie. On note alors $\\phi\\equiv\\bot$. Une contradiction est fausse dans tout modèle. Elle n\u0026rsquo;a donc pas de modèle ; une contradiction ne peut jamais avoir lieu.\nTautologies importantes :\nidempotence de la conjonction et de la disjonction $(P\\land P)\\leftrightarrow P$$(P\\lor P)\\leftrightarrow P$ commutativité de la conjonction, disjonction et de la double implication $(P\\lor Q) \\leftrightarrow (Q\\lor P)$$(P\\land Q) \\leftrightarrow (Q\\land P)$$(P\\leftrightarrow Q) \\leftrightarrow (Q\\leftrightarrow P)$ associativité de la disjonction, de la conjonction et de la double implication $((P\\lor Q) \\lor R)\\leftrightarrow (P\\lor(Q\\lor R))$$((P\\land Q) \\land R)\\leftrightarrow (P\\land(Q\\land R))$$((P\\leftrightarrow Q) \\leftrightarrow R)\\leftrightarrow (P\\leftrightarrow(Q\\leftrightarrow R))$ distributivité de la disjonction par rapport à la conjonction et réciproquement $(P\\lor (Q\\land R))\\leftrightarrow ((P\\lor Q)\\land (P\\lor R))$$(P\\land (Q\\lor R))\\leftrightarrow ((P\\land Q)\\lor (P\\land R))$ lois d'absorption $(P\\land(P\\lor Q))\\leftrightarrow P$$(P\\lor(P\\land Q))\\leftrightarrow P$ lois de De Morgan $\\neg (P\\lor Q) \\leftrightarrow (\\neg P \\land \\neg Q)$$\\neg (P\\land Q) \\leftrightarrow (\\neg P \\lor \\neg Q)$ contraposée $(P\\rightarrow Q) \\leftrightarrow (\\neg Q \\rightarrow \\neg P)$ Un raisonnement consiste généralement à partir d\u0026rsquo;une tautologie, la prémisse, et à aboutir à de nouvelles tautologies à partir d\u0026rsquo;implications (une tautologie étant vraie dans tout modèle, elle implique nécessairement une autre tautologie).\nDans un raisonnement par l\u0026rsquo;absurde (preuve par contradiction), pour montrer que $\\phi$ est une tautologie, on suppose que $\\neg \\phi$ est vraie dans au moins un modèle pour aboutir à une contradiction $\\psi$ ($\\psi\\equiv\\bot$). Dans les modèles où $\\neg \\phi$ est vraie, $\\neg \\phi \\rightarrow \\psi$ est également vraie. Or $\\neg\\phi\\rightarrow\\psi\\equiv \\neg\\neg\\phi\\lor\\psi \\equiv \\phi\\lor\\bot \\equiv \\phi$. Donc dans les modèles où $\\neg\\phi$ est vraie, $\\phi$ est vraie. Par conséquent, il n\u0026rsquo;y a pas de modèle où $\\neg \\phi$ est vraie. D\u0026rsquo;où $\\phi\\equiv\\top$.\nFormes normales Supposons que l\u0026rsquo;on ait un nombre fini de variables propositionnelles $\\set{P_1,P_2,\\ldots,P_n}$.\nOn va construire une formule $\\phi$ qui n\u0026rsquo;est vraie que dans un modèle $\\mathcal{M}$ associé à une distribution de vérité $\\delta$ :\n$\\displaystyle \\phi = (\\epsilon_1 P_1\\land \\epsilon_2 P_2\\land\\cdots\\land\\epsilon_n P_n) = \\bigwedge_{i=1}^n \\epsilon_i P_i$ où $\\epsilon_i$ désigne $\\neg$ si $\\delta(P_i) = 0$ et $\\not \\! \\neg$ si $\\delta(P_i) = 1$ (avec $\\not \\! \\neg P= P$).\nLa table de vérité aura bien des zéros partout sauf dans la ligne correspondant au modèle $\\mathcal{M}$.\nPassons maintenant d\u0026rsquo;une formule qui vérifie un seul modèle à une formule qui en vérifie plusieurs. Choisissons $\\set{\\mathcal{M}_i:i\\in I}$ parmi les $2^n$ modèles possibles. En appelant $\\phi$ la formule vraie dans $\\mathcal{M}_i$, il suffit de former la formule suivante :\n$\\displaystyle \\phi = \\bigvee_{i\\in I}\\phi_i$\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/geometrie/geo1/",
	"title": "Pythagore et Thalès",
	"tags": [],
	"description": "",
	"content": " Le triangle Pythagore Pythagore a vécu à Samos (une île grecque) au 6e siècle avant notre ère. Malgré le nom qu\u0026rsquo;on lui donne, le théorème de Pythagore était connu et utilisé par les Babyloniens et les Indiens des siècles avant Pythagore. Mais peut-être que Pythagore a été le premier à l\u0026rsquo;introduire en Grèce et cela a suffi à lui octroyer une reconnaissance éternelle.\nThéorème de Pythagore :\nSi un triangle est rectangle, alors le carré de la longueur de l’hypoténuse est égal à la somme des carrés des longueurs des deux autres côtés. Réciproque du théorème de Pythagore :\nSi dans un triangle le carré de la longueur du plus grand côté est égal à la somme des carrés des longueurs des deux autres côtés, alors ce triangle est rectangle. Contraposée du théorème de Pythagore :\nSi dans un triangle le carré de la longueur du plus grand côté n\u0026rsquo;est pas égal à la somme des carrés des longueurs des deux autres côtés, alors ce triangle n\u0026rsquo;est pas rectangle. Contraposée de la réciproque du théorème de Pythagore :\nSi un triangle n\u0026rsquo;est pas rectangle, alors le carré de la longueur du plus grand côté n\u0026rsquo;est pas égal à la somme des carrés des longueurs des deux autres côtés. La démonstration historique par Euclide du théorème de Pythagore :\nDeux démonstrations par découpage et réarrangement :\nCes deux démonstrations par découpage (et toutes leurs cousines) reposent sur la possibilité de paver le plan avec deux carrés de côtés différents.\nUne démonstration plus \u0026ldquo;physique\u0026rdquo; (reposant sur ce qu\u0026rsquo;implique une relation de proportionnalité sur une surface) :\nHistoire du théorème : Mésopotamie\nLes historiens des mathématiques et assyriologues ont découvert à la fin des années 1920 que s\u0026rsquo;était forgée en Mésopotamie (l\u0026rsquo;ancien Irak), à l\u0026rsquo;époque paléo-babylonienne une culture mathématique dont l\u0026rsquo;objet n\u0026rsquo;était pas purement utilitariste.\nPlusieurs des tablettes d\u0026rsquo;argile qui ont été retrouvées et analysées montrent que la relation entre les longueurs des côtés du rectangle et celle de sa diagonale (soit entre les longueurs des côtés d’un triangle rectangle) était connue et utilisée pour résoudre des problèmes calculatoires.\nDes \u0026ldquo;tablettes cadastrales\u0026rdquo; (dont la plus ancienne date de -2340 à -2200 ) établies pour le commerce ou l\u0026rsquo;administration de parcelles exposent ainsi cette connaissance. La tablette Si427 datant de -1900 à -1600, découverte à Sippar (Irak), montre ainsi un terrain avec tour, aire de battage et marécage dont une des parcelles a été mise en vente par son propriétaire. Elle présente des découpes de trapèzes et de triangles rectangles ainsi que quelques triplets pythagoriciens.\nLa tablette Plimpton 322 datant de vers -1800 donne une liste ordonnée de nombres associés à des triplets pythagoriciens, soit des entiers (a, b, c) satisfaisant la relation a2 + b2 = c2. La tablette ne donne que deux nombres du triplet, mais les associe explicitement au plus petit côté et à la diagonale d\u0026rsquo;un rectangle. La première colonne ne contient pas un de 3 éléments du triplet mais une combinaison des 3 dont l\u0026rsquo;interprétation varie selon les hypothèses et les reconstitutions des parties manquantes. Ce pourrait être une tablette d\u0026rsquo;exercice pour étudiant mais la régularité des éléments laissent penser à une table. Les inscriptions sont en caractère cunéiforme, en base 60 avec numérotation de position.\nIl n\u0026rsquo;y a pas trace de l\u0026rsquo;énoncé d\u0026rsquo;un théorème, et les historiens préfèrent souvent utiliser un autre mot, certains parlent par exemple de « règle de Pythagore ». Ni celle-ci, ni le principe qui la sous-tend ne sont explicitement énoncés non plus, mais les exemples montrent bien qu\u0026rsquo;une règle générale est connue.\nLa datation et l\u0026rsquo;origine exacte des tablettes d\u0026rsquo;argile n\u0026rsquo;est pas toujours évidente, beaucoup de celles-ci ont été achetées sur le marché des antiquités comme la tablette Plimpton 322, mais les historiens peuvent s\u0026rsquo;appuyer sur des éléments linguistiques, et les similarités avec celles dont l\u0026rsquo;origine est connue, ayant été obtenues par des fouilles archéologiques régulières. Les traces que l\u0026rsquo;on a des cultures antérieures rendent peu vraisemblable la découverte de la « règle de Pythagore », avant -2300, celle-ci pourrait apparaître entre -2025 et -1825.\nInde\nEn Inde, un énoncé du théorème, sous sa forme la plus générale, apparaît dans l\u0026rsquo;Apastamba, l\u0026rsquo;un des Śulba-Sūtras, ces traités du cordeau qui codifient les règles des constructions destinées aux rituels védiques. Ceux-ci ont été rédigés entre le viiie et le ive siècle avant notre ère (par ailleurs certains triplets pythagoriciens sont mentionnés dans des textes bien antérieurs). Les Sulbasutras parlent du rectangle et de sa diagonale, plutôt que de triangle.\nChine\nLe théorème apparaît également en Chine dans le Zhoubi suanjing (« Le Classique mathématique du Gnomon des Zhou »), un des plus anciens ouvrages mathématiques chinois. Ce dernier, écrit probablement durant la dynastie Han (206 av. J.-C. à 220), regroupe des techniques de calcul datant de la dynastie Zhou (xe siècle av. J.-C. à -256). Le théorème ou procédure s’énonce de la manière suivante :\n« En réunissant l’aire (mi) de la base (gou) et l’aire de la hauteur (gu), on engendre l’aire de l’hypoténuse. »\nMais la question se pose de savoir si ce théorème — ou cette procédure — était muni ou non d’une démonstration. Sur ce point les avis sont partagés. Le théorème, sous le nom de Gougu (à partir des mots « base » et « altitude »), est repris dans le Jiuzhang suanshu (Les neuf chapitres sur l\u0026rsquo;art mathématique, 100 av. J.-C. à 50), avec une démonstration, utilisant un découpage et une reconstitution, qui ne ressemble pas à celle d’Euclide et qui illustre l\u0026rsquo;originalité du système démonstratif chinois.\nSource : Wikipédia Deux vieux exos Un problème babylonien Ce problème a été découvert sur des tablettes d’argile babylonienne entre -2000 et -1600.\nune perche est posée verticalement contre un mur. Si son extrémité haute glisse de 6 unités vers le bas contre le mur, de combien d’unité glisse horizontalement l’extrémité basse ?\nRéponse (cliquer pour afficher) 18 Un problème chinois Un problème plus difficile datant d’entre -250 et +50 (en Chine, le théorème de Pythagore se nomme Goo Gu 勾股).\nDans une mare, un lotus dépasse de 10 cm à la verticale. Un coup de vent le pousse de 60 cm et la fleur touche alors la surface. Quelle est la profondeur de la mare ?\nRéponse (cliquer pour afficher) 175 cm Le théorème de Pythagore peut aussi nous aider à trouver la distance de l\u0026rsquo;horizon comme dans cette activité.\nEscargot de Pythagore L\u0026rsquo;escargot de Pythagore, spirale de Théodore ou encore spirale d\u0026rsquo;Anderhub est une figure qui permet de construire géométriquement les racines carrées des entiers consécutifs.\nCette construction est utilisée dans la jolie preuve de la dépendance en $v^2$ de l\u0026rsquo;énergie cinétique par Johann Bernoulli.\nThalès Deux triangles semblables (ayant les mêmes angles) ont leurs côtés correspondants proportionnels. On parle de théorème de Thalès lorsque les deux triangles partagent un sommet.\nL\u0026rsquo;histoire de Thalès et de la pyramide serait en fait une légende et Thalès de Millet n\u0026rsquo;aurait pas grand chose à voir avec le théorème qui porte son nom.\nCertains textes de l\u0026rsquo;Antiquité grecque font référence aux travaux de Thalès de Milet au vie siècle av. J.-C., dont aucun écrit ne nous est parvenu. Cependant, aucun texte ancien n\u0026rsquo;attribue la découverte du théorème de Thalès à celui-ci. L\u0026rsquo;attribution en France du théorème à Thalès semble associée à la mesure de la hauteur d\u0026rsquo;une pyramide égyptienne que celui-ci aurait effectuée.\nDans son commentaire sur les Éléments d\u0026rsquo;Euclide, Proclus affirme que la géométrie avait été découverte en Égypte, et transportée en Grèce par Thalès après son voyage dans cette contrée. Selon une anecdote rapportée par Pline l\u0026rsquo;Ancien, Plutarque et Diogène Laërce, lors de ce voyage Thalès aurait obtenu la hauteur d\u0026rsquo;une des pyramides en mesurant l\u0026rsquo;ombre de celle-ci. Pour Pline de même que pour Diogène Laërce (qui se réfère à Hieronymus de Rhodes, un auteur actif au iiie siècle av. J.-C., ce qui est déjà autour de trois siècles après Thalès), Thalès attend que son ombre soit égale à sa taille pour mesurer l\u0026rsquo;ombre de la pyramide dont il déduit alors la hauteur.\n« Hiéronyme dit que Thalès mesura les pyramides d'après leur ombre, ayant observé le temps où notre propre ombre égale notre hauteur.\u0026nbsp;» La version que donne Plutarque dans Le Banquet des Sept Sages est clairement romancée :\n« Ainsi, vous, Thalès, le roi d'Égypte vous admire beaucoup, et, entre autres choses, il a été, au-delà de ce qu'on peut dire, ravi de la manière dont vous avez mesuré la pyramide sans le moindre embarras et sans avoir eu besoin d'aucun instrument. Après avoir dressé votre bâton à l'extrémité de l'ombre que projetait la pyramide, vous construisîtes deux triangles par la tangence d'un rayon, et vous démontrâtes qu'il y avait la même proportion entre la hauteur du bâton et la hauteur de la pyramide qu'entre la longueur des deux ombres. » La version de Plutarque fait intervenir des rapports de proportionnalité, et donc peut renvoyer au théorème de Thalès. Ce n\u0026rsquo;est pas vraiment le cas de la version plus élémentaire rapportée par Pline et Diogène Laërce, qui correspond très probablement à la version originale de Hieronymus. De toute façon, comme le remarque Maurice Caveing, « il est peu vraisemblable que le souverain d\u0026rsquo;un pays qui, plus de 1 000 ans avant Thalès, connaissait le calcul du seqed, ait ignoré comment mesurer la hauteur des pyramides ».\nSource : Wikipédia La vidéo suivante présente une démonstration du théorème de Thalès et de sa réciproque (permettant de s\u0026rsquo;assurer du parallélisme entre deux droites) à partir de calculs d\u0026rsquo;aires de triangles.\nGrâce au théorème de Thalès, on peut partager de manière égale un segment sans faire la moindre mesure.\nPrésentation d\u0026rsquo;un petit problème posé à l\u0026rsquo;examen d\u0026rsquo;entrée au MIT (grande université scientifique américaine) en 1869 :\nGrâce à Thalès, on peut aussi savoir si deux bateaux vont se rencontrer (on dit faire route de collision).\nSupposons qu\u0026rsquo;un bateau A avance selon un cap fixe (direction constante) à vitesse constante et continue de voir dans la même direction un bateau B, lui aussi en mouvement rectiligne uniforme (suffit de remarquer que le bateau B reste aligné avec un repère fixe pris sur le bateau A). Alors les droites joignant les deux bateaux à un instant donné sont parallèles.\nAppelons C le point de croisement entre les deux trajectoires des bateaux et appelons $t_C$ et ${t\u0026rsquo;}_C$ les instants respectifs où les bateaux A et B arrivent en C.\nAppliquons le théorème de Thalès :\n$$\\frac{A(t_C)A(t_i)}{A(t_C)A(t_0)}=\\frac{B({t\u0026rsquo;}_C)B(t_i)}{B({t\u0026rsquo;}_C)B(t_0)} \\Rightarrow\\frac{v_A(t_C-t_i)}{v_A(t_C-t_0)}=\\frac{v_B({t\u0026rsquo;}_C-t_i)}{v_B({t\u0026rsquo;}_C- t_0)}$$\nOn en déduit $t_C = {t\u0026rsquo;}_C$, ce qui implique que les deux bateaux arrivent ensemble en C. Il va y avoir collision ! Donc si on voit un bateau toujours dans la même direction, la plus grande méfiance s\u0026rsquo;impose.\nDroite d\u0026rsquo;Euler "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/quantique/spekkens/",
	"title": "Théorie-jouet de Spekkens",
	"tags": [],
	"description": "",
	"content": " Théorie-jouet de Spekkens note\nSpekkens, Robert W. (March 19, 2007). \u0026ldquo;Evidence for the epistemic view of quantum states: A toy theory\u0026rdquo;. Physical Review A. 75 (3): 032110 🌐\nLeifer, M. S. « Is the Quantum State Real? An Extended Review of ψ-ontology Theorems », Quanta, vol. 3, no 1, p. 67–155 (2014) 🌐\nKnee, G. C. « Isolation of the Conceptual Ingredients of Quantum Theory by Toy Theory Comparison », mémoire de Master of Science, Imperial College London, soutenu le 20 septembre 2010 🌐\nSpekkens propose en 2007 un modèle jouet qui reproduit qualitativement la physique d’un qubit à partir d\u0026rsquo;une théorie classique à laquelle on ajoute un principe limitant la connaissance qu\u0026rsquo;on peut obtenir sur n\u0026rsquo;importe quel système. La théorie permet d\u0026rsquo;éclairer certains phénomènes quantiques sous un autre jour, ce qui pour l\u0026rsquo;auteur devrait nous inciter à adopter une vision épistémique de l\u0026rsquo;état quantique. Dans cette vision, l\u0026rsquo;état quantique devient porteur d\u0026rsquo;une information incomplète sur une réalité sous-jacente, cachée. Étant donnée que la théorie de Spekkens est locale et que les inégalités de Bell impliquent qu\u0026rsquo;elle ne peut reproduire toute la mécanique quantique, d\u0026rsquo;où sa catégorisation en théorie-jouet.\nLe système élémentaire Le plus petit objet de la théorie de Spekkens est appelé système élémentaire. C\u0026rsquo;est l\u0026rsquo;objet ontique, l\u0026rsquo;objet réel de la théorie. Il peut se trouver dans l’un des quatre états ontiques possibles. On peut penser par exemple à un dé tétraédrique dont chaque face modélise un état possible.\nOn représente un système élémentaire par une rangée de quatre cases (une par état ontique).\nNotre « opinion » sur le système, appelée état épistémique, est représentée par une disjonction (ou logique) d\u0026rsquo;états ontiques. Par exemple $a \\lor b$ signifie : « le système est soit dans l’état ontique a, soit dans l’état b ».\nDifférents degrés de connaissance :\nignorance totale\u0026nbsp;:\n$$1\\lor2\\lor3\\lor4$$ connaissance partielle\u0026nbsp;:\n$$1\\lor2$$ connaissance totale\u0026nbsp;:\n$$1$$ La base ontique d’un état épistémique est l’ensemble des états ontiques compatibles avec lui. Exemple : la base ontique de $1 \\lor 2 \\lor 3 \\lor 4$ est $\\{1, 2, 3, 4\\}$. Pour un système élémentaire, si la base ontique d’un état épistémique contient $n$ états, alors la distribution de probabilité est uniforme sur ces $n$ états. Chaque case colorée porte la probabilité $1/n$. Conséquences :\nDans l’état d’ignorance maximale ($1 \\lor 2 \\lor 3 \\lor 4$), chaque case vaut $1/4$.\nDans l’état de connaissance partielle ($1\\lor2$), chaque case vaut $1/2$.\nEt dans l’état de connaissance parfaite ($1$), la case unique vaut $1$.\nLorsqu’on connaît tout, l’état épistémique coïncide avec l’état ontique, de même qu’une distribution de Dirac traduit la certitude parfaite sur une variable classique. Le principe d\u0026rsquo;équilibre de la connaissance Jusqu\u0026rsquo;ici, seuls des phénomènes classiques peuvent être reproduits par ce système. Pour obtenir des effets « quantiques » (comme la non‑commutation), il faut ajouter une contrainte informationnelle qui limite ce que l’on peut savoir sur l’état ontique. C’est précisément ce que fait le principe d\u0026rsquo;équilibre de la connaissance.\nSchématiquement, le principe énonce qu\u0026rsquo;on ne peut jamais connaître plus de la moitié de l’information complète.\nQuelques définitions pour préciser les choses :\nEnsemble canonique\u0026nbsp;:\nensemble minimal de questions oui/non suffisantes pour identifier sans ambiguïté l’état ontique d'un système donné. Pour un système élémentaire dans les états ontiques 1, 2, 3 ou 4, un méthode inefficace pour déterminer l\u0026rsquo;état serait : $$\\left\\{ \\begin{array}{l} \\text{ \u0026lsquo;Est-ce 1 ?\u0026rsquo;},\\\\ \\text{ \u0026lsquo;Est-ce 2 ?\u0026rsquo;},\\\\ \\text{ \u0026lsquo;Est-ce 3 ?\u0026rsquo;},\\\\ \\text{ \u0026lsquo;Est-ce 4 ?\u0026rsquo;} \\end{array}\\right\\} $$\nAlors que l\u0026rsquo;ensemble canonique parviendrait au même résultat avec seulement deux questions : $$\\left\\{ \\begin{array}{l} \\text{ \u0026lsquo;Est-ce 1 ou 2 ?\u0026rsquo;},\\\\ \\text{ \u0026lsquo;Est-ce 3 ou 4 ?\u0026rsquo;} \\end{array}\\right\\} $$\nChaque question coupe l’espace des possibles en deux. Pour un système élémentaire à 4 états, 2 questions suffisent donc (pas forcément ces deux là).\nLa mesure de la connaissance $K$ selon Spekkens est définie comme le nombre maximal de questions aux réponses connues parmi tous les ensembles canoniques. Et sa mesure de l'ignorance $I$ se définit comme la différence entre la taille de l’ensemble canonique et $K$. On peut maintenant définir plus proprement le principe d\u0026rsquo;équilibre des connaissances :\nDans un état de connaissance maximale, à tout instant et pour tout système, $K=I$. Une conséquence immédiate est que la connaissance maximale est incomplète.\nPour le système élémentaire : $K=I=1$. Une seule question peut être résolue, l’autre reste sans réponse. Cela revient à ne pouvoir écarter que deux états ontiques au maximum et donc en laisser deux possibles.\nIl y a $\\binom{4}{2}=6$ états possibles différents de connaissance maximale qu\u0026rsquo;on appellera bits-jouets (\u0026ldquo;bit\u0026rdquo; car il ne faut qu\u0026rsquo;un bit classique pour répondre à la question restée sans réponse dans l\u0026rsquo;ensemble canonique pour un système élémentaire) :\nEn ajoutant l\u0026rsquo;état d\u0026rsquo;ignorance maximale, on complète l\u0026rsquo;ensemble des sept états épistémiques autorisés pour un seul système élémentaire.\nCe déficit de connaissance, imposé a priori, suffit à reproduire plusieurs effets clés de la mécanique quantique sans quitter un cadre fondamentalement classique.\nAnalogue d\u0026rsquo;un qubit état pur On peut établir une correspondance bi‑univoque entre les sept états permis d’un bit‑jouet (épistémiques) et sept états particulièrement importants en information quantique : les six états propres $±1$ des trois matrices de Pauli $X$, $Y$, $Z$ et l’état maxim­alement mixte. Le « dictionnaire » est :\n$$ \\begin{array}{rcl} 1\\lor2 \u0026amp;\\longleftrightarrow\u0026amp; \\lvert0\\rangle \\\\ 3\\lor4 \u0026amp;\\longleftrightarrow\u0026amp; \\lvert1\\rangle \\\\ 1\\lor3 \u0026amp;\\longleftrightarrow\u0026amp; \\lvert+\\rangle \\\\ 2\\lor4 \u0026amp;\\longleftrightarrow\u0026amp; \\lvert-\\rangle \\\\ 2\\lor3 \u0026amp;\\longleftrightarrow\u0026amp; \\lvert{+\\mathrm i}\\rangle \\\\ 1\\lor4 \u0026amp;\\longleftrightarrow\u0026amp; \\lvert{-\\mathrm i}\\rangle \\\\ 1\\lor2\\lor3\\lor4 \u0026amp;\\longleftrightarrow\u0026amp; \\tfrac{\\mathbb I}{2} \\end{array} $$\nReprésentations géométriques :\nDeux états épistémiques sont disjoints si leurs bases ontiques n’ont aucun élément commun.\nLes couples analogues aux paires orthogonales ($\\lvert0\\rangle$, $\\lvert1\\rangle$), ($\\lvert+\\rangle$, $\\lvert-\\rangle$), ($\\lvert{+\\mathrm i}\\rangle$, $\\lvert{-\\mathrm i}\\rangle$) deviennent ainsi trois paires de bits‑jouets diamétralement opposées, leur intersection vide se lisant d’un coup d’œil (aucune case colorées en commun).\nLa fidélité $\\mathcal{F}[\\bar{p},\\bar{q}]$ entre deux états $a\\lor b\\lor c\\lor d$ et $e\\lor f\\lor g\\lor h$, où $p_k$ est un vecteur de probabilités uniformes pour $a$, $b$, $c$ et $d$ de somme unité, et $q_k$ est un vecteur de probabilités uniformes pour $e$, $f$, $g$ et $h$ de somme unité est définie par : $$\\mathcal{F}[\\bar{p},\\bar{q}] \\;=\\; \\sum_{k}\\sqrt{p_k}\\sqrt{q_k}$$\nLa fidélité est une mesure de la non disjonction. La disjonction et l\u0026rsquo;égalité sont deux cas particuliers de fidélités respectives $\\mathcal{F}=0$ et $\\mathcal{F}=1$.\nExemple pour les états épistémiques $1\\lor 3$ et $1\\lor 2$ :\nChaque état autorise exactement deux états ontiques et, par le principe d’uniformité, leur assigne la probabilité $1/2$.\nLes valeurs de $p_k$ et $q_k$ sont donc :\nÉtat ontique$p_k$ pour $1\\lor 3$$q_k$ pour $1\\lor 2$ $1$$1/2$$1/2$ $2$$0$$1/2$ $3$$1/2$$0$ $4$$0$$0$ Calcul de la fidélité :\n$$ \\begin{aligned} \\mathcal{F}[\\bar{p},\\bar{q}] \u0026amp;= \\sum_{k=1}^{4}\\sqrt{p_k}\\,\\sqrt{q_k}\\\\ \u0026amp;=\\sqrt{\\tfrac12}\\sqrt{\\tfrac12} \\;+\\; \\sqrt{0}\\sqrt{\\tfrac12} \\;+\\; \\sqrt{\\tfrac12}\\sqrt{0} \\;+\\; \\sqrt{0}\\sqrt{0} \\\\ \u0026amp;= \\tfrac12 \\;+\\; 0 \\;+\\; 0 \\;+\\; 0 \\\\ \u0026amp;= \\tfrac12 \\end{aligned} $$\nEt on obtient bien la même valeur de fidélité quantique entre les états qubits correspondants, $1\\lor3 \\;\\longleftrightarrow\\; |+\\rangle$, $1\\lor2 \\;\\longleftrightarrow\\; |0\\rangle$ : $|\\langle+|0\\rangle|^{2} = 1/2$.\nLa combinaison convexe de deux états épistémiques disjoints est l\u0026rsquo;union de leurs bases ontiques si cette union forme un état épistémique valide. Sinon, et pour les états non-disjonoints, la combinaison convexe est non définie.\nLa superposition cohérente de deux états épistémiques disjoints $(a\\lor b)$ et $(c\\lor d)$ avec $a,b≠c,d$, et $a\u0026lt;b,c\u0026lt;d$ est définie par :\n$$ \\begin{aligned} (a\\lor b)\\; +_1 \\; (c\\lor d) \u0026amp;= a\\lor c,\\\\ (a\\lor b)\\; +_2 \\; (c\\lor d) \u0026amp;= b\\lor d,\\\\ (a\\lor b)\\; +_3 \\; (c\\lor d) \u0026amp;= b\\lor c,\\\\ (a\\lor b)\\; +_4 \\; (c\\lor d) \u0026amp;= a\\lor d \\end{aligned} $$\nLes opérations binaires cohérentes associent des bits-jouets purs à d\u0026rsquo;autres bits-jouets purs (générant un analogue de la superposition cohérente de la mécanique quantique). Dans la représentation graphique, on associe à deux états diamétralement opposés (donc disjoints) un des quatre autres états.\nLes quatre opérations binaires cohérentes $+_1$, $+_2$, $+_3$, $+_4$ sont analogues aux superpositions de poids égaux de deux états purs de mécanique quantique avec une phase relative $\\theta$ valant respectivement $0$, $\\pi$, $\\pi/2$, $3\\pi/2$.\nExemple : en théorie jouet, $(1\\lor 2)+_4(3\\lor 4)=(1\\lor 4)$, et en mécanique quantique, $\\tfrac1{\\sqrt{2}} (|0\\rangle + \\mathrm{e}^{\\mathrm{i}\\frac{3\\pi}{2}} |1\\rangle)=\\tfrac1{\\sqrt{2}} (|0\\rangle -\\mathrm{i} |1\\rangle)=|-\\mathrm{i}\\rangle$.\nTransformations Dans la théorie-jouet, toute transformation valable doit conserver la fidélité entre états épistémiques. C’est l’analogue exact de la conservation du produit scalaire par les transformations unitaires en mécanique quantique. Autrement dit, si deux états ont une fidélité $\\mathcal{F}$ avant l’opération, ils auront la même fidélité après.\nLes transformations permises sont donc seulement les permutations des quatre états ontiques. En effet, si une transformation envoyait plusieurs états ontiques sur un seul, elle pourrait faire passer un état épistémique légal (respectant le principe d’équilibre des connaissances) vers un état illégal qui le viole.\nExemple de la permutation $(123)(4)$ (permutation cyclique des états ontiques 1, 2 et 3 et absence de permutation pour l\u0026rsquo;état 4).\nDe $1\\lor 2$, on obtient $2 \\lor 3$. Et de $3\\lor 4$, on obtient $1\\lor 4$. La fidélité entre états initiaux (nulle ici) est bien conservée entre les états finaux.\nEt cela reste vrai pour les 24 permutations des états ontiques éléments du groupe $S_4$.\nMesure Dans la théorie-jouet comme en mécanique quantique, une mesure se doit de fournir un résultat mais aussi un état post-mesure.\nEn quantique, l\u0026rsquo;état post-mesure est le vecteur propre de l\u0026rsquo;observable associé à la valeur propre mesurée.\nEn théorie-jouet, l\u0026rsquo;état post-mesure doit continuer à obéir au principe d\u0026rsquo;équilibre de la connaissance. Autrement dit, la mise à jour doit éliminer tous les états ontiques incompatibles avec l’issue observée mais sans jamais laisser moins de deux états ontiques possibles pour un bit-jouet.\nLe système élémentaire possède 4 états ontiques ; une mesure ne peut donc que les scinder en deux paquets de deux, ce qui donne $\\binom{4}{2}=6$ partitions possibles. Mais certaines de ces partitions ne sont qu\u0026rsquo;un réétiquetage comme $\\{a,b\\}\\,|\\,\\{c,d\\}$ et $\\{c,d\\}\\,|\\,\\{a,b\\}$ qui sont équivalentes. Il ne reste finalement que trois directions de mesure analogues aux trois axes $X$, $Y$, $Z$ de mesure d’un qubit.\nOn note la mesure permettant de distinguer $a\\lor b$ de $c\\lor d$ avec les chiffres romains $\\color{#1DB100}\\text{I}$ et $\\color{#D41876}\\text{II}$ :\n$$ \\begin{array}{|c|c|c|c|} \\hline \\color{#1DB100}\\text{I} \u0026amp; \\color{#1DB100}\\text{I} \u0026amp;\\color{#D41876} \\text{II} \u0026amp;\\color{#D41876} \\text{II}\\\\ \\hline \\end{array} = \\{1\\lor 2,3\\lor 4\\} \\leftrightarrow \\text{mesure sur l\u0026rsquo;axe }Z $$\n$$ \\begin{array}{|c|c|c|c|} \\hline \\color{#1DB100}\\text{I} \u0026amp;\\color{#D41876} \\text{II} \u0026amp; \\color{#1DB100}\\text{I} \u0026amp;\\color{#D41876} \\text{II}\\\\ \\hline \\end{array} = \\{1\\lor 3,2\\lor 4\\} \\leftrightarrow \\text{mesure sur l\u0026rsquo;axe }X $$\n$$ \\begin{array}{|c|c|c|c|} \\hline \\color{#1DB100}\\text{I} \u0026amp;\\color{#D41876} \\text{II} \u0026amp;\\color{#D41876} \\text{II} \u0026amp; \\color{#1DB100}\\text{I}\\\\ \\hline \\end{array} = \\{1\\lor 4,2\\lor 3\\} \\leftrightarrow \\text{mesure sur l\u0026rsquo;axe }Y $$\nDeux états épistémiques disjoints (tout comme des états quantiques orthogonaux) sont parfaitement distinguables par un choix appropriés de la direction de mesure. La mesure $\\begin{array}{|c|c|c|c|} \\hline \\color{#1DB100} \\text{I} \u0026amp;\\color{#D41876} \\text{II} \u0026amp;\\color{#1DB100} \\text{I} \u0026amp; \\color{#D41876}\\text{II}\\\\ \\hline \\end{array}$ permet par exemple de séparer avec certitude $1\\lor 3$ et $2\\lor 4$.\nDonc si on utilise $\\begin{array}{|c|c|c|c|} \\hline \\color{#1DB100} \\text{I} \u0026amp;\\color{#D41876} \\text{II} \u0026amp;\\color{#1DB100} \\text{I} \u0026amp; \\color{#D41876}\\text{II}\\\\ \\hline \\end{array}$ sur $2\\lor 4$, l\u0026rsquo;état post-mesure est toujours $2\\lor 4$.\nCertaines mesures peuvent laisser une ambigüité comme $\\begin{array}{|c|c|c|c|} \\hline \\color{#1DB100}\\text{I} \u0026amp;\\color{#D41876} \\text{II} \u0026amp;\\color{#1DB100} \\text{I} \u0026amp;\\color{#D41876} \\text{II}\\\\ \\hline \\end{array}$ sur $1\\lor 2$. Puisque l\u0026rsquo;état ontique est soit en $1$, soit en $2$, les deux résultats de mesures $1\\lor 3$ et $2\\lor 4$ sont possibles et obtenus avec la même probabilité.\nIl est important de noter que l\u0026rsquo;incertitude ici n\u0026rsquo;est pas fondamentale mais la conséquence du manque de connaissance initiale sur la position de l\u0026rsquo;état ontique.\nImaginons que $\\begin{array}{|c|c|c|c|} \\hline \\color{#1DB100}\\text{I} \u0026amp;\\color{#D41876} \\text{II} \u0026amp;\\color{#1DB100} \\text{I} \u0026amp;\\color{#D41876} \\text{II}\\\\ \\hline \\end{array}$ sur $1\\lor 2$ donne $1\\lor 3$. Cela nous indique où était l\u0026rsquo;état ontique (en $1$) mais le principe d\u0026rsquo;équilibre de la connaissance nous interdisant de connaître l\u0026rsquo;état ontique actuel, cela impose que la mesure ait perturbé le système en distribuant l\u0026rsquo;état ontique aléatoirement sur les deux états possibles mesurés $1$ ou $3$.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc2/",
	"title": "TQC-2",
	"tags": [],
	"description": "",
	"content": " Théorie quantique des champs \u0026ndash; Partie 2 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Très souvent une simple traduction.\nLe premier encadré gris est issu de No-Nonsense Classical Mechanics de Jakob Schwichtenberg.\nRetour sommaire\nRevenons dans un premier temps à la mécanique du 19e siècle, en particulier la mécanique hamiltonienne pour constater sa similarité avec la mécanique quantique avant de présenter la théorie classique des champs.\nThéorie classique des champs Du Lagrangien à l\u0026rsquo;Hamiltonien Le taux de variation du Lagrangien est donné par :\n$$\\frac{dL}{dt} = \\frac{\\partial L}{\\partial q_i} \\dot{q}_i + \\frac{\\partial L}{\\partial \\dot{q}_i} \\ddot{q}_i$$\nEt en utilisant les équations d\u0026rsquo;Euler-Lagrange, on obtient :\n$$\\frac{dL}{dt} = \\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{q}_i} \\right) \\dot{q}_i + \\frac{\\partial L}{\\partial \\dot{q}_i} \\ddot{q}_i = \\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{q}_i} \\dot{q}_i \\right)$$\nOn définit le moment canonique conjugué $p_i$ :\n$$p_i = \\frac{\\partial L}{\\partial \\dot{q}_i}$$\nCela permet de réécrire l\u0026rsquo;équation précédente en lui donnant la forme d\u0026rsquo;une équation de conservation :\n$$ \\frac{d}{dt} (p_i \\dot{q}_i - L) = 0 $$\nOn appelle Hamiltonien $H$ la quantité conservée et on montrera plus loin qu\u0026rsquo;elle correspond à l\u0026rsquo;énergie du système :\n$$H = p_i \\dot{q}_i - L$$\nLe passage de $L$ à $H$ correspond mathématiquement à une transformation de Legendre.\nLa transformation de Legendre permet d\u0026rsquo;encoder différemment l\u0026rsquo;information d\u0026rsquo;une fonction. En particulier, elle permet de changer la dépendance en une coordonnée en sa coordonnée conjuguée. Ici, elle va nous permettre de passer de $\\dot{q}$ à $p$.\nImaginons une fonction $L(v)$ convexe et calculons la pente $p(v)=\\frac{\\partial L(v)}{\\partial v}$ (un fonction convexe voit sa pente croître de manière monotone ce qui implique que $p$ et $v$ sont en bijection (si la fonction est concave, il suffit de considérer son opposée)).\nOn peut aussi écrire que la fonction de départ est la primitve de sa pente : $L(v)=\\int_0^vp(v\u0026rsquo;)\\mathrm{d}v\u0026rsquo;$. $L(v)$ devient ainsi l\u0026rsquo;aire sous la courbe définie par la pente $p(v)$.\nMais comme on s\u0026rsquo;est assuré grâce à la convexité de $L$ qu\u0026rsquo;à chaque $v$ corresponde un et un unique $p$, on peut aussi considérer la fonction $v(p)$.\nAppelons $H(p)$ l\u0026rsquo;aire sous la courbe de $v(p)$ : $H(p)\\equiv\\int_0^p v(p\u0026rsquo;)\\mathrm{d}p\u0026rsquo;$.\nOn peut voir sur le schéma suivant que ces deux aires ont un lien très simple : leur somme vaut $p\\times v$ !\nOn a ainsi $H(p) = pv - L(v)$.\nPar conséquent, la transformée de Legendre du Lagrangien $L(q,\\dot{q})$ par rapport à $\\dot{q}$ est donnée par $H(q,p)=p\\dot{q}(p)-L(q,\\dot{q})$ en appelant $p$ la pente $\\frac{\\partial L}{\\partial\\dot{q}}$.\nFaisons varier $H$ :\n$$ \\begin{aligned} \\delta H \u0026amp;= p_i \\delta \\dot{q}_i + \\delta p_i \\dot{q}_i - \\frac{\\partial L}{\\partial q_i} \\delta q_i - \\frac{\\partial L}{\\partial \\dot{q}_i} \\delta \\dot{q}_i\\\\ \u0026amp;= \\delta\\dot{q}_i\\left(\\cancel{p_i - \\frac{\\partial L}{\\partial \\dot{q}_i}}\\right)+ \\delta p_i \\dot{q}_i - \\frac{\\partial L}{\\partial q_i} \\delta q_i \\\\ \u0026amp;=\\delta p_i \\dot{q}_i -\\frac{\\partial L}{\\partial q_i}\\delta q_i \\\\ \u0026amp;=\\delta p_i {\\color{#D41876}\\dot{q}_i} - {\\color{#0076BA}\\dot{p}_i}\\delta q_i \\end{aligned} $$\nEt comme $H$ ne dépend que de $q_i$ et $p_i$, on a :\n$$ \\delta H = {\\color{#0076BA}\\frac{\\partial H}{\\partial q_i}} \\delta q_i + {\\color{#D41876}\\frac{\\partial H}{\\partial p_i}} \\delta p_i $$\nPar identification, on obtient les équations d\u0026rsquo;Hamilton qui permettent de déterminer les équations du mouvement du système d\u0026rsquo;une nouvelle façon :\n$$ {\\color{#D41876} \\frac{\\partial H}{\\partial p_i} = \\dot{q}_i} \\qquad {\\color{#0076BA}\\frac{\\partial H}{\\partial q_i} = -\\dot{p}_i} $$\nDéfinissons maintenant le crochet de Poisson $\\{A,B\\}$ :\n$$ \\{A, B\\} = \\frac{\\partial A}{\\partial q_i} \\frac{\\partial B}{\\partial p_i} - \\frac{\\partial A}{\\partial p_i} \\frac{\\partial B}{\\partial q_i} $$\nPuis considérons une fonction $F$ des coordonnées généralisées $q_i$ et $p_i$. Le taux de variation de $F$ est donné par :\n$$ \\begin{aligned} \\frac{\\mathrm{d}F}{\\mathrm{d}t} \u0026amp;= \\frac{\\partial F}{\\partial t} + \\frac{\\partial F}{\\partial q_i}{\\color{#0076BA}\\dot{q}_i} + \\frac{\\partial F}{\\partial p_i}{\\color{#D41876}\\dot{p}_i}\\\\ \u0026amp;= \\frac{\\partial F}{\\partial t} + \\frac{\\partial F}{\\partial q_i}\\left({\\color{#0076BA}-\\frac{\\partial H}{\\partial p_i}}\\right) + \\frac{\\partial F}{\\partial p_i}{\\color{#D41876}\\frac{\\partial H}{\\partial q_i}}\\\\ \u0026amp;=\\frac{\\partial F}{\\partial t} + \\{F, H\\} \\end{aligned} $$\nEt si $F$ n\u0026rsquo;est pas une fonction du temps :\n$$ \\frac{dF}{dt} = \\{F, H\\} $$\nDonc si $\\{F, H\\}=0$, alors $F$ est une constante du mouvement !\nIl y a une interprétation géométrique à cette relation.\nSi $F=F(\\boldsymbol{q},\\boldsymbol{p})$, alors le vecteur $(\\dot{\\boldsymbol{q}},\\dot{\\boldsymbol{p}})=\\left(\\frac{\\partial H}{\\partial\\boldsymbol{p}},-\\frac{\\partial H}{\\partial \\boldsymbol{q}}\\right)$ est tangent à la surface $F(\\boldsymbol{q},\\boldsymbol{p})=$ constante. En effet $\\boldsymbol{\\nabla} F \\cdot (\\dot{\\boldsymbol{q}}, \\dot{\\boldsymbol{p}}) = \\left(\\frac{\\partial F}{\\partial \\boldsymbol{q}},\\frac{\\partial F}{\\partial \\boldsymbol{p}}\\right)\\cdot (\\dot{\\boldsymbol{q}},\\dot{\\boldsymbol{p}})= \\frac{\\partial F}{\\partial q_i} \\frac{\\partial H}{\\partial p_i} - \\frac{\\partial F}{\\partial p_i} \\frac{\\partial H}{\\partial q_i}=\\{F,H\\}=0$.\nEt de même, $(\\dot{\\boldsymbol{q}},\\dot{\\boldsymbol{p}})$ est tangent à la surface $H(\\boldsymbol{q},\\boldsymbol{p})=$ constante puisque $\\boldsymbol{\\nabla} H \\cdot (\\dot{\\boldsymbol{q}}, \\dot{\\boldsymbol{p}}) = \\left(\\frac{\\partial H}{\\partial \\boldsymbol{q}},\\frac{\\partial H}{\\partial \\boldsymbol{p}}\\right)\\cdot (\\dot{\\boldsymbol{q}},\\dot{\\boldsymbol{p}})= -\\dot{\\boldsymbol{p}}\\cdot\\dot{\\boldsymbol{q}}+\\dot{\\boldsymbol{q}}\\cdot\\dot{\\boldsymbol{p}}=0$\nLe lien entre crochets de Poisson et loi de conservation rappelle bien sûr le rôle du commutateur en mécanique quantique.\nLe taux de variation de l\u0026rsquo;espérance quantique de l\u0026rsquo;opérateur $\\hat{F}$ est en effet donné par :\n$$ \\frac{d \\langle \\hat{F} \\rangle}{dt} = \\frac{1}{i \\hbar} \\langle [\\hat{F}, \\hat{H}] \\rangle $$\nLe parallélisme entre mécanique quantique et formalisme hamiltonien pousse à jeter un pont entre les deux :\n$$ \\{ A, B \\} \\rightarrow \\frac{1}{\\mathrm{i} \\hbar} \\langle [\\hat{A}, \\hat{B}] \\rangle $$\nVérifions le parallèle pour le couple (position, moment conjugué) :\n$$ \\begin{aligned} \\{ q_j, p_k \\} \u0026amp;= \\frac{\\partial q_j}{\\partial q_i}\\frac{\\partial p_k}{\\partial p_i}-\\frac{\\partial p_k}{\\partial q_i}\\frac{\\partial q_j}{\\partial p_i}\\\\ \u0026amp;= \\delta_{ij}\\delta_{ik}-0\\times 0\\\\ \u0026amp;= \\delta_{jk}\\\\ \\end{aligned} $$\nCe qui donnerait avec le pont $[\\hat{q}_j,\\hat{p}_k]=\\mathrm{i}\\hbar\\delta_{jk}$ qui est bien le commutateur quantique entre position et impulsion.\nEn relativité restreinte Cherchons le Lagrangien d\u0026rsquo;une particule libre de masse $m$ dans le cadre de la relativité restreinte.\nL\u0026rsquo;action $S=\\int_{t_1}^{t_2} L \\mathrm{d}t$ de la particule se doit d\u0026rsquo;être invariante de Lorentz. L\u0026rsquo;introduction du temps propre $\\mathrm{d}\\tau=\\frac{\\mathrm{d}t}{\\gamma}$ (le seul véritable invariant) semble s\u0026rsquo;imposer : $S = \\int_{\\tau_1}^{\\tau_2}L\\gamma\\mathrm{d}\\tau$.\n$L\\gamma$ doit alors à son tour être invariant de Lorentz et on ne voit pas vraiment d\u0026rsquo;autre possibilité que d\u0026rsquo;être constant, d\u0026rsquo;où $L=\\frac{K}{\\gamma}$ où $K$ est une constante. Or on sait qu\u0026rsquo;aux petites vitesses, on doit retrouver $L=\\frac{1}{2}mv^2\\, (+\\text{cste})$, et comme $\\gamma^{-1} \\approx 1 - \\frac{1}{2} \\frac{v^2}{c^2} $ lorsque $v\\ll c$, on obtient $K\\left(1-\\frac{1}{2}\\left(\\frac{v^2}{c^2}\\right)\\right)=\\frac{1}{2}mv^2+\\text{cste}$. Cela impose $K=-mc^2$. Et finalement :\n$$ S=-mc^2\\int_{\\tau_1}^{\\tau_2}\\mathrm{d}\\tau=-mc\\int_a^b \\mathrm{d}s $$\navec $\\mathrm{d}s=\\sqrt{c^2\\mathrm{d}t^2-\\mathrm{d}x^2-\\mathrm{d}y^2-\\mathrm{d}z^2}$\nPar principe de moindre action $\\delta S = 0$ or $\\delta \\int_a^b \\mathrm{d}s=0$ est maximum le long d\u0026rsquo;une ligne droite. On retrouve donc que la trajectoire d\u0026rsquo;une particule libre est une ligne droite.\nIl est par exemple facile de se convaincre mathématiquement que le chemin purement temporel joignant A à B est le plus long possible puisque $c\\mathrm{d}t\u0026gt;\\sqrt{c^2\\mathrm{d}t^2-\\mathrm{d}x^2}$ (c\u0026rsquo;est la magie de la métrique minkowskienne que de rendre plus long un chemin sans détour).\nOn en déduit l\u0026rsquo;expression du moment conjugué d\u0026rsquo;une particule libre (sa quantité de mouvement) :\n$$ \\boldsymbol{p}=\\frac{\\partial L}{\\partial \\boldsymbol{v}}=\\frac{\\partial }{\\partial \\boldsymbol{v}}\\left( -mc^2\\sqrt{1-v^2/c^2}\\right)=\\gamma m \\boldsymbol{v} $$\nEt son énergie :\n$$ E=H=\\boldsymbol{p}\\cdot\\boldsymbol{v}-L=\\gamma m v^2 +\\frac{mc^2}{\\gamma}=\\gamma m c^2\\left[\\frac{v^2}{c^2}+\\left(1-\\frac{v^2}{c^2}\\right)\\right]=\\gamma m c^2 $$\nEn relativité restreinte, on assemble l\u0026rsquo;énergie et le moment en un quadrivecteur $p^\\mu=(\\frac{E}{c},\\boldsymbol{p})$ (ou $p_\\mu=(\\frac{E}{c},-\\boldsymbol{p})$).\nParticule chargée dans un champ électromagnétique Donnons une charge $q$ à notre particule et couplons-la à un champ électromagnétique. Ce dernier peut être décrit par un champ quadrivectoriel $A^\\mu(x)=\\left(\\frac{V(x)}{c},\\boldsymbol{A}(x)\\right)$ où $V(x)$ est le potentiel électrique (scalaire) et $\\boldsymbol{A}(x)$ est le potentiel magnétique (vectoriel). L\u0026rsquo;interaction avec le champ correspond à une énergie $-qA_\\mu\\mathrm{d}x^\\mu$ et donc l\u0026rsquo;action s\u0026rsquo;écrit :\n$$ S = \\int_{t_1}^{t_2} \\left( - \\frac{mc^2}{\\gamma} + q\\boldsymbol{A} \\cdot \\boldsymbol{v} - qV \\right) dt $$\nLe Lagrangien est l\u0026rsquo;intégrande et donc le moment canonique conjugué est donné par :\n$$ \\boldsymbol{p} = \\frac{\\partial L}{\\partial \\boldsymbol{v}} = \\gamma m \\boldsymbol{v} + q\\boldsymbol{A} $$\nSystème d’unités utilisées en TQC :\nunités de Lorentz-Heaviside\nElles simplifient l\u0026rsquo;écriture des équations de l\u0026rsquo;électromagnétisme en posant : $$ \\mu_0=\\epsilon_0=1 $$ Les équations de Maxwell deviennent : $$ \\begin{array}{ll} \\boldsymbol{\\nabla} \\cdot \\boldsymbol{E} = \\rho \u0026amp;\\boldsymbol{\\nabla} \\times \\boldsymbol{E} = -\\frac{1}{c}\\frac{\\partial \\boldsymbol{B}}{\\partial t}\\\\ \\boldsymbol{\\nabla} \\cdot \\boldsymbol{B} = 0 \u0026amp;\\boldsymbol{\\nabla} \\times \\boldsymbol{B} = \\frac{1}{c}\\left( \\boldsymbol{J} + \\frac{\\partial \\boldsymbol{E}}{\\partial t}\\right) \\end{array} $$\nunités naturelles\nComme les vitesses s\u0026rsquo;expriment en fraction de $c$ et les spins en unités de $\\hbar$, il est plus commode de les utiliser comme étalon de mesure en posant : $$ \\hbar=c=1 $$\nOn définit le tenseur antisymétrique de second rang $F_{\\mu\\nu}$ comme :\n$$ F_{\\mu \\nu} = \\partial_{\\mu} A_{\\nu} - \\partial_{\\nu} A_{\\mu} $$\nC\u0026rsquo;est le tenseur du champ électromagnétique (il ressemble à un rotationnel à 4 dimensions). Ses éléments contiennent les composantes des champs $\\boldsymbol{E}$ et $\\boldsymbol{B}$.\nEn notant que $\\partial^\\mu=\\left(\\frac{\\partial}{\\partial t},-\\nabla\\right)$ et $\\partial_\\mu=\\left(\\frac{\\partial}{\\partial t},\\nabla\\right)$, on obtient les composantes du champ à partir de :\n$\\boldsymbol{B}=\\boldsymbol{\\nabla} \\times \\boldsymbol{A}$, qui donne ${\\color{#0076BA}B^i}=-\\varepsilon^{ijk}\\partial_jA_k=-\\frac{1}{2}\\varepsilon^{ijk}F^{jk}$ où $\\varepsilon^{ijk}$ est le symbole de Levi-Civita, $\\boldsymbol{E}=-\\frac{\\partial \\boldsymbol{A}}{\\partial t}-\\boldsymbol{\\nabla} V$, qui donne ${\\color{#D41876}E^i}=-\\partial^0 A^i+\\partial^i A^0=-F^{0i}=F^{i0}$. $$ F_{\\mu \\nu} = \\begin{pmatrix} 0 \u0026amp;\\color{#D41876} E_1 \u0026amp; \\color{#D41876}E_2 \u0026amp; \\color{#D41876}E_3 \\\\ \\color{#D41876}-E_1 \u0026amp; 0 \u0026amp; \\color{#0076BA}-B_3 \u0026amp; \\color{#0076BA}B_2 \\\\ \\color{#D41876}-E_2 \u0026amp;\\color{#0076BA} B_3 \u0026amp; 0 \u0026amp;\\color{#0076BA} -B_1 \\\\ \\color{#D41876}-E_3 \u0026amp;\\color{#0076BA} -B_2 \u0026amp;\\color{#0076BA} B_1 \u0026amp; 0 \\end{pmatrix} $$\n$$ F^{\\mu \\nu} = \\begin{pmatrix} 0 \u0026amp; \\color{#D41876}{-E^1} \u0026amp; \\color{#D41876}-E^2 \u0026amp; \\color{#D41876}-E^3 \\\\ \\color{#D41876}E^1 \u0026amp; 0 \u0026amp;\\color{#0076BA} -B^3 \u0026amp; \\color{#0076BA}B^2 \\\\ \\color{#D41876}E^2 \u0026amp;\\color{#0076BA} B^3 \u0026amp; 0 \u0026amp;\\color{#0076BA} -B^1 \\\\ \\color{#D41876}E^3 \u0026amp;\\color{#0076BA} -B^2 \u0026amp; \\color{#0076BA}B^1 \u0026amp; 0 \\end{pmatrix} $$\nOn cherche à nouveau un invariant de Lorentz pour le Lagrangien du champ, ce qui nous amène logiquement au produit scalaire du tenseur champ :\n$$ F_{\\mu \\nu} F^{\\mu \\nu} = 2({\\color{#0076BA}\\boldsymbol{B}}^2 - {\\color{#D41876}\\boldsymbol{E}}^2) $$\nEt le Lagrangien peut s\u0026rsquo;écrire ainsi :\n$$ L = -\\frac{1}{4} \\int \\mathrm{d}^3 x \\, F_{\\mu \\nu} F^{\\mu \\nu} $$\nLe facteur $1/4$ se justifiera par la suite.\nEnfin, la conservation locale de la charge s\u0026rsquo;exprime par l\u0026rsquo;équation de continuité :\n$$ \\frac{\\partial \\rho}{\\partial t} + \\boldsymbol{\\nabla} \\cdot \\boldsymbol{J} = \\partial_{\\mu} J^{\\mu} = 0 $$\nChamps classiques Un champ classique est une bestiole qui se nourrit d\u0026rsquo;une position dans l\u0026rsquo;espace-temps et qui pond l\u0026rsquo;amplitude du champ en ce point. La sortie peut être un scalaire (ex : température), un nombre complexe, un vecteur (ex : champ magnétique), un tenseur (ex : $F_{\\mu\\nu}(x)$) ou tout objet plus complexe. On obtient alors respectivement un champ scalaire, un champ scalaire complexe, un champ vectoriel, un champ tensoriel, etc.\nLes champs sont définis localement.\nLes valeurs du champ vivent dans un espace supplémentaire \u0026ldquo;au-dessus\u0026rdquo; de l\u0026rsquo;espace-temps. Pour un champ scalaire, par exemple, l\u0026rsquo;amplitude pouvant prendre une valeur réelle en chaque point de l\u0026rsquo;espace-temps, c\u0026rsquo;est comme ci passait en se point un axe réel supplémentaire. Et pour un champ dont l\u0026rsquo;amplitude s\u0026rsquo;ébat dans un espace plus complexe, c\u0026rsquo;est comme ci on avait collé une copie de cet espace en tout point de l\u0026rsquo;espace-temps. L\u0026rsquo;espace total obtenu (espace de base + copies en tout point de l\u0026rsquo;espace des amplitudes) s\u0026rsquo;appelle un fibré.\nDensité lagrangienne et hamiltonienne Le but ici est de formuler des Lagrangiens et Hamiltoniens dans le langage des champs classiques.\nDans le cas d\u0026rsquo;un réseau linéaire discret de ressorts de constante de raideur $K$ et de masselottes de masses $m$ séparées d\u0026rsquo;une longueur $\\ell$, on avait écrit l\u0026rsquo;Hamiltonien suivant :\n$$ H = \\sum_j \\frac{p_j^2}{2m} + \\frac{1}{2} K(q_{j+1} - q_j)^2 $$\nEt le Lagrangien :\n$$ L = \\sum_j \\frac{p_j^2}{2m} - \\frac{1}{2} K(q_{j+1} - q_j)^2 $$\nOn passe à la limite continue :\n$$ \\begin{aligned} \\ell \u0026amp;\\rightarrow 0\\\\ q_j \u0026amp;\\rightarrow \\phi(x,t)\\\\ \\sum_j \u0026amp;\\rightarrow \\frac{1}{\\ell}\\int\\mathrm{d}x\\\\ \\frac{q_{j+1}-q_j}{\\ell} \u0026amp;\\rightarrow \\frac{\\partial \\phi(x,t)}{\\partial x} \\end{aligned} $$\nOn obtient :\n$$ H = \\int \\mathrm{d}^3x \\left[\\frac{1}{2} \\rho \\left( \\frac{\\partial \\phi}{\\partial t} \\right)^2 + \\frac{1}{2} \\mathcal{T} \\left(\\boldsymbol{\\nabla}\\phi \\right)^2 \\right] $$\n$$ L = \\int \\mathrm{d}^3x \\left[\\frac{1}{2} \\rho \\left( \\frac{\\partial \\phi}{\\partial t} \\right)^2 - \\frac{1}{2} \\mathcal{T} \\left( \\boldsymbol{\\nabla} \\phi \\right)^2 \\right] $$\nOn a introduit la masse linéique $\\rho=m/\\ell$ et la tension du ressort $\\mathcal{T}=K\\ell$.\nDéfinissons les densités hamiltonienne et lagrangienne comme :\n$$ H = \\int \\mathrm{d}^3x \\, \\mathcal{H} $$\n$$ L = \\int \\mathrm{d}^3x \\, \\mathcal{L} $$\nL\u0026rsquo;intérêt du passage à cette densité devient plus claire si on regarde ce que devient la formule de l\u0026rsquo;action : $S=\\int\\mathrm{d}^4x\\mathcal{L}$. Le temps et l\u0026rsquo;espace sont maintenant traités sur un pied d\u0026rsquo;égalité, ce qui sied beaucoup mieux à une théorie relativiste.\n$\\mathcal{H}$ et $\\mathcal{L}$ sont généralement fonction de $\\phi$, $\\dot{\\phi}$ et $\\phi\u0026rsquo;$.\nDéfinissons le moment conjugué $\\pi(x)$ à partir de la dérivée fonctionnelle :\n$$ \\pi(x) = \\frac{\\delta L}{\\delta \\dot{\\phi}} = \\frac{\\partial \\mathcal{L}}{\\partial \\dot{\\phi}} $$\nCela permet de relier $\\mathcal{H}$ et $\\mathcal{L}$ :\n$$ \\mathcal{H} = \\pi \\dot{\\phi} - \\mathcal{L} $$\nDans le cas de l\u0026rsquo;exemple masselottes-ressorts, on obtient :\n$$ \\mathcal{L}=\\frac{1}{2}\\rho\\left(\\frac{\\partial\\phi}{\\partial t}\\right)^2-\\frac{1}{2}\\mathcal{T}(\\boldsymbol{\\nabla}\\phi)^2 $$\n$$ \\mathcal{H}=\\frac{1}{2}\\rho\\left(\\frac{\\partial\\phi}{\\partial t}\\right)^2+\\frac{1}{2}\\mathcal{T}(\\boldsymbol{\\nabla}\\phi)^2 $$\n$$ \\pi=\\rho\\frac{\\partial\\phi}{\\partial t} $$\nLe principe de moindre action $\\delta S = 0$ sur $S=\\int\\mathrm{d}^4x\\mathcal{L}(\\phi,\\partial_\\mu\\phi)$ donne la version quadridimensionnelle des équations d\u0026rsquo;Euler-Lagrange :\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\phi} - \\partial_\\mu \\left( \\frac{\\partial \\mathcal{L}}{\\partial (\\partial_\\mu \\phi)} \\right) = 0 $$\nRegardons ce que cela donne pour le champ électromagnétique.\n$$ \\mathcal{L} = -\\frac{1}{4}F_{\\mu\\nu}F^{\\mu\\nu}=\\frac{1}{2} (\\boldsymbol{E}^2 - \\boldsymbol{B}^2) $$\nEn l\u0026rsquo;absence de potentiel électrique ($V=0$), on a $A^\\mu=(0,\\boldsymbol{A})$ et donc $E^i=F^{i0}=\\partial^iA^0-\\partial^0 A^i=-\\partial^0 A^i$.\nPar conséquent $\\mathcal{L} =\\frac{1}{2} (\\boldsymbol{E}^2 - \\boldsymbol{B}^2)=\\frac{1}{2}(\\dot{\\boldsymbol{A}}^2 - \\boldsymbol{B}^2)$.\nEt le moment conjugué est $\\pi^i=\\partial\\mathcal{L}/\\partial(\\partial_0A_i)$ (puisqu\u0026rsquo;ici $\\phi=A$) et donc $\\boldsymbol{\\pi}=-\\dot{\\boldsymbol{A}}=\\boldsymbol{E}$, ce qui donne :\n$$ \\mathcal{H} = \\pi^i\\dot{A}_i-\\mathcal{L} = \\frac{1}{2} \\left( \\boldsymbol{E}^2 + \\boldsymbol{B}^2 \\right) $$\nEt les équations d\u0026rsquo;Euler-Lagrange donnent :\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A_\\mu} - \\partial_\\lambda \\left( \\frac{\\partial \\mathcal{L}}{\\partial (\\partial_\\lambda A_\\mu)} \\right) = 0 $$\nLe premier terme est nul puisque $\\mathcal{L}=\\frac{1}{4}F_{\\mu\\nu}F^{\\mu\\nu}$ ne contient que des dérivées de $A_\\mu$. Et en réécrivant le second terme comme $\\partial_\\lambda F^{\\lambda\\mu}$, on obtient :\n$$ \\partial_\\lambda F^{\\lambda\\mu} = 0 $$\nÉcriture compacte des deux équations de Maxwell inhomogènes dans le vide ($\\boldsymbol{\\nabla}\\cdot\\boldsymbol{E}=0$ et $\\boldsymbol{\\nabla}\\times\\boldsymbol{B}=\\dot{\\boldsymbol{E}}$).\nEn couplant linéairement le quadrivecteur densité de courant $J^\\mu=(\\rho,\\boldsymbol{J})$ au champ électromagnétique, on obtient un nouveau Lagrangien :\n$$ \\mathcal{L} = -\\frac{1}{4} F_{\\mu\\nu}F^{\\mu\\nu} - J^\\mu A_\\mu $$\nOn a maintenant\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A_\\mu} = - J^\\mu $$\nEt donc\n$$ \\partial_\\lambda F^{\\lambda\\mu} = J^\\mu $$\nOn retrouve les deux équations de Maxwell inhomogènes avec charges ($\\boldsymbol{\\nabla}\\cdot\\boldsymbol{E}=\\rho$ et $\\boldsymbol{\\nabla}\\times\\boldsymbol{B}=\\boldsymbol{J}+\\dot{\\boldsymbol{E}}$).\nLe facteur $1/4$ dans le Lagrangien se justifie donc a posteriori par le fait qu\u0026rsquo;il nous ait donné les bonnes équations.\nDans la suite, lorsqu\u0026rsquo;on parlera de Lagrangien, il s\u0026rsquo;agira le plus souvent en réalité de la densité lagrangienne.\nLa résolution des équations d\u0026rsquo;Euler-Lagrange va produire tous les modes d\u0026rsquo;oscillations et donc les vecteurs d\u0026rsquo;onde autorisés, c\u0026rsquo;est-à-dire les valeurs particulières $k_n$ qui survivent à la dissipation. Par principe de superposition, l\u0026rsquo;onde la plus générale est faite de la somme pondérée des différents modes possibles :\n$$ \\phi(x, t) = \\sum_{\\boldsymbol{k}_n} a_{\\boldsymbol{k}_n} \\mathrm{e}^{-\\mathrm{i}(\\omega t - \\boldsymbol{k}_n \\cdot \\boldsymbol{x})} $$\nOr on sait maintenant interpréter ces modes normaux comme des oscillateurs harmoniques qui peuvent donc être quantifiés, aboutissant à des solutions sous forme de particules.\nMécanique quantique relativiste, première tentative Equation de Klein-Gordon Rappelons le raisonnement permettant d\u0026rsquo;aboutir à l\u0026rsquo;équation de Schrödinger (cadre : particule libre en mécanique quantique non-relativiste) :\non part de la relation de dispersion qui lie énergie et impulsion de la particule : $E=\\frac{\\boldsymbol{p}^2}{2m}$ ; on transforme $E$ et $\\boldsymbol{p}$ en opérateurs : $E\\rightarrow\\hat{E}$ et $\\boldsymbol{p}\\rightarrow \\hat{\\boldsymbol{p}}$ ; on substitue $\\hat{E}=\\mathrm{i}\\hbar\\frac{\\partial}{\\partial t}$ et $\\hat{\\boldsymbol{p}}=-\\mathrm{i}\\hbar\\boldsymbol{\\nabla}$ ; on obtient l\u0026rsquo;équation de Schrödinger : $$ \\mathrm{i} \\hbar \\frac{\\partial \\phi(x,t)}{\\partial t} = - \\frac{\\hbar^2}{2m} \\boldsymbol{\\nabla}^2 \\phi(\\boldsymbol{x},t) $$\n$\\phi(\\boldsymbol{x},t)$ est la fonction d\u0026rsquo;onde et les solutions de l\u0026rsquo;équation sont des ondes planes $\\phi(\\boldsymbol{x},t)=N\\mathrm{e}^{-\\mathrm{i}(\\omega t-\\boldsymbol{k}\\cdot\\boldsymbol{x})}$ où $N$ est une constante de normalisation (il s\u0026rsquo;agit ici d\u0026rsquo;une onde incidente). On peut réécrire la solution sous forme quadrivectorielle $\\phi(x)=\\mathrm{e}^{-\\mathrm{i} p\\cdot x}$.\nAppliquons les opérateurs impulsion et énergie sur la solution :\n$$ \\begin{aligned} \\hat{\\boldsymbol{p}}\\phi(\\boldsymbol{x},t)=\\hbar\\boldsymbol{k}\\phi(\\boldsymbol{x},t)\\\\ \\hat{E}\\phi(\\boldsymbol{x},t)=\\hbar\\omega\\phi(\\boldsymbol{x},t) \\end{aligned} $$\nUne onde incidente a donc une impulsion et une énergie positives.\nPour obtenir une équation d\u0026rsquo;onde relativiste, on va tenter le même cheminement.\nL\u0026rsquo;équation de dispersion d\u0026rsquo;une particule relativiste est donnée par :\n$$ E = \\left( \\boldsymbol{p}^2 c^2 + m^2 c^4 \\right)^{\\frac{1}{2}} $$\nEt avec les même substitutions ($E\\rightarrow\\hat{E}=\\mathrm{i}\\hbar\\frac{\\partial}{\\partial t}$ et $\\boldsymbol{p}\\rightarrow \\hat{\\boldsymbol{p}}=-\\mathrm{i}\\hbar\\boldsymbol{\\nabla}$), on obtient :\n$$ \\mathrm{i} \\hbar \\frac{\\partial \\phi}{\\partial t} = \\left( -\\hbar^2 c^2 \\boldsymbol{\\nabla}^2 + m^2 c^4 \\right)^{\\frac{1}{2}} \\phi $$\nDeux problèmes :\nl\u0026rsquo;équation n\u0026rsquo;a pas l\u0026rsquo;air covariante, que faire avec la racine carrée ? Comment prend-on la racine carrée d\u0026rsquo;un opérateur différentiel ? Esquivons les deux problème en partant du carré de la relation de dispersion. On obtient maintenant :\n$$ -\\hbar^2 \\frac{\\partial^2 \\phi}{\\partial t^2} = \\left( -\\hbar^2 c^2 \\boldsymbol{\\nabla}^2 + m^2 c^4 \\right) \\phi $$\nIl s\u0026rsquo;agit de l\u0026rsquo;équation de Klein-Gordon. Par soucis de clarté, on repart en unités naturelles ($\\hbar=c=1$) :\n$$ -\\frac{\\partial^2 \\phi(x, t)}{\\partial t^2} = \\left( -\\boldsymbol{\\nabla}^2 + m^2 \\right) \\phi(\\boldsymbol{x}, t) $$\nTout semble maintenant parfaitement covariant. En notant $\\partial^2=\\partial_\\mu\\partial^\\mu=\\frac{\\partial^2}{\\partial t^2}-\\boldsymbol{\\nabla}^2$, on peut réécrire joliment l\u0026rsquo;équation de Klein-Gordon :\n$$ (\\partial^2 + m^2)\\phi(x)=0 $$\nC\u0026rsquo;est Schrödinger qui découvrit le premier l\u0026rsquo;équation de Klein-Gordon\u0026hellip; mais il la vite rejetée car elle ne donnait pas la bonne structure fine pour l\u0026rsquo;atome d\u0026rsquo;Hydrogène. Il n\u0026rsquo;a finalement gardé que sa limite non-relativiste, l\u0026rsquo;équation de Schrödinger (on verra plus loin comment on passe de l\u0026rsquo;une à l\u0026rsquo;autre).\nPour résoudre l\u0026rsquo;équation, tentons la solution $\\phi(\\boldsymbol{x},t)=N\\mathrm{e}^{-\\mathrm{i}Et+\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}=N\\mathrm{e}^{-\\mathrm{i}p\\cdot x}$ (en unités naturelles, $\\boldsymbol{k}=\\boldsymbol{p}$ et $\\omega=E$ et comme on est plus intéressé par l\u0026rsquo;énergie et l\u0026rsquo;impulsion des particules, ce sont eux qu\u0026rsquo;on utilise).\nEn substituant dans l\u0026rsquo;équation, on retrouve la relation de dispersion $E^2=\\boldsymbol{p}^2+m^2$. $\\phi$ est donc bien une solution et tout semble parfait jusqu\u0026rsquo;au moment où on constate que pour obtenir l\u0026rsquo;énergie de la particule, il faut prendre la racine carrée de l\u0026rsquo;équation de dispersion\u0026hellip; Deux solutions coexistent : $E=\\pm(\\boldsymbol{p}^2+m^2)^{\\frac{1}{2}}$. Peut-on juste ignorer la solution négative en arguant qu\u0026rsquo;elle est non physique ?\nCourants de probabilité et densités Les énergies négatives, c\u0026rsquo;est déjà pas mal incommodant\u0026hellip; mais elles engendrent quelque chose de peut-être encore plus dur à avaler : des densités de probabilité négatives !\nLa densité de probabilité $\\rho$ et le courant de probabilité $\\boldsymbol{j}$ obéissent à l\u0026rsquo;équation de continuité :\n$$ \\frac{\\partial \\rho}{\\partial t} +\\boldsymbol{\\nabla} \\cdot \\boldsymbol{j} = 0 $$\nDémonstration\u0026nbsp;: On multiplie l\u0026rsquo;équation de Klein-Gordon pour $\\phi$ par $\\phi^*$ $$ \\begin{aligned} \\phi^*\\left(-\\hbar^2 \\frac{\\partial^2 \\phi}{\\partial t^2}\\right) \u0026amp;= \\phi^*\\left( -\\hbar^2 c^2 \\boldsymbol{\\nabla}^2 + m^2 c^4 \\right) \\phi\\\\ \u0026amp; = -\\hbar^2 c^2 \\phi^*\\boldsymbol{\\nabla}^2 \\phi+ m^2 c^4 |\\phi^2| \\end{aligned} $$\nPuis on multiplie l\u0026rsquo;équation de Klein-Gordon pour $\\phi^*$ par $\\phi$ :\n$$ -\\hbar^2 \\phi\\frac{\\partial^2 \\phi^*}{\\partial t^2} = -\\hbar^2 c^2 \\phi\\boldsymbol{\\nabla}^2 \\phi^*+ m^2 c^4 |\\phi^2| $$\nEn soustrayant membre à membre les deux équations, on obtient :\n$$ \\left( \\phi^*\\frac{\\partial^2 \\phi}{\\partial t^2}- \\phi\\frac{\\partial^2 \\phi^*}{\\partial t^2}\\right)=c^2\\left( \\phi^*\\boldsymbol{\\nabla}^2 \\phi- \\phi\\boldsymbol{\\nabla}^2 \\phi^*\\right) $$\nQu\u0026rsquo;on peut réécrire :\n$$ \\mathrm{i}\\hbar\\frac{\\partial}{\\partial t}\\left( \\phi^*\\frac{\\partial \\phi}{\\partial t}- \\phi\\frac{\\partial \\phi^*}{\\partial t}\\right)=-\\mathrm{i}\\hbar c^2\\boldsymbol{\\nabla}\\cdot\\left( \\phi^*\\boldsymbol{\\nabla} \\phi- \\phi\\boldsymbol{\\nabla}\\phi^*\\right) $$\nEn effet, $\\frac{\\partial \\phi^*}{\\partial t}\\frac{\\partial \\phi}{\\partial t}-\\frac{\\partial \\phi}{\\partial t}\\frac{\\partial \\phi^*}{\\partial t}=0$, et de même $\\boldsymbol{\\nabla} \\phi^*\\cdot\\boldsymbol{\\nabla} \\phi-\\boldsymbol{\\nabla} \\phi\\cdot\\boldsymbol{\\nabla} \\phi^*=0$.\nOn retrouve bien l\u0026rsquo;équation de continuité en posant :\n$$ \\rho=\\mathrm{i}\\hbar\\left(\\phi^* \\frac{\\partial \\phi}{\\partial t}-\\phi \\frac{\\partial \\phi^*}{\\partial t}\\right) $$\n$$ \\boldsymbol{J}=- \\mathrm{i}\\hbar c^2\\left(\\phi^* \\boldsymbol{\\nabla} \\phi-\\phi \\boldsymbol{\\nabla} \\phi^*\\right) $$\nqui devient en notation quadrivectorielle :\n$$ \\partial_\\mu j^\\mu = 0 $$\nEt le courant de probabilité quadrivectoriel s\u0026rsquo;écrit :\n$$ j^\\mu(x) = \\mathrm{i} \\left[ \\phi^*(x) \\partial^\\mu \\phi(x) - \\phi(x) \\partial^\\mu \\phi^*(x) \\right] $$\nEn substituant la solution $\\phi(x)=N\\mathrm{e}^{-\\mathrm{i}p\\cdot x}$, on obtient une composante temporelle de la probabilité de courant, la densité de probabilité, à l\u0026rsquo;allure inquiétante :\n$$ j^0 = \\rho = 2|N|^2 E $$\nComme $E$ peut être négative, $\\rho$ aussi, et il faudrait alors donner un sens à des probabilités négatives\u0026hellip;\nL\u0026rsquo;interprétation de Feynman des énergies négatives Feynman a proposé une interprétation audacieuse des états à énergie négative solutions de l\u0026rsquo;équation de Klein-Gordon : il s\u0026rsquo;agirait de particules remontant le temps, des particules antiparticules !\nConsidérons l\u0026rsquo;équation classique gouvernant le mouvement d\u0026rsquo;une particule chargée dans un champ électromagnétique :\n$$ m \\frac{\\mathrm{d}^2 x^\\mu}{\\mathrm{d}\\tau^2} = q F^{\\mu\\nu} \\frac{\\mathrm{d}x_\\nu}{\\mathrm{d}\\tau}​ $$\nChanger le signe du temps propre $\\tau$ dans l\u0026rsquo;équation a la même conséquence que changer le signe de la charge $q$. Donc une particule remontant le temps est équivalente a une particule de charge opposée avec un temps s\u0026rsquo;écoulant normalement.\nPour nos solutions de la forme $\\mathrm{e}^{-\\mathrm{i}(Et-\\boldsymbol{p}\\cdot\\boldsymbol{x})}$ (avec $E\u0026lt;0$), changer $t\\rightarrow -t$ doit être compensé par $E\\rightarrow -E$ pour rétablir le sens normal d\u0026rsquo;écoulement du temps. Mais le reversement du temps touche aussi l\u0026rsquo;impulsion $\\boldsymbol{p}\\rightarrow\\boldsymbol{-p}$. Cela donne au final $\\mathrm{e}^{-\\mathrm{i}(Et+\\boldsymbol{p}\\cdot\\boldsymbol{x})}$ avec $E\u0026gt;0$. La particule incidente d\u0026rsquo;énergie négative devient une particule émise d\u0026rsquo;énergie positive.\nOn sait maintenant gérer les énergies négatives : une particule d\u0026rsquo;énergie négative se transforme en antiparticule d\u0026rsquo;énergie positive en changeant le signe de la charge et de l\u0026rsquo;impulsion tridimensionnelle.\nUne solution générale de l\u0026rsquo;équation de Klein-Gordon pour une énergie positive particulière est donnée par la superposition de deux états :\n$$ \\phi(x) = \\left[ \\begin{array}{c} \\text{Particule reçue} \\\\ \\text{d\u0026rsquo;énergie positive} \\\\ \\propto \\mathrm{e}^{-\\mathrm{i}(Et - \\boldsymbol{p} \\cdot \\boldsymbol{x})} \\end{array} \\right] + \\left[ \\begin{array}{c} \\text{Antiparticule émise} \\\\ \\text{d\u0026rsquo;énergie positive} \\\\ \\propto \\mathrm{e}^{+\\mathrm{i}(Et - \\boldsymbol{p} \\cdot \\boldsymbol{x})} \\end{array} \\right] $$\nQuelques Lagrangiens de champs classiques Champ scalaire sans masse Un champ scalaire $\\phi(x)$ assigne une amplitude scalaire à toute position $x$ de l\u0026rsquo;espace-temps. Le Lagrangien ne dépend que du taux de variation temporel $\\partial_\\phi$ et spatial $\\boldsymbol{\\nabla}$ de $\\phi$. Et pour que $\\mathcal{L}$ respecte les canons relativistes, on choisit :\n$$ \\mathcal{L}=\\frac{1}{2} \\partial^\\mu \\phi \\partial_\\mu \\phi=\\frac{1}{2}\\left(\\partial_\\mu \\phi\\right)^2 $$\nIl se développe en $\\mathcal{L}=\\frac{1}{2}\\left(\\partial_0 \\phi\\right)^2-\\frac{1}{2} \\boldsymbol{\\nabla}^2 \\phi$ pour lui donner un air de $\\mathcal{L}=$ (énergie cinétique)- (énergie potentielle). Comme d\u0026rsquo;habitude, le facteur ($\\frac{1}{2}$, ici) est là pour faire coller les prédictions du Lagrangien à ce qu\u0026rsquo;on connait.\nComme $\\frac{\\partial \\mathcal{L}}{\\partial \\phi}=0$ et $\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_\\mu \\phi\\right)}=\\partial^\\mu \\phi$, l\u0026rsquo;équation d\u0026rsquo;Euler-Lagrange implique l\u0026rsquo;équation de mouvement suivante :\n$$ \\partial_\\mu\\partial^\\mu\\phi =0 $$\nCe n\u0026rsquo;est autre que l\u0026rsquo;équation des ondes (équation de d\u0026rsquo;Alembert) $\\partial^2=0$ ou $\\frac{\\partial^2 \\phi}{\\partial t^2}-\\boldsymbol{\\nabla}^2 \\phi=0$.\nEt les solutions ondulent sous la forme :\n$$ \\phi(x, t)=\\sum_{\\boldsymbol{p}} a_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i}\\left(E_{\\boldsymbol{p}} t-\\boldsymbol{p} \\cdot \\boldsymbol{x}\\right)} $$\navec une équation de dispersion donnée par :\n$$ E_{\\boldsymbol{p}}=c|\\boldsymbol{p}| $$\n(ou $E_{\\boldsymbol{p}}=|\\boldsymbol{p}|$ avec $c=1$).\nComme $E_{\\boldsymbol{p}}=0$ en $|\\boldsymbol{p}|=0$, on dit que la relation de dispersion est sans gap.\nCette équation décrira plus loin l\u0026rsquo;énergie des excitations quantiques du système et puisque c\u0026rsquo;est la version $m=0$ de la relation de dispersion relativiste $E_p=\\sqrt{p^2+m^2}$, on parle de particules non massives ainsi que d\u0026rsquo;un champ scalaire sans masse.\nL\u0026rsquo;équation d\u0026rsquo;onde étant linéaire, les ondes solutions obéissent au principe de superposition. Les particules correspondantes sont alors dites libres ou sans interaction. Si on les envoie les unes contres les autres, elles se traversent sans se voir.\nChamp scalaire avec masse Pour inclure une masse, on fait dépendre $\\mathcal{L}$ non plus seulement de $\\partial_\\mu\\phi$ mais aussi du champ $\\phi$ lui-même en introduisant un terme d\u0026rsquo;énergie potentielle $U(\\phi)\\propto\\phi^2$ qui va traduire le coût d\u0026rsquo;avoir un champ plutôt que du vide à cet endroit.\n$$ \\mathcal{L}=\\frac{1}{2}\\left(\\partial_\\mu \\phi\\right)^2-\\frac{1}{2} m^2 \\phi^2, $$\nMontrons à partir des équations d\u0026rsquo;Euler-Lagrange que le paramètre $m$ est bien une masse.\nComme on a $\\frac{\\partial \\mathcal{L}}{\\partial \\phi}=-m^2 \\phi$ et $\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_\\mu \\phi\\right)}=\\partial^\\mu \\phi$, on obtient :\n$$ \\left(\\partial_\\mu \\partial^\\mu+m^2\\right) \\phi=0 $$\nL\u0026rsquo;équation du mouvement de ce champ est donc l\u0026rsquo;équation de Klein-Gordon !\nSes solutions sont à nouveau :\n$$ \\phi(x, t)=\\sum_{\\boldsymbol{p}} a_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i}\\left(E_{\\boldsymbol{p}} t-\\boldsymbol{p} \\cdot \\boldsymbol{x}\\right)} $$\navec la relation de dispersion :\n$$ E^2_{\\boldsymbol{p}} = \\boldsymbol{p}^2 + m^2 $$\nAvoir $m\\neq 0$ crée un gap dans la relation de dispersion (à $\\boldsymbol{p}=0$, $E_{\\boldsymbol{p}}=\\pm m$) correspondant à la masse de la particule.\nÀ nouveau, les équations du mouvement sont linéaires et donc les particules ainsi décrites n\u0026rsquo;interagissent pas.\nSource externe On veut maintenant introduire des interactions. Le plus simple est de faire interagir un champ scalaire avec un potentiel externe. On décrit le potentiel par une fonction $J(x)$ (source externe) qui interagit avec le champ via le terme $-J(x)\\phi(x)$ dans le potentiel. Cela donne le Lagrangien :\n$$ \\mathcal{L}=\\frac{1}{2}\\left[\\partial_\\mu \\phi(x)\\right]^2-\\frac{1}{2} m^2[\\phi(x)]^2+J(x) \\phi(x) $$\nL\u0026rsquo;équation du mouvement devient :\n$$ \\left(\\partial_\\mu \\partial^\\mu+m^2\\right) \\phi(x)=J(x) . $$\nOn a ainsi maintenant une équation différentielle inhomogène.\nLa théorie $\\phi^4$ Comment faire interagir des particules les unes avec les autres (ou des champs avec des champs) ? La recette la plus simple consiste à ajouter un terme d\u0026rsquo;énergie potentielle $U(\\phi)$ proportionnel à $\\phi^4$ au Lagrangien scalaire. S\u0026rsquo;il crée bien des interactions, ce terme empêche dans le même temps de trouver des solutions aux équations autrement que par une théorie des perturbations\u0026hellip;\n$$ \\mathcal{L}=\\frac{1}{2} \\partial^\\mu \\phi \\partial_\\mu \\phi-\\frac{1}{2} m^2 \\phi^2-\\frac{1}{4!} \\lambda \\phi^4, $$\nEt cela donne une équation du mouvement pas très jolie :\n$$ \\left(\\partial^2+m^2\\right) \\phi=-\\frac{\\lambda}{3!} \\phi^3 . $$\nDeux champs scalaires Pour faire interagir des particules, on peut aussi décrire deux types de particules via deux champs différents $\\phi_1(x)$ et $\\phi_2(x)$ et les faire interagir grâce au potentiel $U(\\phi_1,\\phi_2)=g(\\phi_1^2+\\phi_2)^2$ où $g$ paramétrise la force de l\u0026rsquo;interaction.\nDévelopper le carré donne des termes d\u0026rsquo;interaction propre en $\\phi^4$ mais aussi un terme d\u0026rsquo;interaction mutuelle $2\\phi_1^2\\phi_2^2$.\nLa densité lagrangienne est donnée par :\n$$ \\mathcal{L}=\\frac{1}{2}\\left(\\partial_\\mu \\phi_1\\right)^2-\\frac{1}{2} m^2 \\phi_1^2+\\frac{1}{2}\\left(\\partial_\\mu \\phi_2\\right)^2-\\frac{1}{2} m^2 \\phi_2^2-g\\left(\\phi_1^2+\\phi_2^2\\right)^2 $$\nOn remarque que certaines transformations des champs gardent invariant le Lagrangien. On peut ainsi opérer une rotation des champs dans l\u0026rsquo;espace abstrait $\\phi_1$-$\\phi_2$. On a alors $\\phi_1 \\rightarrow \\phi_1^{\\prime}$ et $\\phi_2 \\rightarrow \\phi_2^{\\prime}$ avec :\n$$ \\binom{\\phi_1^{\\prime}}{\\phi_2^{\\prime}}=\\left(\\begin{array}{cc} \\cos \\theta \u0026amp; -\\sin \\theta \\\\ \\sin \\theta \u0026amp; \\cos \\theta \\end{array}\\right)\\binom{\\phi_1}{\\phi_2} $$\nOn dit que les particules décrites par ce Lagrangien ont un degré de liberté interne. L\u0026rsquo;invariance de la physique par rapport aux rotations d\u0026rsquo;un angle $\\theta$ dans l\u0026rsquo;espace $\\phi_1$-$\\phi_2$ exprime une symétrie $SO(2)$ de la théorie.\n$SO(2)$ est le groupe spécial orthogonal à 2 dimensions. Il correspond aux matrices $2\\times2$ orthogonales avec un déterminant égal à 1.\nC\u0026rsquo;est une symétrie continue et on va voir plus loin que les symétries continues conduisent à des quantités conservées.\nChamp scalaire complexe On peut simplifier le Lagrangien à deux champs scalaires précédent en passant à des champs scalaires complexes $\\psi$ et $\\psi^\\dagger$ définis par :\n$$ \\begin{aligned} \\psi \u0026amp; =\\frac{1}{\\sqrt{2}}\\left[\\phi_1+\\mathrm{i} \\phi_2\\right] \\\\ \\psi^{\\dagger} \u0026amp; =\\frac{1}{\\sqrt{2}}\\left[\\phi_1-\\mathrm{i} \\phi_2\\right] \\end{aligned} $$\nOn obtient :\n$$ \\mathcal{L}=\\partial^\\mu \\psi^{\\dagger} \\partial_\\mu \\psi-m^2 \\psi^{\\dagger} \\psi-g\\left(\\psi^{\\dagger} \\psi\\right)^2 $$\nLe nouveau champ scalaire complexe $\\psi$ contient, comme avant, deux degrés de liberté. Le nouveau Lagrangien se retrouve maintenant invariant par rapport aux rotations dans le plan complexe $\\psi \\rightarrow \\psi \\mathrm{e}^{\\mathrm{i} \\alpha}$ et $\\psi^{\\dagger} \\rightarrow \\mathrm{e}^{-\\mathrm{i} \\alpha} \\psi^{\\dagger}$ qui expriment une symétrie $U(1)$.\n$U(1)$ est le groupe des transformations unitaires à 1 dimension.\nL\u0026rsquo;équivalence entre les deux descritptions précédentes découlent de l\u0026rsquo;isomorphisme entre $SO(2)$ et $U(1)$ noté $SO(2)\\simeq U(1)$.\nThéorie $\\psi^\\dagger\\psi\\phi$ Terminons par une théorie décrivant 3 types de particules. On additionne les Lagrangiens de champ scalaire complexe (avec une masse $m$) et de champ scalaire réel (avec une masse $\\mu$) et on ajoute un terme d\u0026rsquo;interaction entre les deux $g\\psi^\\dagger\\psi\\phi$ :\n$$ \\mathcal{L}= \\partial^\\mu \\psi^{\\dagger} \\partial_\\mu \\psi-m^2 \\psi^{\\dagger} \\psi+\\frac{1}{2}\\left(\\partial_\\mu \\phi\\right)^2-\\frac{1}{2} \\mu^2 \\phi^2-g \\psi^{\\dagger} \\psi \\phi $$\nComme on le verra plus tard, cette théorie ressemble beaucoup à l\u0026rsquo;électrodynamique quantique.\nDans le chapitre suivant, on va passer de ces champs classiques à des champs quantiques et vérifier que les particules émergent bien comme leurs excitations.\nChapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/pca/",
	"title": "Analyse en composantes principales",
	"tags": [],
	"description": "",
	"content": " Analyse en composantes principales Les axes qui tiraillent territorialement la France Détaillons un exemple sur le modèle de celui des prénoms dans la vidéo.\nL\u0026rsquo;idée ici est de récupérer plein de séries de données départementales disparates sur le site de l\u0026rsquo;INSEE pour voir si les composantes principales font ou non ressurgir une géographie identifiable de ce brouillard de chiffres. On peut alors s\u0026rsquo;amuser à balbutier des interprétations pour ces axes de différentiation territoriaux et probablement irriter ainsi fortement tout sociologue un tant soit peu éduqué. C\u0026rsquo;est la magie même de l\u0026rsquo;ACP de permettre à un quidam de se prendre pour un sociologue\u0026hellip;\nCherchant à étudier la variabilité entre départements, à faire ressortir leurs spécificités, on choisit ici de n\u0026rsquo;avoir que des données sous forme de taux quitte à rendre relative certaines données brutes en divisant par la population du département. Certes, un vecteur associant les variables non relatives aurait probablement constitué la 1re composante principale puisque la démographie explique alors une large part de la variance, et comme chaque composante est orthogonale aux suivantes, cela aurait peut-être éliminé ce biais dès la 2e composante\u0026hellip; Mais autant s\u0026rsquo;en débarrasser dans les données pour éviter toute confusion.\nUn tableau de 96 lignes (les départements métropolitains) et 51 colonnes (code du département + nom du département + 49 variables statistiques) constitue les données de base. Un export en .csv (accès au fichier) et direction Pandas.\nimport pandas as pd file_path = \u0026#34;tableauINSEE.csv\u0026#34; data = pd.read_csv(file_path, delimiter=\u0026#39;;\u0026#39;) On nettoie un peu en changeant les virgules françaises en points américains comme séparateurs décimaux afin que Pandas daigne associer des nombres (des flottants) aux données.\ndepartements = data.columns[2:] data_cleaned = data[columns_to_use].replace(\u0026#39;,\u0026#39;, \u0026#39;.\u0026#39;, regex=True).apply(pd.to_numeric, errors=\u0026#39;coerce\u0026#39;) Puis on standardise les vecteurs : pour chaque série de 96 valeurs (les valeurs départementales de chaque catégorie statistique), on centre les données en soustrayant la moyenne et on les met à l’échelle en divisant par l’écart type, de sorte que les coordonnées des vecteurs aient tous une moyenne de 0 et un écart type de 1.\nfrom sklearn.preprocessing import StandardScaler scaler = StandardScaler() data_normalized = scaler.fit_transform(data_cleaned) data_normalized_df = pd.DataFrame(data_normalized, columns= departements) Voilà par exemple la carte de la variable \u0026ldquo;cheptel bovin\u0026rdquo;. Une carte = un vecteur à 96 dimensions.\nTraçons la matrice de corrélation de ces 49 variables pour faire connaissance.\nimport plotly.express as px # Calculer la matrice de corrélation correlation_matrix = data_normalized_df.corr() # Créer un heatmap de la matrice de corrélation fig = px.imshow( correlation_matrix, labels=dict(x=\u0026#34;Variables\u0026#34;, y=\u0026#34;Variables\u0026#34;, color=\u0026#34;Corrélation\u0026#34;), x=correlation_matrix.columns, y=correlation_matrix.columns, color_continuous_scale=\u0026#39;Temps\u0026#39; ) Enfin, on applique l\u0026rsquo;analyse en composantes principales sur les vecteurs normalisés.\nRq : les fonctions utilisées pour la normalisation et l\u0026rsquo;ACP viennent de différents modules de la bibliothèque Scikit-learn spécialisée dans le machine learning.\nfrom sklearn.decomposition import PCA pca = PCA() pca_components = pca.fit_transform(data_normalized) pca_df = pd.DataFrame(pca_components, columns=[f\u0026#39;Composante {i+1}\u0026#39; for i in range(pca_components.shape[1])]) Traçons le \u0026ldquo;scree plot\u0026rdquo; qui représente le pourcentage de variance expliquée en fonction des composantes.\nimport numpy as np import plotly.graph_objs as go # Récupérer la variance expliquée pour les 20 premières composantes principales explained_variance = pca.explained_variance_ratio_[:20] # Calculer la variance expliquée cumulée explained_variance_cumulative = np.cumsum(explained_variance) # Création du scree plot fig = go.Figure() # Ajouter les barres pour la variance expliquée individuelle fig.add_trace(go.Bar( x=[f\u0026#39;Composante {i+1}\u0026#39; for i in range(len(explained_variance))], y=explained_variance, name=\u0026#39;Variance expliquée individuelle\u0026#39;, marker=dict(color=\u0026#39;rgba(55, 128, 191, 0.7)\u0026#39;), )) # Ajouter la ligne pour la variance expliquée cumulée fig.add_trace(go.Scatter( x=[f\u0026#39;Composante {i+1}\u0026#39; for i in range(len(explained_variance))], y=explained_variance_cumulative, name=\u0026#39;Variance expliquée cumulée\u0026#39;, mode=\u0026#39;lines+markers\u0026#39;, line=dict(color=\u0026#39;rgb(255, 0, 0)\u0026#39;), )) # Mise en page du graphique fig.update_layout( title=\u0026#39;Scree Plot\u0026#39;, xaxis_title=\u0026#39;Nombre de composantes principales\u0026#39;, yaxis_title=\u0026#39;Pourcentage de variance expliquée\u0026#39;, showlegend=True, template=\u0026#39;plotly_white\u0026#39; ) # Afficher le graphique fig.show() On constate que la 1re composante explique presque 40% de la variance et qu\u0026rsquo;arrivé à la 6e, on dépasse 80% d\u0026rsquo;explication.\nAffichons les valeurs de la première composante sur une carte de France. Les départements scorant positivement fort sur cette composante apparaisse jaune et ceux scorant le plus négativement sont en bleu.\n# Ajouter les codes départements et leurs noms à la dataframe des composantes principales pca_df[\u0026#39;Code\u0026#39;] = data[\u0026#39;Code\u0026#39;] pca_df[\u0026#39;Département\u0026#39;] = data[\u0026#39;Département\u0026#39;] # Choisissez la composante à afficher composante = \u0026#39;Composante 1\u0026#39; # Créer la carte fig = px.choropleth_mapbox( pca_df, geojson=\u0026#34;https://france-geojson.gregoiredavid.fr/repo/departements.geojson\u0026#34;, # GeoJSON pour les départements français locations=\u0026#39;Code\u0026#39;, featureidkey=\u0026#34;properties.code\u0026#34;, color=composante, color_continuous_scale=\u0026#34;Viridis\u0026#34;, mapbox_style=\u0026#34;white-bg\u0026#34;, # Fond de carte blanc zoom=5, center={\u0026#34;lat\u0026#34;: 46.603354, \u0026#34;lon\u0026#34;: 1.888334}, # Centre de la France opacity=1, labels={composante: composante}, hover_name=\u0026#39;Département\u0026#39;, # Afficher le nom du département hover_data={\u0026#39;Code\u0026#39;: False, composante: True}, # Supprimer le code du département et afficher la composante ) fig.show() Cette composante semble opposer les départements dont la population vit pour une large part en ville aux départements plus ruraux.\nDévoilons la recette de cette 1re composante en détaillant les proportions dans lesquelles interviennent chaque ingrédient (les variables statistiques). Pour cela, on trace l\u0026rsquo;histogramme des différentes charges (ou loadings) triées. Les coefficients de charges correspondent aux poids respectifs de chaque variable dans le vecteur.\nnumero_composante = 1 composante = f\u0026#39;Composante {numero_composante}\u0026#39; # Récupérer les loadings pour la composante choisie loadings_composante = loadings[composante] # Trier les loadings par valeur décroissante loadings_composante_sorted = loadings_composante.sort_values(ascending=False) # Créer un DataFrame pour Plotly Express loadings_df = pd.DataFrame({ \u0026#39;Variable\u0026#39;: loadings_composante_sorted.index, \u0026#39;Loading\u0026#39;: loadings_composante_sorted.values }) # Créer un histogramme des loadings avec une coloration en fonction des valeurs fig = px.bar( loadings_df, x=\u0026#39;Variable\u0026#39;, y=\u0026#39;Loading\u0026#39;, color=\u0026#39;Loading\u0026#39;, # Utilisation des valeurs pour la coloration color_continuous_scale=\u0026#39;Viridis\u0026#39;, # Choix de l\u0026#39;échelle de couleurs template=\u0026#39;plotly_white\u0026#39;, title=f\u0026#39;Loadings triés pour la {composante}\u0026#39;, height=600, ) # Supprimer la colorbar si vous ne souhaitez pas l\u0026#39;afficher fig.update_coloraxes(showscale=False) # Afficher le graphique fig.show() Les marqueurs de l’urbanité sont bien opposés à ceux de la ruralité.\nLes différences de mode de vie entre ces environnements semblent donc responsables du plus gros de la distinction entre départements (du moins pour le lot de variables choisis qui sont malgré tout très disparates).\nOn peut renforcer cette interprétation en vérifiant comment une nouvelle variable, la \u0026ldquo;part de la population vivant dans une unité urbaine en 2017\u0026rdquo; score sur un plan formé par les deux premières composantes.\nLa variable \u0026ldquo;population urbaine\u0026rdquo; représentée par un segment rouge a comme prévu une direction proche de la composante 1 et lui est donc fortement corrélée (on a aussi placé les départements dans ce plan en les coloriant en fonction de leur valeur de la variable population urbaine).\nUne fois cette opposition ville-campagne retirée, que reste-t-il pour expliquer les 60% de variance résiduelle entre départements ?\nD\u0026rsquo;après la deuxième composante, l\u0026rsquo;axe sur lequel les valeurs des départements s\u0026rsquo;étalent le plus semble correspondre à une dimension socio-économique. Cela semble montrer que la répartition des richesses sur le territoire est loin d\u0026rsquo;être homogène à l\u0026rsquo;échelle des départements ; certains concentrent les marqueurs de richesse et d\u0026rsquo;autres accumulent les stigmates de la pauvreté.\nLa troisième composante matérialise une opposition géographique entre le Nord et le Sud (avec Paris dans le camp du Sud) grâce à un axe allant des ouvriers aux artisans/commerçants.\nLa quatrième composante distingue principalement positivement Paris avec de forts taux de fonctionnaires, de médecins et de mariages de même sexe, et des petits taux de voitures par ménage et d\u0026rsquo;employés. Le contraste est maximale avec la Corse, le nord des Alpes et la grande couronne.\nLa cinquième composante semble très politique avec un axe allant de Mélenchon-Lassalle à Zemmour-Le Pen. Le vote protestataire semble ainsi se polariser territorialement. L\u0026rsquo;urbanité de la Seine-Saint-Denis en soutien de Mélenchon allié au Sud-Ouest rural de Lassalle contre un vote d\u0026rsquo;extrême-droite plus à l\u0026rsquo;Est.\nParis score bizarrement très fort négativement bien que le vote pour l\u0026rsquo;extrême droite y soit plus rare qu\u0026rsquo;ailleurs.\nDétaillons les valeurs pour chaque catégorie statistique normalisée à Paris (on a conservé l\u0026rsquo;ordre des charges de la composante 5).\nVoilà ce que deviennent ces valeurs après multiplication par les charges de la composante 5.\nEt en sommant toutes ces valeurs, on obtient la projection de Paris et la composante 5. Le résultat d\u0026rsquo;environ -3,8 est bien celui attribuée à Paris dans la carte de la composante. On voit ainsi que les variables politiques ne sont pas celles qui participent au score de Paris.\nLa sixième composante oppose bizarrement le vote Pécresse aux mariages\u0026hellip; Et fait ressortir géographiquement le centre par rapport aux bords.\nEn comparant la carte du vote Pécresse à celle du taux de mariage, on observe bien une très légère complémentarité mais cela n\u0026rsquo;a rien de concluant et c\u0026rsquo;est confirmé par la corrélation de -0,063 entre les deux variables. On ne peut pas vraiment se contenter des extrémités de l\u0026rsquo;axe pour tenter une interprétation.\nL\u0026rsquo;attelage leader de cette composante est pour le moins curieux : les Hauts-de-Seine et la Creuse ! Mais les variables qui tirent leurs scores sont en bonne partie différentes. Un vote Pécresse en 2022 relativement plus fort qu\u0026rsquo;ailleurs semble d\u0026rsquo;ailleurs le seul vrai point commun.\nLe semblant d\u0026rsquo;homogénéité géographique avec un centre qui s\u0026rsquo;oppose aux bords laisse cependant supposer que quelque chose se trame dans cette composante et on peut hasarder quelques hypothèses sociologiques de bistrot. La composante semble agglomérer une France rurale âgée au tissu sociale distendue (plus de morts, moins de mariages) votant plus qu\u0026rsquo;ailleurs pour la droite historique. Et elle lui adjoint deux départements (Hauts-de-Seine et Yvelines) abritant une haute bourgeoisie plus dense qu\u0026rsquo;ailleurs ayant pour seul point commun notable de voter pour cette même droite.\nLa septième composante a pour pôles la proportion d\u0026rsquo;écoles privées d\u0026rsquo;un côté et les mariages de même sexe de l\u0026rsquo;autre. L\u0026rsquo;opposition est amusante mais le reste du vecteur ne semble pas vouloir se prêter à des interprétations faciles pas plus que la répartition géographique (au-delà de la zone bretonne au sens large ou le privé est surreprésenté).\nÀ la louche ou plutôt au godet d\u0026rsquo;une pelleteuse, on a donc les axes de séparation territoriaux suivant (à l\u0026rsquo;échelle des départements), par ordre de pertinence :\nVille contre campagne Riches contre pauvres Ouvriers contre artisans/commerçants Paris contre le reste Vote radical de gauche et protestataire rural contre vote d\u0026rsquo;extrême droite Droite historique contre le reste Religieux contre mariage pour tous "
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/geometrie/geo2/",
	"title": "Cercle et Trigo",
	"tags": [],
	"description": "",
	"content": " Cercle et triangle Trigonométrie La loi des sinus a contribué à la définition du mètre, la première unité \u0026ldquo;universelle\u0026rdquo; lors de la révolution française.\nDelambre et Méchain furent missionner pour mesurer le plus précisément possible un bout de méridien entre Dunkerque et Barcelone, la toute neuve définition du mètre étant :\nun mètre est la dix millionième partie d\u0026rsquo;un quart de méridien terrestre\nLe cercle répétiteur de Borda fut leur arme secrète. Il permettait alors de mesurer des angles avec une incertitude de quelques secondes seulement. Mesurer des distances avec cette précision est beaucoupl plus difficile ! L\u0026rsquo;idée fut alors de réaliser des triangulations : on vise des sommets faciles à repérer et on note les angles entre eux. Ils réalisèrent des dizaines de visées tout le long du bout de méridien étudié et le recouvrit de triangles.\nUne seule distance mesurée avec une grande précision permit alors de déterminer toutes les autres. En effet, grâce à la loi des sinus, la connaissance de la longueur d\u0026rsquo;un côté et de deux angles dans un triangle permet de déterminer les longueurs des deux autres côtés.\nThalès l\u0026rsquo;avait semble-t-il déjà réalisé puisqu\u0026rsquo;il aurait mis au point la méthode suivante pour connaître la distance entre un navire et la côte : deux observateurs $A$ et $C$ s\u0026rsquo;éloignent en direction opposée le long du trait de côte et mesurent les angles $\\alpha = \\widehat{CAB}$ et $\\gamma = \\widehat{ACB}$ entre la direction du bateau et la côte ainsi que la distance $AC$ qui les sépare l\u0026rsquo;un de l\u0026rsquo;autre. On a alors $d=AC\\times\\frac{\\sin\\left(\\gamma\\right)\\sin\\left(\\alpha\\right)}{\\sin\\left(\\pi-(\\alpha+\\gamma)\\right)}$ !\nPreuve :\n$$\\beta=180^\\circ - (\\alpha+\\gamma)$$ $$AB = AC\\times\\frac{\\sin\\gamma}{\\sin\\beta}$$ $$d=AB\\sin\\alpha$$\nThéorème de l\u0026rsquo;angle inscrit et de l\u0026rsquo;angle au centre Nom d\u0026rsquo;un théorème et système éducatif :\nL\u0026rsquo;éducation secondaire se démocratise au cours du xixe siècle. La géométrie, qui a souvent pour but d\u0026rsquo;exercer à la logique, en est un élément important. Les manuels se répandent, et leurs auteurs se mettent à introduire plus ou moins systématiquement des éléments historiques, en particulier en attribuant des noms de mathématicien aux théorèmes. L\u0026rsquo;époque est également au renouveau des recherches historiques sur les mathématiques de la Grèce antique. C\u0026rsquo;est dans ce contexte qu\u0026rsquo;à partir de la fin du xixe siècle le nom de Thalès est attribué à deux théorèmes distincts. En France puis en Italie c\u0026rsquo;est à un théorème du livre VI des Éléments d\u0026rsquo;Euclide, soit celui sur la proportionnalité des segments découpés par une ligne parallèle à un côté dans un triangle, soit sa généralisation pour deux sécantes. En Allemagne c\u0026rsquo;est au théorème de l\u0026rsquo;angle inscrit dans un demi-cercle. Ces dénominations sont bien établies dans les années 1920, époque à laquelle d\u0026rsquo;autres pays adoptent l\u0026rsquo;une ou l\u0026rsquo;autre (mais pas l\u0026rsquo;Angleterre ni les États-Unis).\nCes choix correspondent à des traditions différentes d\u0026rsquo;enseignement de la géométrie, plutôt fidèle à Euclide et à l\u0026rsquo;ordre d\u0026rsquo;exposition des Éléments en Allemagne, alors qu\u0026rsquo;une tradition plus anti-euclidienne s\u0026rsquo;est développée en France depuis La Ramée. Le choix s\u0026rsquo;est porté en France et en Italie, sur un théorème qui met en évidence un invariant de la géométrie affine, le rapport de mesures algébriques sur une droite, ce qui correspond au développement en Europe de la géométrie projective et affine au xixe siècle. Ce théorème est mis en avant dans les débuts de l\u0026rsquo;apprentissage de la géométrie, alors que l\u0026rsquo;Allemagne privilégie le théorème de Pythagore, et choisit de donner le nom de Thalès à un théorème lié au triangle rectangle.\nDans un cas comme dans l\u0026rsquo;autre, le choix du nom n\u0026rsquo;a finalement pas obéi a des considérations strictement historiques. L\u0026rsquo;histoire est instrumentalisée au service d\u0026rsquo;un choix didactique : il s\u0026rsquo;agit de mettre un théorème en avant, en lui attribuant le nom d\u0026rsquo;un mathématicien célèbre, d\u0026rsquo;où des choix différents dans des traditions d\u0026rsquo;enseignement différentes.\nSource : Wikipédia "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/meca/coriolis/",
	"title": "Coriolis",
	"tags": [],
	"description": "",
	"content": " Coriolis Simulations de Troyens dans un système où la grosse masse $M$ et la petite masse $m$ sont dans les proportions $m=M\\frac{\\mu}{1-\\mu} $ avec $\\mu=1/200$\nExplication des bandes de vents dominants (thèse + planètes).\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/quantique/epistovsonto/",
	"title": "Épistémique vs. ontique",
	"tags": [],
	"description": "",
	"content": " Épistémique vs. ontiques En mécanique quantique, l’évolution d’un système isolé est unitaire, c’est‑à‑dire décrite par un opérateur qui conserve la norme (ou la probabilité totale) du vecteur d’état ; cette dynamique, entièrement déterministe, est gouvernée par l’équation de Schrödinger. Dès qu’une mesure intervient, toutefois, la règle de Born (postulat de projection) impose une seconde loi : l’état s’effondre de façon non unitaire et aléatoire pour livrer l’un des résultats possibles, puis se fige afin que le même résultat soit retrouvé si l’on répète immédiatement la mesure. Malgré ce clivage, ce cadre théorique a révolutionné notre compréhension du monde et propulsé des avancées technologiques spectaculaires1.\nRéconcilier ces deux modes d’évolution — l’un continu, réversible et probabilistiquement conservatif, l’autre discontinu, irréversible et intrinsèquement stochastique — reste l’un des défis conceptuels majeurs de la physique moderne. C\u0026rsquo;est le problème de la mesure. D\u0026rsquo;ailleurs, la simple question \u0026ldquo;C\u0026rsquo;est quoi mesurer un système ?\u0026rdquo; n\u0026rsquo;a pas à ce jour de réponse claire, loin de là.\nDistinction de Harrigan et Spekkens Une partie de la communauté scientifique a adopté une attitude d\u0026rsquo;esquive en décrétant que l\u0026rsquo;appareil théorique n\u0026rsquo;est qu\u0026rsquo;une machinerie performante pour prédire les résultats des expériences, rien de plus. Le reste n\u0026rsquo;est que balivernes philosophiques. Cette approche instrumentiste n\u0026rsquo;attribue dès lors aucune réalité à l\u0026rsquo;état quantique.\nLes instrumentistes font partie du camp ψ-épistémique pour lesquels l\u0026rsquo;état quantique ψ n\u0026rsquo;est que porteur d\u0026rsquo;information sans réalité physique. Ils se sont très tôt réunis dans l\u0026rsquo;\u0026ldquo;école de Copenhague\u0026rdquo; qui a longtemps servi de canon à l\u0026rsquo;enseignement de la quantique. Leur mantra un peu caricatural est le célèbre \u0026ldquo;shut up and calculate\u0026rdquo;.\nDe l\u0026rsquo;autre côté du spectre, on a les interprétations ψ-ontiques pour lesquelles l\u0026rsquo;état quantique traduit une réalité physique.\nIl y a malgré tout des ψ‑épistémiques moins hardcores qui imaginent une réalité cachée derrière l\u0026rsquo;état quantique. L\u0026rsquo;état quantique représente alors notre connaissance (incomplète) de cette réalité de la même façon qu\u0026rsquo;une distribution de probabilité dans l\u0026rsquo;espace des phases représente notre connaissance de la position ponctuelle d\u0026rsquo;une particule. Le modèle jouet de Spekkens illustre ça très bien.\nSupposons qu\u0026rsquo;une théorie ou modèle associe au système une propriété physique $\\lambda$ (la réalité) qui détermine la probabilité des différents résultats obtenus lors de la mesure du système.\nLa théorie quantique va, elle, associer un état quantique $|\\psi\\rangle$ à un système préparé d\u0026rsquo;une certaine façon. Mais l\u0026rsquo;état physique $\\lambda$ n\u0026rsquo;est pas nécessairement fixé de manière unique par la préparation qui a produit $|\\psi\\rangle$. On dira que la préparation a produit un état physique $\\lambda$ selon une distribution de probabilité $\\mu_\\psi(\\lambda)$.\nSupposons maintenant que pour toute paire d\u0026rsquo;états quantiques $|\\psi_1\\rangle$ et $|\\psi_2\\rangle$, les distributions $\\mu_1(\\lambda)$ et $\\mu_2(\\lambda)$ ne se recouvrent jamais. L\u0026rsquo;état quantique $|\\psi\\rangle$ peut alors être inféré de manière unique à partir de l\u0026rsquo;état physique du système. Ça définit bien au final une \u0026ldquo;propriété physique\u0026rdquo; pour l\u0026rsquo;état quantique (puisqu\u0026rsquo;il y a une correspondance un pour un entre état quantique et propriété physique $\\lambda\\leftrightarrow\\psi$). C\u0026rsquo;est la vision ψ-ontique.\nÀ l\u0026rsquo;inverse, s\u0026rsquo;il y a recouvrement pour au moins une paire d\u0026rsquo;états quantiques, alors $|\\psi\\rangle$ peut être vu que comme un simple porteur d\u0026rsquo;information sans réalité. Comment l\u0026rsquo;état quantique pourrait être l\u0026rsquo;incarnation d\u0026rsquo;une réalité si la même réalité physique peut se cacher derrière différents états quantiques ? C\u0026rsquo;est bien la possibilité même du chevauchement qui rend un ψ‑épistémique ψ‑épistémique (où serait l\u0026rsquo;incertitude sans cela ?).\nCette distinction entre état épistémique et état ontique reposant sur le non‑recouvrement a été développée par Harrigan et Spekkens dans un article de 2010 :\nHarrigan, N. \u0026amp; Spekkens, R. W. “Einstein, Incompleteness, and the Epistemic View of Quantum States”, Foundations of Physics 40 (2), 125–157 (février 2010) 🌐\nAttraits de la vision ψ-épistémique Tous les ψ‑épistémiques s\u0026rsquo;accordent sur le statut probabiliste de la fonction d\u0026rsquo;onde. Comme dans le modèle jouet de Spekkens, ce point de vue permet de démystifier certaines bizarreries quantiques et c\u0026rsquo;est une approche de ce type que j\u0026rsquo;utilise dans la vidéo ci-dessous pour tenter de rendre plus accessible l\u0026rsquo;état quantique.\nUn argument fort pour cette vision probabiliste de la fonction d\u0026rsquo;onde est justement le fait que la théorie quantique peut être considérée comme une généralisation non commutative de la théorie classique des probabilités comme l\u0026rsquo;a montré von Neumann.\nMais un argument à mes yeux bien plus convaincant pour la ψ-épistémologie est la dissolution qu\u0026rsquo;elle entraîne du problème de la mesure. Rappelons-nous que le problème nait de la double règle d\u0026rsquo;évolution du formalisme quantique : l\u0026rsquo;une douce et continue lorsque le système est isolé et non observé et l\u0026rsquo;autre instantanée et discontinue lors d\u0026rsquo;une mesure. Comme une mesure n\u0026rsquo;est théoriquement qu\u0026rsquo;une interaction physique, on peut se demander pourquoi elle ne pourrait pas être modélisée par l\u0026rsquo;équation de Schrödinger. Mais c\u0026rsquo;est en suivant cette voie qu\u0026rsquo;on se retrouve avec des situations apparemment absurdes comme celle du chat mort et vivant.\nLe problème ne se pose en fait que pour des états ψ-ontiques ; seulement alors se retrouve-t-on avec un chat réellement dans deux états superposés. Pour un ψ‑épistémique, la fonction d\u0026rsquo;onde n\u0026rsquo;a jamais représenté que l\u0026rsquo;étendue de nos connaissances, le chat peut très bien être mort ou vivant avant qu\u0026rsquo;on regarde. La description par une superposition ne reflète que notre ignorance de la possibilité qui s\u0026rsquo;est réalisée. L\u0026rsquo;étendue de la fonction d\u0026rsquo;onde dans l\u0026rsquo;espace des phases n\u0026rsquo;est pour un ψ‑épistémique qu\u0026rsquo;un brouillard d\u0026rsquo;incertitude.\nThéorème PBR et nécessité de l\u0026rsquo;ontologie Les ψ-ontiques associent, eux, une réalité à la fonction d\u0026rsquo;onde. On peut les classer en deux catégories (en appelant $\\lambda$ la propriété physique associée au système, cf. plus haut) :\nles purs et durs pour lesquels la fonction d\u0026rsquo;onde est la réalité fondamentale ($\\lambda\\equiv\\psi$),\net ceux qui supplémentent $\\psi$ avec des variables additionnelles $\\xi$ ($\\lambda=(\\psi,\\xi)$).\nA priori, les ψ-ontistes n\u0026rsquo;ont pas grand-chose pour eux ; le problème de la mesure redevient un problème sérieux, la fonction d\u0026rsquo;onde vit dans un espace à grande dimension, les inégalités de Bell imposent une non localité des variables cachées et le théorème de Kochen‑Specker impose leur contextualité. Pour les ψ-ontistes purs et durs, c\u0026rsquo;est moins compliqué à avaler puisqu\u0026rsquo;ils ont signé en connaissance de cause (la quantique est non locale et contextuelle), mais pour ceux qui espéraient cacher des propriétés plus sympathiques dans $\\xi$, c\u0026rsquo;est râpé.\nLes ψ‑ontiques devaient passer pour de sacrés masos aux yeux des ψ‑épistémiques jusqu\u0026rsquo;au jour où le théorème PBR leur est tombé dessus.\nPusey, M. F. ; Barrett, J. ; Rudolph, T. (2012). \u0026ldquo;On the reality of the quantum state\u0026rdquo;. Nature Physics. 8 (6): 475–478 🌐\nRevenons à la distinction entre ψ‑ontiques et ψ‑épistémiques résumée par ce schéma :\nLes trois physiciens Pusey, Barrett et Rudolph ont réussi à montrer que si les distributions de probabilité d\u0026rsquo;une propriété physique (ontologiques) se recouvrent ($\\mu_1(\\lambda)\\cdot\\mu_2(\\lambda)=q\u0026gt;0$) pour deux états quantiques distincts $|\\psi_1\\rangle$ et $|\\psi_2\\rangle$ (plus précisément si la mesure de l\u0026rsquo;intersection $\\Delta$ des supports n\u0026rsquo;est pas nulle), alors il y a une contradiction avec les prédictions de la théorie quantique.\nPBR est donc un no-go theorem. Pour des hypothèses somme-toute assez raisonnables, il prouve qu\u0026rsquo;une interprétation épistémique de la fonction d\u0026rsquo;onde n\u0026rsquo;est pas compatible avec des propriétés physiques réelles2.\nIl aurait suffit pour prouver que $|\\psi\\rangle$ est épistémique de trouver une seule paire d\u0026rsquo;états dont les densités de probabilité se recouvrent alors que pour prouver qu\u0026rsquo;il est ontique, il faut s\u0026rsquo;assurer que c\u0026rsquo;est le cas d\u0026rsquo;aucune paire\u0026hellip; Mais c\u0026rsquo;est bien ce que PBR a réussi à faire !\nL\u0026rsquo;absence de recouvrement quand les deux états quantiques sont orthogonaux n\u0026rsquo;est pas vraiment surprenante. Par contre, le fait que cela tienne pour des états non orthogonaux est plus troublant ($\\mu_1(\\lambda) \\mu_2(\\lambda)=0$ pour tout $\\lambda$ même si $\\left\\langle\\psi_1 \\mid \\psi_2\\right\\rangle \\neq 0$).\nPourquoi c\u0026rsquo;est étonnant ?\nSi on a $\\left\\langle\\psi_1 \\mid \\psi_2\\right\\rangle \\neq 0$, on obtient en introduisant la résolution de l\u0026rsquo;identité pour une base quelconque de l\u0026rsquo;espace ($1=\\int \\mathrm{d} a|a\\rangle\\langle a|$) $\\int \\mathrm{d} a\\left\\langle\\psi_1 \\mid a\\right\\rangle\\left\\langle a \\mid \\psi_2\\right\\rangle \\neq 0 $. Donc pour au moins un $a$, $\\left\\langle\\psi_1 \\mid a\\right\\rangle\\left\\langle a \\mid \\psi_2\\right\\rangle \\neq 0$. Et comme $\\rho_i(a)=\\left|\\left\\langle a \\mid \\psi_i\\right\\rangle\\right|^2$, ça donne finalement $\\rho_1(a) \\rho_2(a) \\neq 0$ pour au moins un $a$.\nMoralité : les distributions de probabilités quantiques se recouvrent sans que cela ne soit le cas des distributions de $\\lambda$ !\n►\u0026nbsp;Idée de la preuve de PBR\u0026nbsp;: Brossons la preuve sur l\u0026rsquo;exemple d\u0026rsquo;une paire d\u0026rsquo;états non orthogonaux (PBR le généralise à une paire arbitraire).\nSupposons un espace de Hilbert à 2 dimensions avec une base orthogonale $|0\\rangle$, $|1\\rangle$.\nEt soit une autre base orthogonale $|+\\rangle$, $|-\\rangle$, avec $|\\pm\\rangle=\\frac{|0\\rangle \\pm |1\\rangle}{\\sqrt{2}}$.\nPrenons la paire non orthogonale $|0\\rangle$, $|+\\rangle$ ($\\langle 0 \\mid+\\rangle=1 / \\sqrt{2}$).\nLe but est de démontrer que $\\mu_0(\\lambda) \\mu_{+}(\\lambda)=0$ pour tout $\\lambda$. On va le prouver par l\u0026rsquo;absurde en supposant qu\u0026rsquo;il y a une probabilité $q\u0026gt;0$ telle que les deux états quantiques résultent d\u0026rsquo;un $\\lambda$ dans la région $\\Delta$ de recouvrement.\nConsidérons maintenant deux systèmes dont les états physique ne sont pas corrélés. On peut les obtenir par exemple en opérant deux copies d\u0026rsquo;un dispositif de préparation indépendamment. Chaque système peut être préparé tel que son état quantique soit $|0\\rangle$ ou $|+\\rangle$.\nLes états physiques $\\lambda_1$ et $\\lambda_2$ ont alors la probabilité $q^2\u0026gt;0$ d\u0026rsquo;être tous les deux dans la région de chevauchement $\\Delta$.\nCela signifie qu\u0026rsquo;il y a une probabilité non nulle que l\u0026rsquo;état physique des deux systèmes soit compatible avec n\u0026rsquo;importe lequel des quatre états quantiques possibles $|0\\rangle \\otimes|0\\rangle$, $|0\\rangle \\otimes|+\\rangle$, $|+\\rangle \\otimes|0\\rangle$ et $|+\\rangle \\otimes|+\\rangle$.\nLes deux systèmes sont ramenés ensemble et mesurés dans une base orthogonale particulière : $\\left|\\phi_1\\right\\rangle=\\frac{1}{\\sqrt{2}}[|0\\rangle|1\\rangle+|1\\rangle|0\\rangle]$, $\\left|\\phi_2\\right\\rangle=\\frac{1}{\\sqrt{2}}[|0\\rangle|-\\rangle+|1\\rangle|+\\rangle]$, $\\left|\\phi_3\\right\\rangle=\\frac{1}{\\sqrt{2}}[|+\\rangle|1\\rangle+|-\\rangle|0\\rangle]$ et $\\left|\\phi_4\\right\\rangle=\\frac{1}{\\sqrt{2}}[|+\\rangle|-\\rangle+|-\\rangle|+\\rangle]$.\nCette base a été choisie pour avoir $\\left\\langle\\phi_1 \\mid 00\\right\\rangle=0$, $\\left\\langle\\phi_2 \\mid 0+\\right\\rangle=0$, $\\left\\langle\\phi_3 \\mid+0\\right\\rangle=0$ et $\\left\\langle\\phi_4 \\mid++\\right\\rangle=0$.\nQuel que soit le résultat de la mesure ($|\\phi_1\\rangle$, $|\\phi_2 \\rangle$, $|\\phi_3 \\rangle$ ou $|\\phi_4 \\rangle$), il élimine forcément l\u0026rsquo;une des possibilités ($|00\\rangle$, $|0+\\rangle$, $|+0\\rangle$ ou $|++\\rangle$). Cela entraîne que la probabilité de recouvrement des quatre états physiques est nulle. Contradiction !\nIl y a $q^2$ chance que l\u0026rsquo;appareil de mesure ne sache pas laquelle des quatre méthode de préparation a été utilisée, et lorsque ça arrive, il prend le risque de produire un résultat qui n\u0026rsquo;est sensé jamais arriver.\nLes hypothèses du théorème sont :\nil existe une réalité fondamentale $\\lambda$, des $\\lambda$ préparés séparément sont statistiquement indépendants, les prédictions statistiques de la mécanique quantique sont correctes, la réalité de l\u0026rsquo;état quantique (ψ-ontique) est définie comme le non recouvrement des distributions de probabilité de $\\lambda$. Selon ces hypothèses, $|\\psi\\rangle$ est réel ! Quelle que soit la propriété physique fondamentale $\\lambda$, pour un $\\lambda$ donné, $|\\psi\\rangle$ peut être déterminé de manière unique.\nPour certains, comme Antony Valentini, il s\u0026rsquo;agit du résultat le plus important concernant les fondations de la mécanique quantique depuis les inégalités de Bell.\nnote\nParmi les puristes ($\\lambda\\equiv|\\psi\\rangle$), des extra puristes ont décidé d\u0026rsquo;avaler la pilule des dimensions élevées pour regagner la localité et la séparabilité des états. En admettant que notre espace à trois dimensions n\u0026rsquo;est que l\u0026rsquo;ombre d\u0026rsquo;un espace multidimensionnelle dans lequel évolue la fonction d\u0026rsquo;onde, on évacue en effet toutes les galères (extra dimensions mises à part), du moins jusqu\u0026rsquo;à la mesure\u0026hellip;\nCette petite page explique l\u0026rsquo;attrait de ce compromis.\nvoir ici pour une description succincte des fondations de la mécanique quantique dans son interprétation \u0026ldquo;classique\u0026rdquo;).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nEt c\u0026rsquo;est maintenant au tour des ψ-épistémiques de se lancer dans des circonvolutions à base de recherche de loopholes pour échapper aux conclusions du théorème !\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/info/ia/",
	"title": "IA",
	"tags": [],
	"description": "",
	"content": " Intelligence Artificielle Avant de parler d\u0026rsquo;intelligence artificielle, il convient de s\u0026rsquo;interroger sur le terme intelligence.\nL\u0026rsquo;intelligence arificielle (IA) est une discipline scientifique qui a vu officiellement le jour en 1956. Elle repose sur la conjecture selon laquelle toutes les fonctions cognitives, en particulier l\u0026rsquo;apprentissage, le raisonnement, le calcul, la perception, la mémorisation, voire la découverte scientifique ou la créativité artistique, peuvent être décrites avec une précision telle qu\u0026rsquo;il serait possible de les reproduire sur des ordinateurs.\nHistoire de l\u0026rsquo;IA La première programmeuse : Ada Lovelace De Turing au Big Data et Deep Learning Apprentissage Machine : apprendre à prédire L\u0026rsquo;apprentissage automatique (Machine Learning) est à l\u0026rsquo;intersection de l\u0026rsquo;IA et d\u0026rsquo;un autre champ scientifique : la science des données (data science).\nArthur Samuel définit l\u0026rsquo;apprentissage automatique ainsi en 1959 :\nLa capacité à apprendre sans avoir été spécifiquement programmé pour.\nEn pratique, il s\u0026rsquo;agit de produire des réponses adaptées aux données fournies en entrée (identifier des motifs, des tendances, construire des modèles, faire des prédictions). L\u0026rsquo;apprentissage automatique n\u0026rsquo;est donc ni plus ni moins que du traitement de données visant à prédire des résultats en fonction des données entrantes.\nLe diagramme suivant1 survole le domaine.\nOn va s\u0026rsquo;intéresser à l\u0026rsquo;apprentissage automatique dit \u0026ldquo;classique\u0026rdquo;2 qui regroupe des algorithmes très simples nés dans les années 50 et toujours utilisés aujourd\u0026rsquo;hui à peu près partout. Cette branche de l\u0026rsquo;apprentissage automatique se décompose prinicipalement en deux familles d\u0026rsquo;algorithmes : l\u0026rsquo;apprentissage supervisé et son pendant, l\u0026rsquo;apprentissage non supervisé. Nous allons étudier un algorithme star de chacune de ces familles. Algorithme des k plus proches voisins \u0026ndash; Exemple d\u0026rsquo;apprentissage supervisé L\u0026rsquo;algorithme des k plus proches voisin (k-nearest neighbors ou KNN) est une des techniques les plus simples en apprentissage automatique. Sa facilité d\u0026rsquo;utilisation et sa rapidité en font un outil de choix dans l\u0026rsquo;industrie.\nKNN est un algorithme d\u0026rsquo;apprentissage supervisé ; cela signifie que l\u0026rsquo;algorithme nécessite des données classifiées en amont qui vont lui servir à trouver la bonne étiquette pour d\u0026rsquo;autres données non encore classifiées.\nSuivant la nature de l\u0026rsquo;étiquette, KNN peut servir à :\nune classification des nouvelles données si les étiquettes sont des catagories ; une régression si les étiquettes sont des nombres. Principe KNN enregistre, dans un premier temps, tous les points de données étiquetées qui vont lui servir à l\u0026rsquo;apprentissage (c\u0026rsquo;est le training set). Puis, quand arrive un point de donnée non étiqueté, l\u0026rsquo;algorithme calcule sa distance aux autres points et sélectionne les k plus proches. On a alors deux cas possibles :\nsi les étiquettes sont des catégories, l\u0026rsquo;algorithme calcule le mode des catégories des voisins sélectionnés (catégorie la plus représentée). si les étiquettes sont des nombres, l\u0026rsquo;algorithme calcule la moyenne des étiquettes des voisins sélectionnés. L\u0026rsquo;algorithme des k plus proches voisins est non paramétrique dans le sens où aucun modèle mathématique de classification ou régression n\u0026rsquo;est construit à partir des données (pas de paramètre à ajuster) puisque toutes les données d\u0026rsquo;apprentissage sont enregistrées telles quelles.\nCela signifie qu\u0026rsquo;on ne présuppose rien de particulier sur les données (à part que des points proches appartiennent à la même catégorie). L\u0026rsquo;algorithme est donc particulièrement robuste (les données parlent d\u0026rsquo;elles-même) et simple à mettre à jour (suffit d\u0026rsquo;ajouter les nouvelles données d\u0026rsquo;apprentissage).\nChoix de k Comme le montre la petite animation ci-dessus, le choix de k modifie le résultat obtenu.\nSi k est trop petit, le moyennage est faible et donc la variabilité va être très grande. On parle alors de surapprentissage (overfitting). En augmentant k, les résultats obtenus se stabilisent (vote de la majorité) et les erreurs diminuent, jusqu\u0026rsquo;au moment où la boule à l\u0026rsquo;intérieur de laquelle se fait le moyennage devient trop grosse, amenant in fine l\u0026rsquo;algorithme a choisir systématiquement la catégorie majoritaire, quel que soit le point\u0026hellip; On augmente alors le biais (ici, le biais est le préjudice en faveur du plus grand nombre). L\u0026rsquo;ajustement ne suit plus les variations, on parle de sous-apprentissage (underfitting). Pour résumer :\nvariance biais Cas d\u0026rsquo;une régression Cas d\u0026rsquo;une classification k trop petit $\\rightarrow$ overfitting forte faible k trop grand $\\rightarrow$ underfitting faible fort tip\nLorsqu\u0026rsquo;on ne connaît rien sur les données, on peut toujours commencer par prendre la racine carrée du nombre de points dans l\u0026rsquo;ensemble d\u0026rsquo;entraînement comme k de départ.\nLe choix de k est donc affaire de compromis. Pour le rendre plus scientifique, on peut chercher à mesurer la performance de l\u0026rsquo;algorithme pour différentes valeurs de k.\nMais comment mesure-t-on la performance d\u0026rsquo;un algorithme d\u0026rsquo;apprentissage automatique ?\nValidation \u0026ndash; Matrice de confusion La matrice de confusion permet d\u0026rsquo;évaluer la qualité des prédictions d\u0026rsquo;un algorithme.\nPrenons l\u0026rsquo;exemple de l\u0026rsquo;utilisation de KNN sur une banque d\u0026rsquo;images de chiffres écrits à la main et plus spécifiquement concentrons-nous sur sa capacité à reconnaître des \u0026ldquo;3\u0026rdquo;.\nOn découpe l\u0026rsquo;espace en 4 cadrans. Sur une dimension, on regroupe d\u0026rsquo;un côté les données pertinentes (les 3) et de l\u0026rsquo;autre le reste des données (les non 3), et on décompose l\u0026rsquo;autre dimension en prédictions positives (les 3 prédits) et négatives (les non 3 prédits).\nPuis on compte dans chaque cadran le nombre de données correspondant au recouvrement des prédictions et de la réalité. Un nom issu du vocabulaire des diagnostics médicaux est attribué à chacun de ces cadrans :\nles vrais positifs VP (les 3 identifiés comme des 3), les vrais négatifs VN (les non 3 identifiés comme des non 3), les faux positifs FP (les non 3 identifiés comme des 3), les faux négatifs FN (les 3 identifiés comme des non 3). À partir de ces effectifs, on peut calculer 3 grandeurs permettant d\u0026rsquo;évaluer la qualité de la prédiction :\nPrécision\nNombre de données bien prédites parmi les prédictions positives : $$\\frac{VP}{VP+FP}$$\nRappel ou sensibilité\nNombre de données bien prédites parmi les données positives : $$\\frac{VP}{VP+FN}$$\nExactitude (accuracy)\n$$\\frac{VP+VN}{VP+VN+FP+FN}$$\ninfo\nUn algorithme peut très bien être très précis (les prédictions positives sont bien des 3), mais peu sensible, avec un faible taux de rappel (parmi tous les 3, peu ont été identifiés).\nÀ l\u0026rsquo;inverse, on peut avoir une bonne sensibilité (la plupart des vrais 3 ont été identifiés comme tel), mais peu précis (beaucoup de chiffres identifiés comme des 3 sont en fait d\u0026rsquo;autres chiffres).\ntip\nOn peut tout aussi bien définir la matrice de confusion avec les prédictions sur les lignes et la réalité sur les colonnes.\nMaintenant qu\u0026rsquo;on sait évaluer l\u0026rsquo;algorithme, cherchons la valeur de k qui maximise l\u0026rsquo;exactitude.\nDans le graphe ci-dessous, on a tracé l\u0026rsquo;exactitude de l\u0026rsquo;algorithme pour la reconnaissance des \u0026ldquo;9\u0026rdquo; en fonction de la valeur de k. Si notre but est de reconnaître le mieux possible les 9 manuscrits, il semblerait que la valeur de k optimale soit 18.\nExemple d'utilisation de KNN pour reconnaître un chiffre Algorithme des k-moyennes \u0026ndash; Exemple d\u0026rsquo;apprentissage non supervisé Le boulot de l\u0026rsquo;algorithme des k-moyennes (k-means) n\u0026rsquo;est pas d\u0026rsquo;étiqueter les données, mais de les regrouper par famille. C\u0026rsquo;est donc un algorithme de partitionnement des données (clustering).\nContrairement à KNN, l\u0026rsquo;algorithme des k-moyennes ne nécessite pas de données préétiquetées. Il fait ainsi parti des algorithmes d\u0026rsquo;apprentissage automatique non-supervisé (il se débrouille tout seul avec les données mystères).\nPar contre, l\u0026rsquo;algorithme partage avec KNN sa grande simplicité d\u0026rsquo;emploi et son efficacité qui le rendent lui aussi très populaire dans l\u0026rsquo;industrie.\nPrincipe L\u0026rsquo;algorithme dépend d\u0026rsquo;un seul paramètre en plus des données : le nombre de partitions (clusters) k.\nOn commence par choisir k points au hasard dans l\u0026rsquo;espace des données (il peut s\u0026rsquo;agir de k points de données ou de k autres points). Ce sont les k centres (ou centroïdes).\ntip\nPlus les points choisis au départ sont éloignés les uns des autres, mieux c\u0026rsquo;est. Une amélioration de l\u0026rsquo;algorithme de base proposée en 2007, k-means++, s\u0026rsquo;en assure.\nOn attribue ensuite à chaque centre tous les points de données qui lui sont le plus proches, formant ainsi k groupes.\nEnfin, on déplace chaque centre au barycentre de son groupe.\nOn répète les deux dernières opérations (attribution des points les plus près et déplacement des centres) tant que les centres bougent d\u0026rsquo;une itération à l\u0026rsquo;autre.\nL\u0026rsquo;algorithme vise à résoudre au final un problème d\u0026rsquo;optimisation ; son but est en effet de trouver le minimum de la distance entre les points à l\u0026rsquo;intérieur de chaque partition.\nMathématiquement, étant donné un ensemble de points $(x_1,x_2,\\ldots,x_n)$, on cherche à partitionner les $n$ points en $k$ ensembles $S=\\{S_1,S_2,\\ldots,S_k\\}$ en minimisant la grandeur : $$I = \\sum_{i=1}^{k}\\sum_{x_j \\in S_i}||x_i-\\mu_i||^2$$ où $\\mu_i$ est le barycentre des points dans $S_i$.\n$I$ est la variance intra-classe ou inertie intra-classe (terme surtout utilisé en anglais).\nChoix de k Choisir le bon nombre de clusters est crucial pour l\u0026rsquo;algorithme des k-moyennes, comme l\u0026rsquo;illustre l\u0026rsquo;exemple suivant :\nMais ce n\u0026rsquo;est pas toujours simple (contrairement à l\u0026rsquo;exemple) de deviner le bon nombre de clusters juste en inspectant les données. Alors comment faire ?\nOn pourrait se dire qu\u0026rsquo;il suffit de prendre le modèle avec la plus faible inertie. Mais malheureusement, l\u0026rsquo;inertie n\u0026rsquo;est pas une métrique adaptée au choix de k puisqu\u0026rsquo;elle ne fait que descendre quand k augmente\u0026hellip; Logique : plus il y a de clusters, plus la distance intra-cluster diminue\u0026amp;nbsp!\nTraçons l\u0026rsquo;inertie en fonction de k pour y voir plus clair : On remarque qu\u0026rsquo;ici, le nombre de clusters idéal correspond au point d\u0026rsquo;inflexion de la courbe (ou, si on imagine un bras, au coude).\nConfirmons en simulant des données séparées en 5 tas et en retraçant la courbe. Là encore, le coude indique le nombre k idéal.\nOn semble donc avoir trouver une tactique utilisable lorsqu\u0026rsquo;on n\u0026rsquo;a pas d\u0026rsquo;autres indices.\ntip\nIl existe des méthodes plus précises pour déterminer k, mais elles sont aussi plus gourmandes en calcul. La plus répandue utilise les coefficients de silhouette de chaque point (différence entre la distance moyenne avec les points du même groupe (cohésion) et la distance moyenne avec les points des autres groupes voisins (séparation)).\nLimites L\u0026rsquo;algorithme des $k$-moyennes se confronte à une difficulté classique en apprentissage automatique, et plus généralement pour tout problème d\u0026rsquo;optimisation : obtenir un minimum global plutôt qu\u0026rsquo;un minimum local.\nLa convergence vers un des minima locaux dépend crucialement de la position initiale des centres.\nDans l\u0026rsquo;exemple suivant, on obtient 3 partitionnements différents pour 3 initialisations différentes des centres.\nOn vérifie que les centres sont bien bloqués sur leur position dans les deux premiers cas puisqu\u0026rsquo;aucun changement d\u0026rsquo;attribution n\u0026rsquo;est possible.\nDans l\u0026rsquo;algo classique, pour pallier au mieux ce problème, on initialise les centres aléatoirement et on relance l\u0026rsquo;algorithme un certain nombre de fois pour ne garder au final que la solution qui minimise l\u0026rsquo;inertie intra-classe.\nAutre souci des k-moyennes : des difficultés pour partitionner des clusters de différentes tailles, différentes densités ou des formes non sphériques.\nApplications L\u0026rsquo;algorithme des k-moyennes ne présuppose rien sur les données et peut s\u0026rsquo;avérer, par le fait, très utile en première approche dans un rôle de défricheur.\nL\u0026rsquo;algorithme permet aussi de trancher des débats de la plus haute importance sur les couleurs comme \u0026ldquo;est-ce plus vert que bleu ?\u0026rdquo; en organisant un combat entre les centroïdes de chaque couleur.\nDans la même veine, on peut utiliser k-moyennes pour segmenter une image par couleur, ce qui peut s\u0026rsquo;avérer intéressant pour identfier des zones (comme des forêts) sur des données satellite ou pour compresser des images. Le choix de k correspond alors au nombre de couleurs qu\u0026rsquo;on veut garder.\nLa simplicité de k-moyennes en fait un bon outil de dégrossissage des données, y compris sur des données déjà étiquetées. Cela permet de réduire leur dimensionnalité, avant d\u0026rsquo;utiliser des algorithmes plus complexes d\u0026rsquo;apprentissage supervisé.\nJeux d\u0026rsquo;accessibilité sur un graphe Cette partie du cours a moins à voir avec les jeux vidéo qu\u0026rsquo;avec la modélisation de systèmes réactifs (automate bancaire, système-environnement), les problèmes de contrôle, la théorie de la décision, les problèmes de routage sur Internet, l\u0026rsquo;économie\u0026hellip; Autant de domaînes admettant une description en terme d\u0026rsquo;opposition entre adversaires sur une arêne (un des adversaires pouvant modéliser l\u0026rsquo;environnement). La détermination d\u0026rsquo;une stratégie gagnante résout alors le problème posé en assurant sa correction.\nL\u0026rsquo;arène On entendra ici par jeu :\ndes jeux à deux joueurs ($J_1$ et $J_2$ ou Eve et Adam) à information complète : les deux joueurs savent tout (pas comme aux cartes) alternés (pas comme à chifoumi) non randomisés (pas de hasard) L\u0026rsquo;arène dans laquelle le jeu prend place est un graphe orienté biparti.\nbipa\nUn graphe biparti (ou bipartite) $G$ est un graphe dont l\u0026rsquo;ensemble des sommets peut être divisé en deux sous-ensembles de sommets disjoints $S_1$ et $S_2$ ($S_1$ et $S_2$ sont une partition de $S$ : $S_1\\cup S_2=S$, $S_1\\cap S_2=\\varnothing$) tels que chaque arête de $G$ a une extrémité dans $S_1$ et l\u0026rsquo;autre dans $S_2$.\ntip\nUn graphe est biparti si on peut colorier tous les sommets du graphe avec seulement deux couleurs de manière à ce que deux sommets voisins n\u0026rsquo;aient jamais la même couleur (on parle alors de 2-coloriage).\nOn peut montrer qu\u0026rsquo;un graphe est biparti si et seulement si il ne possède pas de cycle de longueur impaire.\nDémonstration de \u0026ldquo;graphe biparti $\\Leftrightarrow$ pas de cycle impair\u0026rdquo; :\nOn montre $\\Rightarrow$ en constatant l\u0026rsquo;impossibilité d\u0026rsquo;un 2-coloriage sur un cycle impair. Donc tout graphe contenant un cycle impair ne peut pas être biparti.\nEt on montre $\\Leftarrow$ en essayant de créer un 2-coloriage depuis un sommet ; tant qu\u0026rsquo;on ne rencontre pas de cycle, pas de problème.\nTous les sommets à une distance impaire du sommet de départ sont coloriés d\u0026rsquo;une couleur et tous ceux à une distance paire sont coloriés de l\u0026rsquo;autre couleur.\nDonc un graphe sans cycle est toujours biparti.\nPlace aux cycles maintenant. Supposons que deux des chemins partant d\u0026rsquo;un sommet se rejoignent, alors on a deux possibilités :\nla jonction se fait entre deux sommets de couleurs différentes et le 2-coloriage reste possible.\nDans ce cas, on joint un chemin de longueur pair et un chemin de longueur impair, ce qui donne un cycle de longueur pair (avec le +1 de l\u0026rsquo;arête de la jonction). la jonction se fait entre sommets de la même couleur rendant impossible le 2-coloriage.\nDans ce deuxième cas, on obtient nécessairement un cycle de longueur impaire puisqu\u0026rsquo;on joint deux chemins de la même parité (+ 1 de l\u0026rsquo;arête de jonction).\nSeuls les cycles impairs font donc échouer le 2-coloriage. Tout graphe non biparti contient au moins un cycle impair.\nRetournons aux jeux\u0026hellip;\nDeux joueurs, $J_1$ et $J_2$, s\u0026rsquo;affrontent sur un graphe orienté biparti $G=(S,A)$ où $S$ est constitué des sommets contrôlés par le joueur 1, $S_1$, et de ceux contrôlés par le joueur 2, $S_2$. Chaque sommet est une position valide du jeu et chaque arête est un mouvement autorisé entre ces positions.\nIl manque encore une condition de gain pour rendre le jeu intéressant ; dans le cas d\u0026rsquo;un jeu d\u0026rsquo;accessibilité, on attribue à chaque joueur un sous-ensemble de sommets correspondant à des états gagnants qu\u0026rsquo;il convient d\u0026rsquo;atteindre pour\u0026hellip; gagner. Il peut aussi exister un sous-ensemble de sommets correspondant à des états de partie nulle.\nUn jeu d\u0026rsquo;accessibilité est alors défini par un quadruplet $(G,S_1,S_2,F)$ où $(G,S_1,S_2)$ est une arène et $F$ est l\u0026rsquo;ensemble des sommets gagnants pour $J_1$.\nExemples Chomp Chomp est un jeu où les deux adversaires mangent à tour de rôle des carrés de chocolat d\u0026rsquo;une tablette avec la \u0026ldquo;contrainte\u0026rdquo; de manger tous les carrés à droite et au-dessus du carré choisi. Le perdant doit manger le brocoli qui reste à la fin.\nEve commence le jeu avec la tablette ci-dessous composé de 5 carrés. Il existe alors 9 configurations possibles, pas toutes atteignables par les deux joueurs.\nTraçons l\u0026rsquo;arène (les ronds bleus sont les positions contrôlées par Eve et les carrés roses par Adam).\n$F$ (sommet gagnant pour Eve) est le carré rose avec un brocoli puisque l\u0026rsquo;atteindre signifie qu\u0026rsquo;Adam se retrouve avec le légume à manger.\nUne des nombreuses variantes du jeu de Nim Un tas d\u0026rsquo;allumettes est disposé devant Eve et Adam. Eve joue en premier et peut retirer autant d\u0026rsquo;allumettes qu\u0026rsquo;elle le souhaite du moment qu\u0026rsquo;elle en prend au moins une et qu\u0026rsquo;elle en laisse au moins une. C\u0026rsquo;est ensuite au tour d\u0026rsquo;Adam de retirer des allumettes avec pour tous les tours qui suivent une contrainte supplémentaire : on ne peut pas retirer plus de deux fois le nombre d\u0026rsquo;allumettes prises par son adversaire au tour précédent. Le joueur qui retire la dernière allumette gagne. Il n\u0026rsquo;y a pas de match nul.\nTraçons le graphe du jeu en supposant que l\u0026rsquo;on commence avec 5 allumettes et étiquetons les sommets avec le couple (nombre d\u0026rsquo;allumettes présentes, nombres d\u0026rsquo;allumettes prenables). L\u0026rsquo;étiquette du nœud de départ est donc (5,4).\nOn a indiqué en jaune le sommet à atteindre pour Eve (sommet de $F$).\nLe graphe est ici plutôt simple, mais on verra dans le TP que pour des nombres d\u0026rsquo;allumettes plus grand, on sera content de pouvoir confier la tâche de sa construction à python.\ntip\nCe jeu est une variante du jeu de Nim (voir TP) comme en fait tout jeu impartial à deux joueurs (théorème de Sprague-Grundy). Un jeu impartial est un jeu tour par tour dans lequel les coups autorisés, ainsi que les gains obtenus, dépendent uniquement de la position, et pas du joueur dont c\u0026rsquo;est le tour. C\u0026rsquo;est le cas de Chomp qui est donc, lui aussi, un jeu de Nim déguisé\u0026hellip; Un jeu qui n\u0026rsquo;est pas impartial est appelé jeu partisan (le morpion ou les échecs par exemple).\nMorpion (tic-tac-toe) Pas besoin de rappeler les règles du morpion (oxo en belgique).\nDans cet exemple, Eve démarre sur un jeu déjà avancé, elle a les ronds.\nCette fois-ci, $F$ contient plusieurs sommets (toujours indiqués en jaune).\nStratégie Gagner la partie, c\u0026rsquo;est arriver sur un sommet de $F$. Comment savoir si Eve peut ou non gagner selon sa position de départ ? Et si elle le peut, comment mettre au point pour elle une stratégie gagnante ?\nPositions gagnantes et attracteurs Pour déterminer l\u0026rsquo;ensemble des positions gagnantes pour Eve sur l\u0026rsquo;arène, on travaille récursivement depuis les sommets de $F$ en suivant les deux préceptes suivants :\nun sommet d\u0026rsquo;Eve est gagnant si un de ses arcs sortants le lie à un sommet gagnant.\nEve n\u0026rsquo;a alors plus qu\u0026rsquo;à emprunter ce chemin.\nun sommet d\u0026rsquo;Adam est gagnant (pour Eve) si tous ses arcs sortants le lie à un sommet gagnant.\nEn effet, Adam ne peut alors pas éviter de mettre Eve dans une position gagnante. Formalisons un peu tout ça en définissant la suite $Attr_i(F)$ qui contient l\u0026rsquo;ensemble des sommets gagnants après $i$ étapes : $$ \\begin{array}{lll} Attr_0(F) \u0026amp;= \u0026amp;F \\\\ Attr_{i+1}(F) \u0026amp;= \u0026amp;Attr_{i}(F) \\\\ \u0026amp;\u0026amp;\\cup \\{s \\in S_1|Succ(s)\\cap Attr_i(F) ≠ \\varnothing \\} \\\\ \u0026amp;\u0026amp;\\cup \\{s\\in S_2| Succ(s)\\subseteq Attr_i(F)\\} \\end{array} $$ Étant donné que $Attr_i(F) \\subseteq Attr_{i+1}(F) \\subseteq S$, pour tout $i≥0$, si on suppose le graphe fini, la suite est croissante et bornée et donc stationnaire (à partir d\u0026rsquo;un certain $i=i_0$, $Attr_i(F)$ est constante, et si $|G|=n$, $i_0$ vaut au plus $n-1$).\ninfo\nOn appelle attracteur de $F$ pour le joueur $J_1$ la limite de $Attr_i(F)$. On le note $Attr(F)$.\nTout sommet dans l\u0026rsquo;attracteur est une position gagnante pour $J_1$.\nLe complémentaire d\u0026rsquo;un attracteur est appelé piège. Si le joueur 1 est sur une position n\u0026rsquo;appartenant pas à son attracteur (et donc à son piège), cela signifie que :\nsi c\u0026rsquo;est son tour, tous les mouvements possibles restent dans le piège, si c\u0026rsquo;est le tour de l\u0026rsquo;adversaire, celui-ci a toujours au moins une possibilité de laisser le joueur 1 dans le piège. Cette position est donc perdante\u0026hellip;\nDétermination \u0026ldquo;à la main\u0026rdquo; de l\u0026rsquo;attracteur dans les exemples précédents L\u0026rsquo;attracteur dans l\u0026rsquo;exemple du jeu Chomp contient 9 sommets, dont le somme de départ.\nDans le cas de la variante de Nim, l\u0026rsquo;attracteur se réduit à $Attr(G) = \\{(0,0,1) , (1,1,0) , (2,2,0) \\}$ où la troisième valeur des triplets correspond au joueur qui contôle le sommet (0 pour Eve et 1 pour Adam). Le sommet de départ $(5,4,0)$ n\u0026rsquo;est pas dedans $\\Rightarrow$ c\u0026rsquo;est perdu pour Eve 😭..\nEnfin, sur l\u0026rsquo;exemple du morpion, l\u0026rsquo;attracteur contient 13 sommets dont celui de départ.\nProgramme permettant de calculer l\u0026rsquo;attracteur On peut écrire un programme récursif calculant l\u0026rsquo;attracteur en temps linéaire en $|S | + |A|$ (rappelons-nous que le parcours complet d\u0026rsquo;un graphe est au mieux en $O(|S | + |A|)$ car cela correspond à parcourir les $|S|$ sommets et les $|A|$ arêtes). Pour éviter de calculer plusieurs fois le même élément, l\u0026rsquo;algorithme tient à jour, pour chaque sommet $s$, un compteur n des successeurs non encore inspectés (sous la forme d\u0026rsquo;un dictionnaire) .\ndef attracteur(G: dict, F: list) -\u0026gt; list: \u0026#34;\u0026#34;\u0026#34; préconditions: G est est un graphe sous forme de liste d\u0026#39;adjacence implémentée par un dictionnaire F est la liste des sommets gagnants pour le joueur 1 postcondition: la fonction retourne l\u0026#39;attracteur de F pour le joueur 1 sous forme d\u0026#39;un dictionnaire dont les clés sont les sommets de G et les valeurs True ou False suivant que le sommet appartienne ou non à l\u0026#39;attracteur \u0026#34;\u0026#34;\u0026#34; Pred = inverseGraphe(G) n = {s:len(G[s]) for s in G} Attr = {s:False for s in G} for sommet in F: Joueur1 = True propage(sommet,Joueur1,Attr,Pred,n) return Attr def propage(sommet,Joueur1,Attr,Pred,n): if Attr[sommet]: return Attr[sommet] = True for s in Pred[sommet]: n[s] -= 1 # un successeur de s en moins if Joueur1 or (n[s] == 0) : propage(s,not Joueur1,Attr,Pred,n) Stratégie gagnante moryless\nUne stratégie sans mémoire est une fonction $\\sigma$ qui assigne un mouvement autorisé à un joueur pour chaque position non terminale : $\\forall s\\in S, (s,\\sigma(s))\\in A.$\nUn joueur sur une position $s$ suit une stratégie s\u0026rsquo;il emprunte le chemin $\u0026lt;s,\\sigma(s),\\sigma^2(s),\\ldots\u0026gt;$. Elle est dite sans mémoire car pour une position donnée, la stratégie est indépendante du chemin qui y a mené ($\\sigma$ ne dépend que du sommet).\nUne stratégie sans mémoire gagnante depuis une position donnée garantit la victoire au joueur en un nombre de coups limité. Pour le joueur 1, une stratégie gagnante garantit d\u0026rsquo;arriver sur un sommet de $F$. Mais suivant la position de départ, une telle stratégie n\u0026rsquo;existe pas forcément\u0026hellip;\nEn construisant l\u0026rsquo;attracteur, on répond à notre première question : la position d\u0026rsquo;Eve est-elle gagnante ? Il suffit de vérifier qu\u0026rsquo;elle appartient à l\u0026rsquo;attracteur.\nSi c\u0026rsquo;est le cas, une stratégie gagnante est facile à mettre en place ; il faut faire en sorte que chaque déplacement sur le graphe (chaque coup joué) se fasse vers un sommet de l\u0026rsquo;attracteur. Chaque coup d\u0026rsquo;Eve vers un sommet de l\u0026rsquo;attracteur piège aussi le coup suivant d\u0026rsquo;Adam dans l\u0026rsquo;attracteur.\nComme son nom l\u0026rsquo;indique, l\u0026rsquo;attracteur attire irrémédiablement vers $F$, assurant la victoire au joueur 1.\nLe joueur 2 aussi, bien sûr, a son attracteur, et il appartient au complémentaire de l\u0026rsquo;attracteur du joueur 1, piège du joueur 1. Donc un seul écart du joueur 1 en dehors de son attracteur, et s\u0026rsquo;en est fini pour lui, le joueur 2 peut le condanner à rester dans le piège. Revenons à nos exemples :\nPour Chomp, le joueur 1 appartient à l\u0026rsquo;attracteur, ce qui signifie que sa position de départ est gagnante. Par conséquent, il a une stratégie gagnante. Mais attention à ne pas se tromper au début ! Sur 5 mouvements possibles, le seul assurant la victoire est de manger le carré en haut à droite. Pour la variante de Nim, c\u0026rsquo;est foutu\u0026hellip; Quelle que soit notre stratégie, elle sera perdante. Enfin, pour Tic-tac-toe, la victoire tend les bras au joueur 1. Et, sans surprise, son premier mouvement doit être de prendre le milieu. Arbre et minimax Pour des jeux comme les échecs, le graphe est bien trop gros pour pouvoir appliquer nos méthodes précédentes. Alors comment s\u0026rsquo;en sortir ?\nL\u0026rsquo;idée est de se contenter d\u0026rsquo;une recherche partielle autour de la position actuelle donnant à l\u0026rsquo;algorithme seulement quelques coups d\u0026rsquo;avance. Il évalue alors les différentes positions futures possibles en leur attribuant un score issu d\u0026rsquo;une heuristique.\nheuri\nUne heuristique est une méthode de calcul qui fournit rapidement une solution réalisable, pas nécessairement optimale ou exacte, pour un problème d\u0026rsquo;optimisation difficile. Elle s\u0026rsquo;impose quand les algorithmes de résolution exacte sont impraticables, à savoir de complexité polynomiale de haut degré, exponentielle ou plus.\nUne heuristique est donc un compromis entre d\u0026rsquo;un côté l\u0026rsquo;optimalité (trouver la meilleure solution) et/ou la complétude (trouver toutes les solutions) de l\u0026rsquo;algorithme et de l\u0026rsquo;autre côté sa vitesse.\nPour implémenter efficacement cette recherche partielle, l\u0026rsquo;idée est d\u0026rsquo;utiliser un arbre plutôt que l\u0026rsquo;arêne précédente. Quelle différence ? L\u0026rsquo;absence de cycle qui va permettre d\u0026rsquo;élaguer ! On le paye au prix de la redondance des sommets (le même sommet peut apparaître plusieurs fois dans l\u0026rsquo;arbre). Pour chaque position, les différents coups possibles correspondent aux différentes branches et on avance ainsi niveau par niveau jusqu\u0026rsquo;aux feuilles représentant les positions terminales.\nLe gros avantage d\u0026rsquo;un arbre est qu\u0026rsquo;il permet facilement de ne garder que quelques niveaux (on raccourci alors toutes les branches jusqu\u0026rsquo;à la profondeur considérée).\nMuni de cet arbre, l\u0026rsquo;algorithme se lance dans un parcours en profondeur jusqu\u0026rsquo;à la profondeur maximale stipulée. Lorsque ce niveau est atteint (2, par exemple, si on veut que l\u0026rsquo;IA ait deux coups d\u0026rsquo;avance), l\u0026rsquo;algorithme évalue les feuilles grâce à son heuristique, puis il propage ce score vers les niveaux supérieurs en suivant le principe du minimax :\nsur un niveau correspondant au joueur 1, on sélectionne la valeur maximale parmi les branches, et sur un niveau correspondant à l\u0026rsquo;adversaire, on sélectionne la valeur minimale. Le pricipe est de maximiser les gains du joueur 1 tout en minimisant ceux du joueur 2. La valeur est au final transmise à la racine (la position depuis laquelle on a lancé la recherche) et la meilleure branche est sélectionnée.\nPrenons l\u0026rsquo;exemple du jeu de Morpion. Une heuristique possible pour évaluer un plateau pourrait consister à compter $+1$ pour chaque alignement encore possible pour le joueur et $-1$ pour ceux encore possibles pour son adversaire.\nL\u0026rsquo;arbre total du morpion n\u0026rsquo;est pas si gros ; le facteur de ramification $b$ est de 5 en moyenne et il y a au plus 9 niveaux (9 coups), ce qui donne $\\approx 5^9 = 1\\,953\\,125$ (à titre de comparaison, aux échecs, $b\\approx35$ et un partie dure en moyenne 100 coups, ce qui donne $b^m\\approx10^{54}$ sommets à inspecter\u0026hellip;).\nL\u0026rsquo;algorithme minimax appliqué au morpion inspecte en réalité environ 4 fois moins de sommets que les deux millions prédits, car ce chiffre ne tient pas compte des nombreuses parties potentielles se terminant avant le neuvième coup. Il en inspecte néanmoins beaucoup plus qu\u0026rsquo;il ne faudrait (il n\u0026rsquo;y a que $9!=362\\,880$ coups possibles si l\u0026rsquo;ordi commence et seulement $8!=40\\,320$ si l\u0026rsquo;humain a l\u0026rsquo;honneur), ce qui illustre le fait qu\u0026rsquo;un arbre contient beaucoup de sommets redondants par rapport au graphe du jeu dont il est tiré (c\u0026rsquo;est le prix à payer pour casser les cycles).\nLa taille modéré de l\u0026rsquo;arbre permet de l\u0026rsquo;explorer jusqu\u0026rsquo;aux parties finales, mais on peut constater en jouant avec le petit programme ci-dessous que la réduction à une profondeur d\u0026rsquo;un seul niveau grâce à l\u0026rsquo;heuristique3 est tout autant redoutable en inspectant presque 4000 fois moins de sommets !\nCela prouve qu\u0026rsquo;au morpion, 1 coup d\u0026rsquo;avance, c\u0026rsquo;est bien suffisant\u0026hellip; Mais ce n\u0026rsquo;est pas vraiment le cas aux échecs, où les plus grands joueurs (comme Kasparov) prévoient jusqu\u0026rsquo;à 12 coups à l\u0026rsquo;avance ! Deep Blue, qui a battu Kasparov en 1997, cherchait jusqu\u0026rsquo;à une profondeur typiquement comprise entre 6 et 16, mais pouvait aller jusqu\u0026rsquo;à 40 dans certaines situations.\nDans le cas des échecs, l\u0026rsquo;heuristique permettant d\u0026rsquo;évaluer un état de l\u0026rsquo;échiquier est bien plus complexe qu\u0026rsquo;au morpion. Elle doit prendre en compte la quantité de pièces et pions restants, la qualité des pièces et les positions de tout ce beau monde (domination du centre, structure compacte, etc.).\nLa taille du jeu de Go rend vaine toute tentative de type minimax. C\u0026rsquo;est au point que les grands joueurs de Go ont longtemps refusé de jouer contre des ordinateurs, non par peur de perdre, mais parcequ\u0026rsquo;ils les trouvaient trop mauvais. C\u0026rsquo;est l\u0026rsquo;essor du deep learning, et donc une philosophie basée sur l\u0026rsquo;apprentissage plus que sur la stratégie, qui a permis à la machine de devenir un adversaire coriace à ce jeu-là aussi.\nSac à dos et heuristique Voyons enfin une autre utilisation d\u0026rsquo;heuristique avec le problème du sac à dos, ici dans sa version \u0026ldquo;0/1\u0026rdquo; (knapsack 0/1 en anglais).\nLe problème du sac à dos est un problème classique d\u0026rsquo;optimisation avec d\u0026rsquo;importantes applications théoriques et industrielles.\nSoit $x\\in\\mathbb{N}^{*}$, soit $v$ une séquence de $n$ éléments appartenant à $\\mathbb{N}^{*}$, et soit $p$ une séquence de $n$ éléments appartenant à $\\{1,2,\u0026hellip;,c\\}$.\nNous appelons $c$ la capacité, $v$ la séquence de valeurs et $p$ la séquence de poids.\nLe problème du sac à dos consiste à mettre des objets de poids $p$, dans un sac à dos qui peut contenir un poids maximal $c$, de telle sorte que la valeur des objets choisis est maximisée.\nPlus formellement, cela revient à maximiser $$\\text{val}(x)=\\sum_{i=1}^n x[i]\\cdot v[i]$$ sous la contrainte $c≥\\sum_{i=1}^n x[i]\\cdot p[i]$, où $x\\in\\{0,1\\}^n$ indique les objets choisis.\nExemple : supposons que le sac ait une capacité de 900 et que l\u0026rsquo;on cherche à y placer les objets suivants\nobjets 🥏 🎺 🥊 🧸 🪠 ⏰ valeurs $v$ 5 50 65 20 10 12 poids $p$ 320 700 845 180 70 420 Le plus simple pour arriver à nos fins est de suivre une stratégie gloutonne (une stratégie étape par étape où un critère de classement permet de sélectionner le prochain objet à ajouter). Un critère qui semble prometteur est le ratio valeur/poids de chaque objet. L\u0026rsquo;idée est alors de placer les objets dans le sac dans l\u0026rsquo;ordre inverse de leur ratio. Cela semble une bonne stratégie puisque les objets ajoutés maximisent ainsi la valeur qu\u0026rsquo;ils apportent par rappor à la place qu\u0026rsquo;ils prennent. Mais si l\u0026rsquo;approche gloutonne a l\u0026rsquo;avantage d\u0026rsquo;être très simple, le revers de la médaille est qu\u0026rsquo;elle est à courte vue, on perd la vision d\u0026rsquo;ensemble. Et dans certaines configurations, comme c\u0026rsquo;est le cas dans notre exemple, cela s\u0026rsquo;avère contre-productif.\nobjets 🥏 🎺 🥊 🧸 🪠 ⏰ ratio $v/p$ 1/64 1/14 1/13 1/9 1/7 1/35 L\u0026rsquo;approche gloutonne nous encourage ici à placer d\u0026rsquo;abord 🪠 dans le sac, puis 🧸, et c\u0026rsquo;est tout. Plus de place pour l\u0026rsquo;objet suivant (🥊). On obtient finalement une valeur de 30 et un poids de 250.\nCet exemple nous montre que l\u0026rsquo;approche gloutonne ne garantit pas l\u0026rsquo;optimalité (loin de là même, vu la place qu\u0026rsquo;il reste dans le sac\u0026hellip;). On pourrait néanmoins facilement améliorer les choses en continuant d\u0026rsquo;essayer de placer les éléments de ratio plus grand sans s\u0026rsquo;arrêter au premier blocage. On tente 🎺, trop grosse, puis ⏰, là ça rentre, et enfin 🥏 qui ne loge pas. On obtient ainsi une valeur de 42 et un poids de 670. Mais même ainsi, on n\u0026rsquo;a pas obtenu la réponse optimale.\nPuisqu\u0026rsquo;on suppose, dans cette version \u0026ldquo;0/1\u0026rdquo; du problème qu\u0026rsquo;un objet est soit présent, soit absent (pas de fraction et pas de multiple), on peut représenter l\u0026rsquo;ensemble des possibilités par un arbre binaire.\nUne méthode sûre pour résoudre le problème consiste alors à parcourir l\u0026rsquo;arbre dans son entièreté et regarder la valeur et le poids de chaque branche complète (de la racine jusqu\u0026rsquo;à la feuille), pour choisir au final la branche la plus rémunératrice et repectant la contrainte de capacité.\nC\u0026rsquo;est la méthode par force brute.\nUn algorithme récursif possible pour faire ce travail (à chaque embranchement, on compare avec et sans l\u0026rsquo;objet) :\ndef sacadosBrute(v,p,c,i,valeur,poids): n = len(v) if i == n: if poids \u0026gt; c: return 0 else: return valeur else: valeurAvec = valeur + v[i] poidsAvec = poids + p[i] return max(sacadosBrute(v,p,c,i+1,valeur,poids),sacadosBrute(v,p,c,i+1,valeurAvec,poidsAvec)) Comme vous l\u0026rsquo;aurez deviné, la résolution du problème du sac à dos par force brute est en $O(2^n)$ où $n$ est le nombre d\u0026rsquo;objets. Donc au-delà de quelques dizaines d\u0026rsquo;objets, c\u0026rsquo;est mort\u0026hellip;\nPour améliorer les choses, on peut utiliser la méthode \u0026ldquo;séparation et évaluation\u0026rdquo; (branch and bond ou BB en anglais) qui vise à élaguer l\u0026rsquo;arbre autant que faire se peut.\nArrivé à un certain sommet de l\u0026rsquo;arbre, si l\u0026rsquo;objet se trouvant en-dessous amène à un dépassement de la capacité, cela ne sert plus à rien de continuer sa branche, alors on coupe.\nL\u0026rsquo;autre idée est d\u0026rsquo;utiliser la détermination de la valeur optimale du sac par la méthode gloutonne comme une heuristique ; si sous un sommet, la somme des valeurs des objets restant aboutit à une valeur totale inférieure à l\u0026rsquo;heuristique, on coupe.\nVoilà un code possible :\ndef sacadosBB(v,p,c,i,valeur,poids,meilleure,potentiel): nbappels += 1 n = len(v) meilleure = max(meilleure,valeur) if i == n: if poids \u0026gt; c: return 0 else: return valeur elif valeur + potentiel[i] \u0026lt; meilleure: return valeur else: valeurAvec = valeur + v[i] poidsAvec = poids + p[i] sol = sacadosBB(v,p,c,i+1,valeur,poids,meilleure,potentiel) if poidsAvec \u0026lt;= c: sol = max(sol,sacadosBB(v,p,c,i+1,valeurAvec,poidsAvec,meilleure,potentiel)) return sol Cela donne un arbre bien plus clairsemé sur notre exemple (et avec, qui plus est, une heuristique qui ne nous aide pas des masses, car très mauvaise).\nUne autre technique possible est d\u0026rsquo;utiliser la programmation dynamique qui est présentée dans la vidéo ci-dessous.\nApprentissage profond L\u0026rsquo;apprentissage profond (deep learning) révolutionne le secteur de l\u0026rsquo;IA dans les années 2010. Il consiste à entraîner un ordinateur à “apprendre” en analysant un grand nombre d’exemples. Il fait cela à l’aide de structures mathématiques appelées réseaux de neurones, qui s’inspirent vaguement du fonctionnement du cerveau humain. L’idée de “profondeur” vient du fait que les réseaux de neurones utilisés dans le deep learning ont de nombreuses couches. Chaque couche effectue une partie de l’analyse et transmet ses résultats à la suivante. C’est un peu comme si un problème complexe était résolu par une série d’étapes simples, chacune se concentrant sur un détail particulier.\nQuoi de mieux que la leçon inaugurale au Collège de France d\u0026rsquo;un de ses fondateurs, Yann Le Cun, pour nous expliquer de quoi il retourne.\nMalheureusement, le mème qui suit résume aujourd\u0026rsquo;hui assez bien notre compréhension fine du fonctionnement des modèles de deep learning : on est devant une boite noire qui fait le job demandé sans que l\u0026rsquo;on comprenne trop comment\u0026hellip;\nAvènement des grands modèles de langage Les grands modèles de langage (LLM en anglais) révolutionnent à leur tour l\u0026rsquo;IA dans les années 2020. Avec leur architecture non-récurrente, le transformeur, basée sur un mécanisme dit d\u0026rsquo;attention, ils peuvent avec succès traiter des données séquentielles tout en étant parallélisable lors de l\u0026rsquo;entrainement (cela permet des gains en performance énormes et ainsi d\u0026rsquo;augmenter de plusieurs ordres de grandeur le nombre de paramètres de leurs modèles, d\u0026rsquo;où le qualificatif \u0026ldquo;grand\u0026rdquo;).\nStephen Wolfram a écrit un long article pédagogique sur le fonctionnement de ChatGPT (l\u0026rsquo;agent conversationnel basé sur un LLM qui a sidéré le grand public lors de sa mise à disposition en 2022).\nLe génial 3Blue1Brown a réalisé une série de vidéos sur les LLM, mais il a aussi pensé aux gens pressés avec cette vidéo introductive :\nUn champ de recherche immense s\u0026rsquo;est ouvert pour tenter de comprendre le fonctionnement interne des LLM. Des chercheurs d\u0026rsquo;Anthropic (l\u0026rsquo;entreprise à l\u0026rsquo;origine de Claude) ont trouver une méthode pour aller en quelque sorte sonder le \u0026ldquo;cerveau\u0026rdquo; de Claude jusqu\u0026rsquo;à localiser l\u0026rsquo;emplacement de différents concepts et réussissant même à booster l\u0026rsquo;activation d\u0026rsquo;un concept par rapport aux autres. C\u0026rsquo;est ainsi qu\u0026rsquo;est né Golden Gate Claude qui pu, pendant 24h, interagir avec les utilisateurs et partager sa totale obsession pour le pont de San Francisco. Dans l\u0026rsquo;exemple d\u0026rsquo;interaction suivant, à la fois drôle et douloureux, Claude semble lutter contre sa psychose :\nIA et éthique L\u0026rsquo;intelligence artificielle repose en grande partie sur la collecte de données et leur traitement. Or des biais très importants aux conséquences potentiellement dramatique peuvent s\u0026rsquo;immiscer lors de cette étape cruciale.\nPrenons l\u0026rsquo;exemple du biais des sruvivants qui est un des plus importants biais de sélection.\nBiais des survivants Il tire son nom des efforts du statisticien Abraham Wald du Statistical Research Group qui analysait les impacts sur les bombardiers amériacains revenus de mission pendant la seconde guerre mondiale afin de déterminer les zones où il fallait améliorer le blindage. En voyant cette image l\u0026rsquo;erreur de logique consisterait à vouloir blinder les zones les plus impactées. C\u0026rsquo;est tomber dans le biais des survivants ! En effet, les avions étudié sont ceux qui sont revenus, les \u0026ldquo;survivants\u0026rdquo;, ce qui signifie que les impacts reçus ne les ont pas détruit. Par contre, l\u0026rsquo;absence d\u0026rsquo;impact dans certaines zones pourrait indiquer qu\u0026rsquo;il s\u0026rsquo;agit là d\u0026rsquo;endroits critiques où un tir provoque plus certainement une avarie grave et donc l\u0026rsquo;absence de survivants y présentant des impacts. Il faut en conclusion blinder d\u0026rsquo;avantage les zones sans impact !\nC\u0026rsquo;est le biais des survivants qui nous fait dire par exemple dire que les constructions anciennes étaient plus solides ou la musique meilleure il y a quelques décennies.\nLe biais du survivant s\u0026rsquo;immisce partout où il y a sélection puisque le critère de sélection laisse fatalement de côté les données ne respectant pas les critères et il faut donc se garder de conclusions générales ne prenant pas en compte la population écartée.\nEt au final, le biais des survivants n\u0026rsquo;est bien qu\u0026rsquo;une forme de biais de sélection. En effet, un biais de sélection désigne généralement toute situation où l’échantillon utilisé pour une analyse ou une prise de décision n’est pas représentatif de la population globale en raison de la méthode utilisée pour sélectionner les données. Avec le biais des survivants, l\u0026rsquo;erreur de représentativité consiste à considérer, comme son nom l\u0026rsquo;indique, uniquement les survivants, mais ce n\u0026rsquo;est qu\u0026rsquo;un exemple parmi d\u0026rsquo;autre de sélection non représentative de données. L\u0026rsquo;entraînement des IA, en particulier celui des IA génératives, est très sensible à ces défauts de représentation de la même façon qu\u0026rsquo;un enfant élevé dans un milieu particulier (secte par exemple) aura beaucoup de mal à se faire une représentation adaptée du reste du monde.\nUne fois récoltées, les données sont traîtées et en particulier, on cherche à établir des corrélations entre des groupes de données. En effet, ces corrélations permettent d\u0026rsquo;extrapoler dans des zones de l\u0026rsquo;espace vides de données et ainsi prédire les valeurs ou catégories de données manquantes. Par contre, on doit alors faire attention à un autre biais classique résumé par un des mantras de la statistique : \u0026ldquo;La corrélation n\u0026rsquo;implique pas la causalité\u0026rdquo;.\nLa corrélation n\u0026rsquo;implique pas la causalité Pour expliquer la corrélation entre deux évènements, on a trop vite fait de supposer une relation de cause à effet. C\u0026rsquo;est une possibilité mais ce n\u0026rsquo;est pas la seule !\nAutres explications possibles de la corrélation ?\nRéponse (cliquer pour afficher) Hasard. Un troisième facteur cause les deux autres (exemple classique de la corrélation entre la vente de glace et les attaques de requins ou entre le nombre d'enfants et le nombre de cigognes - on parle d'ailleurs parfois d'effet cigogne). Peut être que la relation de cause à effet est dans l'autre sens que le sens envisagé. "
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/godel/",
	"title": "Logique modale",
	"tags": [],
	"description": "",
	"content": " Il y a différentes logiques de la prouvabilité dont celle de Gödel-Löb. $\\Box\\phi$ y est interprétée comme \u0026ldquo;la formule $\\phi$ est prouvable\u0026rdquo;.\nToute formule peut être codée par un entier (il suffit par exemple de regarder le code binaire du fichier de traitement de texte où cette formule est écrite et d\u0026rsquo;ajouter \u0026ldquo;1\u0026rdquo; à gauche du code pour éviter la perte éventuelle de zéros non significatifs). De même, on peut coder un arbre de preuve (puisqu\u0026rsquo;il peut lui-même s\u0026rsquo;écrire avec un logiciel de traitement de texte). Dès lors, une formule peut très bien parler du fait qu\u0026rsquo;une autre formule est démontrable puisqu\u0026rsquo;il s\u0026rsquo;agit de dire qu\u0026rsquo;il existe un entier possédant les propriétés qui le caractérisent comme étant le code d\u0026rsquo;une preuve de cette formule. C\u0026rsquo;est ce que fait la formule $\\Box\\phi$.\nL\u0026rsquo;arithmétique de Peano $\\text{PA}$ est une théorie - ensemble d\u0026rsquo;axiomes - en logique du premier ordre dont l\u0026rsquo;arithmétique des entiers est un modèle. L\u0026rsquo;idée d\u0026rsquo;associer un nombre entier à une formule ou à une preuve a permis à Gödel d\u0026rsquo;injecter dans la théorie de l\u0026rsquo;arithmétique des déclarations sur la théorie elle-même. Les nombres gagnent ainsi la capacité méta de parler d\u0026rsquo;eux-mêmes.\nLe premier théorème d\u0026rsquo;incomplétude de Gödel s\u0026rsquo;appuie sur une formule affirmant sa propre non prouvabilité $\\neg\\Box\\phi\\leftrightarrow \\phi$. Si on peut démontrer la formule $\\neg\\Box\\phi\\leftrightarrow \\phi$ dans un système donné, alors on prouve qu\u0026rsquo;il existe une formule non démontrable et pourtant vraie, ce qui impose l\u0026rsquo;incomplétude du système.\nLa démonstration de Gödel est un impresssionnant tour de force de 30 pages, mais elle devient bien plus triviale avec la notion de machine de Turing. La plupart des langages de programmation (comme Python) sont Turing-complet, ce qui signifie qu\u0026rsquo;ils sont équivalents à une machine de Turing universelle capable de calculer tout ce qui est calculable. Or Turing a aussi découvert ce qu\u0026rsquo;une telle machine ne sait pas faire : déterminer (en un temps fini) si un programme (une suite d\u0026rsquo;instructions) va tourner à l\u0026rsquo;infini ou finir par s\u0026rsquo;arrêter. C\u0026rsquo;est le problème de l\u0026rsquo;arrêt.\nPour prouver qu'aucun programme ne peut résoudre le problème de l'arrêt, Turing a raisonné par l'absurde\u0026nbsp;:\nsupposons qu'un tel programme $\\text{P}$ existe. On peut alors construire un programme $\\text{P'}$ à partir de $\\text{P}$ qui, lorsqu'on lui fournit le code d'un programme $\\text{Q}$ en entrée Le théorème de Löb stipule que les formules dont on peut prouver que $\\Box\\phi\\rightarrow\\phi$ sont les formules prouvables :\n$\\Box(\\Box\\phi\\rightarrow\\phi)\\rightarrow\\Box\\phi$. La logique de la prouvabilité de Gödel-Löb consiste à partir du système $\\text{\\bf K}$ (d\u0026rsquo;ailleurs la formule $\\text{K}$ ne décrit ici rien d\u0026rsquo;autre que le modus ponens ; si j\u0026rsquo;ai une preuve de $\\phi\\rightarrow\\psi$ et une preuve de $\\phi$, alors j\u0026rsquo;ai une preuve de $\\psi$) auquel on ajoute l\u0026rsquo;axiome $\\text{(4)}=\\Box P\\rightarrow \\Box\\Box P$ (si j\u0026rsquo;ai une preuve de $P$, alors j\u0026rsquo;ai une preuve du fait que j\u0026rsquo;ai une preuve de $P$) ainsi que le théorème de Löb ($\\text{L}$).\nEn appelant $\\mathscr{C}_{tr.,b.d.}$ la classe des modèles de Kripke à la fois transitifs et bornés à droite (sans suite infinie de mondes), alors on a le résultat suivant :\n$ \\Gamma\\vdash_\\text{\\bf GL} \\phi$ si et seulement si $\\Gamma\\Vdash_{\\mathscr{C}_{tr.,b.d.}}\\phi$ "
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique4/",
	"title": "Logique modale",
	"tags": [],
	"description": "",
	"content": " info\nNotes de lecture du livre La logique pas à pas de Jacques Duparc que je paraphrase allégrement.\nLogique modale Syntaxe et sémantique Systèmes logiques Différentes logiques modales Une petite communauté de dragons vivent sur une île aux règles spéciales. Si un dragon apprend qu\u0026rsquo;il a les yeux bleus, c\u0026rsquo;est la honte et il doit partir le soir même. Mais les autres dragons n\u0026rsquo;ont pas le droit de lui dire et aucun de ces dragons n\u0026rsquo;a de reflet et ne peut donc constater la couleur de ses propres yeux.\nTrois dragons peuplent l\u0026rsquo;île, Apophis, Bahamut et Caraxès, lorsqu\u0026rsquo;un jour, un étranger débarque.\nL\u0026rsquo;étranger déclare :\nl\u0026rsquo;un de vous au moins a les yeux bleus.\nLe premier soir, aucun dragon ne part.\nLe deuxième soir, toujours rien.\nLe troisième soir, ils partent tous les trois\u0026hellip;\nLa logique modale va permettre de modéliser ce petit problème.\nSyntaxe La syntaxe de la logique modale est celle du calcul des propositions à laquelle on ajoute deux opérateurs unaires, les opérateurs de modalité :\nLa boite $\\Box$ Le diamant $\\Diamond$ On va être amené à non pas définir un seul $\\Box$ et $\\Diamond$ mais potentiellement une infinité (dénombrable).\nOn écrira alors $[i]$ et $\\langle i \\rangle$, où $i$ varie dans un ensemble $I$.\nLe langage $\\mathcal{L}$ de la logique modale est l\u0026rsquo;ensemble suivant :\n$\\mathcal{L}=VAR\\cup\\set{\\top,\\bot,\\neg,\\lor,\\land\\rightarrow,\\leftrightarrow,(,)}\\cup\\set{[i]:i\\in\\mathbb{N}}\\cup\\set{\\langle i\\rangle:i\\in\\mathbb{N}}$\nSoient $\\phi$, $\\psi$, $\\theta$ trois formules, on note $\\phi[\\theta/\\psi]$ la substitution uniforme de la formule $\\theta$ à toutes les occurrences de $\\psi$ dans $\\phi$.\nUne substitution uniforme consiste dans un premier temps à retirer toutes les occurrences de $\\psi$ puis à remplacer chacune par $\\theta$.\nSémantique Système de transition On va utiliser les modèles de Kripke, c\u0026rsquo;est-à-dire des graphes dirigés, pour interpréter les formules de la logique modale. Mais on se cantonnera ici à la logique classique (les interprétations de la négation et de l\u0026rsquo;implication seront traditionnels et non intuitionnistes). Chaque nœud va correspondre à des mondes possibles qui sont ou non liés entre eux par des arcs.\nOn appelle ces graphes dirigés des systèmes de transition (frame en anglais) :\nPour $I$ un ensemble non vide, un système de transition étiqueté par $I$ (ou de signature $I$) est une structure relationnelle (un graphe dirigé étiqueté) $$\\mathcal{S}=(N,A)$$ où $N$ est un ensemble non vide dont les éléments sont appelés nœuds et $A$ est un ensemble de ralations binaires sur $\\mathbb{N}$ indicées par $I$ ($A=\\set{A_i\\subseteq N\\times N|i\\in I}$).\nUn système de transition $\\mathcal{S}(N,A)$ (avec $A=\\set{A_i\\subseteq N\\times N|i\\in I}$) est dit\nréflexif si, pour tout $i\\in I$ et pour tout $a\\in N$, il vérifie $a\\xrightarrow{i}a$\u0026nbsp;; symétrique si, pour tout $i\\in I$ et pour tout $a,b\\in N$, dès qu'il vérifie $a\\xrightarrow{i}b$, il vérifie aussi $b\\xrightarrow{i}a$\u0026nbsp;; transitif si, pour tout $i\\in I$ et pour tout $a,b,c\\in N$, dès qu'il vérifie à la fois $a\\xrightarrow{i}b$ et $b\\xrightarrow{i}c$, il vérifie aussi $a\\xrightarrow{i}c$\u0026nbsp;; famélique si, pour tout $i\\in I$ et pour tout $a,b\\in N$, dès qu'il vérifie $a\\xrightarrow{i} b$ alors $a=b$\u0026nbsp;; dense si, pour tout $i\\in I$ et pour tout $a,c\\in N$, dès qu'il vérifie $a\\xrightarrow{i} c$, alors il existe $b$ tel que $a\\xrightarrow{i} b$ et $b\\xrightarrow{i} c$\u0026nbsp;; non borné à droite si, pour tout $i\\in I$ et pour tout $a\\in N$, il existe un nœud $b\\in N$ tel que $a\\xrightarrow{i}b$\u0026nbsp;; euclidien si, pour tout $i\\in I$ et pour tout $a,b,c\\in N$, dès qu'il vérifie à la fois $a\\xrightarrow{i} b$ et $a\\xrightarrow{i} c$, alors il vérifie aussi $b\\xrightarrow{i} c$. Satisfaction Un système de transition se mue en modèle de la logique modale (modèle de Kripke) dès qu\u0026rsquo;il est équipé d\u0026rsquo;une valuation portant sur les variables étudiées. Une valuation indique pour chaque nœud du graphe les variables qui sont forcées en ce nœud.\nSoient $I$ un ensemble non vide et $\\mathcal{S}=(N,A)$ un système de transition étiqueté par $I$.\nUne valuation sur ce système de transition $\\mathcal{S}$ est une fonction : $$ \\begin{array}{ccc} \\mathcal{V}: VAR \u0026amp; \\to \u0026amp; \\mathcal{P}(N)\\\\ \\phantom{\\mathcal{V}:}P \u0026amp; \\mapsto \u0026amp; \\set{a|a\\Vdash P} \\end{array} $$\nLa satisfaction d\u0026rsquo;une formule $\\phi$ au nœud $a$ (au sein de $\\mathcal{S}$ et pour la valuation $\\mathcal{V}$) est notée $a\\Vdash\\phi$ et se définit par induction sur la hauteur de $\\phi$ :\n$a\\Vdash\\top$ et $a\\nVdash\\bot$ $a\\Vdash P$ ssi $a\\in\\mathcal{V}(P)$ (pour une variable $P$ quelconque) $a\\Vdash\\neg\\phi$ ssi $a\\nVdash\\phi$ $a\\Vdash\\phi\\lor\\psi$ ssi ($a\\Vdash\\phi$ ou $a\\Vdash\\psi$) $a\\Vdash\\phi\\land\\psi$ ssi ($a\\Vdash\\phi$ et $a\\Vdash\\psi$) $a\\Vdash\\phi\\rightarrow\\psi$ ssi ($a\\nVdash\\phi$ ou $a\\Vdash\\psi$) $a\\Vdash\\phi\\leftrightarrow\\psi$ ssi (($a\\Vdash\\phi$ et $a\\Vdash\\psi$) ou ($a\\nVdash\\phi$ et $a\\nVdash\\psi$)) $a\\Vdash[i]\\phi$ ssi pour tout $b$, si $a\\xrightarrow{i}b$, alors $b\\Vdash\\phi$ $a\\Vdash\\langle i\\rangle\\phi$ ssi il existe $b$ tel que $a\\xrightarrow{i}b$ et $b\\Vdash\\phi$ S\u0026rsquo;il n\u0026rsquo;existe pas de nœud $b$ tel que $a\\xrightarrow{i}b$, alors quelle que soit la formule $\\phi$, $a\\Vdash\\langle i\\rangle\\phi$ est toujours fausse. Autrement dit, $a\\nVdash \\langle i\\rangle\\phi$.\nPar contre, s\u0026rsquo;il n\u0026rsquo;existe pas de nœud $b$ tel que $a\\xrightarrow{i}b$, alors $a\\Vdash[i]\\phi$ est toujours vraie quelle que soit la formule $\\phi$. En effet pour que $a\\Vdash[i]\\phi$ soit vérifiée il faut que si $a\\xrightarrow{i}b$, alors $b\\Vdash\\phi$. Mais s\u0026rsquo;il n\u0026rsquo;y a aucun $a\\xrightarrow{i}b$ alors l\u0026rsquo;implication est tojours vérifiée.\nLa satisfaction d\u0026rsquo;une formule en un nœud $a$ d\u0026rsquo;un système de transition $\\mathcal{S}$ équipé d\u0026rsquo;une valuation $\\mathcal{V}$ dépend de ces trois ingrédients et on notera donc $\\langle \\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\phi$.\nRevenons à nos dragons :\nExemple des dragons aux yeux bleus les mondes possibles (les nœuds du graphe) vont correspondre aux différentes combinaisons possibles de couleurs d\u0026rsquo;yeux pour les trois dragons $A$, $B$ et $C$. On a deux possibilités (yeux bleus ou non) pour chacun des trois dragons. Cela fait 8 mondes possibles.\nLes arcs entre mondes vont être étiquetés chacun par un des trois dragons et vont correspondre aux différents mondes que ce dragon pense possible depuis le monde où il est.\nPar exemple, dans le monde où seul $B$ a les yeux bleus (le monde 3) $A$ imagine aussi possible le monde où $A$ et $B$ ont tous les deux les yeux bleus (le monde 5). Une flèche étiquetée par $A$ rejoint donc ces deux mondes possibles. Et bien sûr, dans le monde où $A$ et $B$ ont les yeux bleus, $A$ imagine possible le monde où seul $B$ a les yeux bleus (il y a donc une flèche de 5 vers 3). Et comme c\u0026rsquo;est vrai dans chaque cas et pour les trois dragons, cela montre que ce système de transition est symétrique. Il est de plus évident que chaque monde semble possible pour les dragons qui s\u0026rsquo;y trouvent. Le système est donc aussi réflexif.\nImaginons que l\u0026rsquo;on soit dans le monde 2. L\u0026rsquo;ensemble des mondes possibles se simplifient alors grandement.\nLa déclaration de l\u0026rsquo;étranger n\u0026rsquo;est pas une surprise pour $B$ et $C$ qui voient bien que le pauvre $A$ a les yeux bleus et est donc sans effet pour eux. Par contre $A$ comprend que l\u0026rsquo;autre monde qu\u0026rsquo;il pensait possible, le monde béni où il n\u0026rsquo;avait pas des yeux infames (le monde 1) est en fait impossible.\nAvant la déclaration de l\u0026rsquo;étranger, $2\\Vdash\\color{#00AB8E}\\langle A\\rangle \\color{#000}\\neg A$ puisqu\u0026rsquo;il y a une flèche étiquetée par $A$ qui rejoint 1 (qui réalise $\\neg A$). On peut traduire ça par : \u0026ldquo;dans le monde 2, Apophis croit possible qu\u0026rsquo;il n\u0026rsquo;ait pas les yeux bleus\u0026rdquo;. De même $2\\Vdash\\color{#FEAE00}\\langle B\\rangle \\color{#000}\\neg B$ à cause de la flèche étiquetée par $B$ vers le monde 5 et $2\\Vdash\\color{#FF42A1}\\langle C\\rangle \\color{#000}\\neg C$ à cause de la flèche étiquetée par $C$ vers le monde 6. Donc chaque dragon croit encore possible d\u0026rsquo;avoir les yeux bleus. D\u0026rsquo;autre part, on peut écrire que $2\\Vdash\\color{#FF42A1}[C] \\color{#000} A$ puisque les trois arcs étiquetés par $C$ qui partent de 2 vont dans des mondes (2, 5 et 6) où $A$ est réalisée. Ça peut se lire : \u0026ldquo;dans le monde 2, Caraxès sait qu\u0026rsquo;Apophis a les yeux bleus\u0026rdquo;.\nOn peut aussi compliquer les énoncés avec par exemple : $6 \\Vdash \\color{#FF42A1}\\langle C\\rangle \\color{#FEAE00}\\langle B\\rangle \\color{#000} (B\\land\\neg C)$ (une flèche rose part de 6 vers 2 et une flèche jaune part de 2 vers 5 où $B$ a les yeux bleus mais pas $C$).\nAprès la déclaration de l\u0026rsquo;étranger, le monde 1 disparaît !\nSi $B$ et $C$ sont toujours dans le doute quant à la couleur de leurs yeux, pour $A$, les jeux sont faits. En effet, on a dorénavant $2\\Vdash\\color{#00AB8E}[A] \\color{#000} A$. Et le soir même, Apophis s\u0026rsquo;en va tout penaud.\nEn généralisant, dans les mondes où seul un dragon a les yeux bleus (2, 3 et 4), ce dragon le comprend au moment de la déclaration de l\u0026rsquo;étranger et se retire le soir même.\nImaginons maintenant que l\u0026rsquo;on soit dans le monde 5 où Apophis et Bahamut ont les yeux bleus.\nLa déclaration de l\u0026rsquo;étranger ne fait pas beaucoup d\u0026rsquo;effet car ils étaient déjà tous les trois au courant qu\u0026rsquo;au moins l\u0026rsquo;un d\u0026rsquo;eux avait les yeux bleus\u0026hellip; Par conséquent, le soir même, personne ne part. Et ça par contre, ça change pas mal de choses ! En effet, les mondes 2 et 3 deviennet d\u0026rsquo;un coup impossibles puisque on a vu que les mondes où seul un des dragon a les yeux bleus voient ce dragon décamper dès le premier soir.\nOn a maintenant : $5\\Vdash (\\color{#00AB8E}[A] \\color{#000} A)\\land (\\color{#FEAE00}[B] \\color{#000} B)$. $A$ et $B$ ont acquis la certitude d\u0026rsquo;avoir les yeux bleus et s\u0026rsquo;envolent au deuxième soir.\nLes deux dragons aux yeux bleus des mondes 7 ou 6 auraient fait de même.\nLa dernière situation possible correspond au monde 8.\nOn reprend le raisonnement précédent. Personne ne part au premier soir, mais cette fois-ci, ça ne permet d\u0026rsquo;éliminer aucun monde possible\u0026hellip; Et par conséquent, au deuxième soir, personne ne s\u0026rsquo;envole. Mais là, bingo ! Ça élimine les mondes où seuls deux dragons ont les yeux piscine. Finalement, ils comprennent tous les trois qu\u0026rsquo;ils sont banis et s\u0026rsquo;envolent au troisième soir.\nLa situation de départ correspondait donc au monde 8.\nOn peut remarquer en passant différentes équivalences :\n$a\\Vdash\\neg\\color{#00AB8E}[A] \\color{#000} \\phi$ ssi $a\\Vdash \\color{#00AB8E}\\langle A\\rangle \\color{#000} \\neg \\phi$\nPour A, ne pas savoir quelque chose est équivalent à croire possible sont contraire. $a\\Vdash\\color{#00AB8E}[A] \\color{#000} \\phi$ ssi $a\\Vdash \\neg\\color{#00AB8E}\\langle A\\rangle \\color{#000} \\neg \\phi$\nPour A, savoir quelque chose est identique à ne pas croire possible le contraire de cette chose. $a\\Vdash\\neg\\color{#00AB8E}[A] \\color{#000} \\neg\\phi$ ssi $a\\Vdash \\color{#00AB8E}\\langle A\\rangle \\color{#000} \\phi$\nPour A, croire possible quelque chose revient à ne pas savoir le contraire de cette chose. Modèles et classes Soient $\\phi$ une formule, $I$ un ensemble non vide, $\\mathcal{S}=(N,A)$ un système de transition étiqueté par $I$, $a$ un nœud de $\\mathcal{S}$ et $\\mathcal{V}$ une valuation,\n$\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}\\phi$ ssi pour tous nœuds $a'$, $\\langle\\mathcal{S},\\mathcal{V},a'\\rangle\\Vdash\\phi$ $\\langle\\mathcal{S},a\\rangle\\Vdash^{\\forall \\mathcal{V}}\\phi$ ssi pour toutes valuations $\\mathcal{V'}$, $\\langle\\mathcal{S},\\mathcal{V'},a\\rangle\\Vdash\\phi$ $\\mathcal{S}\\Vdash^{\\forall a,\\forall \\mathcal{V}}\\phi$ ssi pour tous nœuds $a'$ et toutes valuations $\\mathcal{V'}$, $\\langle\\mathcal{S},\\mathcal{V'},a'\\rangle\\Vdash\\phi$ Un modèle de la logique modale (modèle de Kripke) est un système de transition équipé d\u0026rsquo;une valuation : $$\\mathcal{M}=\\langle\\mathcal{S},\\mathcal{V}\\rangle$$ Une formule $\\phi$ est vraie (ou satisfaite) dans le modèle $\\langle\\mathcal{S},\\mathcal{V}\\rangle$ si elle est forcée en chacun des nœuds du système de transition : $$\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}\\phi$$\nLa notion de vérité dans un modèle de Kripke est donc une notion de vérité globale puisqu\u0026rsquo;elle prend en compte l\u0026rsquo;ensemble des nœuds du système de transition.\nPrenons l\u0026rsquo;exemple du système de transition $\\mathcal{S}$ équipé de la valuation $\\mathcal{V}$ ci-dessous :\nOn a bien $\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}Q\\lor\\neg Q$. Par contre, comme $b\\Vdash Q$ alors que $a\\Vdash\\neg Q$, $\\langle\\mathcal{S},\\mathcal{V}\\rangle\\nVdash^{\\forall a}Q$ et $\\langle\\mathcal{S},\\mathcal{V}\\rangle\\nVdash^{\\forall a}\\neg Q$.\nOn peut donc avoir une formule $\\phi\\lor\\psi$ satisfaite sans que ni $\\phi$ ni $\\psi$ ne le soient !\ninfo\nOn retrouve les modèles du calcul des propositions en se restreignant aux systèmes de transition contenant un seul nœud. Dit autrement, une formule $\\phi$ de profondeur modale nulle est une formule du calcul des propositions.\nSoit $\\mathcal{M}$ un modèle du calcul des propositions défini par la distribution de valeur de vérité $\\delta_\\mathcal{M}$ qui associe aux variables apparaissant dans $\\phi$ la valeur 1 lorsqu\u0026rsquo;elles sont vraies dans $\\mathcal{M}$ et 0 sinon ($\\delta_\\mathcal{M}(P)=1$ ssi $\\mathcal{M}\\models P$). La formule $\\phi$ st vraie dans $\\mathcal{M}$ (noté $\\mathcal{M}\\models\\phi$) si et seulement si cette même formule $\\phi$ est vraie dans le système de transition $\\mathcal{S}_\\mathcal{M}=(N,A)$, où $N=\\set{a}$ et $A=\\emptyset$, équipé de la valuation $\\mathcal{V}_{\\delta_\\mathcal{M}}$ définie par : $\\mathcal{V}_{\\delta_\\mathcal{M}}(P)=\\set{a}$ si et seulement si $\\delta_\\mathcal{M}(P)=1$ et $\\mathcal{V}_{\\delta_\\mathcal{M}}(P)=\\emptyset$ si et seulement si $\\delta_\\mathcal{M}(P)=0$.\nCela s\u0026rsquo;écrit : $\\mathcal{M}\\models\\phi$ ssi $\\langle\\mathcal{S},\\mathcal{V}_{\\delta_\\mathcal{M}}\\rangle\\Vdash^{\\forall a}\\phi$\nSoit $\\phi$ une formule de la logique modale et $\\mathcal{S}$ un système de transition.\nLa formule $\\phi$ est valide dans $\\mathcal{S}$ si : $$\\mathcal{S}\\Vdash^{\\forall a,\\forall \\mathcal{V}} \\phi$$\nLa notion de validité est globale, elle aussis. Et à nouveau, une formule $\\phi\\lor\\psi$ peut être valide dans un système de transition sans que ni $\\phi$, ni $\\psi$ ne le soit.\nSoient $\\phi$ une formule de profondeur modale nulle et $\\mathcal{S}=(N,A)$ un système de transition.\nLes affirmations suivantes sont équivalentes :\n$\\mathcal{S}\\Vdash^{\\forall a,\\forall\\mathcal{V}} \\phi$ Il existe $a\\in N$ tel que $\\langle\\mathcal{S},a\\rangle\\Vdash^{\\forall \\mathcal{V}}\\phi$ Pour $\\mathcal{T}=(\\set{a},\\emptyset)$, $\\langle\\mathcal{T},a\\rangle\\Vdash^{\\forall \\mathcal{V}}\\phi$ $\\phi$ est une tautologie du calcul des propositions On va montrer la suite des implications suivante $(1)\\rightarrow(2)$, $(2)\\rightarrow(3)$, $(3)\\rightarrow(4)$ et $(4)\\rightarrow(1)$. La circularité nous permettra alors d\u0026rsquo;obtenir toutes les autres implications. Par exemple, $(2)\\rightarrow(1)$ découle de $(2)\\rightarrow(3)\\rightarrow(4)\\rightarrow(1)$.\n$(1)\\rightarrow(2)$ et $(2)\\rightarrow(3)$ sont immédiates. Pour montrer $(3)\\rightarrow(4)$, nous allons montrer la contraposée $\\neg(4)\\rightarrow\\neg(3)$.\nSupposons que $\\phi$ n'est pas une tautologie du calcul des propositions. Il existe donc un modèle $\\mathcal{M}$ pour lequel la formule est fausse. Équipons alors le système de transition $\\mathcal{T}=(\\set{a},\\emptyset)$ de la valuation où pour chacune des variables propositionnelles $P$ apparaissant dans $\\phi$, $a\\in\\mathcal{V}(P)$ si et seulement si $\\mathcal{M}\\models P$.\nPar définition de la satisfaction locale d'une formule $\\langle \\mathcal{T},\\mathcal{V},a\\rangle\\Vdash \\phi$ ssi $\\mathcal{M}\\models\\phi$.\nComme notre hypothèse est que $\\mathcal{M}\\not\\models\\phi$, on en déduit que $\\langle \\mathcal{T},\\mathcal{V},a\\rangle\\nVdash \\phi$. On a donc trouvé une valuation qui contredit $(3)$. Pour montrer $(4)\\rightarrow(1)$, on passe également par la contraposée $\\neg(1)\\rightarrow\\neg(4)$.\nOn suppose que $\\mathcal{S}\\Vdash^{\\forall a,\\forall \\mathcal{V}}\\phi$ n'est pas satisfaite, et donc qu'il existe une valuation $\\mathcal{V}$ et un nœud $a$ tels que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\phi$. On définit le modèle du calcul des propositions $\\mathcal{M}$ par $\\mathcal{M}\\models P$ si et seulement si $a\\in\\mathcal{V}(P)$, et ce pour chaque variable propositionnelle apparaissant dans $\\phi$. On a donc $\\mathcal{M}\\not\\models\\phi$. Par conséquent, $\\phi$ n'est pas une tautologie. Soit $\\phi$ une formule et $\\mathscr{C}$ une classe de systèmes de transition.\n$\\mathscr{C}\\Vdash\\phi$ ssi pour tout $\\mathcal{S}\\in\\mathscr{C}, \\mathcal{S}\\Vdash^{\\forall a,\\forall\\mathcal{V}}\\phi$ ($\\phi$ est valide dans tout système de la classe $\\mathscr{C}$).\nSoit $\\mathscr{C}$ la classe de tous les systèmes de transition.\n$\\mathscr{C}\\Vdash\\Box(P\\rightarrow Q)\\rightarrow(\\Box P\\rightarrow \\Box Q)$\nSoient $\\mathcal{S}$ un système de transition, $a$ un nœud de $\\mathcal{S}$ et $\\mathcal{V}$ une valuation quelconque.\nSi $\\langle \\mathcal{S},\\mathcal{V}, a\\rangle\\Vdash\\Box(P\\rightarrow Q)$, alors pour tout $b$ tel que $a\\longrightarrow b$, $\\langle \\mathcal{S},\\mathcal{V}, b\\rangle\\Vdash P\\rightarrow Q$.\nPour montrer $\\langle \\mathcal{S},\\mathcal{V}, a\\rangle\\Vdash\\Box P\\rightarrow \\Box Q$, il suffit de supposer $\\langle \\mathcal{S},\\mathcal{V}, a\\rangle\\Vdash\\Box P$ et de montrer $\\langle \\mathcal{S},\\mathcal{V}, a\\rangle\\Vdash\\Box Q$.\nOr, si $\\langle \\mathcal{S},\\mathcal{V}, a\\rangle\\Vdash\\Box P$, alors $\\langle \\mathcal{S},\\mathcal{V}, b\\rangle\\Vdash P$ pour tout $b$ tel que $a\\longrightarrow b$. Et comme par hypothèse $\\langle \\mathcal{S},\\mathcal{V}, b\\rangle\\Vdash P\\rightarrow Q$, on en déduit $\\langle \\mathcal{S},\\mathcal{V}, b\\rangle\\Vdash Q$.\nEt comme c\u0026rsquo;est vrai pour tout $b$ successeur de $a$, on en déduit $\\langle \\mathcal{S},\\mathcal{V}, a\\rangle\\Vdash\\Box Q$.\nPour tout système de transition $\\mathcal{S}$, les affirmations suivantes sont équivalents :\n$\\mathcal{S}$ est réflexif Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\Box\\phi\\rightarrow\\phi)$ $(1)\\Rightarrow(2)$\nSi un nœud $a$ quelconque, pour une valuation quelconque, vérifie $a\\Vdash\\Box \\phi$, alors par définition, pour tout $b$ tel que $a\\longrightarrow b$, $b\\Vdash\\phi$. Puisque $\\mathcal{S}$ est réflexif, $a$ est lui-même un de ces $b$ et donc $a\\Vdash\\phi$. $(2)\\Rightarrow(1)$\nSupposons que $\\mathcal{S}$ ne soit pas réflexif. Il s'en suit qu'il existe un nœud $a$ tel que $a\\,\\,\\not\\!\\!\\longrightarrow a$. Soit $\\phi=P$ une formule de hauteur nulle et $\\mathcal{V}$ une valuation telle que $b\\in\\mathcal{V}(P)$ si et seulement si $a\\longrightarrow b$. Par définition de $\\mathcal{V}$, on a à la fois $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box P$ et $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash P$, donc $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash (\\Box P\\rightarrow P)$. Cela prouve ainsi que $\\mathcal{S}\\nvdash ^{\\forall a,\\forall\\mathcal{V}}(\\Box P\\rightarrow P)$, ce qui contredit l'hypothèse. On note $\\mathscr{C}_{ref.}$ la classe de tous les systèmes de transition réflexifs.\nPour tout système de transition $\\mathcal{S}$, les affirmations suivantes sont équivalents :\n$\\mathcal{S}$ est famélique Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\phi\\rightarrow\\Box\\phi)$ $(1)\\Rightarrow(2)$\nUn système de transition $\\mathcal{S}$ muni de la valuation $\\mathcal{V}$ satisfait la formule $\\phi\\rightarrow\\Box\\phi$ au nœud $a$ si, $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\phi$ entraîne $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\phi$. Autrement dit, dès qu'une formule est satisfaite en un nœud, elle doit être satisfaite en chacun des successeurs de ce nœud.\nSi le seul successeur possible d'un nœud est lui-même (système famélique), l'implication tient toujours. $(2)\\Rightarrow(1)$\nDans le cas où il existe un nœud $a$ possédant un successeur $b$ différent de lui-même, il suffit de considérer la formule $\\phi=P$ et la valuation $\\mathcal{V}$ qui vérifie $a\\in\\mathcal{V}(P)$ et $b\\not\\in\\mathcal{V}(P)$ pour obtenir\u0026nbsp;:\n$\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\phi$ $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\Box\\phi$ $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\phi\\rightarrow\\Box\\phi$ Pour tout système de transition $\\mathcal{S}$, les affirmations suivantes sont équivalents :\n$\\mathcal{S}$ est transitif Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\Box\\phi\\rightarrow \\Box\\Box\\phi )$ $(1)\\Rightarrow(2)$\nSoient $\\mathcal{S}$ un système de transition transitif, $\\mathcal{V}$ une valuation et $a$ un nœud.\nSi $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\phi$ alors pour tout nœud $b$ tel que $a\\longrightarrow b$, $\\langle\\mathcal{S},\\mathcal{V},b\\rangle\\Vdash\\phi$.\nPour montrer $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\Box\\phi$, il suffit de montrer que pour tout $c$ tel qu'il existe $b$ vérifiant $a\\longrightarrow b$ et $b\\longrightarrow c$, on a $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\Vdash\\phi$.\nOr puisque $\\mathcal{S}$ est transitif, $a\\longrightarrow b$ et $b\\longrightarrow c$ impliquent $a\\longrightarrow c$. D'où $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\phi$ entraîne $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\Vdash\\phi$. $(2)\\Rightarrow(1)$\nSoit un système de transition non transitif $\\mathcal{S}$ et soient $a$, $b$ et $c$ trois nœuds de $\\mathcal{S}$ tels que $a\\longrightarrow b$, $b\\longrightarrow c$ et $a\\not\\!\\!\\longrightarrow c$. Soit $\\mathcal{V}$ une valuation qui vérifie $d\\in\\mathcal{V}(P)$ si et seulement si $a\\rightarrow d$.\nPour $\\phi=P$, nous obtenons donc à la fois $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\phi$ et $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\nVdash\\phi$ ce qui montre que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\Box\\Box\\phi$. On note $\\mathscr{C}_{tr.}$ la classe de tous les systèmes de transition transitifs.\nPour tout système de transition $\\mathcal{S}$, les affirmations suivantes sont équivalents :\n$\\mathcal{S}$ est symétrique Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\phi\\rightarrow \\Box\\Diamond\\phi)$ On peut traduire $\\Box\\Diamond\\phi$ par \u0026ldquo;pour tout successeur possible, $\\phi$ est vrai dans au moins un successeur\u0026rdquo;.\n$(1)\\Rightarrow(2)$\nSoient $\\mathcal{S}$ un système de transition symétrique, $\\mathcal{V}$ une valuation et $a$ un nœud.\nOn suppose $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash \\phi$. Pour tout nœud $a$, soit $a$ n'a pas de successeur et la formule $\\phi\\rightarrow \\Box\\Diamond\\phi$ est trivialement vérifiée (toute formule commençant par $\\Box$ est vraie en un nœud sans successeur), soit $a$ possède un ou plusieurs successeurs et chacun d'entre eux possède alors $a$ comme successeur par symétrie du graphe. Tous ces nœuds ont donc au moins un successeur, $a$, où $\\phi$ est vraie. Par conséquent, tous les successeurs de $a$ forcent $\\Diamond \\phi$. Et finalement, on a bien $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash \\Box\\Diamond\\phi$. $(2)\\Rightarrow(1)$\nS'il existe deux nœuds $a$ et $b$ tels que $a\\longrightarrow b$ mais $b\\,\\,\\not\\!\\!\\longrightarrow a$, il suffit de considérer une valuation telle que $a\\Vdash P$ et $c\\Vdash\\neg P$ pour tout nœud $c$ tel que $b\\longrightarrow c$ pour obtenir $a\\nVdash P \\rightarrow\\Box\\Diamond P$. On note $\\mathscr{C}_{sym.}$ la classe de tous les systèmes de transition symétriques.\nPour tout système de transition $\\mathcal{S}$, les affirmations suivantes sont équivalents :\n$\\mathcal{S}$ est dense Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\Box\\Box\\phi\\rightarrow \\Box\\phi)$ $(1)\\Rightarrow(2)$\nSoient $\\mathcal{S}$ un système de transition dense, $\\mathcal{V}$ une valuation et $a$ un nœud.\nSupposons que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\Box\\phi$ est vérifié et montrons que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\phi$ l'est également.\nSoit $c$ un nœud vérifiant $a\\longrightarrow c$. $\\mathcal{S}$ étant dense, il existe $b$ tel que $a\\longrightarrow b$ et $b\\longrightarrow c$. Par conséquent, l'hypothèse $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\Box\\phi$ entraîne aussitôt $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\Vdash \\phi$. $(2)\\Rightarrow(1)$\nSoit un système de transition non dense $\\mathcal{S}$ qui vérifie $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\Box\\Box\\phi\\rightarrow \\Box\\phi)$ pour n'importe quelle formule $\\phi$.\nSi $\\mathcal{S}$ n'est pas dense, il existe deux nœuds $a$ et $c$ tels que $a\\longrightarrow c$ sans qu'aucun $b$ ne vérifie à la fois $a\\longrightarrow b$ et $b\\longrightarrow c$.\nConsidérons la valuation $\\mathcal{V}$ définie par $d\\in\\mathcal{V}(P)$ si et seulement si $d\\neq c$.\nIl s'en suit que si $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\nVdash P$ alors $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\Box P$, et pourtant $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\Box P$. D'où $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\Box\\Box P \\rightarrow \\Box P$. Pour tout système de transition $\\mathcal{S}$, les affirmations suivantes sont équivalents :\n$\\mathcal{S}$ est non borné à droite Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\Box\\phi\\rightarrow \\Diamond\\phi)$ $(1)\\Rightarrow(2)$\nSoient $\\mathcal{S}$ un système de transition non borné à droite, $\\mathcal{V}$ une valuation et $a$ un nœud.\nOn suppose que $a\\Vdash\\Box P$. La seule possibilité pour que $a\\nVdash\\Diamond P$, c'est que le nœud $a$ n'est aucun successeur or l'hypothèse que $\\mathcal{S}$ est non broné à droite nous l'interdit, donc nécessairement $a\\Vdash\\Diamond P$. $(2)\\Rightarrow(1)$\nSi un système de transition n'est pas borné à droite, il existe un nœud $a$ sans successeur. Supposons une valuation $\\mathcal{V}$ telle que seul $a\\in \\mathcal{V}(P)$. On a alors $a\\Vdash \\Box P$ mais aussi $a\\nvdash \\Diamond P$. On note $\\mathscr{C}_{n.b.d.}$ la classe de tous les systèmes de transition non bornés à droite.\nPour tout système de transition $\\mathcal{S}$, les affirmations suivantes sont équivalents :\n$\\mathcal{S}$ est euclidien Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\Diamond\\phi\\rightarrow \\Box\\Diamond\\phi)$ $(1)\\Rightarrow(2)$\nSoient $\\mathcal{S}$ un système de transition euclidien, $\\mathcal{V}$ une valuation et $a$ un nœud.\nS'il existe un nœud $c$ tel que $a\\longrightarrow c$ et $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\Vdash \\phi$, alors pour tout nœud $b$ tel que $a\\longrightarrow b$, il existe un nœud $d$, tel que $b\\longrightarrow d$ et $\\langle\\mathcal{S},\\mathcal{V},d\\rangle\\Vdash \\phi$. En effet, il suffit de prendre $d=c$ puisque le caractère euclidien du graphe assure que de $a\\longrightarrow b$ et $a\\longrightarrow c$, nous obtenons $b\\longrightarrow c$. $(2)\\Rightarrow(1)$\nSi un système de transition n'est pas euclidien, il existe des nœuds $a$, $b$, $c$, avec $a\\longrightarrow b$ et $a\\longrightarrow c$, mais sans arc $b\\longrightarrow c$, alors il suffit de considérer une valuation telle que $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\Vdash P$ et pour tout nœud $d\\neq c$, $\\langle\\mathcal{S},\\mathcal{V},d\\rangle\\nVdash P$. On obtient ainsi $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash \\Diamond P$, mais comme il n'y a pas d'arc de $b$ vers $c$, $\\langle\\mathcal{S},\\mathcal{V},b\\rangle\\nVdash \\Diamond P$ et par conséquent $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash \\Box\\Diamond P$. Soit $\\Gamma$ un ensemble de formules. On écrit :\n$\\mathcal{S}\\Vdash^{\\forall a, \\forall\\mathcal{V}} \\Gamma$ si et seulement si pour toute formule $\\phi\\in\\Gamma$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall\\mathcal{V}} \\phi$\n$\\mathcal{S}$ est un système de transition tel qu\u0026rsquo;en tout nœud et quelle que soit la valuation considérée, il satisfait tout ce que \u0026ldquo;raconte\u0026rdquo; $\\Gamma$. C\u0026rsquo;est donc par sa seule forme que ce système de transition vérifie l\u0026rsquo;ensemble des formules de $\\Gamma$.\n$\\mathscr{C} \\Vdash\\Gamma$ si et seulement si pour toute formule $\\phi\\in\\Gamma$, $\\mathscr{C}\\Vdash \\phi$\nMême affirmation que la précédente mais pour une classe de systèmes de transition toute entière plutôt que pour un seul représentant. En ce sens, la plus grande classe $\\mathscr{C}$ qui vérifie $\\mathscr{C}\\Vdash\\Gamma$ octroie une sorte de caractérisation par la seule forme des graphes de ce que raconte $\\Gamma$.\nConséquences sémantiques Soient $\\Gamma$ un ensemble de formules et $\\mathscr{C}$ une classe de systèmes de transition.\n$\\phi$ est une conséquence sémantique globale de $\\Gamma$ (noté $\\Gamma\\models^{\\forall a}_\\mathscr{C}\\phi$) si et seulement si pour tout système de transition $\\mathcal{S}\\in \\mathscr{C}$, et toute valuation $\\mathcal{V}$, si $\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}\\Gamma$ alors $\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}\\phi$\nPlus simplement, une formule est conséquence d\u0026rsquo;un ensemble d\u0026rsquo;hypothèses si chaque fois que ces hypothèses sont vraies dans un modèle de la classe considérée, alors cette formule est également vraie dans ce modèle.\nSoient $\\Gamma$ un ensemble de formules et $\\mathscr{C}$ une classe de systèmes de transition.\n$\\phi$ est une conséquence sémantique locale de $\\Gamma$ (noté $\\Gamma\\models_\\mathscr{C}\\phi$) si et seulement si pour tout système de transition $\\mathcal{S}\\in \\mathscr{C}$, toute valuation $\\mathcal{V}$, et tout nœud $a\\in N$, si $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Gamma$ alors $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\phi$\nLa conséquence sémantique locale est plus forte que la globale au sens où si $\\Gamma\\models_\\mathscr{C}\\phi$ alors $\\Gamma\\models^{\\forall a}_\\mathscr{C}\\phi$. L\u0026rsquo;inverse est généralement faux comme le montre l\u0026rsquo;exemple suivant :\nSoient $\\mathscr{C}$ la classe de tous les systèmes de transition, $\\Gamma=\\set{P}$ et $\\phi=\\Box P$.\nOn a $\\Gamma\\models^{\\forall a}_\\mathscr{C}\\phi$.\nEn effet, considérons un modèle quelconque $\\langle\\mathcal{S},\\mathcal{V}\\rangle$ qui vérifie $\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}\\Gamma$, ce qui revient ici à affirmer que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash P$ est vraie pour tout nœud $a$. En particulier, pour tout nœud $b$ tel que $a\\longrightarrow b$, la relation $\\langle\\mathcal{S},\\mathcal{V},b\\rangle\\Vdash P$ est vérifiée, ce qui signifie que tout nœud $a$ vérifie $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash \\Box P$.\nPar contre, il est facile de trouver un modèle où $\\Gamma\\models_\\mathscr{C}\\phi$ est faux :\nDans ce modèle, on a $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash P$ et $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash \\Box P$ tip\nPlus prosaïquement, la conséquence locale dit que partout où il y a $\\Gamma$, il y a $\\phi$, alors que la conséquence globale dit que s\u0026rsquo;il y a $\\Gamma$ partout alors il y a $\\phi$ partout.\nIl existe un lien entre les deux conséquences sémantiques :\n$\\Gamma\\models^{\\forall a}_\\mathscr{C}\\phi$ si et seulement si $\\set{\\Box^n\\psi|n\\in\\mathbb{N},\\psi\\in\\Gamma}\\models_\\mathscr{C}\\phi$ (où $\\Box^n\\psi = \\underbrace{\\Box\\Box\\ldots\\Box}_{n}\\psi$ si $n\u0026gt;0$ et $\\psi$ si $n =0$)\n$\\Box^n$ permet en quelque sorte de généraliser la vérité d\u0026rsquo;un nœud puisque dire qu\u0026rsquo;on a $\\Box^n\\psi$ en $a$ implique que $\\psi$ est vraie en tout nœud atteignable depuis $a$.\n$\\Leftarrow$\nSi on a $\\Gamma$ en tout nœud, alors en un nœud $a$ quelconque, on a nécessairement $\\Box^n\\psi$ vraie pour tout $n$ et tout $\\psi$ dans $\\Gamma$. Or par hypothèse, cela implique que $\\phi$ est vraie en ce nœud. Et comme la raisonnement ne dépend pas du nœud $a$, cela montre que $\\phi$ est vraie en tout nœud. $\\Rightarrow$\nDémontrons la contraposée\u0026nbsp;: si $\\set{\\Box^n\\psi|n\\in\\mathbb{N},\\psi\\in\\Gamma}\\not\\models_\\mathscr{C}\\phi$ alors $\\Gamma\\not\\models^{\\forall a}_\\mathscr{C}\\phi$.\nSoient $\\mathcal{S}=(N,A)$ un système de transition, $\\mathcal{V}$ une valuation et $a\\in N$ un nœud, tels que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\set{\\Box^n\\psi|n\\in\\mathbb{N},\\psi\\in\\Gamma}$ alors que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\phi$.\nFabriquons le système de transition $\\mathcal{T}=(N',A')$ qui correspond à la restriction de $\\mathcal{S}$ à l'ensemble des nœuds accessibles depuis $a$ et munissons-le d'une valuation $\\mathcal{V}'$ identique à $\\mathcal{V}$ pour ces nœuds.\nPar construction, dans le modèle $\\langle\\mathcal{T},\\mathcal{V}'\\rangle$ ainsi formé, $\\psi$ est vraie en tout nœud\u0026nbsp;: $\\langle\\mathcal{T},\\mathcal{V}'\\rangle\\Vdash^{\\forall a}\\psi$. Cela entraîne que $\\langle\\mathcal{T},\\mathcal{V}'\\rangle\\Vdash^{\\forall a}\\Gamma$, et pourtant $\\langle\\mathcal{T},\\mathcal{V}',a\\rangle\\nVdash\\phi$ par hypothèse, ce qui implique $\\Gamma\\not\\models^{\\forall a}_\\mathscr{C}\\phi$. Si $\\mathscr{C}_\\phi$ et $\\mathscr{C}_\\psi$ désignent respectivement tous les systèmes dans lesquels $\\phi$ est vraie et tous ceux où $\\psi$ est vraie, alors $\\phi\\models^{\\forall a}_\\mathscr{C}\\psi$ est équivalent à l\u0026rsquo;inclusion $\\mathscr{C}_\\phi\\subseteq\\mathscr{C}_\\psi$.\nDonc si on a à la fois $\\phi\\models^{\\forall a}_\\mathscr{C}\\psi$ et $\\psi\\models^{\\forall a}_\\mathscr{C}\\phi$, alors $\\mathscr{C}_\\phi = \\mathscr{C}_\\psi$.\nThéories, équivalence et bisimulation On appelle théorie du modèle $\\langle\\mathcal{S},\\mathcal{V}\\rangle$ l\u0026rsquo;ensemble de toutes les formules qui sont vraies dans ce modèle : $$Th_\\mathcal{M}=\\set{\\phi|\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}\\phi}$$\nLa théorie d\u0026rsquo;un modèle est composée de toutes les formules décrivant ce modèle.\nDeux modèles sont distinguables par la logique modale s\u0026rsquo;ils n\u0026rsquo;ont pas la même théorie.\nOn appelle théorie locale d\u0026rsquo;un modèle $\\langle\\mathcal{S},\\mathcal{V}\\rangle$ au nœud $a$ l\u0026rsquo;ensemble de toutes les formules qui sont vraies dans ce modèle : $$Th_{(\\mathcal{M},a)}=\\set{\\phi|\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\phi}$$\nSoient $\\mathcal{M}$ et $\\mathcal{N}$ deux modèles de la logique modale.\n$\\mathcal{M}$ et $\\mathcal{N}$ sont équivalents s'ils ont la même théorie\u0026nbsp;:\n$\\mathcal{M} \\equiv \\mathcal{N}$ si et seulement si $Th_\\mathcal{M}=Th_\\mathcal{N}$ $\\mathcal{M}$ et $\\mathcal{N}$ sont localement équivalents s'ils partagent la même théorie locale\u0026nbsp;:\n$(\\mathcal{M},a) \\equiv (\\mathcal{N},b)$ si et seulement si $Th_{(\\mathcal{M},a)}=Th_{(\\mathcal{N},b)}$ Les deux modèles suivants sont équivalents ($\\mathcal{M} \\equiv \\mathcal{N}$).\nEt par ailleurs, $(\\mathcal{M},a) \\equiv (\\mathcal{N},a')$, $(\\mathcal{M},b) \\equiv (\\mathcal{N},b')$, $(\\mathcal{M},b) \\equiv (\\mathcal{N},c')$.\nLa bisimulation va nous permettre de reconnaître que ces modèles sont localement équivalents. Deux modèles correspondent en quelque sorte à deux univers parallèles. Mais il se peut que depuis un des mondes du premier univers, tout paraisse exactement identique à ce que l\u0026rsquo;on perçoit depuis un monde du deuxième univers. Bien que les univers soient différents, ses \u0026ldquo;habitants\u0026rdquo; ne peuvent pas les différentier. On dit qu\u0026rsquo;ils sont indiscernables ou bisimilaires. Deux mondes sont indiscernables s\u0026rsquo;il n\u0026rsquo;existe pas de formule vraie dans l\u0026rsquo;un et fausse dans l\u0026rsquo;autre.\nUn jeu à deux joueurs à information parfaite, le jeu de bisimulation, est défini de telle sorte que le joueur appelé Duplicateur possède une stratégie gagnante si et seulement si les deux mondes (associés à leurs modèles respectifs) qui interviennent dans le jeu sont indiscernables.\nPlus besoin de passer en revue toutes les formules possibles pour déterminer si deux mondes sont ou non discernables\u0026hellip;\nSoient $\\mathcal{M}=\\langle\\mathcal{S}=(N,A) , \\mathcal{V}\\rangle$ et $\\mathcal{N}=\\langle\\mathcal{T}=(M,B),\\mathcal{W}\\rangle$ deux modèles de la logique modale et soient $a\\in N$ et $b\\in M$ deux nœuds.\nLe jeu de bisimulation $\\mathbb{Bis}((\\mathcal{M},a),(\\mathcal{N},b))$ est défini comme suit :\nil comprend deux joueurs appelés Duplicateur et Corrupteur.\nLe Duplicateur ($\\text{D}_{up}$) cherche à montrer que les deux modèles sont localement bisimilaires, alors que le Corrupteur ($\\text{C}_{or}$) au contraire veut montrer qu'ils ne le sont pas.\nUn coup, pour chacun des joueurs, consiste à choisir un nœud dans un système de transition. Les nœuds de départ étant $a$ dans $\\mathcal{S}$ et $b$ dans $\\mathcal{T}$. À chaque tour, pour une position $a'$ dans $\\mathcal{M}$ et une position $b'$ dans $\\mathcal{N}$, $\\text{C}_{or}$ choisit d'abord de jouer dans $\\mathcal{S}$ ou dans $\\mathcal{T}$ puis il choisit un arc permettant d'arriver à un successeur du nœud $a'$ ou $b'$ et $\\text{D}_{up}$ répond en choisissant un successeur dans l'autre modèle tel que les nœuds choisit dans chaque modèle satisfont les mêmes variables propositionnelles. Fin de la partie\u0026nbsp;: S'il existe une variable $P$ telle que la position initiale vérifie $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash P$ si et seulement si $\\langle\\mathcal{T},\\mathcal{W},b\\rangle\\nVdash P$, alors $\\text{C}_{or}$ l'emporte. Si un joueur ne peut plus avancer alors il perd la partie. Si la partie est infinie, $\\text{D}_{up}$ l'emporte. Exemple 1 :\nLe Corrupteur possède une stratégie gagnante\u0026nbsp;: $\\text{C}_{or}$ choisit de joueur dans $\\mathcal{N}$ puis va en $b'$. $\\text{D}_{up}$ n'a pas d'autre choix que d'aller en $b$. $\\text{C}_{or}$ choisit à nouveau de joueur dans $\\mathcal{N}$ et va en $c'$. $\\text{D}_{up}$ perd car il ne peut plus avancer. Exemple 2 :\nLe Duplicateur possède une stratégie gagnante\u0026nbsp;: Si $\\text{C}_{or}$ choisit d'aller en $b'$ ou en $c'$, $\\text{D}_{up}$ va en $b$ (il n'a pas trop le choix en même temps...). Si $\\text{C}_{or}$ choisit d'aller en $b$, $\\text{D}_{up}$ va en $b'$ ou en $c'$ (c'est sans importance). Le Corrupteur ne peut pas jouer son deuxième coup et perd la partie. Exemple 3 :\nLe modèle $\\mathcal{N}$ est défini par\u0026nbsp;: l'ensemble des nœuds est $\\mathbb{N}$, de tout nœud $n\\in\\mathbb{N}$ part deux arcs\u0026nbsp;: $n\\longrightarrow n+1$ et $n\\longrightarrow n+2$ $2n\\Vdash P$ et $2n+1\\nVdash P$ Le Duplicateur possède une stratégie gagnante qui consiste à tout moment de la partie\u0026nbsp;: si $\\text{C}_{or}$ joue dans $\\mathcal{N}$ et va sur $n$, à aller sur $a$ si $n$ est pair et sur $b$ sinon, si $\\text{C}_{or}$ joue dans $\\mathcal{M}$ et va sur $a$, à aller sur $n+2$ si $n$ est pair et sur $n+1$ sinon, si $\\text{C}_{or}$ joue dans $\\mathcal{M}$ et va sur $b$, à aller sur $n+1$ si $n$ est pair et sur $n+2$ sinon, De la sorte, le jeu continue indéfiniment. Théorème de Henessy-Milner : $(\\mathcal{M},a)\\equiv(\\mathcal{N},b)$ si et seulement si $\\text{D}_{up}$ a une stratégie gagnante dans $\\mathbb{Bis}((\\mathcal{M},a),(\\mathcal{N},b))$.\nSuite : Systèmes logiques "
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique5/",
	"title": "Logique modale",
	"tags": [],
	"description": "",
	"content": " info\nNotes de lecture du livre La logique pas à pas de Jacques Duparc que je paraphrase allégrement.\nLogique modale Syntaxe et sémantique Systèmes logiques Différentes logiques modales Axiomatique : les systèmes logiques Nous cherchons l\u0026rsquo;équivalent en logique modale des tautologies pour le calcul des propositions ; des formules vraies dans tous les modèles, ou plutôt dans toute une classe de modèles car comme on va le voir, la topologie d\u0026rsquo;un modèle contraint la vérité à l\u0026rsquo;intérieur de celui-ci.\nLa logique de la classe $\\mathscr{C}$ est la classe de toutes les formules qui sont valides dans tous les systèmes de transition de $\\mathscr{C}$ : $\\mathbb{Log}_\\mathscr{C}=\\set{\\phi|\\text{pour tout }\\mathcal{S}\\in\\mathscr{C},\\mathcal{S}\\Vdash^{\\forall a,\\forall \\mathcal{V}} \\phi}$\nCette logique doit contenir toutes les formules qui sont des tautologies du calcul des propositions puisque ces formule de profondeur modale nulle (pas d\u0026rsquo;opérateurs modaux) sont nécessairement forcées en tout nœud de n\u0026rsquo;importe quel modèle.\nSi la logique contient la formule $\\phi$, elle doit aussi contenir $\\Box\\phi$. En effet, si $\\phi$ est forcée en tout nœud, alors le successeur de tout nœud force aussi $\\phi$. On appelle ça la nécessitation (ou généralisation) d'une formule. De plus, si $\\phi$ appartient à la logique, la satisfaction de $\\phi$ en un nœud $a$ donné est indépendante de la valuation $\\mathcal{V}$ (que $P$ soit vraie ou fausse, $P$ est forcée au nœud $a$). Par conséquent, que $\\psi$ soit vraie ou fausse, la satisfaction de la formule $\\phi[\\psi/P]$ ne varie pas. Toutes substitutions uniformes opérées sur les variables d'une formule appartenant à la logique sera aussi dans la logique. Enfin, la logique de la classe de tous les systèmes de transition est close par modus ponens\u0026nbsp;; si $\\phi \\rightarrow\\psi$ et $\\psi$ appartiennent toutes deux à la logique, alors $\\psi$ également. En effet, le modus ponens préserve la validité puisque si un modèle vérifie en un noud à la fois $\\psi$ et $\\psi\\rightarrow\\phi$, alors (par définition de l'interprétation de l'implication) il vérifie également $\\phi$. Pour déterminer des nouvelles tautologies en calcul des propositions, on a utilisé des systèmes de déduction (à la Hilbert, déduction naturelle, calcul des séquents). La modalité complique un peu les choses et seul le système à la Hilbert va pouvoir s\u0026rsquo;adapter facilement.\nUn axiome est une formule. Un système formel est un ensemble d'axiomes. Soient $I$ un ensemble non vide, $\\phi$ une formule et $\\textbf{S}$ un système formel.\nUne preuve de la formule $\\varphi$ dans le système formel $\\textbf{S}$ est une suite finie de formule $\\langle\\varphi_1,\\varphi_2,\\ldots,\\varphi_n\\rangle$ telle que :\n$\\varphi_n=\\phi$ chaque $\\varphi_l$ vérifie l'une des quatre conditions suivantes\u0026nbsp;: $\\varphi_l \\in \\textbf{S}$ ($\\varphi_l$ est un axiome) $\\varphi_l$ est obtenue par application de la règle du modus ponens à deux formules d'indice inférieurs $\\varphi_j$ et $\\varphi_k$ où $j , k \u003c l$ ($\\varphi_j = \\varphi_k\\rightarrow\\varphi_l$) $\\varphi_l = \\varphi_k[\\theta /P]$ où $k \u003c l$ (substitution d'une variable $P$ quelconque par une formule $\\theta$ quelconque dans une formule d'indice inférieure) $\\varphi_l = [i]\\varphi_j$ où $j \u003c l$ ($\\varphi_l$ est obtenue à partir de l'application de la règle de nécessitation appliquée à une formule d'indice inférieure pour une étiquette $i\\in I$ quelconque) Exemple 1 :\nSi $\\textbf{S}$ désigne l\u0026rsquo;ensemble des tautologies du calcul des propositions, alors la suite\n$$\\langle P\\rightarrow(P\\lor\\neg P),\\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q),\\Box (\\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q)),\\Box (\\Diamond \\phi\\rightarrow(\\Diamond \\phi\\rightarrow\\lor\\neg\\Diamond \\phi))\\rangle$$ est une preuve de $\\Box (\\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q)),\\Box (\\Diamond \\phi\\rightarrow(\\Diamond \\phi\\rightarrow\\lor\\neg\\Diamond \\phi)$ dans ce système. En effet :\n$\\varphi_1=P\\rightarrow(P\\lor\\neg P)$ est une tautologie du calcul des propositions et donc un axiome du système formel $\\textbf{S}$, $\\varphi_2 = \\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q) = P\\rightarrow(P\\lor\\neg P)[\\Diamond Q /P]$ est la substitution de la formule $\\Diamond Q$ à $P$ dans $\\varphi_0$, $\\varphi_3 = \\Box (\\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q))$ est obtenu par nécessitation de $\\varphi_1$, $\\varphi_4 = \\Box (\\Diamond \\phi\\rightarrow(\\Diamond \\phi\\rightarrow\\lor\\neg\\Diamond \\phi) = \\varphi_3[\\phi/Q]$ C\u0026rsquo;est plus élégant de représenter l\u0026rsquo;enchaînement sous forme d\u0026rsquo;arbre :\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$P\\rightarrow(P\\lor\\neg P)$} \\RightLabel{$\\scriptsize\\;sub.$} \\UnaryInfC{$\\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q)$} \\RightLabel{$\\scriptsize\\;nec.$} \\UnaryInfC{$\\Box (\\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q))$} \\RightLabel{$\\scriptsize\\;sub.$} \\UnaryInfC{$\\Box (\\Diamond \\phi\\rightarrow(\\Diamond \\phi\\rightarrow\\lor\\neg\\Diamond \\phi)$} \\end{prooftree} $$ Exemple 2 :\nSi $\\textbf{S}$ désigne l\u0026rsquo;ensemble des tautologies du calcul des propositions auquel on ajoute la formule $P\\rightarrow\\Diamond P$, alors l\u0026rsquo;arbre ci-dessous décrit une preuve de $\\Box(\\phi\\rightarrow\\Diamond\\Diamond\\phi)$ dans ce système :\nSoient $\\textbf{S}$ un système formel et $\\Gamma$ un ensemble de formules appelées hypothèses.\nOn écrit $\\Gamma\\vdash_\\textbf{S}\\phi$ s\u0026rsquo;il existe un ensemble fini de formules $\\set{\\psi_1,\\psi_2,\\ldots,\\psi_n}\\subseteq\\Gamma$ tel que la formule $(\\psi_1\\land\\psi_2\\land\\ldots\\land\\psi_n)\\rightarrow\\phi$ puisse être prouvée dans le système formel $\\textbf{S}$. Autrement dit le séquent $\\vdash_\\textbf{S}(\\psi_1\\land\\psi_2\\land\\ldots\\land\\psi_n)\\rightarrow\\phi$ est vérifiée pour un nombre fini de formules bien choisies de $\\Gamma$.\ninfo\nAttention, ici les hypothèses n\u0026rsquo;ont pas le même sens qu\u0026rsquo;en logique des propositions. En effet, pour prouver le séquent $\\phi\\vdash_\\textbf{S}\\psi$, on ne peut pas ajouter $\\phi$ aux axiomes et tenter d\u0026rsquo;atteindre $\\psi$ à partir des règles (modus ponens, substitution uniforme et nécessitation). Il faut ici réussir à prouver la formule $\\phi\\rightarrow\\psi$ sur la base des seuls axiomes. Si on pouvait faire comme en logique des propositions, prouver $\\phi\\vdash_\\textbf{S}\\Box\\phi$ deviendrait évident : on place $\\phi$ dans les axiomes et on applique la règle de nécessitation. Mais non, il faut pouvoir prouver $\\vdash_\\textbf{S}\\phi\\rightarrow\\Box\\phi$ et comme on va le voir, dans le système $\\textbf{K}$, la formule $P\\rightarrow\\Box P$ n\u0026rsquo;est pas prouvable ($P\\nvdash_\\textbf{K}\\Box P$).\nLe système K Le système $\\textbf{K}$ est le plus petit système formel qui contient :\nchaque tautologie du calcul des propositions $\\text{(K)}$\u0026nbsp;: $\\Box(P\\rightarrow Q)\\rightarrow (\\Box P\\rightarrow \\Box Q)$ $\\text{(dual)}$\u0026nbsp;: $\\Diamond P\\leftrightarrow\\neg\\Box\\neg P$ Le système $\\textbf{K}$ est minimal en ce sens qu\u0026rsquo;il est validé dans tout modèle.\nPour tout modèle $\\mathcal{M}=\\langle\\mathcal{S},\\mathcal{V}\\rangle$ et tout nœud $a$, si $a\\Vdash\\Box(P\\rightarrow Q)$ et $a\\Vdash\\Box P$, cela signifie que pour tout nœud $b$ tel que $a\\longrightarrow b$, $b\\Vdash P\\rightarrow Q$ et $b\\Vdash P$, par conséquent $b\\Vdash Q$ et donc $a\\Vdash \\Box Q$.\nCela montre que l\u0026rsquo;axiome $\\text{(K)}$ est valide dans tous les systèmes de transition.\nPour montrer que $P\\rightarrow\\Box P$ n\u0026rsquo;est pas prouvable dans $\\textbf{K}$, il suffit donc de présenter un modèle (et le système $\\textbf{K}$ autorise tous les systèmes de transition) où l\u0026rsquo;implication est fausse pour au moins un nœud. Or on a déjà vu un tel modèle :\nDans ce modèle, on a $a\\Vdash P$ mais aussi $a\\nVdash \\Box P$ et par conséquent $a\\nVdash P\\rightarrow\\Box P$. Cet axiome $\\text{(K)}$ (K en l\u0026rsquo;honneur de Kripke) est appelé axiome de distribution car il permet de distribuer l\u0026rsquo;opérateur modal $\\Box$ à l\u0026rsquo;intérieur de l\u0026rsquo;implication. On l\u0026rsquo;appelle aussi parfois axiome d\u0026rsquo;omniscience logique car il stipule que si un agent sait que $P$ implique $Q$, alors s’il sait également que $P$ il doit savoir que $Q$. En d’autres termes, l\u0026rsquo;agent doit connaître toutes les conséquences logiques de ce qu\u0026rsquo;il sait déjà.\nL\u0026rsquo;axiome $\\text{(dual)}$ permet, lui, de passer d\u0026rsquo;un opérateur modal à l\u0026rsquo;autre et est aussi valide dans tout sytème de transition (être sûr de la présence d\u0026rsquo;un truc revient à ne pas être sûr de son absence, ou pour rester sur la structure du système de transition, dire qu\u0026rsquo;il existe un chemin vers $P$ revient à ne pas dire que tous les chemins vont vers $\\neg P$).\nLa relation sœur permettant de passer de l\u0026rsquo;opérateur boite à l\u0026rsquo;opérateur diamant $\\Box P \\leftrightarrow \\neg\\Diamond\\neg P$ (si tous les chemins mènent à $P$, il n\u0026rsquo;existe pas de chemin menant à $\\neg P$) est elle aussi vraie partout.\nCes deux relations soulignent bien la nature duale des opérateurs $\\Diamond$ et $\\Box$.\nMais ce système formel n\u0026rsquo;est pas encore suffisamment riche pour contenir toutes les formules valides dans tous les modèles quels qu\u0026rsquo;ils soient. Pour cela, il faudrait également considérer toutes les substitutions uniformes possibles (entre autre dans la formule $\\text{(K)}$). On passe alors aux logiques modales.\nLogique modale normale Une logique modale normale $\\mathbb{L}$ est un ensemble de formules qui contient le système formel $\\textbf{K}$ et qui est clos par :\nmodus ponens $$ \\begin{prooftree} \\AxiomC{$\\phi$} \\AxiomC{$\\phi\\rightarrow\\psi$} \\RightLabel{$\\scriptsize\\;mod.p.$} \\BinaryInfC{$\\psi$} \\end{prooftree} $$ substitution uniforme $$ \\begin{prooftree} \\AxiomC{$\\phi$} \\RightLabel{$\\scriptsize\\;sub.$} \\UnaryInfC{$\\phi[\\theta/P]$} \\end{prooftree} $$ nécessitation $$ \\begin{prooftree} \\AxiomC{$\\phi$} \\RightLabel{$\\scriptsize\\;nec.$} \\UnaryInfC{$\\Box\\phi$} \\end{prooftree} $$ Toute logique modale normale contient chaque formule $\\phi$ prouvable dans le système formel $\\textbf{K}$ (si $\\vdash_\\textbf{K}\\phi$ alors $\\phi$ fait partie de toute logique modale normale).\nLa plus petite logique modale normale est construite sur la base du système formel $\\textbf{K}$. On la note $\\mathbb{L}_\\textbf{K}$. Le lien entre la logique normale $\\mathbb{L}_\\textbf{K}$ et le système formel $\\textbf{K}$ est le suivant : $\\phi \\in \\mathbb{L}_\\textbf{K}$ ssi $\\vdash_\\textbf{K}\\phi$.\nQuelques axiomes usuels $\\text{(K) :}$ $\\Box(P\\rightarrow Q)\\rightarrow(\\Box P\\rightarrow \\Box Q)$\nOn l\u0026rsquo;a vu, l\u0026rsquo;axiome $\\text{(K)}$ est à la base des logiques modales normales.\n$\\text{(D) :}$ $\\Box P\\rightarrow \\Diamond P$\nLa formule $\\text{(D)}$ dit que si quelque chose est nécessaire, alors elle doit être possible. On peut aussi l\u0026rsquo;interpréter différemment dans d\u0026rsquo;autres types de logiques que nous allons entrevoir ensuite : en logique déontique, la formule dit que quelque chose d\u0026rsquo;obligatoire doit être permis ; en logique épistémique, elle dit que si je sais $P$, alors il est faux que je sache $\\neg P$ ; et en logique doxastique, que si je crois que $P$, alors il est faux que je crois que $\\neg P$.\n$\\text{(D)}$ est parfois appelé axiome de consistance.\nComme on l\u0026rsquo;a vu plus tôt, un système de transition qui valide $\\text{(D)}$ n\u0026rsquo;admet pas d\u0026rsquo;impasse. Autrement dit, les arcs doivent être non bornés à droite.\n$\\text{(T) :}$ $\\Box P\\rightarrow P$\nLa formule $\\text{(T)}$ dit que si quelque chose est nécessaire, alors elle est. On l\u0026rsquo;appelle parfois axiome de factivité.\nEn logique déontique : ce qui est obligatoire est.\nEn logique épistémique : si je sais $P$, alors $P$ est avéré.\nEn logique doxastique : si je crois que $P$, alors $P$ est également avéré.\nEt nous avons vu qu\u0026rsquo;un système de transition valide $\\text{(T)}$ si et seulement si ce système est réflexif. $\\text{(T)}$ est donc aussi l\u0026rsquo;axiome de réflexivité.\n$\\text{(B) :}$ $P\\rightarrow \\Box\\Diamond P$\nLa formule $\\text{(B)}$ dit que si quelque chose est vraie, alors il est nécessaire qu\u0026rsquo;elle soit possible.\nEn logique déontique : ce qui est, est obligatoirement permis.\nEn logique épistémique : si je $P$ est avéré, alors je sais que je ne sais pas $\\neg P$.\nEn logique doxastique : si $P$ est avéré, alors je crois que je ne crois pas $\\neg P$.\nEt nous avons vu qu\u0026rsquo;un système de transition valide $\\text{(B)}$ si et seulement si ce système est symétrique. $\\text{(B)}$ est donc aussi l\u0026rsquo;axiome de symétrie.\n$\\text{(4) :}$ $\\Box P\\rightarrow \\Box\\Box P$\nLa formule $\\text{(4)}$ dit que si quelque chose est nécessaire, alors elle est nécessairement nécessaire.\nEn logique déontique : si quelque chose est obligatoire, c\u0026rsquo;est obligatoire qu\u0026rsquo;elle le soit.\nEn logique épistémique : si je sais $P$, alors je sais que je sais $P$. Et transitivement, on sait que l\u0026rsquo;on sait que l\u0026rsquo;on sait\u0026hellip; Autrement dit, avec la formule (4) nous décrivons des agents parfaitement rationnel.\nOn appelle $\\text{(4)}$ l\u0026rsquo;axiome d\u0026rsquo;introspection positive dans le cadre de la logique épistémique.\nEt nous avons vu qu\u0026rsquo;un système de transition valide $\\text{(4)}$ si et seulement si ce système est transitif. $\\text{(4)}$ est donc aussi l\u0026rsquo;axiome de transitivité.\n$\\text{(5) :}$ $\\Diamond P\\rightarrow \\Box\\Diamond P$\nLa formule $\\text{(5)}$ dit que s\u0026rsquo;il n\u0026rsquo;est pas nécessaire que quelque chose ne soit pas, alors elle est nécessaire.\nEn logique déontique : si quelque chose est permise, c\u0026rsquo;est obligatoire qu\u0026rsquo;elle soit permise.\nEn logique épistémique : si je ne sais pas quelque chose, alors je sais que je ne le sais pas. L\u0026rsquo;interprétation épistémique décrit un agent parfaitement conscient de ce qu\u0026rsquo;il ne sait pas.\nEn logique doxastique : si je ne crois pas quelsue chose, alors je crois que je ne le crois pas. L\u0026rsquo;interprétation doxastique décrit un agent parfaitement conscient de ce qu\u0026rsquo;il ne croit pas.\nOn appelle $\\text{(5)}$ l\u0026rsquo;axiome d\u0026rsquo;introspection négative dans le cadre de la logique épistémique.\nEt nous avons vu qu\u0026rsquo;un système de transition valide $\\text{(5)}$ si et seulement si ce système est euclidien. $\\text{(5)}$ est donc aussi l\u0026rsquo;axiome d\u0026rsquo;euclidité.\nEn prenant ensemble les formules $\\text{(5)}$ et $\\text{(T)}$, on regarde des systèmes de transition à la fois euclidiens et réflexifs. Or la conjonction de ces deux propriétés est équivalentes à la conjonction des trois propriétés d\u0026rsquo;une relation d\u0026rsquo;équivalence !\nUn système de transition est à la fois euclidien et réflexif si et seulement si il est à la fois réflexif, symétrique et transitif.\nAutrement dit, un graphe euclidien et réflexif est en fait ni plus ni moins qu\u0026rsquo;un graphe dans lequel la relation d\u0026rsquo;accessibilité est une relation d\u0026rsquo;équivalence (relation définie par les trois propriétés réflexivité, symétrie et transitivité).\n$\\Rightarrow$\u0026nbsp;:\nréflexif\u0026nbsp;: c'est immédiat symétrique\u0026nbsp;: si on a un arc $a\\longrightarrow b$, alors par réflexivité, on a aussi $a\\longrightarrow a$, et le caractère euclidien impose donc $b\\longrightarrow a$. transitif\u0026nbsp;: supposons qu'on ait deux arcs $a\\longrightarrow b$ et $b\\longrightarrow c$. Comme on a montré la symétrie du graphe, on a aussi un arc $b\\longrightarrow a$ et le caractère euclidien impose donc un arc $a\\longrightarrow c$. $\\Leftarrow$\u0026nbsp;:\nréflexif\u0026nbsp;: c'est immédiat euclidien\u0026nbsp;: supposons qu'on ait deux arcs $a\\longrightarrow b$ et $a\\longrightarrow c$. La symétrie donne un arc $b\\longrightarrow a$ et la transitivité impose alors un arc $b\\longrightarrow c$. Quelques systèmes formels et logiques usuels Par la suite, nous confonderons un système formel et la logique qu\u0026rsquo;il induit par clôture. Les systèmes formels qu\u0026rsquo;on va introduire sont tous des extensions du système $\\textbf{K}$.\n$\\textbf{K}$ $\\textbf{KD}$\u0026nbsp;: $\\textbf{K}\\cup\\set{\\text{(D)}}$ $\\textbf{KB}$\u0026nbsp;: $\\textbf{K}\\cup\\set{\\text{(B)}}$ $\\textbf{KT}$\u0026nbsp;: $\\textbf{K}\\cup\\set{\\text{(T)}}$ $\\textbf{K4}$\u0026nbsp;: $\\textbf{K}\\cup\\set{\\text{(4)}}$ $\\textbf{KB4}$\u0026nbsp;: $\\textbf{K}\\cup\\set{\\text{(B),(4)}}$ $\\textbf{KD4}$\u0026nbsp;: $\\textbf{K}\\cup\\set{\\text{(D),(4)}}$ $\\textbf{KDB}$\u0026nbsp;: $\\textbf{K}\\cup\\set{\\text{(D),(B)}}$ $\\textbf{KTB}$\u0026nbsp;: $\\textbf{K}\\cup\\set{\\text{(T),(B)}}$ $\\textbf{S4} = \\textbf{KT4}$\u0026nbsp;: $\\textbf{K}\\cup\\set{\\text{(T),(4)}}$ $\\textbf{S5} = \\textbf{KT5}$\u0026nbsp;: $\\textbf{K}\\cup\\set{\\text{(T),(5)}}$ Convainquons-nous dans un premier temps que ces axiomes apportent bien quelque chose de plus en montrant par exemple que la logique $\\textbf{KB}$ ne permet pas de prouver l\u0026rsquo;axiome $\\text{(4)}$.\nComme $\\text{(B)}$ est vraie dans tout système de transition symétrique, il suffit de trouver un modèle symétrique où au moins un nœud ne satisfait pas $\\text{(4)}=\\Box P\\rightarrow\\Box\\Box P$ pour montrer que $\\textbf{K4}\\not\\subseteq \\textbf{KB}$.\nComme $P$ est satisfait dans tout les successeurs de $a$, $a\\Vdash\\Box P$. Mais pour avoir $a\\Vdash\\Box\\Box P$, il faudrait que tous les successeurs de $a$ satisfassent $\\Box P$, or ce n'est pas le cas de $b$ (qui a pour successeur $a$ lui-même par symétrie et $a\\nVdash P$). D'où $a\\nVdash \\Box\\Box P$. De même, montrons que $\\textbf{KTB}$ ne peut prouver ni $\\text{(4)}$ ni $\\text{(5)}$.\n$\\text{(T)}$ et $\\text{(B)}$ seront vraies ensemble dans tout système de transition à la fois réflexif et symétrique. Exhibons un tel modèle où au moins un nœud ne satisfait ni $\\text{(4)}=\\Box P\\rightarrow\\Box\\Box P$ ni $\\text{(5)}=\\Diamond P\\rightarrow\\Box\\Diamond P$.\nOn constate que dans ce modèle réflexif et symétrique, $a$ ne satisfait pas $\\text{(4)}$ et $b$ ne satisfait pas une instance de $\\text{(5)}$ où $P=\\neg Q$. Mais on va préférer dans la suite mettre au jour les inclusions inverses (qui contient qui) grâce à la notion de réduction.\nSoient $\\textbf{S}$ et $\\textbf{S\u0026rsquo;}$ deux systèmes formels et $\\phi$ une formule quelconque,\n$\\textbf{S}≤\\textbf{S'}$ si et seulement si $\\vdash_\\textbf{S}\\phi\\rightarrow \\,\\vdash_\\textbf{S'}\\phi$ Cette relation est réflexive ($\\textbf{S}≤\\textbf{S}$) et transitive (si $\\textbf{S}≤\\textbf{S\u0026rsquo;}$ et $\\textbf{S\u0026rsquo;}≤\\textbf{S\u0026rsquo;\u0026rsquo;}$ alors $\\textbf{S}≤\\textbf{S\u0026rsquo;\u0026rsquo;}$).\n$\\textbf{S}≤\\textbf{S\u0026rsquo;}$ se lit $\\textbf{S}$ se réduit à $\\textbf{S\u0026rsquo;}$. Tout ce qui est prouvé à l\u0026rsquo;aide de $\\textbf{S}$ peut également l\u0026rsquo;être à l\u0026rsquo;aide de $\\textbf{S\u0026rsquo;}$.\nSi $\\textbf{S}$ se réduit à $\\textbf{S\u0026rsquo;}$, cela signifie que $\\textbf{S\u0026rsquo;}$ permet de prouver au moins autant de formules (appelées théorèmes) que $\\textbf{S}$ et par conséquent, que la classe des systèmes de transition dans lesquels tous les théorèmes de $\\textbf{S\u0026rsquo;}$ sont valides est incluse dans celle des systèmes de transition qui tous valident les théorèmes de $\\textbf{S}$ (c\u0026rsquo;est plus dur de raconter plus de choses, ça restreint l\u0026rsquo;ensemble des modèles possibles).\nAutrement dit,\nsi $\\mathscr{C}_\\textbf{S}$ et $\\mathscr{C}_\\textbf{S\u0026rsquo;}$ désignent respectivement la classe de tous les systèmes de transition qui valident $\\textbf{S}$ et $\\textbf{S\u0026rsquo;}$ :\n$\\textbf{S}≤\\textbf{S'}$ si et seulement si $\\mathscr{C}_\\textbf{S'}\\subseteq\\mathscr{C}_\\textbf{S}$. Par contre, les logiques modales normales étant des ensembles de formules satisfaites, pour elles l\u0026rsquo;inclusion est dans l\u0026rsquo;autre sens :\nLorsque $\\textbf{S}$ et $\\textbf{S\u0026rsquo;}$ contiennent tous les deux le système formel $\\textbf{K}$, les logiques modales normales qu\u0026rsquo;ils engendrent ($\\mathbb{L}_\\textbf{S}$ et $\\mathbb{L}_\\textbf{S\u0026rsquo;}$) vérifient la relation suivante :\n$\\textbf{S}≤\\textbf{S'}$ si et seulement si $\\mathbb{L}_\\textbf{S}\\subseteq\\mathbb{L}_\\textbf{S'}$. On obtient alors immédiatement les réductions suivantes :\n$\\textbf{K}≤\\textbf{K4}$ $\\textbf{K}≤\\textbf{KB}$ $\\textbf{K}≤\\textbf{KD}$ $\\textbf{K4}≤\\textbf{KB4}$ $\\textbf{K4}≤\\textbf{KD4}$ $\\textbf{KB}≤\\textbf{KB4}$ $\\textbf{KB}≤\\textbf{KDB}$ $\\textbf{KD}≤\\textbf{KDB}$ $\\textbf{KD}≤\\textbf{KD4}$ $\\textbf{KT}≤\\textbf{KTB}$ $\\textbf{KT}≤\\textbf{KT4}$ Montrons maintenant que :\n$\\textbf{KD}≤\\textbf{KT}$ Il suffit de montrer que la formule $\\text{(D)}$ peut être prouvée grâce au système $\\textbf{KT}$, c\u0026rsquo;est-à-dire : $\\vdash_\\textbf{KT} \\Box P\\rightarrow\\Diamond P$.\nOn en déduit immédiatement :\n$\\textbf{KDB}≤\\textbf{KTB}$ $\\textbf{KD4}≤\\textbf{ KT4=S4}$ On peut aussi montrer que :\n$\\textbf{KTB}≤\\textbf{S5}$ Il suffit de montrer que la formule $\\text{(B)}$ peut être prouvée grâce au système $\\textbf{KT5}$, c\u0026rsquo;est-à-dire : $\\vdash_\\textbf{S5} P\\rightarrow\\Box\\Diamond P$.\nMontrons d\u0026rsquo;abord que $\\vdash_\\textbf{KT}P\\rightarrow\\Diamond P$, ce qui montre également que $\\vdash_\\textbf{KT5}P\\rightarrow\\Diamond P$.\nPuis utilisons ce résultat pour montrer que $\\vdash_\\textbf{S5}P\\rightarrow\\Box\\Diamond P$ $\\textbf{S4}≤\\textbf{S5}$ Il suffit de montrer que la formule $\\text{(4)}$ peut être prouvée grâce au système $\\textbf{KT5}$, c\u0026rsquo;est-à-dire : $\\vdash_\\textbf{S5} \\Box P\\rightarrow\\Box\\Box P$.\nOn commence par montrer $\\vdash_\\textbf{S5} \\Box P\\rightarrow\\Box\\Diamond\\Box P$ en ajoutant juste une substitution au théorème précédant :\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;th.$} \\UnaryInfC{$ P\\rightarrow\\Box\\Diamond P$} \\RightLabel{$\\scriptsize\\;sub.$} \\UnaryInfC{$ \\Box P \\rightarrow \\Box\\Diamond \\Box P$} \\end{prooftree} $$ On montre ensuite $\\vdash_\\textbf{S5} \\Diamond\\Box P\\rightarrow \\Box P$ Puis on montre $\\vdash_\\textbf{S5} \\Box\\Diamond\\Box P\\rightarrow \\Box\\Box P$ Pour enfin pouvoir conclure\u0026nbsp;: $\\textbf{KB4}≤\\textbf{S5}$ Il faut montrer à la fois que :\nla formule $\\text{(4)}$ peut être prouvée dans le système $\\textbf{S5}$ et on l'a fait dans la démonstration de la réduction de $\\textbf{S4}$ à $\\textbf{S5}$, la formule $\\text{(B)}$ peut être prouvée dans le système $\\textbf{S5}$ et on l'a fait dans la démonstration de la réduction de $\\textbf{KTB}$ à $\\textbf{S5}$, Représentons tout ça graphiquement en utilisant le code suivant : une flèche allant de $\\textbf{S}$ à $\\textbf{S\u0026rsquo;}$ signifie que la relation $\\textbf{S}≤\\textbf{S\u0026rsquo;}$ est avérée ($\\textbf{S}$ se réduit à $\\textbf{S\u0026rsquo;}$).\nEt comme on l\u0026rsquo;a vu, la réduction d\u0026rsquo;un système formel à un autre implique l\u0026rsquo;inclusion inverse entre les classes de tous les systèmes de transition qui valident ces systèmes formels.\nDans le graphique suivant, une flèche allant de $\\mathscr{C}$ à $\\mathscr{C\u0026rsquo;}$ désigne donc l\u0026rsquo;inclusion inverse $\\mathscr{C}\\supseteq\\mathscr{C\u0026rsquo;}$.\nLe projet est maintenant de préciser le rapport entre axiomatique et sémantique avec l\u0026rsquo;espoir que l\u0026rsquo;ensemble des formules valides sur les structures d\u0026rsquo;un certain type soit identique à l\u0026rsquo;ensemble des axiomes et théorèmes d\u0026rsquo;un certain système (ce qu\u0026rsquo;on a appelé une logique modale), car ainsi la validité formera une interprétation du système. Mais pour s\u0026rsquo;assurer de cette adéquation entre sémantique et axiomatique, il faut d\u0026rsquo;une part obtenir la correction (ou solidité, soundness en anglais) du système (toutes les formules prouvables sont vraies), et sa réciproque, la complétude (toutes les formules vraies sont prouvables).\nDit autrement, on doit pouvoir déduire du système formel tout le vrai (complétude) et rien que le vrai (correction).\nOn va d\u0026rsquo;abord chercher à faire correspondre une logique modale normale à chaque classe de systèmes de transitions.\nSoit $\\mathscr{C}$ une classe de systèmes de transition et $\\mathbb{L}$ une logique modale normale, $\\mathbb{L}$ est correcte par rapport à la classe $\\mathscr{C}$ (noté $\\mathscr{C}$-correcte) si pour toute formule $\\phi$ et pour tout système de transition $\\mathcal{S}$ de la classe $\\mathscr{C}$, si $\\vdash_\\mathbb{L}\\phi$ alors $\\mathcal{S}\\Vdash^{\\forall a,\\forall \\mathcal{V}}\\phi$.\nCe qu\u0026rsquo;on peut aussi écrire :\n$$\\text{si } \\vdash_\\mathbb{L}\\phi \\text{ alors }\\models_{\\mathscr{C}}\\phi$$\nÀ chacune des onze classes, on peut faire correspondre une logique modale normale qui se trouve être correcte pour cette classe.\nsi $\\vdash_\\textbf{K}\\phi$ alors $\\models_{\\mathscr{C}}\\phi$ si $\\vdash_\\textbf{K4}\\phi$ alors $\\models_{\\mathscr{C}_{tr.}}\\phi$ si $\\vdash_\\textbf{KD}\\phi$ alors $\\models_{\\mathscr{C}_{n.b.d.}}\\phi$ si $\\vdash_\\textbf{KD4}\\phi$ alors $\\models_{\\mathscr{C}_{tr.,n.b.d.}}\\phi$ si $\\vdash_\\textbf{KT}\\phi$ alors $\\models_{\\mathscr{C}_{ref.}}\\phi$ si $\\vdash_\\textbf{S4}\\phi$ alors $\\models_{\\mathscr{C}_{ref.,tr.}}\\phi$ si $\\vdash_\\textbf{KB}\\phi$ alors $\\models_{\\mathscr{C}_{sym.}}\\phi$ si $\\vdash_\\textbf{KB4}\\phi$ alors $\\models_{\\mathscr{C}_{sym.,tr.}}\\phi$ si $\\vdash_\\textbf{KDB}\\phi$ alors $\\models_{\\mathscr{C}_{sym.,n.b.d.}}\\phi$ si $\\vdash_\\textbf{KTB}\\phi$ alors $\\models_{\\mathscr{C}_{ref.,sym.}}\\phi$ si $\\vdash_\\textbf{S5}\\phi$ alors $\\models_{\\mathscr{C}_{ref.,sym.,tr.}}\\phi$ On a déjà en fait déjà tout démontré.\nEn effet, on a montré que les axiomes de la logique normale minimale (tautologies du calcul des propositions, $\\text{(K)}$ et $\\text{(dual)}$) sont satisfaites en tout nœud et pour toute valuation de tous les systèmes de transitions. Et on a montré que les trois opérations utilisés pour les preuves (la nécéssitation, la substitution uniforme et le modus ponens) préservent la validité. Donc une formule prouvable dans $\\textbf{K}$ est valide dans tout système de transition ( $\\textbf{K}$ est $\\mathscr{C}$-correcte).\nEnsuite, on a montré que chacun des axiomes modaux supplémentaires ajoutés à $\\textbf{K}$ pour obtenir les autres logiques normales ont une validité limitée à une classe spécifique de systèmes de transition.\n$\\text{(T)}$ est valide dans tout système de transition réflexif. $\\text{(B)}$ est valide dans tout système de transition symétrique. $\\text{(4)}$ est valide dans tout système de transition transitif. $\\text{(D)}$ est valide dans tout système de transition non borné à droite. $\\text{(4)}$ est valide dans tout système de transition euclidien. Et on a montré qu\u0026rsquo;un graphe est à la fois réflexif et euclidien si et seulement si il est réflexif, symétrique et transitif.\nOn représente graphiquement par une flèche \u0026ldquo;$\\textbf{X}\\longrightarrow\\mathscr{C}_x$\u0026rdquo; la relation de correction : \u0026ldquo;si $\\vdash_\\textbf{X}\\phi$, alors $\\models_{\\mathscr{C}_x}\\phi$\u0026rdquo;.\nOn va essayer maintenant d\u0026rsquo;associer les classe aux logiques, la sémantique à la syntaxe.\nSoit $\\mathscr{C}$ une classe de systèmes de transition et $\\mathbb{L}$ une logique modale normale.\n$\\mathbb{L}$ est faiblement complète par rapport à la classe $\\mathscr{C}$ (noté $\\mathscr{C}$-faiblement complète) si pour toute formule $\\phi$\u0026nbsp;:\n$$\\text{si }\\models_\\mathscr{C}\\phi \\text{ alors } \\vdash_\\mathbb{L}\\phi$$ $\\mathbb{L}$ est fortement complète par rapport à la classe $\\mathscr{C}$ (noté $\\mathscr{C}$-fortement complète) si pour toute formule $\\phi$ et tout ensemble de formules $\\Gamma$\u0026nbsp;:\n$$\\text{si } \\Gamma \\models_\\mathscr{C}\\phi \\text{ alors } \\Gamma \\vdash_\\mathbb{L}\\phi$$ On a alors le théorème de forte complétude suivant :\nsi $\\Gamma\\models_{\\mathscr{C}}\\phi$ alors $\\Gamma\\vdash_\\textbf{K}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{tr.}}\\phi$ alors $\\Gamma\\vdash_\\textbf{K4}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{n.b.d.}}\\phi$ alors $\\Gamma\\vdash_\\textbf{KD}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{tr.,n.b.d.}}\\phi$ alors $\\Gamma\\vdash_\\textbf{KD4}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{ref.}}\\phi$ alors $\\Gamma\\vdash_\\textbf{KT}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{ref.,tr.}}\\phi$ alors $\\Gamma\\vdash_\\textbf{S4}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{sym.}}\\phi$ alors $\\Gamma\\vdash_\\textbf{KB}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{sym.,tr.}}\\phi$ alors $\\Gamma\\vdash_\\textbf{KB4}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{sym.,n.b.d.}}\\phi$ alors $\\Gamma\\vdash_\\textbf{KDB}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{ref.,sym.}}\\phi$ alors $\\Gamma\\vdash_\\textbf{KTB}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{ref.,sym.,tr.}}\\phi$ alors $\\Gamma\\vdash_\\textbf{S5}\\phi$ Pour le démontrer on va avoir besoin de nouvelles définitions et de plusieurs lemmes.\nSoient $\\textbf{S}$ un système formel et $\\Gamma$ un ensemble de formules,\n$\\Gamma$ est $\\textbf{S}$-consistant si et seulement si $\\Gamma\\nvdash_\\textbf{S}\\bot$. Dans le cas contraire, $\\Gamma$ est dit $\\textbf{S}$-inconsistant. Lemme d'existence Soient $\\mathscr{C}$ une classe de système de transition et $\\mathbb{L}$ une logique modale normale,\n$\\mathbb{L}$ est $\\mathscr{C}$-(fortement) complète si et seulement si pour tout $\\Gamma\\subseteq\\mathbb{L}$ qui est $\\mathbb{L}$-consistant, il existe $\\mathcal{S}\\in\\mathscr{C},\\mathcal{V}$ et $a$ tels que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Gamma$. $\\Rightarrow$\u0026nbsp;:\npuisque $\\mathbb{L}$ est $\\mathscr{C}$-complète, un ensemble de formules quelconque $\\mathbb{L}$-consistant $\\Gamma\\subseteq\\mathbb{L}$ est nécessairement satisfaisable pour un système de transition de la classe $\\mathscr{C}$. En effet, si ce n'était pas le cas, l'expression $\\Gamma\\models_\\mathscr{C}\\bot$ serait vraie, mais alors nous pourrions en déduire $\\Gamma\\vdash_\\mathbb{L}\\bot$ en utilisant la $\\mathscr{C}$-complétude de $\\mathbb{L}$. $\\Leftarrow$\u0026nbsp;:\nmontrons la contraposée. On suppose donc que $\\mathbb{L}$ n'est pas $\\mathscr{C}$-complète. Soit donc $\\Gamma$ un ensemble de formules et $\\phi$ une formule tels que $\\Gamma\\models_\\mathscr{C}\\phi$ mais $\\Gamma\\nvdash_\\mathbb{L}\\phi$. Il apparaît dès lors que $\\Gamma\\cup\\set{\\neg\\phi}\\nvdash_\\mathbb{L}\\bot$ est vérifiée, car $\\Gamma\\cup\\set{\\neg\\phi}\\vdash_\\mathbb{L}\\bot$ entraînerait $\\Gamma\\vdash_\\mathbb{L}\\neg\\phi\\rightarrow\\bot$ puis, en passant par la contraposée, $\\Gamma\\vdash_\\mathbb{L}\\phi$. Par conséquent $\\Gamma\\cup\\set{\\neg\\phi}$ est $\\mathbb{L}$-consistant et pourtant, pour tout système de transition $\\mathcal{S}\\in\\mathscr{C}$, $\\mathcal{V}$ et $a$, $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\Gamma\\cup\\set{\\neg\\phi}$. L\u0026rsquo;idée pour démontrer la complétude est de chercher un modèle $\\langle\\mathcal{S},\\mathcal{V}\\rangle$ dont le système de transition fait partie de la classe considérée et un nœud $a$ tels que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Gamma$ où $\\Gamma$ est un ensemble de formules $\\mathbb{L}$-consistant.\nOn va mettre au point un tel modèle en modelant les nœuds de son système de distribution directement à partir d\u0026rsquo;ensembles de formules ! La sémantique détourne ainsi la syntaxe à son profit en donnant vie à une sorte de Golem syntaxique.\nSoient $\\mathbb{L}$ une logique modale normale et $\\Gamma$ un ensemble de formules,\n$\\Gamma$ est maximal $\\mathbb{L}$-consistant (MC) si et seulement si $\\Gamma\\nvdash_\\mathbb{L}\\bot$ et pour tout ensemble $\\Gamma \\subsetneq \\Gamma'$, $\\Gamma'\\vdash_\\mathbb{L}\\bot$. Remarques :\nsi $\\Gamma$ est maximal $\\mathbb{L}$-consistant, alors\u0026nbsp;:\n$\\mathbb{L}\\subseteq\\Gamma$ $\\Gamma$ est clos par modus ponens pour toute formule $\\phi$, $\\phi\\in\\Gamma$ ou $\\neg\\phi\\in\\Gamma$ pour toutes formules $\\phi$ et $\\psi$, $\\phi\\lor\\psi\\in\\Gamma$ si et seulement si $\\phi\\in\\Gamma$ ou $\\psi\\in\\Gamma$ Lemme de Lindenbaum Si $\\Gamma$ est $\\mathbb{L}$-consistant, alors il existe $\\Gamma_{max}$ maximal $\\mathbb{L}$-consistant tel que $\\Gamma\\subseteq\\Gamma_{max}$. Soit $(\\phi_n)_{n\\in\\mathbb{N}}$ une énumération de toutes les formules. On définit :\n$\\Gamma_0=\\Gamma$ $\\Gamma_{n+1}=\\begin{cases}\\Gamma_n\\cup\\set{\\phi_n} \\text{ si } \\Gamma_n\\cup\\set{\\phi_n}\\nvdash_\\mathbb{L}\\bot\\\\ \\Gamma_n\\cup\\set{\\neg\\phi_n} \\text{ sinon}\\end{cases}$ $\\displaystyle \\Gamma_{max}=\\bigcup_{n\\in\\mathbb{N}}\\Gamma_n$ $\\Gamma_{max}$ est $\\mathbb{L}$-consistant car sinon il existerait un plus petit entier $n$ tel que $\\Gamma_{n+1}\\vdash_\\mathbb{L}\\bot$. Nous aurions alors $\\Gamma_n\\cup\\set{\\phi_n}\\vdash_\\mathbb{L}\\bot$ et $\\Gamma_n\\cup\\set{\\neg\\phi_n}\\vdash_\\mathbb{L}\\bot$ par construction de $\\Gamma_{max}$. En utilisant le raisonnement par l'absurde, nous obtenons à la fois $\\Gamma_n\\vdash_\\mathbb{L}\\phi_n$ et $\\Gamma_n\\vdash_\\mathbb{L}\\neg\\phi_n$, ce qui entraîne $\\Gamma_n\\vdash_\\mathbb{L}\\bot$, contredisant la fait que par minimalité de l'entier $n$, $\\Gamma_n$ est $\\mathbb{L}$-consistant. $\\Gamma_{max}$ est maximal car sinon il existerait $\\Gamma\u0026rsquo;$ $\\mathbb{L}$-consistant tel que $\\Gamma_{max}\\subsetneq\\Gamma\u0026rsquo;$. Dans ce cas, une formule quelconque $\\psi$ telle que $\\psi\\in\\Gamma\u0026rsquo;\\setminus\\Gamma_{max}$ apparaîtrait dans l\u0026rsquo;énumération $(\\phi_n)_{n\\in\\mathbb{N}}$ de toutes les formules et il existerait ainsi un entier $n$ tel que $\\psi=\\phi_n$. Par construction de $\\Gamma_{max}$, comme $\\phi_n\\not\\in\\Gamma_{max}$, $\\Gamma_n\\cup\\set{\\phi_n}\\vdash_\\mathbb{L}\\bot$ est vérifiée. Donc $\\Gamma_{max}\\cup\\set{\\phi_n}\\vdash_\\mathbb{L}\\bot$ ce qui entraîne $\\Gamma\u0026rsquo;\\vdash_\\mathbb{L}\\bot$, contredisant la $\\mathbb{L}$-consistance de $\\Gamma\u0026rsquo;$.\nSoit $\\mathbb{L}$ une logique modale normale, le modèle canonique $\\mathcal{M}_\\mathbb{L}$ est défini par $\\mathcal{M}_\\mathbb{L}=\\langle N_\\mathbb{L},A_\\mathbb{L},\\mathcal{V}_\\mathbb{L}\\rangle$ avec\u0026nbsp;: $N_\\mathbb{L}=\\set{\\Gamma|\\Gamma\\text{ maximal }\\mathbb{L}\\text{-consistant}}$ $A_\\mathbb{L}$ défini par $\\Gamma\\longrightarrow\\Gamma'$ si et seulement si pour toute formule $\\phi$, si $\\phi\\in\\Gamma'$ alors $\\Diamond\\phi\\in\\Gamma$ (c.-à-d. $\\Gamma\\supseteq\\set{\\Diamond\\phi : \\phi\\in\\Gamma'}$) $\\mathcal{V}_\\mathbb{L}$ est défini par $\\mathcal{V}_\\mathbb{L}(P)=\\set{\\Gamma\\in N_\\mathbb{L}|P\\in\\Gamma}$ (c.-à-d. $\\Gamma\\Vdash P$ si et seulement si $P\\in\\Gamma$) Lemme ($\\longrightarrow$ et $\\Box$) Soit $\\mathbb{L}$ une logique modale normale et $\\mathcal{M}_\\mathbb{L}=\\langle N_\\mathbb{L},A_\\mathbb{L},\\mathcal{V}_\\mathbb{L}\\rangle$ le modèle canonique associé à $\\mathbb{L}$. Pour tout nœud $\\Gamma,\\Gamma\u0026rsquo;\\in N_\\mathbb{L}$, on a :\n$\\Gamma\\longrightarrow\\Gamma'$ si et seulement si pour toute formule $\\phi$, si $\\Box\\phi\\in\\Gamma$ alors $\\phi\\in\\Gamma'$. $\\Rightarrow$\u0026nbsp;:\nRaisonnons par l'absurde en supposant $\\Box\\phi\\in\\Gamma$ et $\\phi\\not\\in\\Gamma'$. Par maximale $\\mathbb{L}$-consistance de $\\Gamma'$, $\\neg\\phi\\in\\Gamma'$. Par définition de l'arc $\\Gamma\\longrightarrow\\Gamma'$, $\\Diamond\\neg\\phi\\in\\Gamma$ et puisque $\\Gamma$ est consistant, $\\neg\\Diamond\\neg\\phi\\not\\in\\Gamma$, d'où $\\Box\\phi\\not\\in\\Gamma$ (où on a utilisé la dualité entre $\\Diamond$ et $\\Box$). Contradiction. $\\Leftarrow$\u0026nbsp;:\nSoit $\\phi\\in\\Gamma'$. Raisonnons par l'absurde en supposant $\\Diamond\\phi\\not\\in\\Gamma$ (ce qui signifie que $\\Gamma\\,\\,\\not\\!\\!\\longrightarrow\\Gamma'$). Alors par maximale $\\mathbb{L}$-consistance $\\neg\\Diamond\\phi\\in\\Gamma$, par conséquent $\\neg\\Diamond\\neg\\neg\\phi\\in\\Gamma$ et donc $\\Box\\neg\\phi\\in\\Gamma$ (où on a utilisé la dualité entre $\\Diamond$ et $\\Box$). Par hypothèse, $\\Box\\neg\\phi\\in\\Gamma$ entraîne $\\neg\\phi\\in\\Gamma'$, ce qui contredit la $\\mathbb{L}$-consistance de $\\Gamma'$. Lemme ($\\longrightarrow$ et $\\Diamond$) Soit $\\mathbb{L}$ une logique modale normale et $\\mathcal{M}_\\mathbb{L}=\\langle N_\\mathbb{L},A_\\mathbb{L},\\mathcal{V}_\\mathbb{L}\\rangle$ le modèle canonique associé à $\\mathbb{L}$. Pour tout nœud $\\Gamma,\\Gamma\u0026rsquo;\\in N_\\mathbb{L}$ :\nSi $\\Diamond\\phi\\in\\Gamma$ alors il existe $\\Gamma'$ tel que $\\Gamma\\longrightarrow\\Gamma'$ et $\\phi\\in\\Gamma'$. Supposons $\\Diamond\\phi\\in\\Gamma$. Pour obtenir $\\Gamma\u0026rsquo;$, on construit d\u0026rsquo;abord $\\Theta=\\set{\\phi}\\cup\\set{\\psi|\\Box\\psi\\in\\Gamma}$. Cet ensemble est consistant car sinon il existerait $\\psi_1,\\ldots,\\psi_k\\in\\Theta$ tels que $\\vdash_\\mathbb{L}(\\psi_1\\land\\ldots\\land\\psi_k)\\rightarrow\\neg\\phi$. Par nécessitation, $\\vdash_\\mathbb{L}\\Box((\\psi_1\\land\\ldots\\land\\psi_k)\\rightarrow\\neg\\phi)$. Et par distributivité (utilisation de l\u0026rsquo;axiome $\\text{(K)}$) $\\vdash_\\mathbb{L}\\Box(\\psi_1\\land\\ldots\\land\\psi_k)\\rightarrow\\Box\\neg\\phi$. Et puisque dans toute logique normale $\\vdash_\\mathbb{L}(\\Box\\psi_1\\land\\ldots\\land\\Box\\psi_k)\\rightarrow\\Box(\\psi_1\\land\\ldots\\land\\psi_k)$, on obtient finalement $\\vdash_\\mathbb{L}(\\Box\\psi_1\\land\\ldots\\land\\Box\\psi_k)\\rightarrow\\Box\\neg\\phi$. Par conséquent $\\Gamma\\vdash_\\mathbb{L}\\Box\\neg\\phi$, ce qui implique que $\\Box\\neg\\phi\\in\\Gamma$ par maximalité, et donc également $\\neg\\Diamond\\phi\\in\\Gamma$ (par dualité). Or par hypothèse, $\\Diamond\\phi\\in\\Gamma$, ce qui entraîne l\u0026rsquo;inconsistance de $\\Gamma$. Contradiction.\nComme on vient de montrer que l\u0026rsquo;ensemble $\\Theta=\\set{\\phi}\\cup\\set{\\psi|\\Box\\psi\\in\\Gamma}$ est $\\mathbb{L}$-consistant, il suffit de prendre pour $\\Gamma\u0026rsquo;$ l\u0026rsquo;ensemble $\\Theta_{max}$ maximal $\\mathbb{L}$-consistant donné par le lemme de Lindenbaum tel que $\\Theta\\subseteq\\Theta_{max}$. On a bien ainsi un nœud de $N_\\mathbb{L}$ contenant $\\phi$.\nLemme de vérité Soit $\\mathbb{L}$ une logique modale normale et $\\mathcal{M}_\\mathbb{L}=\\langle N_\\mathbb{L},A_\\mathbb{L},\\mathcal{V}_\\mathbb{L}\\rangle$ le modèle canonique associé à $\\mathbb{L}$. Pour tout nœud $\\Gamma\\in N_\\mathbb{L}$ :\n$\\langle\\mathcal{M}_\\mathbb{L},\\Gamma\\rangle\\Vdash\\phi$ si et seulement si $\\phi\\in\\Gamma$ La démonstration se fait par induction sur la hauteur de la formule $\\phi$.\nsi $ht(\\phi)=0$, alors $\\phi=\\top$ ou $\\phi=\\bot$ ou $\\phi$ est une variable propositionnelle et $\\Gamma\\Vdash\\phi$ ssi $\\phi\\in\\Gamma$ est la définition même de la valuation $\\mathcal{V}_\\mathbb{L}$. si $ht(\\phi)=n+1$ si $\\phi=\\neg\\psi$, alors $\\Gamma\\Vdash\\neg\\phi$ ssi $\\Gamma\\nVdash\\psi$. Or par hypothèse d'induction $\\Gamma\\nVdash\\psi$ ssi $\\psi\\not\\in\\Gamma$. Par maximalité de $\\Gamma$, on obtient $\\neg\\psi\\in\\Gamma$. si $\\phi=(\\psi_1\\lor\\psi_2)$, alors $\\Gamma\\Vdash(\\psi_1\\lor\\psi_2)$ ssi $\\Gamma\\Vdash\\psi_1$ ou $\\Gamma\\Vdash\\psi_2$.\nPar hypothèse d'induction, $\\Gamma\\Vdash\\psi_1$ ssi $\\psi_1\\in\\Gamma$ et $\\Gamma\\Vdash\\psi_2$ ssi $\\psi_2\\in\\Gamma$. Et par maximalité de $\\Gamma$, $\\psi_1\\in\\Gamma$ ou $\\psi_2\\in\\Gamma$ ssi $(\\psi_1\\lor\\psi_2)\\in\\Gamma$. si $\\phi=(\\psi_1\\land\\psi_2)$, alors $\\Gamma\\Vdash(\\psi_1\\lor\\psi_2)$ ssi $\\Gamma\\Vdash\\psi_1$ et $\\Gamma\\Vdash\\psi_2$.\nPar hypothèse d'induction, $\\Gamma\\Vdash\\psi_1$ ssi $\\psi_1\\in\\Gamma$ et $\\Gamma\\Vdash\\psi_2$ ssi $\\psi_2\\in\\Gamma$. Or par $\\mathbb{L}$-consistance, $\\psi_1\\in\\Gamma$ et $\\psi_2\\in\\Gamma$ ssi $(\\psi_1\\land\\psi_2)\\in\\Gamma$. si $\\phi=(\\psi_1\\rightarrow\\psi_2)$. On déduit ce cas de $\\neg\\psi_1\\lor\\psi_2$. si $\\phi=(\\psi_1\\leftrightarrow\\psi_2)$. On déduit ce cas de $(\\psi_1\\rightarrow\\psi_2)\\land(\\psi_2\\rightarrow\\psi_1)$. si $\\phi=\\Diamond\\psi$, alors $\\Gamma\\Vdash\\Diamond \\psi$ ssi il existe $\\Gamma',\\Gamma\\longrightarrow\\Gamma'$ et $\\Gamma'\\Vdash\\psi$. Or par hypothèse d'induction $\\Gamma'\\Vdash\\psi$ ssi $\\psi\\in\\Gamma'$. Finalement, par définition de la relation $\\Gamma\\longrightarrow\\Gamma'$ et par le lemme ($\\longrightarrow$ et $\\Diamond$) $\\psi\\in\\Gamma'$ ssi $\\Diamond\\psi\\in\\Gamma$. si $\\phi=\\Box\\psi$, alors $\\Gamma\\Vdash\\Box\\phi$ ssi $\\Gamma\\Vdash\\neg\\Diamond\\neg\\psi$ ssi $\\neg\\Diamond\\neg\\psi\\in\\Gamma$ ssi $\\Box\\psi\\in\\Gamma$ (par maximal $\\mathbb{L}$-consistance). Nous voilà parés pour démontrer la complétude forte des onze logiques normales décrites grâce au modèle canonique de chacune.\nLe lemme d\u0026rsquo;existence nous dit que $\\mathbb{L}$ est $\\mathscr{C}$-fortement complète ssi pour tout $\\Gamma\\subseteq\\mathbb{L}$, $\\mathbb{L}$-consistant, il existe $\\mathcal{S}\\in\\mathscr{C}$, $\\mathcal{V}$ et $a$ tels que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Gamma$.\nPour chaque logique normale, nous allons choisir le modèle canonique $\\mathcal{M}_\\mathbb{L}=\\langle N_\\mathbb{L},A_\\mathbb{L},\\mathcal{V}_\\mathbb{L}\\rangle$ associé. Et pour le nœud $a$, nous allons prendre $\\Gamma_{max}$. Le lemme de vérité nous dit alors que $\\langle\\mathcal{M}_\\mathbb{L},\\Gamma_{max}\\rangle\\Vdash\\phi$ ssi $\\phi\\in\\Gamma_{max}$. En conséquence, puisque $\\Gamma\\subseteq\\Gamma_{max}$, il ressort que $\\langle\\mathcal{M}_\\mathbb{L},\\Gamma_{max}\\rangle\\Vdash\\Gamma$.\nIl ne reste plus qu\u0026rsquo;à vérifier à chaque fois que le modèle canonique est bien construit sur la base d\u0026rsquo;un système de transition de la classe $\\mathscr{C}$ considérée.\n$\\mathcal{M}_\\textbf{K}\\in\\mathscr{C}$\u0026nbsp;: comme $\\mathscr{C}$ désigne la classe de tous les systèmes de transition, il n'ya rien à montrer. $\\mathcal{M}_\\textbf{K4}\\in\\mathscr{C}_{tr.}$\u0026nbsp;:\nil faut montrer que pour tout nœud $\\Xi_1$, $\\Xi_2$ et $\\Xi_3$ de $\\mathcal{M}_\\textbf{K4}$, si $\\Xi_1\\longrightarrow \\Xi_2$ et $\\Xi_2\\longrightarrow \\Xi_3$, alors $\\Xi_1\\longrightarrow \\Xi_3$.\nLa formule $\\text{(4)}=\\Box p\\rightarrow\\Box\\Box P$ est équivalente (par dualité) à la formule $\\neg\\Diamond\\neg P\\rightarrow\\neg\\Diamond\\neg\\neg\\Diamond\\neg P$ elle même équivalente à $\\Diamond\\Diamond\\neg P\\rightarrow\\Diamond\\neg P$ (en passant par la contraposée et en éliminant les doubles négations). Comme $\\Diamond\\Diamond\\neg P\\rightarrow\\Diamond\\neg P\\in\\Gamma$, on a aussi $\\Diamond\\Diamond\\neg P\\rightarrow\\Diamond\\neg P\\in\\Xi_1$ et puisque $\\Xi_1$ est clos par substitution uniforme, $\\Diamond\\Diamond\\neg \\neg\\phi\\rightarrow\\Diamond\\neg \\neg\\phi\\in\\Xi_1$. Et donc chaque formule $\\Diamond\\Diamond\\rightarrow\\Diamond$ appartient à $\\Xi_1$.\nPar définition de $\\Xi_1\\longrightarrow \\Xi_2$ et $\\Xi_2\\longrightarrow \\Xi_3$, pour une formule $\\phi$ quelconque, si $\\phi\\in\\Xi_3$, alors $\\Diamond\\phi\\in\\Xi_2$ et $\\Diamond\\Diamond\\phi\\in\\Xi_1$. Et comme $\\Xi_1$ est clos par modus ponens, si à la fois $\\Diamond\\Diamond\\phi\\in\\Xi_1$ et $\\Diamond\\Diamond\\phi\\rightarrow\\Diamond\\phi\\in\\Xi_1$, alors $\\Diamond\\phi\\in\\Xi_1$. Cela prouve que pour toute formule $\\phi$, si $\\phi\\in\\Xi_3$, alors $\\Diamond\\phi\\in\\Xi_1$, et par conséquent $\\Xi_1\\longrightarrow\\Xi_3$. $\\mathcal{M}_\\textbf{KD}\\in\\mathscr{C}_{n.b.d.}$\u0026nbsp;:\nil faut montrer que pour tout nœud $\\Xi_1$ de $\\mathcal{M}_\\textbf{KD}$, il existe $\\Xi_2$ tel que $\\Xi_1\\longrightarrow \\Xi_2$.\nD'après le lemme ($\\longrightarrow$ et $\\Diamond$), si $\\Diamond\\phi\\in\\Xi_1$, alors il existe $\\Xi_2$ tel que $\\Xi_1\\longrightarrow \\Xi_2$ et $\\phi\\in\\Xi_2$. Or l'axiome $\\text{(D)}$ dit précisément $\\Box P\\rightarrow\\Diamond P$. Comme $\\Xi_1$ est clos par substitution uniforme, la formule $\\Box\\top\\rightarrow\\Diamond\\top$ est dans $\\Xi_1$. Or $\\Xi_1$ vérifie nécessairement $\\Box\\top$ (car une tautologie est vraie partout) et comme $\\Xi_1$ est clos par modus ponens, $\\Diamond\\top\\in\\Xi_1$, et donc d'après le lemme ($\\longrightarrow$ et $\\Diamond$), il existe $\\Xi_2$ tel que $\\Xi_1\\longrightarrow \\Xi_2$ et $\\top\\in\\Xi_2$. $\\mathcal{M}_\\textbf{KD4}\\in\\mathscr{C}_{tr.,n.b.d.}$\u0026nbsp;:\nConséquence de 2. et 3. $\\mathcal{M}_\\textbf{KT}\\in\\mathscr{C}_{ref.}$\u0026nbsp;:\nIl faut montrer que $\\Xi\\longrightarrow \\Xi$ est vérifié pour tout nœud $\\Xi$ de $\\mathcal{M}_\\textbf{KT}$.\nLa formule $\\text{(T)}=\\Box P\\rightarrow P$ est équivalente à $\\neg\\Diamond\\neg P\\rightarrow P$ et par contraposition à $\\neg P\\rightarrow\\diamond\\neg P$. Comme $\\Xi$ est clos par substitution uniforme, la formule $\\neg \\neg \\phi\\rightarrow\\diamond\\neg \\neg\\phi$ est dans $\\Xi$ et donc $\\phi\\rightarrow\\Diamond\\phi$ est dans $\\Xi$. La cloture par modus ponens permet alors de déduire que pour toute formule $\\phi\\in\\Xi$, $\\Diamond\\phi\\in\\Xi$, ce qui est la condition nécessaire à la relation $\\Xi\\longrightarrow \\Xi$. $\\mathcal{M}_\\textbf{S4}\\in\\mathscr{C}_{ref.,tr.}$\u0026nbsp;:\nConséquence de 2. et 5. $\\mathcal{M}_\\textbf{KB}\\in\\mathscr{C}_{sym.}$\u0026nbsp;:\nil faut montrer que pour tout couple de nœuds $\\Xi_1$ et $\\Xi_2$ de $\\mathcal{M}_\\textbf{KB}$, $\\Xi_1\\longrightarrow \\Xi_2$ alors $\\Xi_2\\longrightarrow \\Xi_1$.\n$\\text{(B)}= P\\rightarrow \\Box\\Diamond P$ . Comme $\\Xi_1$ est clos par substitution uniforme, les formules $\\phi\\rightarrow\\Box\\Diamond\\phi$ sont dans $\\Xi_1$. La cloture par modus ponens de $\\Xi_1$ nous dit alors que si $\\phi\\in\\Xi_1$, alors $\\Box\\Diamond\\phi\\in\\Xi_1$. Or d'après le lemme ($\\longrightarrow$ et $\\Box$), si $\\Xi_1\\longrightarrow\\Xi_2$ alors pour toute formule $\\phi$, si $\\Box\\phi\\in\\Xi_1$, alors $\\phi\\in\\Xi_2$. Par conséquent, pour tout $\\phi\\in\\Xi_1$, $\\Diamond\\phi\\in\\Xi_2$, ce qui est la condition nécessaire pour définir $\\Xi_2\\longrightarrow\\Xi_1$. $\\mathcal{M}_\\textbf{KB4}\\in\\mathscr{C}_{sym.tr.}$\u0026nbsp;:\nConséquence de 2. et 7. $\\mathcal{M}_\\textbf{KDB}\\in\\mathscr{C}_{sym.,n.b.d.}$\u0026nbsp;:\nConséquence de 3. et 7. $\\mathcal{M}_\\textbf{KTB}\\in\\mathscr{C}_{ref.sym.}$\u0026nbsp;:\nConséquence de 5. et 7. $\\mathcal{M}_\\textbf{S5}\\in\\mathscr{C}_{ref.,sym.tr.}$\u0026nbsp;:\nConséquence de 2. et 5. et 7. On représente graphiquement par une flèche \u0026ldquo;$\\mathscr{C}_x\\longrightarrow\\textbf{X}$\u0026rdquo; la relation de forte-complétude : \u0026ldquo;si $\\Gamma\\models_{\\mathscr{C}_x}\\phi$, alors $\\Gamma\\vdash_\\textbf{X}\\phi$\u0026rdquo;.\nnote\nLa notion de conséquence sémantique $\\models$ demande une satisfaction sur tous les modèle alors que la notion de conséquence syntaxique $\\vdash$ demande seulement l\u0026rsquo;existence d\u0026rsquo;une preuve.\nDit autrement, $\\models$ est définie avec un quantificateur universel $\\forall$ et $\\vdash$ avec un quantificateur existentiel $\\exists$ or la négation du quantificateur universel est le quantificateur existentiel et inversement.\nDémontrer qu\u0026rsquo;une formule est non démontrable est donc très difficile du côté de la théorie de la démonstration puisque cela oblige à montrer que toutes les preuves échouent. Par contre, sur le versant sémantique, il suffit de trouver un modèle où la négation de la formule est satisfaite (un contre-exemple suffit).\nC\u0026rsquo;est la correction qui nous permet le changement de versant (on passe de la syntaxe à la sémantique).\nOn peut trouver d\u0026rsquo;ailleurs un peu douloureux que la correspondance de loin la plus difficile à démontrer (la complétude) apporte si peu d\u0026rsquo;applications concrètes.\nSuite : Différentes logiques modales "
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique6/",
	"title": "Logique modale",
	"tags": [],
	"description": "",
	"content": " Logique modale Syntaxe et sémantique Systèmes logiques Différentes logiques modales Petit condensé de logique modale\u0026nbsp;: En logique modale, un \u0026ldquo;monde possible\u0026rdquo; contient les propositions qui sont vraies dans ce monde. Il correspond à un modèle du calcul des propositions (à une ligne d\u0026rsquo;une table de vérité). Ces mondes possibles sont connectés entre eux par des relations d\u0026rsquo;accessibilité, et les opérateurs de modalité permettent d\u0026rsquo;explorer ces relations. Si $P$ est vraie dans tous les mondes accessibles depuis le monde $a$, alors on écrit $a\\Vdash \\Box P$ et si $P$ est vraie dans au moins un de ces mondes, on utilise $a\\Vdash \\Diamond P$. Un modèle de la logique modale est alors un graphe orienté, appelé système de transition ou structure de Kripke.\nL'un des objectifs fondamentaux des systèmes logiques est de fournir un cadre où les propositions peuvent être systématiquement et formellement prouvées ou réfutées. On cherche au final une recette pour mettre à jour les tautologies de la théorie, les vérités absolues, les formules validées dans tout modèle. Cette exigence conduit à une axiomatisation, l'établissement d'un ensemble de règles d'inférences et de principes de base pour forger une machine à produire du vrai.\nAvec pour axiomes les tautologies du calcul des propositions auxquelles on ajoute l'axiome modal $\\text{(K)}$ (qui distribue $\\Box$ dans une implication), et les règles de nécessitation (si $\\vdash\\phi$, alors on a aussi $\\vdash\\Box\\phi$), de substitution uniforme (une formule prouvable où toute occurence d'une proposition est remplacée par une même formule quelconque est tout autant prouvable) et de modus ponens pour assurer la cohérence de la machinerie, on se retrouve avec la logique modale normale de base $\\mathbf{K}$ qui permet de produire l'ensemble des formules universellement vraies. Toute formule obtenue dans $\\mathbf{K}$ est vraie dans tout monde possible de toute structure de Kripke, et inversement. On dit alors qu'il y a correction et complétude entre le système formel et les modèles associés\u0026nbsp;: $\\vdash_\\textbf{ K}\\phi$ ssi $\\Vdash_\\mathscr{C}\\phi$ où $\\vdash_\\textbf{ K}\\phi$ dit que $\\phi$ a été prouvée dans $\\mathbf{K}$, et $\\Vdash_\\mathscr{C}\\phi$ dit que $\\phi$ est valide (vraie dans tout monde) pour tout modèle de la classe $\\mathscr{C}$ qui désigne l'ensemble des systèmes de transition.\nEn ajoutant de nouveaux axiomes modaux au système formel, on ajoute par là même des contraintes aux modèles associés (les graphes gagnent de nouvelles propriétés). À chaque jeu d'axiomes va correspondre une certaine classe de systèmes de transition où la vérité pourra se déployer. En ajoutant l'axiome $\\text{(B)}=P\\rightarrow \\Box\\Diamond P$, par exemple, la validité des formules obtenues sera limitée aux graphes non orientés $\\mathscr{C}_{sym.}$ puisque l'axiome $\\text{(B)}$ ajoute la symétrie aux structures. Les différentes logiques modales On cherche maintenant à incarner la logique modale avec des interprétations précises des opérateurs modaux. La boîte peut servir à exprimer qu\u0026rsquo;un agent sait quelque chose et son dual, le diamant, exprime alors le fait que cet agent croit seulement possible cette même chose. Ça nous place dans une logique épistémique (logique de la connaissance) et c\u0026rsquo;est principalement cette version qu\u0026rsquo;on a utilisée jusqu\u0026rsquo;ici pour donner corps à la théorie. Mais d\u0026rsquo;autres emplois sont courants et ne rattachent pas forcément les opérateurs modaux à un agent, comme c\u0026rsquo;est le cas de la logique aléthique et la logique déontique. On verra aussi qu\u0026rsquo;en fonction des interprétations, certains axiomes sont plus naturels que d\u0026rsquo;autres et donc que ces différentes logiques modales s\u0026rsquo;épanouissent dans des univers aux topologies différentes.\nLogique aléthique La logique modale est dite aléthique lorsque les opérateurs modaux traduisent la nécessité et la possibilité.\nLes trois cellules rectangulaires sont destinées à être conjointement exhaustives et mutuellement exclusives : chaque proposition peut être nécessaire, contingente (possiblement vraie mais aussi possiblement fausse), ou encore impossible, mais jamais plus qu\u0026rsquo;un de ces trois choix.\nOn peut en déduire l\u0026rsquo;hexagone logique suivant, extension du carré des oppositions d\u0026rsquo;Aristote, dû à Sesmat et Blanché :\nDeux propositions contradictoires ne sont jamais ni vraies, ni fausses en même temps. Deux propositions contraires ne sont jamais vraies en même temps, mais peuvent être fausses en même temps. Deux propositions sous-contraires ne sont jamais fausses en même temps, mais peuvent être vraies en même temps. L\u0026rsquo;axiome $\\text{(K)}=\\Box(P\\rightarrow Q)\\rightarrow(\\Box P\\rightarrow\\Box Q)$, qui rappelons-le est vrai dans tout système, se traduit en logique aléthique par \u0026ldquo;s\u0026rsquo;il est nécessaire que $P$ entraîne $Q$, alors si $P$ est nécessaire, $Q$ est nécessaire également.\u0026rdquo;\nParmi les autres axiomes qu\u0026rsquo;on a pu rencontrer, deux semblent plus adaptés à cette interprétation de la modalité :\nl'axiome $\\text{(T)}=\\Box P\\rightarrow P$\u0026nbsp;: \"s'il est nécessaire que $P$, alors $P$ est réalisée\" (les logiciens du Moyen Âge, déjà, employaient la formule ab necesse esse). l'axiome $\\text{(B)}= P\\rightarrow \\Box\\Diamond P$\u0026nbsp;: \"si $P$ est réalisée, alors il est nécessaire que $P$ soit possible\" (cela semble en effet assez naturel). Le système logique normale adapté à la logique aléthique semble donc être $\\textbf{ KBT}$ et la classe de modèles adaptée est donc celle des systèmes de transition à la fois réflexifs et symétriques.\nAristote est à l\u0026rsquo;origine de la logique aléthique et il en donne un exemple célèbre dans De l\u0026rsquo;interprétation :\nNécessairement il y aura demain une bataille navale ou il n\u0026rsquo;y en aura pas. Mais il n\u0026rsquo;est pas nécessaire qu\u0026rsquo;il y ait demain une bataille navale, pas plus qu\u0026rsquo;il n\u0026rsquo;est nécessaire qu\u0026rsquo;il n\u0026rsquo;y en ait pas. Mais qu\u0026rsquo;il y ait ou qu\u0026rsquo;il n\u0026rsquo;y ait pas demain une bataille navale, voilà qui est nécessaire.\nEn appelant $P$ l\u0026rsquo;affirmation \u0026ldquo;il y aura une bataille navale demain\u0026rdquo;, la première et troisième phrase se traduisent par la même formule, $\\Box(P\\lor\\neg P)$, alors que la deuxième phrase se traduit par $\\neg\\Box P\\land\\neg\\Box\\neg P$.\nLe modèle ci-dessous représente les deux mondes possibles (avec ou sans bataille navale). Les deux formules sont vérifiées en chacun des mondes.\nLa logique modale \u0026ldquo;moderne\u0026rdquo; a elle aussi d\u0026rsquo;abord été aléthique lorsqu\u0026rsquo;elle née du désir de C.I. Lewis, en 1913, de renforcer l\u0026rsquo;implication matérielle $P\\rightarrow Q$. Il a voulu lui substituer une implication stricte qui s\u0026rsquo;avérera finalement une implication matérielle renforcée du caractère de la nécessité. L\u0026rsquo;étude de l\u0026rsquo;implication stricte revenait donc à celle de la logique aléthique. La logique modale s\u0026rsquo;est ensuite développée de manière purement axiomatique jusqu\u0026rsquo;au début des années 60, quand S. Kripke introduit l\u0026rsquo;idée des mondes possibles pour créer une sémantique à la logique modale.\nMais revenons à l\u0026rsquo;implication. La formule $(P\\rightarrow Q)\\lor(Q\\rightarrow P)$ est une tautologie du calcul des propositions. Pour deux propositions quelconques, il y en a donc toujours une qui implique l\u0026rsquo;autre, ce qui paraît absurde puisque les propositions peuvent être totalement indépendantes !\nAvec l\u0026rsquo;implication stricte (implication ordinaire renforcée de la nécessité), le paradoxe disparaît car $\\Box(P\\rightarrow Q)\\lor\\Box(Q\\rightarrow P)$ n\u0026rsquo;est pas valide. En effet, aucun des mondes du modèle ci-dessous ne satisfait la formule.\nLe métaphysicien Charles Hartshorne a produit une \u0026ldquo;preuve\u0026rdquo; de l\u0026rsquo;existence de Dieu qui repose sur la logique $\\textbf{{KB}}$ et qui nécessite un théorème de la logique modale $\\vdash_\\textbf{ K} \\Box(P\\rightarrow Q)\\rightarrow(\\Diamond P\\rightarrow\\Diamond Q)$, l\u0026rsquo;axiome d\u0026rsquo;Anselme $G\\rightarrow\\Box G$ (si Dieu existe, il est nécessaire que Dieu existe) et l\u0026rsquo;axiome de Leibniz $\\Diamond G$ (il est possible que Dieu existe).\nMonsieur Phi a une chouette vidéo sur ce genre d'argument ontologique. Logique déontique Cette logique porte sur l\u0026rsquo;obligation ou la permission.\nLes deux principes de la logique aléthique (les deux axiomes choisis) deviennent clairement faux en logique déontique\u0026nbsp;:\n$\\Box P\\rightarrow P$\u0026nbsp;: si $P$ est obligatoire, alors $P$ est vraie. Ça n'a ici pas de sens puisque c'est le caractère propre de l'obligation que de pouvoir être transgressée\u0026nbsp;! Il ne s'agirait pas sinon d'une obligation mais bien d'une nécessité. $P\\rightarrow \\Box\\Diamond P$\u0026nbsp;: si $P$ est vraie, alors $P$ doit être permise. C'est à nouveau dénué de sens puisque des choses non permises arrivent. Considérer la propositions vraies revient d'ailleurs à tomber dans le sophisme naturaliste qui stipule que tout ce qui est naturel est bon (\"il n'y a pas de vêtements dans la nature, donc il ne faut pas autoriser les vêtements\"). Mais quelle formule s\u0026rsquo;adapte à la logique déontique ? L\u0026rsquo;axiome $\\text{(D)}=\\Box P\\rightarrow\\Diamond P$ affirmant que \u0026ldquo;si quelque chose est obligatoire, alors elle est permise\u0026rdquo;, convient naturellement. En triturant un peu la formule, on obtient $\\Box P\\rightarrow\\Diamond P\\equiv\\neg\\Box P\\lor\\Diamond P\\equiv\\Diamond\\neg P\\lor\\Diamond P$ \u0026ldquo;soit c\u0026rsquo;est permis de faire une chose, soit c\u0026rsquo;est permis de ne pas la faire\u0026rdquo;. Ou encore : $\\Diamond\\neg P\\lor\\Diamond P\\equiv \\neg\\Box P\\lor\\neg\\Box\\neg P\\equiv\\neg (\\Box P\\land\\Box\\neg P)$ \u0026ldquo;ne peuvent être obligatoire à la fois une chose et son contraire\u0026rdquo;. Tout cela semble plutôt évident.\nLe système formel correspondant est donc $\\textbf{ KD}$ et la classe de systèmes de transition associée est $\\mathscr{C}_{n.b.d.}$. La raison de cette topologie ouverte, avec des mondes qui ont tous un échappatoire est qu\u0026rsquo;une impasse est par construction un monde ou rien n\u0026rsquo;est permis !\nPlusieurs paradoxes découlent de cette formalisation de l\u0026rsquo;obligation et de la permission. Penchons-nous d\u0026rsquo;abord sur le paradoxe de Ross (dû à\u0026hellip; Ross).\nL\u0026rsquo;énoncé \u0026ldquo;Il est obligatoire de poster la lettre\u0026rdquo; ($\\Box P$ où $P$ correspond à poster la lettre) entraîne \u0026ldquo;il est obligatoire de poster la lettre ou de la brûler\u0026rdquo; ($\\Box(P\\lor B)$ où $B$ correspond à brûler la lettre). En effet $\\Box P\\rightarrow \\Box(P\\lor B)$ est prouvable sans hypothèse et est donc vérifiée en tout nœud de tout modèle de Kripke d\u0026rsquo;une logique modale normale (puisque l\u0026rsquo;axiome $\\text{(D)}$ n\u0026rsquo;est même pas nécessaire à la preuve).\nOn part de la tautologie $P\\rightarrow (P\\lor Q)$ et de l\u0026rsquo;axiome $\\text{(K)}$ :\nSi l\u0026rsquo;obligation de poster une lettre implique l\u0026rsquo;obligation de la poster ou de la brûler, alors on peut satisfaire l\u0026rsquo;obligation découlant de la première en brûlant la lettre. Certes, la lettre est loin d\u0026rsquo;être postée et on a d\u0026rsquo;ailleurs probablement enfreint un interdit, mais on a pourtant bien aussi satisfait une obligation\u0026hellip;\nL\u0026rsquo;implication a permis une sorte de réduction du poids de l\u0026rsquo;obligation par affaiblissement de $P$ en $P\\lor B$.\nOn rencontre un autre problème en affaiblissant une conjonction comme le montre le paradoxe du Bon Samaritain (dû à Prior) : l\u0026rsquo;énoncé \u0026ldquo;il est obligatoire qu\u0026rsquo;Alice aide Bob qui a été volé\u0026rdquo; peut se traduire par $\\Box(A\\land B)$ où $A$ est \u0026ldquo;Alice aide Bob\u0026rdquo; et $B$ est \u0026ldquo;Bob a été volé\u0026rdquo;. Le problème est que $\\Box(A\\land B)\\rightarrow\\Box B$ est prouvable dans le système $\\textbf{ K}$ ($\\text{(D)}$ n\u0026rsquo;est toujours pas nécessaire à la preuve).\nOn part de la tautologie $(A\\land B)\\rightarrow B$ et de l\u0026rsquo;axiome $\\text{(K)}$ :\nDonc si aider Bob lors d\u0026rsquo;un vol est obligatoire, il devient alors tout aussi obligatoire que Bob soit volé. Bob n\u0026rsquo;est pas ravi de la situation.\nUne façon de se sortir de ces pièges est de maintenir une séparation stricte entre les descriptions des faits et les prescriptions déontiques. Cela évite d\u0026rsquo;inférer des faits (comme l\u0026rsquo;existence d\u0026rsquo;une victime) à partir des obligations morales ou légales.\nMais plus fondamentalement, en habillant modalement les deux tautologies ($P\\rightarrow(P\\lor Q)$ et $(A\\land B\\rightarrow B$) qui ont servi d\u0026rsquo;axiome dans les démonstrations, on permet de les interpréter sous un jour plus concret révélant leur bizarrerie. Le ver était dans le fruit dès l\u0026rsquo;utilisation de l\u0026rsquo;implication matérielle classique (celle que Lewis cherchait justement à dompter avec la modalité).\nPourquoi ces paradoxes sont-ils attachés à la logique déontique alors qu\u0026rsquo;on peut très bien obtenir les mêmes formules dans les autres logiques modales (puisqu\u0026rsquo;aucun axiome n\u0026rsquo;est nécessaire) ?\nLa logique déontique vise à proposer un guide éthique, des normes à suivre, là où la logique aléthique, par exemple, n\u0026rsquo;énonce qu\u0026rsquo;un état de fait. Devant l\u0026rsquo;affirmation que la nécessité du postage d\u0026rsquo;une lettre rend nécessaire son postage ou sa destruction par le feu, on a moins envie de monter au rideau (même si ça reste un peu bizarre). Une chose nécessaire s\u0026rsquo;impose physiquement et non moralement, il n\u0026rsquo;y a pas le choix, et l\u0026rsquo;application brute de la logique \u0026ldquo;mathématique\u0026rdquo; fait donc plus sens. En tant que prescripteur de comportement, la logique déontique a besoin d\u0026rsquo;une plus grande incarnation et de plus d\u0026rsquo;attention sur l\u0026rsquo;interprétation.\nLe dilemme du libre choix (aussi dû à Ross) est une autre curiosité qui voit une hypothèse d\u0026rsquo;allure bénigne se transformer en boîte de Pandore.\n\u0026ldquo;Tu peux soit dormir sur le sofa soit dans la chambre d\u0026rsquo;amis\u0026rdquo; semble impliquer que \u0026ldquo;tu peux dormir sur le sofa et tu peux dormir dans la chambre d\u0026rsquo;amis\u0026rdquo;. On imagine ainsi que l\u0026rsquo;implication $\\Diamond(A\\lor B)\\rightarrow(\\Diamond A\\land\\Diamond B)$ est prouvable dans le système. Mais ce n\u0026rsquo;est pas le cas (voir ci-dessous). Qu\u0026rsquo;à cela ne tienne ! Ajoutons-là aux hypothèses. Cela serait dommage de se passer d\u0026rsquo;une implication si naturelle\u0026hellip; Mais là, patatras, tout devient possible ! Imaginons en effet qu\u0026rsquo;il existe une quelconque possibilité $\\Diamond A$, alors $\\Diamond (A\\lor B)$ aussi est vraie, et ce quel que soit $B$ (s\u0026rsquo;il ya un monde accessible où $A$ est vraie, alors dans ce même monde, $A\\lor B$ est vraie) et grâce à notre pernicieux nouvel axiome, on déduit la vérité de $\\Diamond A\\land\\Diamond B$ qui se réduit à celle de $\\Diamond B$ (puisque par hypothèse $\\Diamond A$ est vraie). On a finalement $\\Diamond A\\rightarrow \\Diamond B$ pour tout $B$. La possibilité de $A$ rend tout possible grâce à cet axiome diabolique.\nOn prouve que $\\Diamond(A\\lor B)\\rightarrow(\\Diamond A\\land\\Diamond B)$ n\u0026rsquo;est pas prouvable dans le système $\\textbf{ KD}$ en trouvant un modèle appartenant à $\\mathscr{C}_{n.b.d.}$ et tel qu\u0026rsquo;au moins un de ses nœuds ne satisfait pas la formule.\nUn dernier pour la route : le dilemme de Sartre (dû à Lemmon). Il décrit l\u0026rsquo;impossibilité de représenter un conflit d\u0026rsquo;obligations en logique déontique standard (à cause de l\u0026rsquo;axiome $\\text{(D)}$). La situation dans laquelle se retrouve Jephté dans la Bible en est une illustration parfaite. Jephté promet à Dieu de sacrifier le premier être vivant qu\u0026rsquo;il croise s\u0026rsquo;il gagne la guerre. Il gagne la guerre et croise sa fille (c\u0026rsquo;est ballot). On a bien conflit d\u0026rsquo;obligations puisque l\u0026rsquo;obligation de ne pas tuer sa fille doit bien figurer quelque part aussi\u0026hellip; On a donc une situation où il est obligatoire de faire une chose $\\Box A$ et aussi obligatoire de ne pas la faire $\\Box \\neg A$. Or $\\Box A\\rightarrow \\Diamond A$ d\u0026rsquo;après $\\text{(D)}$ et par dualité, on obtient $\\Box A\\rightarrow \\neg \\Box\\neg A$. La situation de départ (la satisfaction de $\\Box A$ et $\\neg\\Box A$), bien qu\u0026rsquo;envisageable, aboutit à une inconsistance interdisant donc même son existence dans le système formel $\\textbf{ KD}$ (qui commet sûrement là un pécher).\nMalgré ses \u0026ldquo;petits\u0026rdquo; défauts, la logique déontique connait différentes applications en informatique. En autorisant l\u0026rsquo;expression de raisonnements normatifs, elle permet de spécifier la marche à suivre en cas de comportement anormal. De manière plus général, elle propose un cadre pour gérer les autorisations et politiques d\u0026rsquo;accès, et on la retrouve dans le secteur de l\u0026rsquo;automatisation juridique ou encore celui des contraintes sur les bases de données.\nLogique épistémique Elle permet de raisonner à propos de la connaissance d\u0026rsquo;un ou plusieurs agents.\nC\u0026rsquo;est elle qui est au cœur de notre histoire introductive de dragons ou de cette blague sur des logiciens assoiffés.\nAppelons $1$, $2$ et $3$ les trois logiciens de gauche à droite et $B_i$ l'affirmation \"le logicien $i$ veut une bière\".\nAu départ, $1$ veut une bière (et elle le sait\u0026hellip;), mais elle ne peut pas savoir dans lequel des 4 mondes restant possibles elle se trouve : $[1]B_1\\land\\langle1\\rangle B_2\\land\\langle1\\rangle B_3$. Comme le monde où $B_1\\land B_2\\land B_3$ fait partie des mondes possibles, elle répond \u0026ldquo;je ne sais pas\u0026rdquo;. $2$ en déduit $B_1$. Car si $\\neg B_1$, alors $[1]\\neg B_1$ et donc le monde $B_1\\land B_2\\land B_3$ où tous veulent picoler n\u0026rsquo;est plus accessible et elle aurait répondu non. Voulant elle aussi une bière, $2$ hésite encore entre deux mondes $[2]B_1\\land[2]B_2\\land\\langle2\\rangle B_3$. $3$ en déduit $B_2$ et peut maintenant répondre puisque $[3]B_1\\land[3]B_2\\land[3] B_3$ ($3$ aussi veut une bière bien sûr). Plus qu\u0026rsquo;un monde possible, celui qui satisfait $B_1\\land B_2\\land B_3$. C\u0026rsquo;est un grand \u0026ldquo;Oui !\u0026rdquo;. La règle de nécessitation associée à l\u0026rsquo;axiome $\\text{(K)}$ (valables dans tout nœud de tout système d\u0026rsquo;une logique normale) impliquent l\u0026rsquo;omniscience logique de l\u0026rsquo;agent $i$. En effet, par nécessitation $A\\rightarrow B$ devient $[i](A\\rightarrow B)$ et $\\text{(K)}=[i](A\\rightarrow B)\\rightarrow([i]A\\rightarrow [i]B)$. Par conséquent, l\u0026rsquo;agent connait toutes les conséquences logique ($[i]B$) de ce qu\u0026rsquo;il sait ($[i]A$).\nEn logique modale épistémique, la relation binaire entre deux mondes est une relation d\u0026rsquo;indistinguabilité ; l\u0026rsquo;agent ne sait pas dans quel monde il se trouve. Cette relation est nécessairement symétrique par la notion même d\u0026rsquo;indistinguabilité. Et comme un monde ne pourra jamais être distingué de lui-même, il faut ajouter une relation réflexive de chaque monde à lui-même. Le dernier ingrédient est la transitivité. Si $a$ est indistinguable de $b$ et que $b$ est indistinguable de $c$, alors $a$ est aussi indistinguable de $c$. Une telle relation, symétrique, réflexive et transitive est une relation d\u0026rsquo;équivalence. Et on aurait pu partir de là finalement puisque c\u0026rsquo;est bien l\u0026rsquo;équivalence qui se cache derrière la notion d\u0026rsquo;indistinguabilité.\nLa classe de systèmes de transition assurant une relation d\u0026rsquo;équivalence entre les mondes possibles correspond à la logique $\\textbf{ S5}$.\nDeux jeux d\u0026rsquo;axiomes équivalents permettent d\u0026rsquo;obtenir $\\textbf{ S5}$ : $\\set{(\\text{K}),(\\text{T}),(\\text{B}),(\\text{4})}$ où $\\text{(T)}$ apporte la réflexivité, $\\text{(B)}$ la symétrie et $\\text{(4)}$ la transitivité, ou bien le jeu $\\set{(\\text{K}),(\\text{T}),(\\text{5})}$ où $\\text{(5)}$ apporte l\u0026rsquo;euclidanité sachant qu\u0026rsquo;un système de transition euclidien est à la fois symétrique et transitif.\nLes axiomes doivent être vérifiés dans tous les mondes possibles. Ainsi $\\text{(T)}=[i]P\\rightarrow P$ signifie que dans un monde quelconque, si l\u0026rsquo;agent $i$ sait que $P$, alors $P$ est vrai dans ce monde (l\u0026rsquo;agent ne sait que des choses vraies). Et il faut aussi que soit réalisée $\\text{(5)}=\\neg[i]P\\rightarrow[i]\\neg[i]P$, ce qui signifie que si l\u0026rsquo;agent $i$ ne sait pas quelque chose, alors il sait qu\u0026rsquo;il ne le sait pas. C\u0026rsquo;est le principe d\u0026rsquo;introspection négative.\nL\u0026rsquo;agent de la logique $\\textbf{ S5}$ est une sorte d\u0026rsquo;agent idéal pour lequel l\u0026rsquo;accès à la connaissance est maximal (sans qu\u0026rsquo;il ne sache tout pour autant). Il n\u0026rsquo;est tellement pas humain qu\u0026rsquo;il ne peut pas croire\nOn place parfois l\u0026rsquo;agent dans une logique inférieure, $\\textbf{ S4}$, qui perd la symétrie (et donc l\u0026rsquo;indistinguabilité !). L\u0026rsquo;introspection négative disparaît alors et est remplacée par l\u0026rsquo;introspection positive $\\text{(4)}:[i]P\\rightarrow [i][i] P$ où si l\u0026rsquo;agent sait quelque chose, alors il sait qu\u0026rsquo;il sait. Mine de rien, ça lui limite pas mal l\u0026rsquo;accès à la connaissance par rapport à l\u0026rsquo;autre introspection. L\u0026rsquo;agent reste idéal en ce qu\u0026rsquo;il conserve une omniscience logique totale, mais il peut vivre dans un monde ou il peut croire possible qu\u0026rsquo;il sait quelque chose qu\u0026rsquo;il ne sait pas (la honte).\nDans $\\textbf{ S4}$, l\u0026rsquo;accessibilité n\u0026rsquo;est plus nécessairement réciproque entre les mondes. Cela introduit une hiérarchie, un ordre où certains mondes peuvent être considérés comme des extensions ou des raffinements d\u0026rsquo;autres, des mondes \u0026ldquo;plus informés\u0026rdquo;. C\u0026rsquo;est pertinent pour modéliser la communication dans des réseau où l\u0026rsquo;information n\u0026rsquo;est pas toujours réciproquement échangée (dès que les canaux sont asymétriques comme c\u0026rsquo;est le cas dans la plupart des structures hiérarchiques).\nCouplé à une logique temporelle, cela permet de modéliser un apprentissage progressif où l\u0026rsquo;acquisition d\u0026rsquo;information se fait de manière séquentielle ou cumulative. En robotique ou en intelligence artificielle, cela permet dans la même veine de modéliser des processus de planification où chaque décision ou action mène à un nouvel état du monde qui intègre toutes les connaissances et conséquences des actions précédentes.\nLogique temporelle Il y a beaucoup de logiques temporelles qui multiplient généralement les opérateurs modaux. Mais de base, une logique modale temporelle peut exprimer ce qu\u0026rsquo;on trouve dans le carré des oppositions ci-dessous.\nLa logique temporale peut aussi bien s\u0026rsquo;intéresser au passé qu\u0026rsquo;au futur.\nLe temps dont il est question ici est discret, chaque monde possible correspondant à un instant $t$.\nPas de raison particulière pour que la relation d\u0026rsquo;accessibilité entre ces mondes soit réflexive ou symétrique, par contre, elle se doit d\u0026rsquo;être transitive pour conserver l\u0026rsquo;ordonnancement entre les différents instants considérés. L\u0026rsquo;axiome clé est donc le $\\text{(4)}$ et le futur d\u0026rsquo;un monde correspond alors à l\u0026rsquo;ensemble des mondes qui lui sont accessibles (l\u0026rsquo;absence de réflexivité entraîne que le monde actuel ne fasse pas partie du futur).\nOn peut alors traduire les opérateurs modaux par\u0026nbsp;:\n$\\Box \\phi$ : $\\phi$ sera toujours vérifiée, $\\Diamond \\phi$ : $\\phi$ sera vérifiée à un certain moment. Cela permet d'exprimer des énoncés tels que\u0026nbsp;:\n$\\Box\\neg(\\texttt{started}\\land\\neg\\texttt{ready})$ : il est impossible d\u0026rsquo;atteindre un état où $\\texttt{started}$ est activé, mais pas $\\texttt{ready}$, $\\Box(\\texttt{request}\\rightarrow\\Diamond\\texttt{acknowledged})$ : pour tout état, si une demande (de quelque ressource que ce soit) est effectuée, alors elle sera finalement reconnue, Le temps des logique temporelles peut prendre une forme linéaire ou arborescente.\nDans la logique temporelle linéaire (LTL), les instants s\u0026rsquo;enchaînent en un collier de perles infini. C\u0026rsquo;est idéal pour explorer un système déterministe ou une seule branche d\u0026rsquo;exécution et vérifier efficacement que toutes les spécifications sont satisfaites.\nDans une logique temporelle arborescente (CTL), le champ des possibles s\u0026rsquo;ouvrent. Elle permet de modéliser des systèmes non déterministes ou des choix amenant sur des branches différentes peuvent être faits.\nLa logique temporelle est très utilisée en vérification formelle, où la technique de base est essentiellement le model checking.\nLogique de la prouvabilité Souhaitant éprouver la stabilité de l\u0026rsquo;édifice mathématique, en particulier l\u0026rsquo;arithmétique (la science des nombres, le cœur des maths), Gödel a imaginé une méthode astucieuse, une sorte de hack qui lui permit de plonger les énoncés logiques dans le champ de l\u0026rsquo;arithmétique (en particulier sur l\u0026rsquo;Arithmétique de Peano $\\text{PA}$, une théorie - ensemble d\u0026rsquo;axiomes - en logique du premier ordre dont l\u0026rsquo;arithmétique des entiers est un modèle). En associant à chaque formule un numéro (le numéro de Gödel ou code de Gödel), noté $\\ulcorner\\phi\\urcorner$, il a permis aux nombres de parler d\u0026rsquo;eux-mêmes dans une sorte d\u0026rsquo;épanchement introspectif.\nCe qui semblait à l\u0026rsquo;époque un processus éminemment complexe est devenu trivial à l\u0026rsquo;ère de l\u0026rsquo;informatique. Pour associer un nombre unique à une formule, il suffit par exemple de regarder le code binaire du fichier de traitement de texte où la formule est écrite (et d\u0026rsquo;ajouter éventuellement un \u0026ldquo;1\u0026rdquo; au début si ça commence par des \u0026ldquo;0\u0026rdquo;).\nOn peut même coder sans problème un arbre de preuve complet (tout ce qu\u0026rsquo;on peut écrire dans un traitement de texte finalement). Dès lors, une formule peut très bien parler du fait qu\u0026rsquo;une autre formule est démontrable puisqu\u0026rsquo;il s\u0026rsquo;agit de dire qu\u0026rsquo;il existe un entier possédant les propriétés qui le caractérisent comme étant le code d\u0026rsquo;une preuve de cette formule. C\u0026rsquo;est ce que fait la formule $\\Box\\phi$ où l\u0026rsquo;opérateur de modalité $\\Box$ prend alors le sens de \u0026ldquo;est prouvable\u0026rdquo; (elle équivaut à la fameuse formule de Gödel $G\\leftrightarrow\\neg\\text{Bew}(\\ulcorner G\\urcorner)$, $\\Box$ correspondant à $\\text{Bew}(x)$, \u0026ldquo;bew\u0026rdquo; étant le début du mot allemand beweisbar qui signifie prouvable).\nLe premier théorème d\u0026rsquo;incomplétude de Gödel s\u0026rsquo;appuie sur une formule affirmant sa propre non prouvabilité $\\neg\\Box\\phi\\leftrightarrow \\phi$. Si on peut démontrer la formule $\\neg\\Box\\phi\\leftrightarrow \\phi$ dans un système donné, alors on prouve qu\u0026rsquo;il existe une formule non démontrable et pourtant vraie, ce qui impose l\u0026rsquo;incomplétude du système.\nLemme de diagonalisation :\nPour toute formule arithmétique $f(x)$, il existe une formule arithmétique $P$ telle que $P\\leftrightarrow f(P)$.\nC\u0026rsquo;est l\u0026rsquo;outil permettant à Gödel de construire des formules autoréférentes comme $\\neg\\Box\\phi\\leftrightarrow \\phi$.\nLe lemme dit que pour toute formule $\\phi$, il existe une formule $S(\\ulcorner\\phi\\urcorner)$ qui parle de $\\phi$ (une formule méta en somme) telle qu\u0026rsquo;on peut prouver $\\phi$ si et seulement si on peut prouver $S(\\ulcorner\\phi\\urcorner)$ : $\\vdash\\phi\\leftrightarrow S(\\ulcorner\\phi\\urcorner)$.\nPour démontrer le lemme, on va supposer que $n$ est le numéro d\u0026rsquo;une formule de la forme $A(x)$ où $x$ est une variable et on définit $F(n)=\\ulcorner A(\\ulcorner A(x)\\urcorner)\\urcorner$ et si $n$ ne correspond pas à une formule sous cette forme, alors $F(n)=0$.\nNotons que $F$ dépend du choix de la variable $x$. Ainsi, $F(\\ulcorner y=0 \\urcorner)=\\ulcorner(\\ulcorner y=0 \\urcorner) = 0\\urcorner$ et $F(\\ulcorner x=0 \\urcorner)=\\ulcorner(\\ulcorner x=0 \\urcorner) = 0\\urcorner$.\nSoit $g(w)\\equiv S(F(w))$ et $\\phi\\equiv g(\\ulcorner g(x)\\urcorner)$.\nOn obtient :\n$$ \\begin{aligned} \\phi \u0026amp;= g(\\ulcorner g(x)\\urcorner)\\\\ \u0026amp;= S(F(\\ulcorner g(x)\\urcorner) \\\\ \u0026amp;= S(\\ulcorner g(\\ulcorner g(x)\\urcorner)\\urcorner)\\\\ \u0026amp;= S(\\ulcorner\\phi\\urcorner) \\end{aligned} $$\nTrès informellement, le lemme de diagonalisation exprime l\u0026rsquo;échec de l\u0026rsquo;argument de la diagonale de Cantor pour construire une formule $S(x)$ qui ne ferait pas partie de l\u0026rsquo;ensemble dénombrable des formules prouvables. La clôture de l\u0026rsquo;ensemble des formules prouvables par action d\u0026rsquo;une formule sur les éléments de cet ensemble empêche en effet de \u0026ldquo;diagonaliser à l\u0026rsquo;extérieur\u0026rdquo; du domaine.\nLa formule autoréférente de Gödel est proche du fameux paradoxe du menteur \u0026ldquo;cette phrase est fausse\u0026rdquo;.\nPremier théorème d\u0026rsquo;incomplétude :\nDans un système formel consistant (= cohérent = système dans lequel on ne peut pas prouver le faux) suffisamment expressif pour décrire l\u0026rsquo;arithmétique, on ne peut pas prouver toutes les formules vraies (comme la formule qui affirme sa non prouvabilité). La prouvabilité est plus faible que la vérité.\nMontrons dans la logique $\\textbf{ K4}$ que $\\vdash\\phi\\leftrightarrow\\neg\\Box\\phi$ permet d\u0026rsquo;arriver à $\\vdash\\neg\\Box\\bot\\rightarrow\\neg\\Box\\phi$ ; la formule n\u0026rsquo;est pas prouvable si aucune contradiction n\u0026rsquo;est prouvable. On obtient donc bien l\u0026rsquo;essence du premier théorème d\u0026rsquo;incomplétude.\n$\\phi\\leftrightarrow\\neg\\Box\\phi$ $\\Box\\phi\\rightarrow\\neg\\phi$ (à partir de 1.) $\\Box(\\Box\\phi\\rightarrow\\neg\\phi)$ (nécessitation) $\\Box\\Box\\phi\\rightarrow\\Box\\neg\\phi$ (distribution de la boite grâce à $\\text{(K)}$) $\\Box\\phi\\rightarrow\\Box\\neg\\phi$ (par l'axiome $\\text{(4)}= \\Box \\phi\\rightarrow\\Box\\Box\\phi$ et avec l'étape 4., on déduit $\\Box\\phi\\rightarrow\\Box\\Box\\phi\\rightarrow\\Box\\neg\\phi$) $\\Box\\phi\\rightarrow\\Box\\phi\\land\\Box\\neg\\phi$ (à partir de 5. et $\\Box\\phi\\rightarrow\\Box\\phi$) $\\Box\\phi\\rightarrow\\Box(\\phi\\land \\neg\\phi)$ $\\Box\\phi\\rightarrow\\Box\\bot$ $\\neg\\Box\\bot\\rightarrow \\neg\\Box\\phi$ (contraposée de 8.) Le théorème de Löb utilise un autre formule autoréférente, $\\Box(\\Box\\phi\\rightarrow\\phi)\\rightarrow\\Box\\phi$, s\u0026rsquo;apparentant plutôt, dans son cas, au paradoxe d\u0026rsquo;un diseur de vérité \u0026ldquo;ce que je dis là est vrai\u0026rdquo; (aucune valeur de vérité ne semble applicable puisqu\u0026rsquo;un menteur dirait la même chose).\nEt la surprise est que de tels points fixes sont prouvables et par conséquent vrais.\nLe théorème de Löb stipule que les formules dont on peut prouver que $\\Box\\phi\\rightarrow\\phi$ sont les formules prouvables :\nSi $\\vdash\\Box\\phi\\rightarrow\\phi$, alors $\\vdash \\phi$. Le théorème affirme donc que si on peut prouver une formule en admettant comme axiome qu\u0026rsquo;on peut la prouver, alors on peut la prouver sans l\u0026rsquo;axiome. Le théorème est donc une sorte de méthode Coué logique, une prophétie autoréalisatrice ; \u0026ldquo;si tu crois en toi, ça va le faire\u0026rdquo;. C\u0026rsquo;est bizarre, mais ça se démontre.\nPreuve du théorème de Löb :\nOn suppose $\\vdash \\Box\\phi\\rightarrow\\phi$.\nGrâce au lemme de diagonalisation, on construit la formule arithmétique autoréférente suivante\u0026nbsp;:\n$\\vdash\\sigma\\leftrightarrow (\\Box\\sigma\\rightarrow\\phi)$ (\"ma propre prouvabilité implique $\\phi$\").\n$\\vdash\\Box(\\sigma\\rightarrow (\\Box\\sigma\\rightarrow\\phi))$ (par nécessitation) $\\vdash\\Box\\sigma\\rightarrow \\Box(\\Box\\sigma\\rightarrow\\phi)$ puis $\\vdash\\Box\\sigma\\rightarrow (\\Box\\Box\\sigma\\rightarrow\\Box\\phi)$ (distribution de la boite grâce à $\\text{(K)}$) On a donc $\\vdash \\Box\\sigma\\rightarrow\\Box\\Box\\sigma$ et $\\vdash \\Box\\sigma\\rightarrow\\Box\\phi$ $\\vdash \\Box\\sigma\\rightarrow\\phi$ (car $\\vdash \\Box\\sigma\\rightarrow\\Box\\phi\\rightarrow\\phi$ puisqu'on suppose $\\vdash \\Box\\phi\\rightarrow\\phi$) $\\vdash (\\Box\\sigma\\rightarrow\\phi)\\rightarrow\\sigma$ (sens réciproque de la formule autoréférente) $\\vdash\\sigma$ par modus ponens de 4. et 5. $\\vdash\\Box\\sigma$ par nécessitation. $\\vdash \\phi$ par modus ponens de 4. et 7. Si un système formel est consistant alors il ne peut pas prouver sa propre consistance. Et donc les seuls systèmes qui peuvent prouver leur consistance sont ceux qui sont inconsistants\u0026hellip;\nC\u0026rsquo;est le second théorème d\u0026rsquo;incomplétude de Gödel.\nCe théorème n\u0026rsquo;est autre que la contraposée du théorème de Löb !\nAvec $\\phi=\\bot$, on obtient :\nsi $\\vdash\\neg\\Box\\bot$, alors $\\vdash\\neg\\Box(\\Box\\bot\\rightarrow\\bot)$ et comme $\\Box\\bot\\rightarrow\\bot\\equiv\\neg\\Box\\bot\\lor\\bot\\equiv\\neg\\Box\\bot$, on a finalement si $\\vdash\\neg\\Box\\bot$, alors $\\vdash\\neg\\Box(\\neg\\Box\\bot)$\nPar dualité, $\\Diamond P = \\neg\\Box\\neg P$. Donc $\\Diamond P$ exprime qu\u0026rsquo;on ne peut pas prouver que $P$ est faux. Autrement dit, $\\Diamond P$ affirme que $P$ est consistant (cohérent) dans le système formel.\n$\\neg\\Box\\bot\\rightarrow\\neg\\Box(\\neg\\Box\\bot)$ devient alors $\\Diamond\\top\\rightarrow \\neg\\Box(\\Diamond \\top)$ (\u0026ldquo;si une théorie est consistante, alors on ne peut pas prouver que la théorie est consistante\u0026rdquo;).\nLa logique de la prouvabilité de Gödel-Löb ($\\textbf{ GL}$) est formée à partir du système $\\textbf{ K}$ auquel on ajoute l\u0026rsquo;axiome de Löb $\\text{(L)}=\\Box(\\Box\\phi\\rightarrow\\phi)\\rightarrow\\Box\\phi$ (c\u0026rsquo;est la version formalisée du théorème de Löb).\n$\\vdash_\\textbf{ GL}\\text{(4)}=\\Box\\phi\\rightarrow\\Box\\Box \\phi$\nEt donc $\\textbf{ K4}≤\\textbf{ KL}$ ($\\textbf{ K4}$ se réduit à $\\textbf{ KL}$).\nObtention d\u0026rsquo;un premier théorème : $\\vdash_\\textbf{ K}\\Box P\\rightarrow \\Box ((\\Box P\\land \\Box\\Box P)\\rightarrow(P\\land \\Box P))$\nPuis d'un deuxième\u0026nbsp;: $\\vdash_\\textbf{ K}\\Box (P\\land\\Box P)\\rightarrow \\Box\\Box P$ On les imbrique à l'aide de l'axiome de Löb $\\text{(L)}$\u0026nbsp;: La classe des systèmes de transition qui valide $\\textbf{ GL}$ est donc transitive !\nMontrons de plus que cette classe est irréflexive et asymétrique.\nIl suffit de trouver un modèle qui valide $\\color{#1DB100}\\text{(L)}$ sans valider ni $\\color{#FF42A1}\\text{(B)}=\\Box P\\rightarrow \\Diamond\\Box P$, ni $\\color{#F27200}\\text{(T)}=\\Box P \\rightarrow P$.\nOr un graphe transitif et non réflexif, ni symétrique n\u0026rsquo;est autre qu\u0026rsquo;un arbre. On va aller un peu plus loin en montrant que ces arbres doivent être en plus finis. Et c\u0026rsquo;est plutôt génial que cette logique de la prouvabilité s\u0026rsquo;exprime naturellement sur des arbres finis qui représentent intuitivement la notion de preuve (la racine représentant la formule prouvée, et les feuilles les axiomes).\nLa logique $\\textbf{ GL}$ est correcte et complète par rapport à la classe $\\mathscr{C}_{\\text{arbres finis}}$ des modèles de Kripke à la fois transitifs, irréflexifs, asymétriques et bornés à droite (sans suite infinie de mondes).\nCorrection :\nSi $ \\vdash_\\textbf{ GL} \\phi$, alors $\\models_{\\mathscr{C}_{\\text{arbres finis}}}\\phi$.\nComme $\\text{(L)}$ est le seule axiome ajouté à $\\textbf{ K}$, il faut montrer que l\u0026rsquo;ensemble des systèmes de transition qui valide l\u0026rsquo;axiome de Löb est restreint aux seuls arbres finis (l\u0026rsquo;axiome $\\text{(K)}$ est valide dans $\\mathscr{C}$ et les règles de la logique modale normale conserve la validation).\nSupposons que l\u0026rsquo;axiome ne soit pas satisfait en un nœud $a$ d\u0026rsquo;un arbre fini $\\mathcal{T}=(N,A)$. On a donc $\\langle \\mathcal{T},a\\rangle \\not\\models \\Box(\\Box P\\rightarrow P)\\rightarrow \\Box P$.\nCela implique :\n$\\langle \\mathcal{T},a\\rangle \\models \\Box(\\Box P\\rightarrow P)$, et $\\langle \\mathcal{T},a\\rangle \\not\\models \\Box P$. La deuxième affirmation entraîne qu'il existe un nœud $a'\\in N$ tel que $a\\longrightarrow a'$ et $\\langle \\mathcal{T},a\\rangle \\not\\models P$\nPar contre, la première affirmation nous assure que $\\langle \\mathcal{T},a'\\rangle \\models \\Box P\\rightarrow P$.\nPar conséquent $\\langle \\mathcal{T},a'\\rangle \\not\\models \\Box P$. Cela signifie qu'il existe un nœud $a''\\in N$, successeur de $a'$ ($a'\\longrightarrow a''$) tel que $\\langle \\mathcal{T},a''\\rangle \\not\\models P$.\nPar transitivité, $a \\longrightarrow a''$. Or si on retourne à l'affirmation $1.$, on a aussi $\\langle \\mathcal{T},a''\\rangle \\models \\Box P\\rightarrow P$, ce qui va conduire de fil en aiguille à une séquence infinie de successeurs de $a$, contredisant ainsi que $\\mathcal{T}$ soit fini.\nOn en conclut que l\u0026rsquo;axiome de Löb et donc $\\textbf{ KL}$ est valide dans tout arbre fini.\nComplétude :\nSi $\\models_{\\mathscr{C}_{\\text{arbres finis}}}\\phi$, alors $\\vdash_\\textbf{ GL} \\phi$.\nComme entrevu plus haut, l\u0026rsquo;opérateur de modalité peut s\u0026rsquo;interpréter comme la formule arithmétique de Gödel $\\text{Bew}(x)$. Il s\u0026rsquo;agit plus précisément d\u0026rsquo;une formule de l\u0026rsquo;Arithmétique de Peano $\\mathbf{PA}$. Il se trouve qu\u0026rsquo;on peut traduire toute formule de la logique modale dans $\\textbf{ PA}$ et on montre alors que tout théorème de $\\textbf{ GL}$ est prouvable dans $\\textbf{ PA}$. C\u0026rsquo;est ce qu\u0026rsquo;on peut appeler la correction arithmétique de $\\textbf{ GL}$. Et mieux, Solovay a prouvé aussi sa complétude arithmétique ; toute formule prouvable de l\u0026rsquo;arithmétique est un théorème de $\\textbf{ GL}$.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/",
	"title": "Maths",
	"tags": [],
	"description": "",
	"content": " Un peu de maths Géométrie Le triangle Cercle et triangle Aires et volumes Dimensions Poursuites Pythagore Thalès Droite d'Euler Trigonométrie Angle inscrit Algèbre Équations Identités remarquables Algèbre de Boole Algèbre linéaire Combinatoire et graphes Théorie des groupes Multiplication de polynômes et FFT Arithmétique Infinis dénombrables et indénombrables Numération Nombres algébriques Fractions continues Algorithme d'Euclide Triplets pythagoriciens Divisibilité et nombres premiers Probabilités Loi binomiale Loi normale Densité de probabilités Probabilités conditionnelles et Bayes Chaînes de Markov Curiosités et énigmes Spaghetti et inégalité triangulaire Anniversaires simultanés Paradoxe des deux enfants Paradoxe de Cover Statistiques Les différentes moyennes Monte Carlo Paradoxe de Simpson Test d'hypothèse Analyse en composante principale p-hacking Écart-type expérimental Inférences bayesiennes Théorie des jeux Attaque/Défense "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/meca/action/",
	"title": "Moindre action",
	"tags": [],
	"description": "",
	"content": " Principe de moindre action Maupertuis formule ainsi le principe de moindre action en 1744 :\nLorsqu’il arrive quelque changement dans la nature, la quantité d’action, nécessaire pour ce changement, est la plus petite qui soit possible.\nUne formulation plus moderne serait la minimisation du transfert d\u0026rsquo;énergie entre les réservoirs cinétique et potentiel au cours du temps dévolu au trajet entre deux points.\nMaupertuis reprenait ainsi le flambeau de Fermat qui avait introduit dès 1657 le principe de moindre temps pour expliquer les trajets des rayons lumineux en optique géométrique.\nDe tels principes sont dits variationnels car il s\u0026rsquo;agit d\u0026rsquo;étudier et minimiser les variations entre deux points.\nLeur motivation originelle était métaphysique : par sa perfection, la nature se devait d\u0026rsquo;être optimale. On trouva ensuite des justifications plus scientifiques\u0026hellip;\nConceptuellement, il s\u0026rsquo;agit d\u0026rsquo;une démarche descendante (top-down) consistant à fixer un but global et à en déduire ensuite les équations locales qui ont été nécessaires pour l\u0026rsquo;atteindre. C\u0026rsquo;est l\u0026rsquo;inverse de la démarche ascendante (bottom-up) plus familière (car c\u0026rsquo;est celle enseignée au lycée) qui part des équations locales pour aboutir de proche en proche au global (par un processus itératif).\nEn mécanique classique par exemple, le local, c\u0026rsquo;est Newton : on construit la trajectoire complète en intégrant une équation différentielle depuis une position et une vitesse initiale. Et le global, c\u0026rsquo;est Lagrange ou Hamilton : on fixe deux points et on cherche la bonne trajectoire parmi l\u0026rsquo;ensemble des trajectoires possibles (celle pour laquelle l\u0026rsquo;action est stationnaire).\nLes théories mécaniques sont progressivement passées d\u0026rsquo;ascendantes à descendantes. Pour la lumière, c\u0026rsquo;est l\u0026rsquo;inverse.\nL\u0026rsquo;évolution des idées en optique est passionnante en soi, mais si on s\u0026rsquo;y attarde ici, c\u0026rsquo;est surtout pour l\u0026rsquo;influence importante qu\u0026rsquo;elles ont eue sur certains architectes de la mécanique classique puis quantique. Des éléments propres aux théories optiques vont ainsi progressivement percoler en mécanique.\nOptique Principe de moindre temps de Fermat Reprenons l\u0026rsquo;idée de Fermat pour déterminer le chemin d\u0026rsquo;un rayon lumineux réfracté d\u0026rsquo;un milieu à l\u0026rsquo;autre. L\u0026rsquo;idée clé de cette manière de calculer par minimisation est de se donner un point de départ I et un point d\u0026rsquo;arrivée F et de chercher, parmi toutes les trajectoires possibles les rejoignant, la plus courte.\nOn constate déjà que si I et F sont dans le même milieu homogène, la trajectoire la plus courte sera le segment de droite [IF] et donc le principe de Fermat prédit bien que la lumière se propage rectilignement dans un milieu homogène.\nEt quand le point d\u0026rsquo;arrivée F est dans un autre milieu que le point de départ I, la prise en compte de l\u0026rsquo;effet du milieu comme un ralentissement apparent de la vitesse de la lumière (dans un milieu d\u0026rsquo;indice optique $n$, la lumière voyage à la vitesse $c/n$ où $c$ est sa célérité dans le vide, soit $300\\,000$ km/s) permet de retrouver le phénomène de réfraction : la lumière change de direction en passant d\u0026rsquo;un milieu à l\u0026rsquo;autre. La déviation est telle que le trajet global sera le moins long possible et donc la partie du trajet dans le milieu d\u0026rsquo;indice optique supérieur sera plus petite que l\u0026rsquo;autre.\nDans le programme ci-dessous, les temps de parcours de différents trajets pour rejoindre un point dans l\u0026rsquo;air à un point dans l\u0026rsquo;eau sont comparés. Le trajet minimal est déterminé et on montre qu\u0026rsquo;il respecte la loi de la réfraction de Snell-Descartes.\nC\u0026rsquo;est très élégant, mais n\u0026rsquo;est-ce pas que cela ? un raffinement calculatoire ? Comment la lumière pourrait-elle aller explorer les différents chemins avant de choisir le moins long ? Cela paraît saugrenu\u0026hellip; Du moins du côté particulaire de la description de la lumière, car pour une onde, ce n\u0026rsquo;est pas si bizarre. Une onde est en effet intrinsèquement la modélisation d\u0026rsquo;un comportement collectif. Considérer une onde, c\u0026rsquo;est donc bien considérer différents trajets pris \u0026ldquo;à la fois\u0026rdquo;.\nOndelettes de Huygens Pour apprivoiser ce mouvement collectif, Huygens a l\u0026rsquo;idée en 1678 de décrire tout point atteint par la lumière comme la source d\u0026rsquo;une nouvelle ondelette sphérique qui va se propager depuis ce point. Reconstituer le trajet de la lumière revient alors à regarder où les fronts d\u0026rsquo;onde de ces ondelettes s\u0026rsquo;accumulent pour former une enveloppe, le rayon perçant cette enveloppe perpendiculairement.\nHuygens parvient avec son principe de superposition à retrouver l\u0026rsquo;ensemble des prédictions de Fermat. L\u0026rsquo;optique géométrique tenait donc là deux explications alternatives ; l\u0026rsquo;une variationnelle et l\u0026rsquo;autre à base de fronts d\u0026rsquo;onde.\nPour être plus précis et faciliter le parallèle avec Hamilton ensuite, explicitons les définitions.\nSoit l\u0026rsquo;ensemble des points $q$ jusqu\u0026rsquo;où la lumière issue d\u0026rsquo;un point $q_0$ peut traverser en un temps inférieur ou égal à $t$. La frontière de cet ensemble, $\\Phi_{q_0}(t)$ est appelé le front d\u0026rsquo;onde du point $q_0$ après un temps $t$ et regroupe les points que la lumière peut atteindre en un temps $t$ mais pas avant.\nLe principe de Huygens peut alors s\u0026rsquo;exprimer ainsi :\nsoit $\\Phi_{q_0}(t)$ le front d\u0026rsquo;onde issu du point $q_0$ après un temps $t$. Pour chaque point $q$ de ce front, on considère le front d\u0026rsquo;onde au temps $s$, $\\Phi_{q}(s)$. Alors le front d\u0026rsquo;onde issu du point $q_0$ après le temps $t+s$, $\\Phi_{q_0}(t+s)$ sera l\u0026rsquo;enveloppe des fronts $\\Phi_{q}(s),q\\in \\Phi_{q_0}(t)$.\nEn fixant un point $q_0$, on appelle fonction caractéristique $S_{q_0}(q)$ la fonction qui associe à tout point $q$ le longueur du chemin optique de $q_0$ à $q$.\nLes courbes de niveau de la fonction caractéristique ne sont autre que les fronts d\u0026rsquo;onde.\nL\u0026rsquo;animation ci-dessous montre comment l\u0026rsquo;enveloppe des fronts d\u0026rsquo;onde est naturellement déviée lors d\u0026rsquo;un changement de milieu si la vitesse de propagation change, expliquant ainsi la réfraction.\nMême si ses \u0026ldquo;ondelettes\u0026rdquo; préfigurent évidemment de véritables ondes, Huygens se contente de modéliser l\u0026rsquo;optique géométrique avec sa théorie et il s\u0026rsquo;imagine d\u0026rsquo;ailleurs les ondelettes s\u0026rsquo;évanouir partout ailleurs que sur l\u0026rsquo;enveloppe.\nVers l\u0026rsquo;optique ondulatoire En 1803, Young publie un dessin expliquant comment la lumière passant à travers deux fentes étroites donne naissance à des franges alternativement brillantes et sombres sur un écran.\nOn est encore très proche des ondelettes de Huygens sauf qu\u0026rsquo;on tourne notre attention sur l\u0026rsquo;ensemble du réseau plutôt qu\u0026rsquo;exclusivement sur les enveloppes.\nLa théorie de Huygens nécessite une superposition de suffisamment de sources pour clairement définir ses fronts d\u0026rsquo;onde.\nDans l\u0026rsquo;image précédente, on a additionné des sources sur un petit segment vertical. L\u0026rsquo;effet de moiré nous donne un aperçu du phénomène de diffraction.\nEn étirant le segment avec de nouvelles sources, les enveloppes finissent par mieux se définir ; on retrouve une propagation rectiligne (du moins autour de l\u0026rsquo;axe de symétrie perpendiculaire aux sources).\nSi on peut comprendre géométriquement l\u0026rsquo;apparition de zone brillante par superposition des réseaux d\u0026rsquo;ondelettes, l\u0026rsquo;existence de zone parfaitement sombre est plus mystérieuse. Comment des zones éclairées plusieurs fois pourraient-elles se trouver obscurcies ?\nFresnel et la phase L\u0026rsquo;ingrédient manquant pour expliquer les interférences est apporté en 1818 par Fresnel : il donne une phase aux ondelette ! Un traitement mathématique complet devenait possible et Fresnel fut ainsi capable de mettre au point la théorie de la diffraction.\nFresnel s\u0026rsquo;imagine que la lumière est une vibration transversale de l\u0026rsquo;éther analogue aux ondes mécaniques décrites au siècle précédent par d\u0026rsquo;Alembert (1747).\nIl donne alors aux ondelettes de Huygens le pouvoir d\u0026rsquo;interférer grâce à la phase $\\phi=\\omega t-\\vec{k}\\cdot\\vec{r}$ (avec $\\|\\vec{k}\\|=k=\\frac{n \\omega}{c}$).\nArago écrira :\nQuel est donc le procédé magique qui permet de transformer à volonté la lumière en ombre, le jour en nuit ? Ce procédé excitera plus de surprise encore que le fait en lui-même ; ce procédé consiste à diriger sur le papier, mais par une route légèrement différente, un second rayon lumineux qui, pris isolément aussi, l\u0026rsquo;aurait fortement éclairé. Les deux rayons en se mêlant semblaient devoir produire une illumination plus vive ; le doute à cet égard ne semblait point permis ; eh bien ! ils se détruisent quelquefois tout à fait et l\u0026rsquo;on se se trouve avoir créé les ténèbres en ajoutant de la lumière à la lumière.\nUn fait neuf exige un mot nouveau. Ce phénomène dans lequel des rayons, en se mêlant, se détruisent tout à fait ou seulement en partie, s\u0026rsquo;appelle une interférence.\nLumière ou obscurité se déduisent désormais du terme d\u0026rsquo;interférence entre les différentes sources secondaires en un endroit donné. Et grâce aux interférences, le principe de superposition de Huygens peut maintenant expliquer la diffraction ! En sommant continument les multiples déphasages des ondelettes sur les fronts d\u0026rsquo;onde1, Fresnel est capable de prédire précisément les figures de diffraction obtenues dans plusieurs situations.\nAujourd\u0026rsquo;hui, le principe de superposition moderne mis au point par Fresnel porte le nom de ses deux papas : principe de Huygens-Fresnel.\nFresnel est le grand artisan du basculement de paradigme d\u0026rsquo;une description corpusculaire de la lumière (dont le géant Newton était le fer de lance) vers une description ondulatoire. Un fervent partisan des particules, Poisson, joua aussi un rôle amusant\u0026hellip;\nLorsqu’il éplucha le mémoire de concours de Fresnel, Poisson reprit ses intégrales de diffraction et présenta ses résultats en 1819 devant la commission du « Grand Prix de la Diffraction » de l’Académie des sciences. Les calculs prédisent un point lumineux au centre de l’ombre d’un disque éclairé par une source ponctuelle. Totalement absurde pour un partisan des particules ! Poisson est convaincu qu\u0026rsquo;il tient là l\u0026rsquo;argument définitif contre la théorie ondulatoire. Arago qui présidait la commission voulut en avoir le cœur net et monta l\u0026rsquo;expérience. Il observa alors bien ce qui sera désormais appelée tâche de Fresnel (Arago spot en anglais)2 ! L\u0026rsquo;argument définitif de Poisson fit finalement triompher les ondes3.\nHelmholtz En 1859, Helmholtz résume quasiment toute l\u0026rsquo;optique en une unique équation d\u0026rsquo;onde scalaire (d\u0026rsquo;abord pensée pour l\u0026rsquo;acoustique avant d\u0026rsquo;être adaptée à l\u0026rsquo;optique).\n$$\\nabla^2 \\Psi(\\vec{r})+k^2 n^2(\\vec{r}) \\Psi(\\vec{r})=0$$\nL\u0026rsquo;animation ci-dessous est calculée à partir de cette seule équation. Une série verticale de sources sur la gauche, un obstacle sphérique où l\u0026rsquo;onde vaut zéro et on laisse la magie (ou la physique, c\u0026rsquo;est selon) opérer.\nVotre navigateur ne prend pas en charge la balise video. De l\u0026rsquo;optique ondulatoire à l\u0026rsquo;optique géométrique L\u0026rsquo;optique géométrique n\u0026rsquo;est maintenant plus que la limite aux courtes longueurs d\u0026rsquo;onde de l\u0026rsquo;optique ondulatoire.\nEn injectant dans l\u0026rsquo;équation de Helmholtz une solution de la forme $\\Psi(\\vec{r})=A(\\vec{r}) e^{i k_0 S(\\vec{r})}$ (où la phase réduite $S$ et l\u0026rsquo;amplitude $A$ changent peu sur des distances de l\u0026rsquo;ordre de $\\lambda$) et en ne gardant que le plus grand ordre en $k_0$ (ordre 2), on obtient l\u0026rsquo;équation éikonale : $|\\vec{\\nabla} S|^2=n^2(\\mathbf{r})$.\nLa phase totale étant $k_0S(\\vec{r})$, on définit le vecteur d\u0026rsquo;onde $\\vec{k}(\\vec{r})=\\vec{\\nabla}\\phi=k_0\\vec{\\nabla}S$. En prenant son carré $|\\vec{k}|^2$, on obtient $k_0^2|\\vec{\\nabla} S|^2=k_0^2 n^2(\\vec{r})$. Et donc $|\\vec{k}|=\\frac{2 \\pi}{\\lambda_0} n(\\vec{r})=\\frac{2 \\pi}{\\lambda(\\vec{r})}$. C\u0026rsquo;est bien la norme attendue pour un vecteur d\u0026rsquo;onde dans un milieu d\u0026rsquo;indice $n$ (où la longueur d\u0026rsquo;onde locale vaut $\\lambda=\\lambda_0/n$).\nLes surfaces de phase constante correspondent à $S(\\vec{r})=\\text{cste}$. Et comme le gradient d’une fonction scalaire est orthogonal aux surfaces de niveau, $\\vec{n} =\\vec{\\nabla} S$ est la normale au front d’onde.\nPuisqu\u0026rsquo;on a défini $\\vec{k}$ comme proportionnel à $\\vec{\\nabla} S$, le vecteur d\u0026rsquo;onde est colinéaire à la normale. Sa direction est celle du rayon en optique géométrique.\nEn paramétrant le trajet par l\u0026rsquo;abscisse curviligne $s$, on obtient l\u0026rsquo;équation d\u0026rsquo;un rayon lumineux : $\\frac{d \\vec{r}}{d s}=\\frac{\\vec{k}}{|\\vec{k}|}=\\frac{\\vec{\\nabla} S}{n}$.\nFaraday et Maxwell Dès 1845, Faraday a l\u0026rsquo;intuition que la lumière est une onde électromagnétique. Mais c\u0026rsquo;est 20 ans plus tard que Maxwell clôt définitivement le chapitre de l\u0026rsquo;optique classique dans un bouquet final.\nAvec lui, la lumière est promue onde vectorielle et elle intègre officiellement le spectre du rayonnement du champ électromagnétique (dont elle ne constitue qu\u0026rsquo;une toute petite partie), et sa vitesse est maintenant déterminée théoriquement ($c=\\frac{1}{\\sqrt{\\epsilon_0\\mu_0}}$).\nD\u0026rsquo;un point de vue théorique, la compréhension scientifique de la lumière est donc partie d\u0026rsquo;un principe variationnel global pour terminer sur des équations locales (celles d\u0026rsquo;Helmholtz ou de Maxwell).\nLa mécanique a fait le chemin inverse, on a d\u0026rsquo;abord eu les équations de Newton (locales) avant de se tourner vers les principes variationnels.\nMécanique Bernoulli et la brachystochrone En 1696, Jean Bernoulli utilise l\u0026rsquo;optique pour résoudre un problème de mécanique qui chiffonnait les physiciens de l\u0026rsquo;époque, le problème de la brachystochrone.\nQuelle courbe dévalée par un point matériel pesant sans frottement ni vitesse initiale permet de rejoindre le plus vite le point de départ à un point d\u0026rsquo;arrivée fixé (en aval par rapport au champ de pesanteur supposé uniforme) ?\nBernoulli a en tête le principe de Fermat : le trajet le plus court entre les deux points est celui que suivrait la lumière. Et pour la trajectoire courbée, il fait l\u0026rsquo;analogie avec les mirages : il découpe verticalement l\u0026rsquo;espace en strates d\u0026rsquo;indices différents faisant varier la vitesse. La conservation de l\u0026rsquo;énergie lui donne une vitesse $v=\\sqrt{2gy}$ à la strate d\u0026rsquo;altitude $y$ et la loi de réfraction selon le principe de Fermat lui assure que $\\frac{\\sin\\theta}{v}=\\text{cste}$ et donc $\\frac{\\theta}{\\sqrt{y}}=\\text{cste}$ où $\\theta$ est l\u0026rsquo;angle par rapport à la verticale. Il réussit ainsi à démontrer que la courbe optimale est une cycloïde dont la tangente initiale est verticale au départ et qui est engendrée par un cercle dont le diamètre est la descente maximale du point.\nLa raison pour laquelle $\\frac{\\theta}{\\sqrt{y}}=\\text{cste}$ correspond à une cycloïde est donnée dans cette vidéo de 3Blue1Brown.\nEt le petit programme ci-dessous permet de tester manuellement différentes courbes et vérifier que le temps minimal est obtenu quand on s\u0026rsquo;approche de la cycloïde (courbe jaune).\nLa formulation de ce problème est typiquement variationnelle et cela en fait un tremplin (cycloïdique) vers les théories à venir (surtout que toutes les stars du moment s\u0026rsquo;y confrontent et proposent une solution : le frère de Jean, Jacques Bernoulli, L\u0026rsquo;Hôpital, Leibniz, Newton, etc.).\nEn jetant un pont entre l\u0026rsquo;optique et la mécanique pour résoudre la brachistochrone, Bernoulli préfigure les futurs travaux de Hamilton.\nMaupertuis En 1744, Maupertuis porte dans le domaine de mécanique l\u0026rsquo;idée de Fermat d\u0026rsquo;un principe variationnel permettant de déterminer le trajet des rayons lumineux. Il s\u0026rsquo;agit maintenant d\u0026rsquo;obtenir la trajectoire d\u0026rsquo;un système entre deux points en extrémisant une certaine grandeur : l\u0026rsquo;action.\nPour Maupertuis, l\u0026rsquo;action c\u0026rsquo;est $S_M=\\int_{A\\rightarrow B}mv\\mathrm{d}s$ où $s$ est l\u0026rsquo;abscisse curviligne sur la trajectoire. Anachroniquement, on peut transformer $S_M$ en $\\int_{A\\rightarrow B}2T\\mathrm{d}t$ où $T$ est l\u0026rsquo;énergie cinétique et minimiser l\u0026rsquo;action de Maupertuis revient alors à minimiser l\u0026rsquo;énergie cinétique au cours du temps à énergie totale fixée.\nL\u0026rsquo;action trouve déjà là sa dimension définitive d\u0026rsquo;une énergie multipliée par une durée, ou une quantité de mouvement multipliée par une distance.\nLagrange Un élève de Jean Bernoulli, Euler (le 🐐 des mathématiques), met au point le calcul des variations à partir de considérations géométriques et généralise ainsi les méthodes mises au point pour résoudre le problème de la brachistochrone.\nLagrange donne sa forme actuelle à ce nouveau type de calcul avec une approche purement analytique et il introduit les coordonnées généralisées $q_i$ (qui en plus des positions habituelles peuvent être des positions relatives, des angles, etc.) et la variable temps que n\u0026rsquo;utilisait pas Maupertuis.\nLa définition lagrangienne de l\u0026rsquo;action généralise celle de Maupertuis : $S_L=\\int_{t_1}^{t_2} L(q, \\dot{q}, t) \\mathrm{d} t$ où $L$, le lagrangien, vaut $T-V$, la différence entre l\u0026rsquo;énergie cinétique et l\u0026rsquo;énergie potentielle.\nLe principe de stationnarité $\\delta S_L= 0$ pour des extrémités fixées en temps conduit aux équations d\u0026rsquo;Euler-Lagrange :\n$$\\frac{\\mathrm{d}}{\\mathrm{d} t}\\frac{\\partial L}{\\partial \\dot{q}_i} - \\frac{\\partial L}{\\partial q_i} = 0$$\nDorénavant, le lagrangien sera au cœur de la physique moderne : toute théorie commence par se chercher un lagrangien puisque tout découle de lui. Un des plus grands achèvements humains est ainsi l\u0026rsquo;établissement du lagrangien du modèle standard qui réunit toute la physique hormis la gravité.\nLe lagrangien de l\u0026rsquo;optique géométrique Le principe de moindre temps et le principe de moindre action sont deux principes variationnels. Dans l\u0026rsquo;optique géométrique de Fermat, l\u0026rsquo;action correspond à la durée du trajet entre le point de départ et le point d\u0026rsquo;arrivée. Mais qui joue le rôle du temps chez Fermat ? Répondre revient à déterminer le \u0026ldquo;lagrangien\u0026rdquo; de l\u0026rsquo;optique géométrique.\nSupposons que le rayon se déplace dans un plan $(x,z)$ où $z$ serait l\u0026rsquo;axe optique ou l\u0026rsquo;axe de propagation. Le temps de parcours vaut alors $T=\\frac1c \\int_1^2 n(x,z)\\mathrm{d}\\ell=\\frac1c \\int_1^2 n(x,z)\\sqrt{\\mathrm{d}x^2+\\mathrm{d}z^2}=\\frac1c \\int_1^2 n(x,z)\\sqrt{1+\\frac{\\mathrm{d}x^2}{\\mathrm{d}z^2}}\\,\\mathrm{d}z=\\int_1^2\\mathcal{L}(x,\\tilde{x},z)\\,\\mathrm{d}z$ où on on note $\\tilde{x}\\equiv\\frac{\\mathrm{d}x}{\\mathrm{d}z}$.\nLe rôle du temps de la mécanique est donc joué par la coordonnée $z$ selon l\u0026rsquo;axe de propagation, $x$ garde son rôle et la vitesse $\\dot{x}$ devient l\u0026rsquo;inclinaison du rayon par rapport à la direction de propagation $\\mathrm{d}x/\\mathrm{d}z$. Notons que le lagrangien de l\u0026rsquo;optique a la dimension de l\u0026rsquo;inverse d\u0026rsquo;une vitesse plutôt que d\u0026rsquo;une énergie.\nL\u0026rsquo;équation d\u0026rsquo;Euler-Lagrange s\u0026rsquo;écrit $\\frac{\\mathrm{d}}{\\mathrm{d}z}\\left(\\frac{\\partial\\mathcal{L}}{\\partial \\tilde{x}}\\right)=\\frac{\\partial\\mathcal{L}}{\\partial x}$ et on tire du lagrangien $\\frac{\\partial\\mathcal{L}}{\\partial \\tilde{x}}=\\frac{n(x,z)\\tilde{x}}{c\\sqrt{1+\\tilde{x}^2}}$.\nSupposons que l\u0026rsquo;indice ne dépende pas de $x$ : $n(x,z)=n(z)$. On a alors $\\frac{\\partial\\mathcal{L}}{\\partial x}=0$ et donc $\\frac{\\partial\\mathcal{L}}{\\partial \\tilde{x}}$ est une constante en $z$ conservée le long de l\u0026rsquo;axe de propagation.\nOn constate sur le schéma que $\\tilde{x}=\\tan\\theta$ et donc $\\frac{\\tilde{x}}{\\sqrt{1+\\tilde{x}^2}}=\\sin\\theta$. La constance de $\\frac{\\partial\\mathcal{L}}{\\partial \\tilde{x}}$ selon $z$ implique donc celle de $n(z)\\sin\\theta$. On a retrouvé la loi de Snell-Descartes de la réfraction !\nHamilton Hamilton a pavé la voie pour la transition de la mécanique classique à la mécanique quantique.\nOn passe de la formulation de Lagrange à celle d\u0026rsquo;Hamilton en définissant les moments $p_i=\\partial L / \\partial \\dot{q}_i$ et en utilisant la transformée de Legendre $H(q, p, t)=\\sum_i p_i \\dot{q}_i-L$. Cela permet de remplacer la variable $\\dot{q}$ par le moment conjugué $p$ (ou impulsion).\nL\u0026rsquo;espace des configurations (l\u0026rsquo;espace des coordonnées ${q}$) où prend racine la mécanique lagrangienne est ainsi déménagée vers l\u0026rsquo;espace des phases $(q,p)$ où l\u0026rsquo;impulsion $p$ est considérée comme une coordonnée libre au même titre que $q$ (ce qui ne pouvait pas être le cas de $\\dot{q}=\\frac{\\mathrm{d}q}{\\mathrm{d}t}$)4. L\u0026rsquo;action devient pour Hamilton $S_H=\\int_{t_1}^{t_2}\\left(\\sum_i p_i \\dot{q}_i-H\\right) \\mathrm{d} t$.\nLa stationnarité $\\delta S_H = 0$, pour des extrémités fixes en $(q,t)$ fournit les équations canoniques :\n$\\dot{q}_i=\\frac{\\partial H}{\\partial p_i}, \\quad \\dot{p}_i=-\\frac{\\partial H}{\\partial q_i}$.\nL\u0026rsquo;espace des phases est déjà \u0026ldquo;quantum-ready\u0026rdquo; : les transformations canoniques entre $q$ et $p$ seront les mêmes, les crochets de Poisson (oui le même que la tâche) préfigurent les commutateurs, les symétries d\u0026rsquo;évolution à la Noether s\u0026rsquo;expriment naturellement, etc.\nHamilton-Jacobi Le principe de moindre action à la Lagrange consistant à chercher un chemin dans l\u0026rsquo;espace des configurations qui rende stationnaire l\u0026rsquo;action est tout à fait analogue au principe de Fermat.\nHuygens ayant réussi à reproduire les résultats de Fermat avec ses ondelettes, Hamilton se demande si on ne pourrait pas trouver une formulation équivalente en mécanique. Une mécanique à base d\u0026rsquo;ondelettes et de fronts d\u0026rsquo;onde !\nHamilton y parvient en envisageant l\u0026rsquo;action sous un nouvel aspect. Plutôt que de fixer les points de départ ($q_0,t_0$) et d\u0026rsquo;arrivée ($q_F,t_F$), il laisse ce dernier libre ($q,t$). Il obtient alors :\n$\\displaystyle\\frac{\\partial S}{\\partial q}=p$ $\\displaystyle\\frac{\\partial S}{\\partial t}=-H$ (équation de Hamilton-Jacobi) ► démonstration : Imaginons qu'on fasse varier la position finale en gardant $t$ fixé\u0026nbsp;:\n$q(t) \\rightarrow q(t)+\\delta q$ et $\\delta t =0$ On obtient $\\delta S =\\delta \\int_{t_0}^t L d t=\\int_{t_0}^t \\delta L d t$.\nAvec $L=p \\dot{q}-H(q,p)$, $\\delta L=\\delta p \\dot{q}+p \\delta \\dot{q}-\\frac{\\partial H}{\\partial q} \\delta q -\\frac{\\partial H}{\\partial p} \\delta p$.\nRegroupons les termes en $\\delta p$ : $\\delta L=\\left(\\cancel{\\dot{q}-\\frac{\\partial H}{\\partial p}}\\right) \\delta p + p \\delta \\dot{q} - \\frac{\\partial H}{\\partial q} \\delta q= p \\delta \\dot{q} + \\dot{p} \\delta q$ car $\\dot{q}=\\frac{\\partial H}{\\partial p}$ et $\\dot{p}=-\\frac{\\partial H}{\\partial q}$ d\u0026rsquo;après les équations canoniques.\nEt comme $p \\delta \\dot{q}=\\frac{d}{d t}\\left(p \\delta q\\right)-\\dot{p} \\delta q$, $\\delta L = \\frac{d}{d t}\\left(p_i \\delta q_i\\right)$. D\u0026rsquo;où $\\int_{t_0}^t \\delta L d t=\\left[p_i \\delta q_i\\right]_{t_0}^t$.\nEt comme on ne touche pas aux conditions initiales ($\\delta q(t_0)=0$) :$\\int_{t_0}^t \\delta L d t=p(t) \\delta q(t)$.\nOr, on peut aussi écrire que $\\delta S = \\frac{\\partial S}{\\partial q} \\delta q$.\nPar identification, $\\frac{\\partial S}{\\partial q}=p(t)$.\nPour déterminer la variation de $S$ en fonction du temps d\u0026rsquo;arrivée, laissons la trajectoire évoluer sur son chemin optimal. On a alors $\\mathrm{d}S=\\mathcal{L}\\mathrm{d}t$.\nD\u0026rsquo;autre part, $\\mathrm{d}S=\\mathcal{L}\\mathrm{d}t=\\frac{\\partial S}{\\partial q}\\mathrm{d}q+\\frac{\\partial S}{\\partial t}\\mathrm{d}t$.\nOr on a vu que $\\frac{\\partial S}{\\partial q}=p$, d\u0026rsquo;où $\\mathcal{L}\\mathrm{d}t=\\left(p \\dot{q}+\\frac{\\partial S}{\\partial t}\\right) \\mathrm{d} t$.\nOn en déduit donc que $\\frac{\\partial S}{\\partial t}=\\mathcal{L}-p \\dot{q}=-H$.\nDans le cas d\u0026rsquo;un système conservatif, on déduit de l\u0026rsquo;équation de Hamilton-Jacobi5 : $$S(q, t)=S_{0}(q)-E t$$ Hamilton a alors l\u0026rsquo;intuition géniale d\u0026rsquo;y voir une phase $\\phi(\\vec{r},t) = \\vec{k} \\cdot\\vec{r}-\\omega t$. Mais sa quête n\u0026rsquo;est pas celle d\u0026rsquo;une mécanique purement ondulatoire (il faudra attendre Schrödinger pour ça), il souhaite la \u0026ldquo;forme Huygens\u0026rdquo; de l\u0026rsquo;équivalence entre la mécanique et l\u0026rsquo;optique géométrique.\nPar analogie avec la limite de l\u0026rsquo;optique ondulatoire aux petites longueurs d\u0026rsquo;onde et le principe de Huygens, on peut identifier $S_{0}(q)$ avec la fonction caractéristique donnant la longueur du chemin optique de $q_0$ à $q$.\nComme $\\partial S_0(q) / \\partial q=p$, le vecteur normal à la surface $S_0(q)=\\text{cst}$, $\\vec{\\nabla} S_0(q)$ n\u0026rsquo;est autre que le vecteur impulsion $\\vec{p}$.\nLa trajectoire mécanique suit les rayons (donnés par l\u0026rsquo;impulsion) normaux aux fronts d\u0026rsquo;onde constitués par les surfaces d\u0026rsquo;action réduite constante !\nSchrödinger résume les travaux d\u0026rsquo;Hamilton ainsi :\nLe principe variationnel de Hamilton se trouve correspondre au principe de Fermat pour la propagation d’une onde dans l’espace des configurations (espace des q), et l’équation de Hamilton–Jacobi exprime le principe de Huygens pour cette propagation. Malheureusement, cette conception puissante et d’une importance majeure de Hamilton est, dans la plupart des relectures modernes, dépouillée de son magnifique habit — jugé superflu — au profit d’une représentation plus terne de la correspondance analytique.\nL\u0026rsquo;admiration que voue Schrödinger à Hamilton a été récompensée puisque les travaux d\u0026rsquo;Hamilton se sont révélés essentiels dans la quête de Schrödinger d\u0026rsquo;une équation d\u0026rsquo;onde quantique.\nSi Hamilton jette les base de la révolution quantique, les lubies géométriques de Jacobi préfigurent une autre révolution du 20e siècle, celle de la relativité générale.\nIl transforme en effet la recherche d\u0026rsquo;une trajectoire minimisant l\u0026rsquo;action en la recherche d\u0026rsquo;une géodésique dans l\u0026rsquo;espace des configurations ! Pour un système à énergie fixée, il réécrit ainsi l\u0026rsquo;action réduite comme $W=\\int_{q_1}^{q_2}p^i\\mathrm{d}q_i$ en $\\int_{q_1}^{q_2}ds_J$ en introduisant la métrique de Jacobi : $\\mathrm{d}s_J^2 = 2(E-V(q)) g_{i j}(q) \\mathrm{d} q^i \\mathrm{d} q^j$ (où $g_{ij}$ est une métrique riemannienne). Le principe de Maupertuis devient alors un théorème géométrique.\nL\u0026rsquo;avènement de la mécanique quantique Planck À la fin du 19e siècle, réussir à modéliser l\u0026rsquo;énergie libérée par un corps noir (une cavité dont les seules émissions de rayonnement sont thermiques) en fonction de la fréquence est une énigme qui résiste au physicien.\nLe nombre de modes de vibration augment avec la fréquence et à l\u0026rsquo;équilibre thermique, chaque mode doit recevoir la même énergie, proportionnelle à la température. Moralité, la densité d\u0026rsquo;énergie aux grandes fréquences explose, c\u0026rsquo;est la catastrophe ultraviolette ! Bien sûr, la théorie est en violente contradiction avec l\u0026rsquo;expérience qui présente zéro catastrophe.\nPour tenter de résoudre le problème, Planck utilise ce qu\u0026rsquo;il pense être un expédiant provisoire : il introduit une discrétisation, un pas $h$ dont il compte bien se débarrasser ensuite en le faisant tendre vers zéro.\nEt ça marche ! Il obtient la courbe attendue du corps noir. Mais à son grand désarroi, il n\u0026rsquo;arrive pas à se débarrasser de $h$. En le faisant tendre vers zéro, il perd la courbe. Et l\u0026rsquo;accord avec les résultats expérimentaux lui fixent même une valeur pour son petit pas : $\\pu{6,63E-34 J*s}$. Car oui, ce pas de discrétisation (qui deviendra la constante de Planck $h$) a la dimension d\u0026rsquo;une action, c\u0026rsquo;est un quantum d\u0026rsquo;action !\nPour la première fois, une modélisation de la nature s\u0026rsquo;avère fondamentalement discontinue et c\u0026rsquo;est l\u0026rsquo;action qui refuse de se faire découper infiniment.\nde Broglie Dans son élan fabuleux de 1905, Einstein explique l\u0026rsquo;effet photoélectrique en discrétisant la lumière en photons d\u0026rsquo;énergie $E=h\\nu$, généralisant l\u0026rsquo;idée de Planck au rayonnement.\nDe Broglie comprend alors que la formule $E=h\\nu$ n\u0026rsquo;est pas réservée aux photons puisque le rayonnement échange de l\u0026rsquo;énergie avec la matière.\nIl écrira :\nAprès une longue méditation et réflexion solitaire, j’ai subitement eu l’idée durant l’année 1923, que la découverte faite par Einstein en 1905 doit être généralisée par extension à toute particule de matière et singulièrement l’électron.\nAvec ce quantum d\u0026rsquo;énergie proportionnel à la fréquence, on associe de fait une caractéristique ondulatoire à la matière. De Broglie pousse cette dualité onde-corpuscule un cran plus loin en lui donnant aussi une longueur d\u0026rsquo;onde qu\u0026rsquo;il relie à la quantité de mouvement : $\\lambda=\\frac{h}{p}$.\nOn a maintenant un lien direct entre le $\\int n\\,\\mathrm{d}q$ de Fermat et le $\\int p\\,\\mathrm{d}q$ de Maupertuis (l\u0026rsquo;action réduite d\u0026rsquo;Hamilton) : en appelant $v$ la vitesse de l\u0026rsquo;onde dans le milieu, $n=\\frac{c}{v}=\\frac{c}{\\lambda\\nu}=\\frac{c}{\\frac{h}{p}\\frac{E}{h}}=\\frac{pc}{E}$. Et donc $\\int n\\,\\mathrm{d}q=\\frac{c}{E}\\int p\\,\\mathrm{d}q$.\nDe Broglie parvient avec sa relation à donner du sens aux orbites quantifiées de l\u0026rsquo;atome de Bohr : l\u0026rsquo;électron en orbite évolue sur une trajectoire fermée et maintenant qu\u0026rsquo;on peut lui associer une longueur d\u0026rsquo;onde, un système d\u0026rsquo;ondes stationnaires se met en place.\nEn supposant que la longueur d\u0026rsquo;onde reste la même sur l\u0026rsquo;orbite, on doit avoir : $n=\\oint\\frac{\\mathrm{d}\\ell}{\\lambda}=\\oint\\frac{p}{h}\\mathrm{d}\\ell$ (où $n$ est un entier, plus rien à voir avec l\u0026rsquo;indice optique). Et en supposant une orbite circulaire de rayon $r$, on obtient un moment cinétique $L$ valant : $L=mvr=\\frac{nh}{2\\pi}=n\\hbar$, ce qui est exactement la relation ad hoc introduite par Bohr.\nIl ne restait plus qu\u0026rsquo;à faire le même pas pour la mécanique qu\u0026rsquo;entre Huygens et Fresnel pour l\u0026rsquo;optique. Ou comme le dit de Broglie :\nLa nouvelle dynamique du point matériel (incluant le photon d’Einstein) est à l’ancienne (dynamique classique) ce que l’optique ondulatoire est à l’optique géométrique.\nSchrödinger C\u0026rsquo;est Schrödinger qui franchira le cap et il va suivre pour cela les pas d\u0026rsquo;Hamilton.\nL\u0026rsquo;équation de Hamilton-Jacobi peut s\u0026rsquo;écrire $\\frac{\\partial S}{\\partial t}+H(q, \\nabla S)=0$ en notant $\\nabla S = \\frac{\\partial S}{\\partial q}$.\nPour une particule libre, $H=p^2 / 2 m$ et on obtient : $\\frac{\\partial S}{\\partial t}+\\frac{1}{2 m}|\\nabla S|^2=0$ (car $\\frac{\\partial S}{\\partial q}$ vaut aussi l\u0026rsquo;impulsion $p$).\nL\u0026rsquo;idée-clé de Schrödinger est de remplacer dans l’équation de Helmholtz $\\nabla^2 \\Psi+k_0^2 n^2 \\Psi=0$ l’indice optique $n$ par une fonction liée à l’énergie potentielle $V(q)$ pour trouver l’équation qui régit la « phase » d’une onde mécanique. Pour cela, il va s\u0026rsquo;aider du travail d\u0026rsquo;Hamilton sur l\u0026rsquo;éikonale.\nRappelons-nous qu\u0026rsquo;en optique géométrique, on obtient l\u0026rsquo;équation éikonale $|\\nabla S|^2=n^2(\\mathbf{r})$ en approximant aux petites longueurs d\u0026rsquo;onde l\u0026rsquo;équation de Helmholtz pour une solution scalaire $\\Psi \\propto e^{i k_0 S}$. En faisant la même chose avec une onde mécanique de phase $S_0(q)$, Schrödinger va trouver l\u0026rsquo;équivalent de l\u0026rsquo;indice optique.\nÉquation stationnaire :\nCherchons une fonction d\u0026rsquo;onde $\\Psi(q)$ de la forme $\\Psi(q)=A(q) \\mathrm{e}^{i S_0(q) / \\hbar}$ où $S_0$ est l\u0026rsquo;action réduite liée à l\u0026rsquo;énergie $E$ du système par $H(q, \\nabla S_0)=E $.\nEn substituant dans Helmoltz, on impose une relation du type $\\nabla^2 \\Psi+K(q) \\Psi=0$ et on identifie $K(q)$ pour que la limite $\\hbar\\rightarrow 0$ redonne le bon $|\\nabla S|^2$.\nPour une particule soumise à un potentiel $V(q)$, on obtient l\u0026rsquo;équation stationnaire :\n$$\\color{#D41876}-\\frac{\\hbar^2}{2 m} \\nabla^2 \\Psi(q)+V(q) \\Psi(q)=E \\Psi(q)$$\n► démonstration : Calcul de $\\nabla^2\\Psi$ :\n$\\nabla \\Psi=\\mathrm{e}^{\\frac{\\mathrm{i}}{\\hbar} S_0}\\left(\\nabla A+\\frac{\\mathrm{i}}{\\hbar} A \\nabla S_0\\right)$\n$\\nabla^2 \\Psi=\\nabla \\cdot\\left[\\mathrm{e}^{\\frac{\\mathrm{i}}{\\hbar} S_0}\\left(\\nabla A+\\frac{\\mathrm{i}}{\\hbar} A \\nabla S_0\\right)\\right]=\\mathrm{e}^{\\frac{\\mathrm{i}}{\\hbar} S_0}\\left[\\nabla^2 A+\\frac{2 \\mathrm{i}}{\\hbar} \\nabla A \\cdot \\nabla S_0+\\frac{\\mathrm{i}}{\\hbar} A \\nabla^2 S_0-\\frac{1}{\\hbar^2} A|\\nabla S_0|^2\\right]$ En ne conservant que le terme d\u0026rsquo;ordre dominant en $\\hbar^{-1}$, on obtient $\\nabla^2 \\Psi=-\\mathrm{e}^{\\frac{\\mathrm{i}}{\\hbar} S_0}A \\frac{|\\nabla S_0|^2}{\\hbar^2}$.\nEn substituant dans $\\nabla^2 \\Psi+K(q) \\Psi=0$ et après factorisation par $\\mathrm{e}^{\\frac{\\mathrm{i}}{\\hbar} S_0}$, on a :\n$-A \\frac{|\\nabla S_0|^2}{\\hbar^2}+K(q) A=0 \\quad \\Longrightarrow \\quad K(q)=\\frac{|\\nabla S_0(q)|^2}{\\hbar^2}$ Or pour une particule soumise à un potentiel $V(q)$, on a classiquement $H(q,p)=\\frac{1}{2 m}|\\nabla S_0(q)|^2+V(q)=E$, d\u0026rsquo;où $|\\nabla W(q)|^2=2 m(E-V(q))$.\nPour que dans la limite des petites longueurs d\u0026rsquo;onde, on retrouve $|\\nabla W(q)|^2=2 m(E-V(q))$, il faut donc poser $K(q)=\\frac{2 m}{\\hbar^2}(E-V(q))$.\nCette équation est l’analogue directe de l’équation de Helmholtz pour la lumière, où $E-V$ joue le rôle de $k^2$.\nÉquation dépendant du temps :\nSchrödinger remarque que pour décrire les états non stationnaires, il suffit de remplacer dans l’équation stationnaire $E$ par $\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t}$. En effet, pour une onde monochromatique classique de pulsation $\\omega$, l’amplitude évolue dans le temps comme $\\mathrm{e}^{\\mathrm{i}\\omega t}$. Et avec $E=h\\nu=\\hbar\\omega$, on peut écrire $\\Psi(q,t)=\\Psi(q)\\mathrm{e}^{\\frac{\\mathrm{i}}{\\hbar}E t}$.\nEn dérivant temporellement cette solution, on obtient : $\\frac{\\partial}{\\partial t} \\Psi(q, t)=-\\frac{i E}{\\hbar} \\Psi(q, t) \\Longrightarrow E \\Psi=i \\hbar \\frac{\\partial \\Psi}{\\partial t}$.\nPar identification, on voit que l\u0026rsquo;énergie $E$ agit sur la fonction d\u0026rsquo;onde comme un opérateur dérivée.\nOn obtient alors l’équation de Schrödinger dépendant du temps :\n$$\\color{#D41876}\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t} \\Psi(q, t)=\\left(-\\frac{\\hbar^2}{2 m} \\nabla^2+V(q)\\right) \\Psi(q, t)$$\nVotre navigateur ne prend pas en charge la balise video. Sur la vidéo précédente, j\u0026rsquo;ai envoyé un paquet d\u0026rsquo;onde gaussien à la rencontre d\u0026rsquo;une marche de potentiel (un petit potentiel uniforme positif règne à droite alors qu\u0026rsquo;il est nul à gauche).\nVotre navigateur ne prend pas en charge la balise video. Et dans cette vidéo, on constate que le blob réfracté s\u0026rsquo;éloigne de la normale au dioptre comme le ferait un rayon dans un milieu d\u0026rsquo;indice optique inférieur. On pourrait s\u0026rsquo;inquiéter que le blob aille moins vite dans ce milieu (alors qu\u0026rsquo;en optique, un indice plus faible correspond à une vitesse supérieure). Mais non, tout va bien puisque l\u0026rsquo;\u0026ldquo;indice optique\u0026rdquo; de la mécanique est donné par $\\sqrt{2m(E-V)}$ (qui n\u0026rsquo;est autre que $p$) or $E-V$ est plus faible à droite vu que le potentiel y est plus grand.\nFeynman La contribution de Feynman à la physique est à la confluence de cette longue évolution de l\u0026rsquo;optique et de la mécanique. Le prix Nobel vient en effet récompenser son travail sur l\u0026rsquo;électrodynamique quantique, théorie décrivant l\u0026rsquo;interaction entre la matière et le rayonnement, où il a en particulier remis l\u0026rsquo;action au centre du jeu.\nFeynman semble pourtant à contre-courant du lent cheminement des particules vers les ondes. En effet, pour lui, tout est particule, même la lumière. Il écrit ainsi dans QED: The Strange Theory of Light and Matter (une introduction tout public à l\u0026rsquo;électrodynamique quantique de laquelle j\u0026rsquo;ai tiré ce texte il y a très longtemps) :\nI want to emphasize to you that light comes in this form – particles. It is very important to know light behaves like particles, especially for those of you who have gone to school where you were probably told something about light behaving like waves. I’m going to tell you the way it does behave like particles.\nNéanmoins, ses particules ont des propriétés essentiellement ondulatoires. Selon Feynman, lorsqu\u0026rsquo;une particule emprunte un chemin pour aller d\u0026rsquo;un point I à un point F, elle emporte avec elle une phase qui oscille tout le long du chemin. Que vaut la phase finale pour un chemin entier ? $\\frac{S}{\\hbar}$ où l\u0026rsquo;action $S$ est calculée sur le chemin considéré !\nOn peut retrouver ce $\\frac{S}{\\hbar}$ à partir de la formule de la phase de l\u0026rsquo;optique ondulatoire $kq-\\omega t$ et des relations d\u0026rsquo;Einstein $E=\\hbar\\omega$ et de de Broglie $p=\\hbar k$.\nDécoupons le chemin en tronçons infinitésimaux : sur un de ces intervalles, la phase $\\phi$ varie de $\\mathrm{d}\\phi=k \\mathrm{d}q - \\omega\\mathrm{d}t$. En utilisant les équivalences avec l\u0026rsquo;énergie et l\u0026rsquo;impulsion, la petite variation de phase devient $\\mathrm{d}\\phi=\\frac{1}{\\hbar}(p\\mathrm{d}q-E\\mathrm{d}t)$.\nOn force ensuite $\\mathrm{d}t$ en facteur : $\\mathrm{d}\\phi=\\frac{1}{\\hbar} ( p \\frac{ \\mathrm{d} q }{ \\mathrm{d} t } - E ) \\mathrm{d}t=\\frac{1}{\\hbar}(p\\dot{q}-E)\\mathrm{d}t$. Plus qu\u0026rsquo;à substituer $E$ par l\u0026rsquo;hamiltonien $H$ pour retrouver la formule du lagrangien : $\\mathrm{d}\\phi=\\frac{1}{\\hbar}\\mathcal{L}\\mathrm{d}t$.\nEt pour obtenir la variation de phase sur l\u0026rsquo;entièreté du chemin, on somme les petits déphasages de chaque tronçon infinitésimal, ce qui revient à intégrer le lagrangien le long du chemin : $\\Delta\\phi=\\frac{1}{\\hbar}\\int_\\text{chemin}\\mathcal{L}\\mathrm{d}t$. C\u0026rsquo;est bien l\u0026rsquo;action qu\u0026rsquo;on retrouve ici et la variation de phase sur le chemin s\u0026rsquo;écrit finalement $\\Delta\\phi=\\frac{1}{\\hbar}S_\\text{chemin}$.\nMais pour Feynman, la particule n\u0026rsquo;emprunte pas qu\u0026rsquo;un seul chemin pour aller de I à F, elle emprunte tous les chemins possibles ! Chaque chemin contribue à la probabilité finale de retrouver la particule au point F après être partie du point I.\nPour obtenir la probabilité totale, on se retrouve à additionner entre eux les déphasages obtenus sur chacun des chemins (cela revient à additionner des termes $\\mathrm{e}^{\\mathrm{i}\\frac{S_\\text{chemin}}{\\hbar}}$). Cela ressemble pas mal au principe de superposition de Huygens-Fresnel\u0026hellip;\nFeynman aime décrire cette opération comme une somme de petites flèches dans un plan (l\u0026rsquo;angle de la flèche représente le déphasage). La grande majorité des chemins fournissent des déphasages variant largement les uns par rapport aux autres (la division par $\\hbar$ fait gigoter ça très très vite) et sommer des flèches dont l\u0026rsquo;angle varie quasi aléatoirement ne mène pas très loin\u0026hellip; Seuls les chemins où la phase varie très peu de l\u0026rsquo;un à l\u0026rsquo;autre vont contribuer efficacement, constructivement, à la somme finale. Or les chemins où la phase varie peu sont les chemins où l\u0026rsquo;action varie peu. On se retrouve à chercher les trajectoires où l\u0026rsquo;action est stationnaire redécouvrant le principe de moindre action via le principe de superposition !\nFeynman marie ainsi d\u0026rsquo;une façon nouvelle l\u0026rsquo;optique ondulatoire à la mécanique en passant directement par la phase et le principe de superposition de Huygens-Fresnel plutôt que par l\u0026rsquo;équation d\u0026rsquo;onde d\u0026rsquo;Helmholtz et met dans le même mouvement l\u0026rsquo;action au centre de son formalisme.\nBouclons la boucle en reprenant un exemple donné par Feynman dans QED.\nOn a une source de photons monochromatiques (leur énergie $E$ est donc fixe) et un détecteur. Un écran cache la source au détecteur mais un miroir est placé en dessous.\nLa probabilité que le détecteur capte un photon doit être calculée en utilisant tous les chemins possibles. Pour simplifier, Feynman découpe le miroir en 13 et ne s\u0026rsquo;autorise que les trajets rectilignes.\nVotre navigateur ne prend pas en charge la balise video. Dans cette situation l\u0026rsquo;action obtenue sur chaque chemin dépend seulement de la longueur du trajet (en effet, à énergie constante l\u0026rsquo;action se réduit $\\int p\\mathrm{d}q$ (action réduite) et pour un photon, $p=\\frac{E}{c}$. Donc $E\\frac{L}{c}$ ou encore $E\\tau$, l\u0026rsquo;action et donc le déphasage de chaque chemin est proportionnel au temps de parcours.\nEn sommant sur les chemins, on constate bien que les contributions les plus importantes (car de phases quasi constantes) sont celles proche de la trajectoire la plus courte (on a retrouvé Fermat !).\nSources :\nMesli, A. « Genèse et développement du principe de moindre action. Première partie : de Fermat (1655) à Lagrange (1756) », Photoniques n° 80, p. 34–40 (janvier 2016). Mesli, A. « Genèse et développement du principe de moindre action. Deuxième partie : de Hamilton (1834) à Feynman (1942) », Photoniques n° 81, p. 37–45 (avril 2016). 🌐 Vaquero Vallina, M. « On the Geometry of the Hamilton–Jacobi Equation », thèse de doctorat, Universidad Autónoma de Madrid – Instituto de Ciencias Matemáticas (ICMAT), soutenue le 27 novembre 2015. 🌐 Houchmandzadeh, B. “The Hamilton–Jacobi equation: An alternative approach”, American Journal of Physics 88 (5), 353–359 (Mai 2020) 🌐 Wheeler, N. “Schrödinger’s train of thought”, Reed College Physics Department essay, avril 2006. 🌐 les portraits sont plus ou moins hallucinés par IA. le calcul infinitésimal et intégral fut développé parallèlement par Newton et Leibniz à la fin du 17e siècle.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAmusant ce partage des attributions suivant les pays entre théoricien et expérimentateur.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCe n\u0026rsquo;est pas sans rappeler comment Millikan confirma à son corps défendant l\u0026rsquo;idée du photon d\u0026rsquo;Einstein pour expliquer l\u0026rsquo;effet photoélectrique.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLa dynamique lagrangienne prend place dans le fibré tangent à l\u0026rsquo;espace des configurations $(q,\\dot{q})$, celle d\u0026rsquo;Hamilton est très bien dans l\u0026rsquo;espace des phases $(q,p)$ qui est donc l\u0026rsquo;unique cadre géométrique (techniquement il s\u0026rsquo;agit du fibré cotangent de l\u0026rsquo;espace des configurations).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAvec $S_0$, on retrouve au final l\u0026rsquo;action de Maupertuis qui est renommée action réduite $W=\\int_{q_A}^q p \\,dq$. En effet, $S_M=\\int_{A\\rightarrow B}mv\\,\\mathrm{d}s=\\int_{A\\rightarrow B}p\\,\\mathrm{d}q=\\int_{A\\rightarrow B}(p\\dot{q} - H +H)\\,\\mathrm{d}t=S_H+E(t_B-t_A)$ ($E$ est fixe ici).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique3/",
	"title": "Propositions - Preuve",
	"tags": [],
	"description": "",
	"content": " info\nNotes de lecture du livre La logique pas à pas de Jacques Duparc que je paraphrase allégrement.\nCalcul des propositions Syntaxe Sémantique Preuve Théorie de la démonstration Une preuve ou une démonstration\u0026nbsp;:\nest de longueur finie, est vérifiable mécaniquement, s\u0026rsquo;appuie sur des hypothèses, aboutit à une conclusion. Une démonstration permet de mettre au jour une vérité syntaxique (s\u0026rsquo;appuyant sur un jeu de règles) plutôt que sémantique (faisant intervenir les modèles d\u0026rsquo;une théorie).\nOn a vu que $\\phi$ est une conséquence sémantique de la théorie $\\mathcal{T}$, noté $\\mathcal{T}\\models \\phi$, si $\\phi$ est vraie dans tous les modèles de $\\mathcal{T}$.\nOn va maintenant se pencher sur la conséquence syntaxique de la théorie $\\mathcal{T}$. $\\phi$ est une conséquence syntaxique de $\\mathcal{T}$, noté $\\mathcal{T} \\vdash \\phi$, s\u0026rsquo;il existe une démonstration de $\\phi$ sur la base d\u0026rsquo;hypothèses contenues dans la théorie $\\mathcal{T}$. Fini les modèles, on ne s\u0026rsquo;intéresse plus qu\u0026rsquo;à la syntaxe par un jeu de réécriture.\nQuel est le lien entre les conséquences sémantiques et syntaxiques ?\nSi on veut que nos démonstrations aboutissent à des résultats ayant du sens, il faut qu\u0026rsquo;ils correspondent aux conséquences sémantiques. Et à l\u0026rsquo;inverse, on souhaite qu\u0026rsquo;une conséquence sémantique puisse se démontrer mécaniquement. Le système de règles de démonstrations va être consciencieusement mis au point afin d\u0026rsquo;aboutir à l\u0026rsquo;équivalence entre les deux conséquences : $\\mathcal{T}\\models \\phi$ si et seulement si $\\mathcal{T} \\vdash \\phi$. Et en particulier, on veut qu\u0026rsquo;une formule soit une tautologie si et seulement si elle est prouvable sans hypothèse : $\\models \\phi$ ssi $\\vdash \\phi$.\nOn va distinguer trois familles de systèmes de démonstration différents pour le calcul des propositions\u0026nbsp;:\nles systèmes axiomatiques, la déduction naturelle, le calcul des séquents. Les systèmes axiomatiques Les systèmes axiomatiques ou systèmes \u0026ldquo;à la Hilbert\u0026rdquo; reposent sur un petit nombre de vérités élémentaires, les axiomes, et sur des règles de construction qui préservent la vérité.\nLes démonstrations obtenues sont concises mais très peu explicatives. Ça marche, mais on ne sait pas pourquoi, faute au manque de structure de ces preuves.\nExemple d\u0026rsquo;un système axiomatique reposant sur le système complet de connecteur $\\set{\\neg,\\rightarrow}$ et comportant 3 axiomes pour une seule règle :\nAxiomes $(1)\\quad\\phi\\rightarrow (\\psi\\rightarrow\\phi)$$(2)\\quad(\\phi\\rightarrow (\\psi\\rightarrow\\theta))\\rightarrow ((\\phi\\rightarrow\\psi)\\rightarrow(\\phi\\rightarrow\\theta))$$(3)\\quad(\\neg\\psi\\rightarrow \\neg\\phi) \\rightarrow ((\\neg \\psi\\rightarrow\\phi)\\rightarrow \\psi)$ Règle modus ponens\u0026nbsp;: de $\\phi$ et $\\phi\\rightarrow \\psi$ on déduit $\\psi$ La déduction de $\\phi$ à partir d'un ensemble de formules $\\Gamma$ est une suite finie de formules $\\langle \\phi_1,\\phi_2,\\ldots,\\phi_n\\rangle$ telle que\u0026nbsp;:\nchaque $\\phi_i$ vérifie une des trois conditions suivantes : $\\phi_i$ est un axiome, $\\phi_i$ est une hypothèse ($\\phi_i \\in \\Gamma$), $\\phi_i$ est obtenue à partir de l\u0026rsquo;application de la règle du modus ponens à deux formules $\\phi_j$ et $\\phi_k$ telles que $j,k≤i$. $\\phi_n=\\phi$ Exemple de la démonstration de la formule $(\\phi\\rightarrow\\phi)$ sans hypothèse\u0026nbsp;:\n$ \\begin{array}{llr} (1)\u0026amp; (\\phi\\rightarrow ((\\phi\\rightarrow \\phi)\\rightarrow\\phi))\\rightarrow ((\\phi\\rightarrow (\\phi\\rightarrow \\phi))\\rightarrow(\\phi\\rightarrow\\phi)) \u0026amp; \\text{(axiome 2 avec } \\psi = (\\phi\\rightarrow \\phi) \\text{ et }\\theta = \\phi )\\\\ (2)\u0026amp; \\phi\\rightarrow ((\\phi\\rightarrow \\phi)\\rightarrow\\phi) \u0026amp; \\text{(axiome 1 avec } \\psi = (\\phi\\rightarrow \\phi) )\\\\ (3)\u0026amp; (\\phi\\rightarrow (\\phi\\rightarrow \\phi))\\rightarrow(\\phi\\rightarrow\\phi) \u0026amp; (\\textit{modus ponens} \\text{ (1) - (2))}\\\\ (4)\u0026amp; \\phi\\rightarrow (\\phi\\rightarrow \\phi) \u0026amp; \\text{(axiome 1 avec } \\psi = \\phi )\\\\ (5)\u0026amp; \\phi\\rightarrow\\phi \u0026amp; (\\textit{modus ponens} \\text{ (3) - (4))} \\end{array} $\nOn ne peut pas dire que la démonstration nous éclaire beaucoup sur la vérité de $\\phi\\rightarrow \\phi$\u0026hellip;\nEn conclusion, les systèmes à la Hilbert sont utiles pour démontrer des trucs mais pas pour étudier les démonstrations elles-mêmes.\nLa déduction naturelle Un séquent, noté $\\Gamma\\vdash\\phi$, est un couple où\u0026nbsp;:\n$\\Gamma$ est un ensemble fini de formules, $\\phi$ est une formule. Remarques\u0026nbsp;:\n$\\Gamma$ représente les hypothèses que l\u0026rsquo;on veut utiliser. $\\phi$ est la conclusion du séquent, la formule à démontrer. $\\vdash$ se lit \u0026ldquo;démontre\u0026rdquo; ou \u0026ldquo;prouve\u0026rdquo;. On écrit $\\vdash\\phi$ si $\\phi$ se démontre sans hypothèse ($\\Gamma=\\emptyset $). Un séquent est prouvable s'il peut être obtenu par une application finie de règles de démonstration. Une formule $\\phi$ est prouvable si le séquent $\\vdash\\phi$ est prouvable. On écrit $\\Gamma\\nvdash\\phi$ si $\\Gamma\\vdash\\phi$ n\u0026rsquo;est pas prouvable.\nLa règle est la brique de base de la démonstration. Une démonstration est ainsi un assemblage de règles généralement représenté sous forme d\u0026rsquo;arbre.\nChaque règle est composée\u0026nbsp;:\nd\u0026rsquo;un ensemble de prémisses (de 0 à 3), chacune étant un séquent. d\u0026rsquo;un séquent conclusion. d\u0026rsquo;une barre horizontale qui sépare les deux (on identifie la règle à droite de la barre). Pour chaque connecteur logique, on a deux types de règles\u0026nbsp;:\nles règles d\u0026rsquo;introduction, et les règles d\u0026rsquo;élimination. On va dans la suite considérer le système complet de connecteur $\\set{\\neg,\\land,\\lor,\\rightarrow}$.\nEn dehors des axiomes représentés par le séquent $\\phi \\vdash\\phi$, les règles peuvent être regroupées en deux catégories\u0026nbsp;:\nles règles logiques (règles d\u0026rsquo;introduction et d\u0026rsquo;élimination des différents connecteurs). les règles structurelles permettant de manipuler les hypothèses ; l\u0026rsquo;affaiblissement permet d\u0026rsquo;ajouter de nouvelles hypothèses et la contraction permet de fusionner deux occurrences de la même hypothèse. Logique minimale Règles de la logique minimale :\nAxiome\n$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\end{prooftree} $ Un séquent dont la conclusion est aussi l'hypothèse est prouvable. Introduction de la conjonction\n$ \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi$} \\AxiomC{$\\Gamma'\\vdash\\psi$} \\RightLabel{$\\;\\scriptsize \\land i$} \\BinaryInfC{$\\Gamma,\\Gamma' \\vdash\\phi\\land\\psi$} \\end{prooftree} $ Si $\\phi$ et $\\psi$ sont prouvées, alors on prouve $\\phi\\land\\psi$. Élimination de la conjonction\n$ \\begin{array}{cc} \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi\\land\\psi$} \\RightLabel{$\\;\\scriptsize \\land e_g$} \\UnaryInfC{$\\Gamma \\vdash\\phi $} \\end{prooftree} \u0026 \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi\\land\\psi$} \\RightLabel{$\\;\\scriptsize \\land e_d$} \\UnaryInfC{$\\Gamma \\vdash\\psi $} \\end{prooftree} \\end{array} $ De $\\phi\\land\\psi$, on peut déduire $\\phi$ et $\\psi$. Introduction de la disjonction\n$ \\begin{array}{cc} \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi$} \\RightLabel{$\\;\\scriptsize \\lor i_g$} \\UnaryInfC{$\\Gamma \\vdash \\phi\\lor\\psi $} \\end{prooftree} \u0026 \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\psi$} \\RightLabel{$\\;\\scriptsize \\lor i_d$} \\UnaryInfC{$\\Gamma \\vdash \\phi\\lor\\phi $} \\end{prooftree} \\end{array} $ Si on a prouvé une formule, alors a fortiori, on a prouvé cette formule ou une autre. Élimination de la disjonction\n$ \\begin{prooftree} \\AxiomC{$\\Gamma \\vdash \\phi \\lor \\psi$} \\AxiomC{$\\Gamma',\\phi \\vdash \\theta$} \\AxiomC{$\\Gamma'',\\psi \\vdash \\theta$} \\RightLabel{$\\;\\scriptsize \\lor e$} \\TrinaryInfC{$\\Gamma,\\Gamma',\\Gamma''\\vdash \\theta$} \\end{prooftree} $ Si on a prouvé $\\phi\\lor\\psi$, alors pour prouver $\\theta$, il suffit de prouver $\\theta$ en supposant $\\phi$ ou prouver $\\theta$ en supposant $\\psi$. Introduction de l'implication\n$ \\begin{prooftree} \\AxiomC{$\\Gamma,\\phi\\vdash\\psi$} \\RightLabel{$\\;\\scriptsize \\rightarrow i$} \\UnaryInfC{$\\Gamma \\vdash\\phi\\rightarrow\\psi$} \\end{prooftree} $ Pour prouver $\\phi\\rightarrow \\psi$, il suffit de prouver $\\psi$ avec $\\phi$ ajoutée aux hypothèses. Élimination de l'implication (modus ponens)\n$ \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi\\rightarrow\\psi$} \\AxiomC{$\\Gamma'\\vdash\\phi$} \\RightLabel{$\\;\\scriptsize \\rightarrow e$} \\BinaryInfC{$\\Gamma,\\Gamma'\\vdash\\psi$} \\end{prooftree} $ Si on a prouvé $\\phi$ et $\\phi\\rightarrow\\psi$, alors on a prouvé $\\psi$. Introduction de la négation\n$ \\begin{prooftree} \\AxiomC{$\\Gamma,\\phi \\vdash\\bot$} \\RightLabel{$\\;\\scriptsize \\neg i$} \\UnaryInfC{$\\Gamma\\vdash\\neg\\phi$} \\end{prooftree} $ Pour montrer $\\neg\\phi$, il suffit d'aboutir à une contradiction en supposant $\\phi$. Élimination de la négation\n$ \\begin{prooftree} \\AxiomC{$\\Gamma \\vdash \\neg\\phi$} \\AxiomC{$\\Gamma' \\vdash \\phi$} \\RightLabel{$\\;\\scriptsize \\neg e$} \\BinaryInfC{$\\Gamma,\\Gamma'\\vdash \\bot$} \\end{prooftree} $ Si on a prouvé à la fois $\\phi$ et $\\neg\\phi$, alors on a prouvé une contradiction. Affaiblissement\n$ \\begin{prooftree} \\AxiomC{$\\Gamma \\vdash \\phi$} \\RightLabel{$\\;\\scriptsize aff$} \\UnaryInfC{$\\Gamma,\\psi\\vdash \\phi$} \\end{prooftree} $ Si on peut prouver $\\phi$ avec les hypothèses $\\Gamma$, alors on peut prouver $\\phi$ en ajoutant d'autres hypothèses à $\\Gamma$ (il peut y avoir des hypothèses qui ne servent pas). Contraction\n$ \\begin{prooftree} \\AxiomC{$\\Gamma,\\psi,\\psi \\vdash \\phi$} \\RightLabel{$\\;\\scriptsize ctr$} \\UnaryInfC{$\\Gamma,\\psi\\vdash \\phi$} \\end{prooftree} $ L'ensemble d'hypothèses $\\Gamma\\cup\\set{\\psi,\\psi}$ est le même que $\\Gamma\\cup\\set{\\psi}$. Chaque règle peut être regardée comme un arbre simple lu de haut en bas et dont la racine est en bas (pour une fois). Le séquent conclusion est donc la racine et les prémisses sont les feuilles.\nL\u0026rsquo;axiome n\u0026rsquo;est qu\u0026rsquo;une racine. Les éliminations de la conjonction, les introductions de la disjonctions, l\u0026rsquo;introduction de l\u0026rsquo;implication, l\u0026rsquo;introduction de la négation, l\u0026rsquo;affaiblissement et la contraction sont des arbres à une racine et une feuille. l\u0026rsquo;introduction de la conjonction, l\u0026rsquo;élimination de l\u0026rsquo;implication et l\u0026rsquo;élimination de la négation sont des arbres à une racine et deux feuilles. l\u0026rsquo;élimination de la disjonction et un arbre à une racine et trois feuilles. Pour construire une preuve, on va assembler ces arbres.\nUne déduction (en logique minimale) dans le système de la déduction naturelle est un arbre fini dont les nœuds sont des séquents $\\left(S_i\\right)_{i≤k}$, et tel que pour chaque nœud $S_i$ de la déduction :\n$S_i$ est une feuille si et seulement si $S_i$ est un axiome, si $S_i$ n'est pas une feuille, alors l'arbre dont $S_i$ est la racine et les enfants de $S_i$ sont les feuilles est une instance de l'une des règles du tableau. Une formule $\\phi$ est déductible des hypothèses $\\Gamma$ s\u0026rsquo;il existe une déduction dont la racine soit le séquent $\\Delta\\vdash\\phi$ pour un ensemble d\u0026rsquo;hypothèses $\\Delta \\subseteq \\Gamma$. On note alors $\\Gamma\\vdash_m \\phi$.\nExemples de preuves :\nProuver que $(\\phi\\rightarrow (\\psi\\rightarrow\\theta))\\rightarrow ((\\phi\\rightarrow\\psi)\\rightarrow(\\phi\\rightarrow\\theta))$ est bien un théorème du calcul des propositions revient à obtenir une déduction du séquent $\\vdash_{\\! m} \\;\\phi\\rightarrow (\\psi\\rightarrow\\theta))\\rightarrow ((\\phi\\rightarrow\\psi)\\rightarrow(\\phi\\rightarrow\\theta)$ :\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\phi\\rightarrow(\\phi\\rightarrow\\theta)\\vdash\\phi(\\psi\\rightarrow\\theta)$} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\RightLabel{$\\;\\scriptsize \\rightarrow e$} \\BinaryInfC{$\\phi\\rightarrow(\\psi\\rightarrow\\theta),\\phi\\vdash\\psi\\rightarrow\\theta$} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\phi\\rightarrow\\psi\\vdash\\phi\\rightarrow\\psi$} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\RightLabel{$\\;\\scriptsize \\rightarrow e$} \\BinaryInfC{$\\phi\\rightarrow\\psi,\\phi\\vdash\\psi$} \\RightLabel{$\\;\\scriptsize \\rightarrow e$} \\BinaryInfC{$\\phi\\rightarrow(\\psi\\rightarrow\\theta),\\phi\\rightarrow\\psi,\\phi,\\phi\\vdash\\theta$} \\RightLabel{$\\;\\scriptsize ctr$} \\UnaryInfC{$\\phi\\rightarrow(\\psi\\rightarrow\\theta),\\phi\\rightarrow\\psi,\\phi\\vdash\\theta$} \\RightLabel{$\\;\\scriptsize \\rightarrow i$} \\UnaryInfC{$\\phi\\rightarrow(\\psi\\rightarrow\\theta),\\phi\\rightarrow\\psi\\vdash(\\phi\\rightarrow\\theta)$} \\RightLabel{$\\;\\scriptsize \\rightarrow i$} \\UnaryInfC{$\\phi\\rightarrow(\\psi\\rightarrow\\theta)\\vdash(\\phi\\rightarrow\\psi)\\rightarrow(\\phi\\rightarrow\\theta)$} \\RightLabel{$\\;\\scriptsize \\rightarrow i$} \\UnaryInfC{$\\vdash (\\phi\\rightarrow(\\psi\\rightarrow\\theta))\\rightarrow((\\phi\\rightarrow\\psi)\\rightarrow(\\phi\\rightarrow\\theta))$} \\end{prooftree} \\phantom{--------} $$ Preuve du séquent $\\vdash_{\\!m}\\; \\phi\\rightarrow\\neg\\neg\\phi$ :\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\neg\\phi\\vdash\\neg\\phi$} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\RightLabel{$\\;\\scriptsize \\neg e$} \\BinaryInfC{$\\neg\\phi,\\phi\\vdash\\bot$} \\RightLabel{$\\;\\scriptsize \\neg i$} \\UnaryInfC{$\\phi\\vdash\\neg\\neg\\phi$} \\RightLabel{$\\;\\scriptsize \\rightarrow i$} \\UnaryInfC{$\\vdash \\phi\\rightarrow\\neg\\neg\\phi$} \\end{prooftree} $$ Par contre, on ne peut pas prouver l\u0026rsquo;implication inverse avec les règles de la logique minimale ($\\nvdash_{\\!m} \\neg\\neg\\phi\\rightarrow\\phi$). On ne peut donc pas éliminer les doubles négations (on verra plus loin qu\u0026rsquo;il manque la règle de l\u0026rsquo;absurdité classique pour y parvenir).\nMontrons que $\\vdash_{\\!m} \\,\\neg\\phi\\leftrightarrow (\\phi\\rightarrow\\bot)$ :\nRq : pour raccourcir les preuves, on va intégrer dorénavant les affaiblissements et contractions aux autres règles.\nLa notion de preuve telle qu\u0026rsquo;on l\u0026rsquo;a défini ne s\u0026rsquo;appuie que sur un enchaînement de règles pour aboutir à la conclusion. Il s\u0026rsquo;agit donc d\u0026rsquo;une conséquence syntaxique des formules en hypothèse.\nDe la même manière qu\u0026rsquo;on a défini l\u0026rsquo;équivalence sémantique à partir de la notion de conséquence sémantique, on va définir une équivalence au sens syntaxique à partir des séquents.\nDeux formules $\\phi$ et $\\psi$ sont équivalentes pour la logique minimale si on a à la fois $\\phi\\vdash_{\\!m}\\psi$ et $\\psi\\vdash_{\\!m}\\phi$, et on écrit $\\phi\\equiv_m\\psi$\nLogique intuitionniste Certains raisonnements ne sont pas possibles avec la logique minimale. On va enrichir le pouvoir expressif de notre système déductif grâce à de nouvelles règles. Ces règles concernent la contradiction $\\bot$, et plus précisément son élimination.\nAbsurdité intuitionniste (ou élimination de la contradiction)\n$ \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\bot$} \\RightLabel{$\\;\\scriptsize \\bot e_i$} \\UnaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $ Si une contradiction découle d\u0026rsquo;un jeu d\u0026rsquo;hypothèses, alors n\u0026rsquo;importe quelle formule est démontrable avec ces mêms hypothèses.\nOn obtient ainsi un nouveau système déductif ; celui de la logique intuitionniste.\nLa règle de l\u0026rsquo;absurdité intuitionniste n\u0026rsquo;est pas démontrable dans le cadre de la logique minimale. Les déductions qui utilisent cette règle ne sont donc pas possibles en logique minimale. Il existe par conséquent des formules qui sont des théorèmes en logique intuitionniste (des séquents démontrables), mais restent non prouvables en logique minimale.\nDeux formules $\\phi$ et $\\psi$ sont équivalentes pour la logique intuitionniste si on a à la fois $\\phi\\vdash_{\\!i}\\psi$ et $\\psi\\vdash_{\\!i}\\phi$, et on écrit $\\phi\\equiv_i\\psi$.\nExemple d'une formule démontrable en logique intuitionniste et non démontrable en logique minimale\u0026nbsp;: $\\neg\\neg(\\neg\\neg\\phi\\rightarrow\\phi)$ $$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\neg\\neg\\phi\\vdash\\neg\\neg\\phi$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax+aff$} \\UnaryInfC{$\\phi,\\neg\\neg\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\phi\\vdash\\neg\\neg\\phi\\rightarrow\\phi$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\neg(\\neg\\neg\\phi\\rightarrow\\phi)$} \\RightLabel{$\\scriptsize\\;\\neg e$} \\BinaryInfC{$\\neg(\\neg\\neg\\phi\\rightarrow\\phi),\\phi\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\neg i$} \\UnaryInfC{$\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\neg\\phi$} \\RightLabel{$\\scriptsize\\;\\neg e$} \\BinaryInfC{$\\neg\\neg\\phi,\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\bot e_i$} \\UnaryInfC{$\\neg\\neg\\phi,\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\neg\\neg\\phi\\rightarrow\\phi$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\neg(\\neg\\neg\\phi\\rightarrow\\phi)$} \\RightLabel{$\\scriptsize\\;\\neg e + ctr$} \\BinaryInfC{$\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\neg i$} \\UnaryInfC{$\\vdash \\neg\\neg(\\neg\\neg\\phi\\rightarrow\\phi)$} \\end{prooftree} $$ La logique classique Absurdité classique\n$ \\begin{prooftree} \\AxiomC{$\\Gamma,\\neg\\phi\\vdash\\bot$} \\RightLabel{$\\;\\scriptsize \\bot e_c$} \\UnaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $ Si une contradiction découle d\u0026rsquo;un jeu d\u0026rsquo;hypothèse auquel on ajoute la négation d\u0026rsquo;une formule, alors la formule est démontrable à partir du jeu d\u0026rsquo;hypothèses de départ.\nEn ajoutant à la logique minimale non pas la règle de l\u0026rsquo;absurdité intuitionniste mais cette règle de l\u0026rsquo;absurdité classique, on obtient la logique classique.\nL\u0026rsquo;absurdité classique n\u0026rsquo;est démontrable ni en logique minimale, ni en logique intuitionniste, alors que l\u0026rsquo;absurdité intuitionniste n\u0026rsquo;est qu\u0026rsquo;un cas particulier de l\u0026rsquo;absurdité classique. $$ \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;aff$} \\UnaryInfC{$\\Gamma,\\neg\\phi\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\bot e_c$} \\UnaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $$ Cela montre que la logique classique est un upgrade par rapport à la logique intuitionniste.\nDes raisonnements comme la loi du tiers exclu ou l\u0026rsquo;élimination des doubles négations deviennent enfin démontrables (ils ne l\u0026rsquo;étaient pas en logique intuitionniste et a fortiori pas non plus en logique minimale).\nLoi du tiers exclu $\\vdash\\phi\\lor\\neg\\phi$ Élimination des doubles négations $\\vdash\\neg\\neg\\phi\\rightarrow\\phi$ Loi de Peirce $\\vdash(\\neg\\phi\\rightarrow\\phi)\\rightarrow\\phi$ La contraposition $\\vdash(\\neg\\psi\\rightarrow\\neg\\phi)\\rightarrow(\\phi\\rightarrow\\psi)$ L\u0026rsquo;ajout de l\u0026rsquo;absurdité classique aux règles de la logique minimale n\u0026rsquo;est pas le seul chemin pour obtenir la logique classique. En utilisant chacun des quatre différents séquents que l\u0026rsquo;on vient de démontrer comme axiome, on peut augmenter soit la logique minimale, soit la logique intuitionniste en logique classique.\nlogique classique = logique intuitionniste + principe du tiers exclu logique classique = logique intuitionniste + loi de Peirce logique classique = logique minimale + élimination des doubles négations logique classique = logique minimale + contraposition Nous allons montré que les inclusions sont vérifiées dans les deux sens.\nPour le sens $\\supset$, c'est déjà fait.\nEn effet, on a démontré plus haut que chacun des axiomes ajoutés peut se démontrer en logique classique sans hypothèse supplémentaire. Sens $\\subset$\u0026nbsp;: il suffit de vérifier à chaque fois que la règle de l'absurde classique peut être obtenue à partir de la logique considérée à laquelle on a ajouté l'axiome. logique classique $\\subset$ logique intuitionniste + principe du tiers exclu\u0026nbsp;:\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\color{#970E53}\\vdash\\phi\\lor\\neg\\phi$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\AxiomC{$\\Gamma,\\neg\\phi\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\bot e_i$} \\UnaryInfC{$\\Gamma,\\neg\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\lor e$} \\TrinaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $$ logique classique $\\subset$ logique intuitionniste + loi de Peirce\u0026nbsp;:\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\color{#970E53}\\neg\\phi\\rightarrow\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\vdash (\\neg\\phi\\rightarrow\\phi)\\rightarrow\\phi$} \\AxiomC{$\\Gamma,\\neg\\phi\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\bot e_i$} \\UnaryInfC{$\\Gamma,\\neg\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\Gamma\\vdash \\neg\\phi\\rightarrow\\phi$} \\RightLabel{$\\scriptsize\\;\\lor e$} \\BinaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $$ logique classique $\\subset$ logique minimale + élimination des doubles négations\u0026nbsp;:\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\color{#970E53}\\vdash\\neg\\neg\\phi\\rightarrow\\phi$} \\AxiomC{$\\Gamma,\\neg\\phi\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\neg i$} \\UnaryInfC{$\\Gamma\\vdash\\neg\\neg\\phi$} \\RightLabel{$\\scriptsize\\;\\lor e$} \\BinaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $$ logique classique $\\subset$ logique minimale + contraposition\nOn va utiliser une instance de la contraposition où $\\psi:=\\phi$ et $\\phi:=\\neg\\bot$\u0026nbsp;:\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\color{#970E53}\\neg\\phi\\rightarrow\\neg\\neg\\bot\\vdash\\neg\\bot\\rightarrow\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\vdash (\\neg\\phi\\rightarrow\\neg\\neg\\bot)\\rightarrow(\\neg\\bot\\rightarrow\\phi)$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\bot\\vdash\\bot$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\neg\\bot\\vdash\\neg\\bot$} \\RightLabel{$\\scriptsize\\;\\rightarrow e$} \\BinaryInfC{$\\bot,\\neg\\bot\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\neg i$} \\UnaryInfC{$\\bot\\vdash\\neg\\neg\\bot$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\vdash \\bot\\rightarrow\\neg\\neg\\bot$} \\AxiomC{$\\Gamma,\\neg\\phi\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\rightarrow e$} \\BinaryInfC{$\\Gamma,\\neg\\phi\\vdash\\neg\\neg\\bot$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\Gamma\\vdash\\neg\\phi\\rightarrow\\neg\\neg\\bot$} \\RightLabel{$\\scriptsize\\;\\rightarrow e$} \\BinaryInfC{$\\Gamma\\vdash\\neg\\bot\\rightarrow\\phi$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\bot\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\neg i$} \\UnaryInfC{$\\vdash\\neg\\bot$} \\RightLabel{$\\scriptsize\\;\\rightarrow e$} \\BinaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $$ info\nLa logique intuitionniste, contrairement à la logique classique, vise à obtenir des preuves constructives. Établir la vérité de $\\phi$ ne suffit pas, il faut la construire étape par étape. Or les raisonnements par l\u0026rsquo;absurde classique ne construisent pas réellement la vérité de $\\phi$ puisqu\u0026rsquo;ils se contentent d\u0026rsquo;établir une contradiction mettant en jeu $\\neg\\phi$. De même, le tiers exclu nous affirme que $\\phi=\\psi\\lor\\neg\\psi$ sans jamais établir la vérité de $\\psi$ ou $\\neg\\psi$.\nExemple classique de l\u0026rsquo;utilisation du tiers exclu en mathématique :\nprouvons qu\u0026rsquo;il existe un couple de nombres irrationnels $(a,b)$ ($a,b\\in\\mathbb{R}\\setminus \\mathbb{Q}$) tels que $a^b$ soit rationnel ($a^b\\in\\mathbb{Q}$).\nSachant que $\\sqrt{2}$ est irrationnel, on distingue deux cas :\nsi $\\sqrt{2}^\\sqrt{2}$ est rationnel, alors on prend $a=b=\\sqrt{2}$\nsi $\\sqrt{2}^\\sqrt{2}$ est irrationnel, alors on prend $a=\\sqrt{2}^\\sqrt{2}$ et $b=\\sqrt{2}$ puisqu\u0026rsquo;ainsi $a^b=2$.\nOn a donc bien répondu à la question puisque d\u0026rsquo;après le principe du tiers exclu, on est dans un cas ou dans l\u0026rsquo;autre. Mais pour savoir laquelle des deux options est la bonne, il faudrait savoir si $\\sqrt{2}^\\sqrt{2}$ est rationnel ou non (et démontrer qu\u0026rsquo;il ne l\u0026rsquo;est pas est beaucoup plus lourd)\u0026hellip;\nEn logique intuitionniste, si l\u0026rsquo;on a réussi à démontrer le séquent $\\vdash_{\\! i}\\phi\\lor\\psi$, c\u0026rsquo;est qu\u0026rsquo;on a démontré un des deux séquents $\\vdash_{\\! i}\\phi$ ou $\\vdash_{\\! i}\\psi$. Il n\u0026rsquo;y a pas de vérité intermédiaire indéterminée.\nUn autre grief contre la logique classique concerne l\u0026rsquo;implication matérielle : $\\phi\\rightarrow\\psi$ est considérée comme vraie si $\\neg\\phi$ sans qu\u0026rsquo;on n\u0026rsquo;ait jamais eu à construire de lien entre $\\phi$ et $\\psi$. La philosophe Dorothy Edgington en a tiré une démonstration de l\u0026rsquo;existence de Dieu grâce à la formule suivante : si Dieu n\u0026rsquo;existe pas ($\\neg G$), il n\u0026rsquo;est pas vrai que si je prie ($P$), alors mes prières seront entendues ($A$). L\u0026rsquo;affirmation ne semble pas choquante. Sous forme de formule, cela donne : $(\\neg G)\\rightarrow \\neg(P\\rightarrow A)$. La formule peut se réécrire en éliminant les implications : $G\\lor\\neg(\\neg P \\lor A)$. Et voilà maintenant le coup de grâce, sous la forme d\u0026rsquo;une deuxième formule : $\\neg P$, \u0026ldquo;je ne prie pas\u0026rdquo;. Et si $P$ est faux, seul $G$ survit dans la formule principale ; \u0026ldquo;Dieu existe\u0026rdquo;.\nLes règles de la logique classique permettent de faire coïncider les notions de conséquence syntaxique et de conséquence sémantique.\nCorrection de la logique classique :\nSoient $\\Gamma$ un ensemble fini de formules et $\\phi$ une formule du calcul des propositions,\nSi $\\Gamma\\vdash_{\\!c}\\phi$ alors $\\Gamma\\models\\phi$\nUne preuve étant une suite finie de séquent, on va procéder par induction sur la longueur de la preuve.\nSi la preuve a une longueur 1, le séquent ne peut être qu'un axiome de la forme $\\phi\\vdash\\phi$.\nReste à montrer que $\\phi\\models\\phi$ est vraie. Et c'est bien sûr le cas\u0026nbsp;: par définition, $\\phi\\models\\phi$ est vraie si $\\phi$ est vraie dans chaque modèle où $\\phi$ est vraie...\nOn suppose maintenant la propriété vraie pour toutes les preuves de longueur $n$.\nUne preuve de longueur $n+1$ est une preuve de longueur $n$ à laquelle on ajoute un séquent qui est soit un axiome (on revient au cas précédent), soit une des règles.\nIl faut montrer que les règles ont été judicieusement choisies pour \"conserver\" la notion de conséquence sémantique entre els prémisses et la conclusion.\nIci, les prémisses sont bien des conséquences sémantiques par application de l'hypothèse d'induction (elles correspondent à des séquents de longueur $≤n$).\nOn doit maintenant vérifier sur chaque règle que le séquent conclusion est une conséquence sémantique des prémisses.\nMontrons-le pour la règle d'élimination de l'implication $ \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi\\rightarrow\\psi$} \\AxiomC{$\\Gamma'\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow e$} \\BinaryInfC{$\\Gamma,\\Gamma'\\vdash\\psi$} \\end{prooftree} $\u0026nbsp;: Par hypothèse d'induction, on sait que $\\Gamma\\models\\phi\\rightarrow\\psi$ et $\\Gamma'\\models\\phi$. Donc dans les modèles de $\\Gamma \\cup \\Gamma'$ (ensemble des modèles où les formules de $\\Gamma$ et $\\Gamma'$ sont vraies simultanément), à la fois $\\phi\\rightarrow\\psi$ et $\\phi$ sont vérifiées. Et comme lorsque $\\phi$ est vraie, $\\phi\\rightarrow\\psi$ n'est vraie que si $\\psi$ est vraie, il en résulte que $\\psi$ est vraie dans ces modèles\u0026nbsp;: $\\Gamma,\\Gamma'\\models\\psi$.\nOn peut montrer de manière équivalente que cela marche pour chaque règle... Une conséquence de la correction de la logique classique est que pour toute théorie finie $\\Gamma$ et pour tout modèle $\\mathcal{M}$ de cette théorie, si une formule est démontrable à partir des hypothèses $\\Gamma$, alors elle est vraie dans $\\mathcal{M}$.\nTout ce que l\u0026rsquo;on peut déduire syntaxiquement d\u0026rsquo;une théorie est vrai dans un modèle quelconque de cette théorie.\nComplétude de la logique classique :\nSoient $\\Gamma$ un ensemble fini de formules et $\\phi$ une formule du calcul des propositions,\nSi $\\Gamma\\models\\phi$ alors $\\Gamma\\vdash_{\\!c}\\phi$\nSoit $\\phi$, une formule quelconque du calcul des propositions dont les variables sont parmis $\\set{P_1,\\ldots,P_n}$, et $\\mathcal{M}$, un modèle possible de $\\phi$. On note :\n$P_i^\\mathcal{M} = P_i$ si $P_i$ est vraie dans $\\mathcal{M}$, $P_i^\\mathcal{M} = \\neg P_i$ sinon. On pose également :\n$\\phi^\\mathcal{M} = \\phi$ si $\\mathcal{M}\\models\\phi$, $\\phi^\\mathcal{M} = \\neg\\phi$ sinon. Par induction sur la hauteur de $\\phi$ on veut montrer le résultat suivant : $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\phi^{\\mathcal{M}}$\nsi $ht(\\phi)=0$, $\\phi$ est une variable propositionnelle $P_i$ et il est alors immédiat que $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}P_i^{\\mathcal{M}}$ si $ht(\\phi)=n+1$, alors on a une des possibilités suivantes\u0026nbsp;: $\\phi=\\neg\\psi$, $\\phi=\\psi_1\\rightarrow\\psi_2$, $\\phi=\\psi_1\\lor\\psi_2$ ou $\\phi=\\psi_1\\land\\psi_2$.\nChaque $\\psi$, de hauteur $n$, respecte par hypothèse la propriété et on doit montrer dans chaque cas que cela implique le même respect de la propriété pour $\\phi$. Prenons par exemple le cas $\\phi=\\psi_1\\rightarrow\\psi_2$ pour voir le type de raisonnement qu\u0026rsquo;il faut mener : Si $\\mathcal{M}\\not\\models\\psi_1$, alors $\\mathcal{M}\\models\\phi$ et donc $\\phi^\\mathcal{M}=\\phi$, et $\\psi_1^\\mathcal{M}=\\neg\\psi_1$.\nPar hypothèse d'induction, $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\neg\\psi_1$\nConsidérons la règle suivante\u0026nbsp;:\nL'utilisation de cette règle et du modus ponens permet d'obtenir\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1\\rightarrow\\psi_2$, ce qui est bien ce que l'on voulait montrer puisque $\\psi_1\\rightarrow\\psi_2=\\phi^\\mathcal{M}$ Si $\\mathcal{M}\\models\\psi_1$, alors\u0026nbsp;: Si $\\mathcal{M}\\models\\psi_2$, alors $\\mathcal{M}\\models\\phi$ et donc $\\phi^\\mathcal{M}=\\phi$, $\\psi_1^\\mathcal{M}=\\psi_1$ et $\\psi_2^\\mathcal{M}=\\psi_2$.\nPar hypothèse d'induction, on a\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1^\\mathcal{M}$ et $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_2^\\mathcal{M}$\nC'est-à-dire\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1$ et $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_2$\nDe $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_2$, on déduit par affaiblissement $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}},\\psi_1 \\vdash_{\\!c}\\psi_2$, puis par introduction de l'implication\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1 \\rightarrow\\psi_2$ Si $\\mathcal{M}\\not\\models\\psi_2$, alors $\\mathcal{M}\\not\\models\\phi$ et donc $\\phi^\\mathcal{M}=\\neg\\phi$, $\\psi_1^\\mathcal{M}=\\psi_1$ et $\\psi_2^\\mathcal{M}=\\neg\\psi_2$.\nPar hypothèse d'induction, on a\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1^\\mathcal{M}$ et $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_2^\\mathcal{M}$\nC'est-à-dire\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1$ et $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\neg\\psi_2$\nOn en déduit\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1\\land\\neg\\psi_2$.\nConsidérons la règle suivante\u0026nbsp;: On applique ensuite la règle du modus ponens pour obtenir $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\neg(\\psi_1\\rightarrow\\psi_2)$ qui est bien le résultat recherché. Supposons qu\u0026rsquo;on ait passé en revue tous les autres cas pour finir de montrer que $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\phi^{\\mathcal{M}}$.\nNous allons maintenant prouver que pour toute formule $\\phi$, si $\\phi$ est une tautologie, alors $\\phi$ est un théorème de la logique classique.\nSoit $\\phi$, une formule quelconque du calcul des propositions dont les variables sont $P_1,\\ldots,P_n$. Et soit $\\mathcal{M}$ un modèle possible de $\\phi$. On a montré que $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\phi^{\\mathcal{M}}$. Or $\\mathcal{M}\\models\\phi$ est vérifié, donc $\\phi^\\mathcal{M}=\\phi$. D\u0026rsquo;où $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\phi$.\nSi $\\phi$ est une tautologie, ce résultat est valable pour n\u0026rsquo;importe quel modèle. Choisissons les modèles $\\mathcal{M}$ et $\\mathcal{M\u0026rsquo;}$ qui distribuent les mêmes valeurs de vérité pour toutes les variables $P_i$ sauf $P_n$. $\\mathcal{M}\\models P_n$ alors que $\\mathcal{M\u0026rsquo;}\\models \\neg P_n$.\nOn a à la fois $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-1}^{\\mathcal{M}},P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\phi$ et $\\set{P_1^{\\mathcal{M\u0026rsquo;}},\\ldots,P_{n-1}^{\\mathcal{M\u0026rsquo;}},P_n^{\\mathcal{M\u0026rsquo;}}} \\vdash_{\\!c}\\phi$.\nD\u0026rsquo;où $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-1}^{\\mathcal{M}},P_n} \\vdash_{\\!c}\\phi$ et $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-1}^{\\mathcal{M}},\\neg P_n} \\vdash_{\\!c}\\phi$ qu\u0026rsquo;on peut aussi écrire $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-1}^{\\mathcal{M}}},P_n \\vdash_{\\!c}\\phi$ et $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-1}^{\\mathcal{M}}},\\neg P_n \\vdash_{\\!c}\\phi$.\nConstruisons la règle suivante en appliquant le tiers exclu :\n$$ \\begin{prooftree} \\AxiomC{} \\UnaryInfC{$\\vdash \\psi\\lor\\neg\\psi$} \\AxiomC{} \\UnaryInfC{$\\Gamma,\\psi\\vdash\\phi$} \\AxiomC{} \\UnaryInfC{$\\Gamma,\\neg\\psi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\lor e$} \\TrinaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $$ En appliquant cette règle à notre cas, on obtient\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-1}^{\\mathcal{M}}} \\vdash_{\\!c}\\phi$ et à nouveau, ce résultat ne dépend pas du modèle $\\mathcal{M}$. On peut donc recommencer le même raisonnement, ce qui donne successivement\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-2}^{\\mathcal{M}}} \\vdash_{\\!c}\\phi, \\set{P_1^{\\mathcal{M}},\\ldots,P_{n-3}^{\\mathcal{M}}} \\vdash_{\\!c}\\phi, \\set{P_1^{\\mathcal{M}},\\ldots,P_{n-4}^{\\mathcal{M}}} \\vdash_{\\!c}\\phi,\\ldots $\nEt en dernière étape, on obtient $P_1\\vdash_{\\!c}\\phi$ et $\\neg P_1\\vdash_{\\!c}\\phi$. D'où l'on déduit $\\vdash_{\\!c}\\phi$ par application du tiers exclu.\nNous avons donc bien réussi à prouver que $\\models\\phi$ implique $\\vdash_{\\!c}\\phi$. Il ne nous reste plus qu\u0026rsquo;à montrer que pour toute formule $\\phi$ et pour tout ensemble fini d\u0026rsquo;hypothèses $\\Gamma=\\set{\\psi_1,\\ldots,\\psi_k}$, $\\Gamma\\models\\phi$ implique $\\Gamma\\vdash_{\\!c}\\phi$.\nIl apparait immédiatement que $\\Gamma\\models\\phi$ si et seulement si $\\models(\\psi_1\\land\\psi_2\\land\\ldots\\land\\psi_k)\\rightarrow\\phi $.\nLe résultat précédent sur les tautologies nous permet de déduire que $\\vdash_{\\!c}(\\psi_1\\land\\psi_2\\land\\ldots\\land\\psi_k)\\rightarrow \\phi$.\nIl est évident par ailleurs que $\\Gamma\\vdash_{\\!c}(\\psi_1\\land\\psi_2\\land\\ldots\\land\\psi_k)$.\nPar modus ponens, on obtient donc $\\Gamma\\vdash_{\\!c}\\phi$.\ntip\nDe façon informelle, la correction stipule que toute formule prouvable est vraie, alors que la complétude, de son côté, affirme que toutes les formules vraies sont prouvables.\nEt dit autrement encore, on peut prouver toute la vérité (complétude) et rien que la vérité (correction).\nCorollaire :\n$\\models\\phi$ ssi $\\vdash_{\\!c}\\phi$\nToute tautologie est démontrable sans utiliser d\u0026rsquo;hypothèse et à l\u0026rsquo;inverse, toute formule démontrable sans hypothèse est une tautologie.\nCorollaire :\nSoient $\\phi$ et $\\psi$ deux formules du calcul des propositions,\n$\\Gamma\\equiv\\psi$ ssi $\\Gamma\\equiv_c\\psi$\nOn a montré finalement que la sémantique du calcul des propositions correspond à la logique classique. On peut d\u0026rsquo;ailleurs parler de \u0026ldquo;sémantique classique\u0026rdquo;. Mais qu\u0026rsquo;en est-il de la logique intuitionniste ? Peut-on définir une sémantique qui lui soit adaptée afin d\u0026rsquo;obtenir un théorème de complétude pour la logique intuitionniste ?\nOui, grâce aux modèles de Kripke.\nLes modèles de Kripke du calcul des propositions intuitionniste On a vu qu\u0026rsquo;une formule est démontrable en logique classique si et seulement si elle est vraie dans tous les modèles (classiques) possibles. On va aboutir à une proposition similaire grâce aux modèles de Kripke : une formule est démontrable en logique intuitionniste si et seulement si elle est réalisée dans tous les modèles de Kripke possibles.\nSoit $\\set{P_1,\\ldots,P_k}$ un ensemble de variables propositionnelles.\n$\\mathcal{K}=(\\mathcal{T}_\\mathcal{K},\\Vdash)$ est un modèle de Kripke arborescent fini sur les variables $P_1,\\ldots,P_k$ si :\n$\\mathcal{T}_\\mathcal{K}$ est un arbre fini dont la relation d'ancestralité entre deux nœuds est noté $\\alpha≤\\beta$ ($\\alpha$ est un ancêtre de $\\beta$). $\\Vdash$ est une relation binaire appelée relation de forcing entre les nœuds de $\\mathcal{T}_\\mathcal{K}$ et les variables propositionnelles qui vérifie\u0026nbsp;:\npour tout nœud $\\alpha$\u0026nbsp;: $\\alpha\\not\\Vdash\\bot$ pour tout nœud $\\alpha$, $\\beta$ et variable propositionnelle $P_i$, si à la fois $\\alpha≤\\beta$ et $\\alpha\\Vdash P_i$ alors on a $\\beta\\Vdash P_i$. info\nUn modèle de Kripke avec un seul nœud peut être vu comme un modèle de la logique classique car les conditions de la relations de forcing ($\\Vdash$) sont alors identiques à celles de la relation de vérité sémantique ($\\models$).\nLa relation de forcing fait en sorte que si une formule est vraie dans un nœud (qui représente un monde, comme on le verra en logique modale), alors elle reste vraie dans tous les nœuds accessibles depuis le nœud de départ (tous les mondes accessibles).\nSoit $\\mathcal{K}=(\\mathcal{T}_\\mathcal{K},\\Vdash)$ un modèle de Kripke arborescent fini sur les variables $P_1,\\ldots,P_k$, $\\alpha$ un nœud de $\\mathcal{T}_\\mathcal{K}$ et $\\phi$, $\\psi$ des formules du calcul des propositions dont les variables sont parmi $\\set{P_1,\\ldots,P_k}$,\n$\\alpha\\Vdash \\phi\\land\\psi$ ssi $\\alpha\\Vdash\\phi$ et $\\alpha\\Vdash\\psi$ $\\alpha\\Vdash \\phi\\lor\\psi$ ssi $\\alpha\\Vdash\\phi$ ou $\\alpha\\Vdash\\psi$ $\\alpha\\Vdash \\phi\\rightarrow\\psi$ ssi pour tout nœud $\\beta$ de $\\mathcal{T}_\\mathcal{K}$ tel que $\\alpha≤\\beta$, si $\\beta\\Vdash\\phi$ alors $\\beta\\Vdash\\psi$.\nOn cherche ici à construire explicitement la vérité de $\\psi$ à partir de celle de $\\phi$ (alors qu'en logique classique, l'établissement de la vérité de $\\phi$ n'est pas nécessaire puisque l'implication est considérée comme vraie soit par l'absence de $\\phi$, soit par la présence de $\\psi$). $\\alpha\\Vdash \\neg\\phi$ ssi pour tout nœud $\\beta$ de $\\mathcal{T}_\\mathcal{K}$ tel que $\\alpha≤\\beta$, $\\beta\\not\\Vdash\\phi$.\nDit autrement, aucun nœud ne force le faux.\nEn effet, la négation $\\neg\\phi$ est interprétée ici comme la formule $\\phi\\rightarrow\\bot$. La définition de la relation de forcing de l'implication dit que\u0026nbsp;: $\\alpha\\Vdash\\phi\\rightarrow\\bot$ ssi pour tout nœud $\\beta$ de $\\mathcal{T}_\\mathcal{K}$ tel que $\\alpha≤\\beta$, si $\\beta\\Vdash\\phi$ alors $\\beta\\Vdash\\bot$. Or la condition $\\beta\\Vdash\\bot$ est interdite par définition des modèles de la logique intuitionniste, donc $\\beta\\Vdash\\bot$ n'est satisfait nulle part. D'où $\\beta\\not\\Vdash\\phi$. Voilà un exemple de modèle de Kripke sur les variables $P$, $Q$, $R$. On y trouve les relations de forcing suivantes :\n$\\theta\\Vdash Q\\land P$ mais $\\beta\\not\\Vdash Q\\land P$ $\\alpha\\Vdash P\\rightarrow Q$ mais $\\alpha\\not\\Vdash Q\\rightarrow P$ $\\beta\\Vdash\\neg P$ mais $\\alpha\\not\\Vdash\\neg P$ $\\alpha\\not\\Vdash P$, $\\alpha\\not\\Vdash \\neg P$ et $\\alpha\\not\\Vdash \\neg\\neg P$ Soit $\\mathcal{K}=(\\mathcal{T}_\\mathcal{K},\\Vdash)$ un modèle de Kripke arborescent fini sur les variables $P_1,\\ldots,P_k$, $\\alpha$ un nœud de $\\mathcal{T}_\\mathcal{K}$, $\\phi$ une formule et $\\Gamma$ un ensemble fini de formules dont les variables sont parmi $\\set{P_1,\\ldots,P_k}$,\n$\\alpha\\Vdash \\Gamma$ ssi $\\alpha\\Vdash\\phi$ pour toute formule de $\\Gamma$. $\\phi$ (respectivement $\\Gamma)$ est réalisée dans $\\mathcal{K}$ si pour tout nœud $\\alpha$ de $\\mathcal{T}_\\mathcal{K}$, $\\alpha\\Vdash\\phi$ (respectivement $\\alpha\\Vdash\\Gamma$). $\\phi$ est une conséquence intuitionniste de $\\Gamma$, noté $\\Gamma\\Vdash_i\\phi$, ssi pour tout modèle de Kripke $\\mathcal{K}=(\\mathcal{T}_\\mathcal{K},\\Vdash)$ et tout nœud $\\alpha$ si $\\alpha\\Vdash\\Gamma$ alors $\\alpha\\Vdash\\phi$.\nEn particulier, on note $\\Vdash_i \\phi$ si $\\phi$ est réalisée dans tout modèle de Kripke. Ce modèle ne réalise ni le tiers exclu $(P\\lor\\neg P)$, ni l\u0026rsquo;élimination des doubles négations $(\\neg\\neg P\\rightarrow P)$.\nEn effet :\n$\\alpha \\not\\Vdash P$ par définition et $\\alpha\\not\\Vdash \\neg P$ puisque $\\alpha≤\\beta$ et $\\beta\\Vdash P$. Donc $\\alpha\\not\\Vdash P\\lor\\neg P$.\n$\\alpha\\not\\Vdash\\neg P$ et $\\beta\\not\\Vdash\\neg P$ d\u0026rsquo;où $\\alpha\\Vdash \\neg\\neg P$ et $\\beta\\Vdash \\neg\\neg P$. D\u0026rsquo;où $\\beta\\Vdash\\neg\\neg P\\rightarrow P$ et $\\alpha\\not\\Vdash\\neg\\neg P\\rightarrow P$ (puisque $\\alpha \\not\\Vdash P$).\nCela permet de constater l\u0026rsquo;adaptation des modèles de Kripke à la logique intuitionniste ; il peut exister des modèles où ni $\\phi$, ni $\\neg\\phi$ ne sont établis comme vrais.\nCorrection et complétude de la logique intuitionniste :\nSoient $\\Gamma$ un ensemble fini de formules et $\\phi$ une formule du calcul des propositions,\n$\\Gamma\\Vdash_i\\phi$ ssi $\\Gamma\\vdash_{\\!i} \\phi$\nCorollaire :\n$\\Vdash_i\\phi$ ssi $\\vdash_{\\!i} \\phi$\nEn abandonnant la condition \u0026ldquo;pour tout nœud $\\alpha$ : $\\alpha\\not\\Vdash\\bot$\u0026rdquo; on obtient le modèle de Kripke adaptée à la logique minimale.\nLa condition de forcing de la négation devient alors $\\alpha\\Vdash\\neg\\phi$ ssi $\\alpha\\Vdash\\phi\\rightarrow\\bot$ (pour tout nœud $\\beta$ de $\\mathcal{T}_\\mathcal{K}$ tel que $\\alpha≤\\beta$, si $\\beta\\Vdash\\phi$ alors $\\beta\\Vdash\\bot$).\nOn obtient cette fois-ci :\nCorrection et complétude de la logique minimale :\nSoient $\\Gamma$ un ensemble fini de formules et $\\phi$ une formule du calcul des propositions,\n$\\Gamma\\Vdash_m\\phi$ ssi $\\Gamma\\vdash_{\\!m} \\phi$\nD\u0026rsquo;où découle $\\Vdash_m\\phi$ ssi $\\vdash_{\\!m} \\phi$\nDans ce modèle de Kripke de la logique minimale, $\\neg\\neg(\\neg\\neg P\\rightarrow P)$ n\u0026rsquo;est pas réalisée.\nEn effet, $\\beta\\not\\Vdash P$ par définition, d\u0026rsquo;où $\\beta\\Vdash \\neg P$. Mais on a également $\\beta\\Vdash\\neg\\neg P$ puisque $\\beta\\Vdash \\bot$. Mais alors $\\beta\\not\\Vdash \\neg\\neg P\\rightarrow P$. Par conséquent $\\beta\\Vdash\\neg(\\neg\\neg P\\rightarrow P)$ et toujours à cause du fait que $\\beta\\Vdash \\bot$, on obtient $\\beta\\Vdash \\neg\\neg(\\neg\\neg P\\rightarrow P)$.\n$\\alpha\\not\\Vdash P$ et $\\beta\\not\\Vdash P$ par définition, d\u0026rsquo;où $\\beta\\Vdash \\neg P$. Mais $\\alpha\\not\\Vdash \\neg\\neg P$ puisque $\\alpha\\not\\Vdash\\bot$. On a aussi $\\alpha\\not\\Vdash\\neg\\neg P\\rightarrow P$ puisque $\\beta\\Vdash \\neg\\neg P$ et $\\beta\\not\\Vdash P$. Par conséquent, $\\alpha\\Vdash \\neg(\\neg\\neg P\\rightarrow P)$ puisqu\u0026rsquo;à la fois $\\alpha\\not\\Vdash\\neg\\neg P \\rightarrow P$ et $\\beta\\not\\Vdash \\neg\\neg P\\rightarrow P$. D\u0026rsquo;où $\\alpha\\not\\Vdash\\neg\\neg(\\neg\\neg P\\rightarrow P)$ puisqu\u0026rsquo;à la fois $\\alpha\\Vdash\\neg(\\neg\\neg P \\rightarrow P)$ et $\\alpha\\not\\Vdash\\bot$.\nIl existe donc un nœud du modèle qui ne force pas la formule $\\neg\\neg(\\neg\\neg P\\rightarrow P)$. Le théorème de complétude de la logique minimale nous dit alors que $\\neg\\neg(\\neg\\neg P\\rightarrow P)$ n\u0026rsquo;est pas un théorème de la logique minimale.\nOr on a montré que cette formule était un théorème de la logique intuitionniste. Le séquent $\\vdash\\neg\\neg(\\neg\\neg P\\rightarrow P)$ est donc prouvable en logique intuitionniste mais pas en logique minimale. Cela prouve que la logique minimale forme un sous-ensemble strict de la logique intuitionniste, elle-même strictement inclue dans la logique classique. Le calcul des séquents Comme la déduction naturelle, le calcul des séquents a été mis au point par Gerhard Gentzen. Son but est de mettre encore mieux en lumière les propriétés mathématiques de la notion de démonstration.\nLes différences principales avec la déduction naturelle sont que la partie droite des séquents n\u0026rsquo;est plus une seule formule conclusion mais un ensemble fini de formules, et les règles d\u0026rsquo;élimination sont remplacées par des règles d\u0026rsquo;introduction à gauche afin de rendre le système de démonstration symétrique.\nUn séquent (noté $\\Gamma\\vdash\\Delta$) est un couple où :\n$\\Gamma$ est un ensemble fini de formules, appelé la partie gauche du séquent, $\\Delta$ est également un ensemble fini de formules, appelé la partie droite du séquent. Intuitivement, un séquent $\\Gamma\\vdash\\Delta$ où $\\Gamma=\\set{\\phi_1,\\ldots,\\phi_n}$ et $\\Delta=\\set{\\psi_1,\\ldots,\\psi_k}$ est interprété comme $\\bigwedge_{1≤i≤n}\\phi_i \\vdash \\bigvee_{1≤j≤k} \\psi_j$. Une conjonction d\u0026rsquo;hypothèses prouve une disjonction de conclusions.\nLa conjonction vide d\u0026rsquo;hypothèse est interprété comme le vrai, alors que la disjonction vide de conclusion est interprétée comme le faux.\n$\\vdash\\Delta$ signifie donc $(\\psi_1\\lor\\psi_2\\lor\\ldots\\lor\\psi_k)$, alors que le séquent $\\Gamma\\vdash$ signifie $(\\phi_1\\land\\phi_2\\land\\ldots\\land\\phi_n)\\rightarrow\\bot$, c\u0026rsquo;est-à-dire $\\neg(\\phi_1\\land\\phi_2\\land\\ldots\\land\\phi_n)$.\nEnfin $\\vdash$, qui signifie \u0026ldquo;vrai implique faux\u0026rdquo;, est interprété comme l\u0026rsquo;absurde (il correspond au séquent $\\vdash\\bot$ de la déduction naturelle).\nLes règles du calcul des séquents sont très proches de celles de la déduction naturelle.\nD\u0026rsquo;un ensemble de prémisses (0, 1 ou 2), on déduit un séquent conclusion.\nLes règles d\u0026rsquo;introduction sont conservées, mais elles deviennent des règles d\u0026rsquo;introduction à droite. Les règles d\u0026rsquo;élimination deviennent, elles, des règles d\u0026rsquo;introduction à gauche.\nLes règles structurelles sont symétrisées et on ajoute un axiome ainsi qu\u0026rsquo;une nouvelle règle, la règle de coupure :\n$$ \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi,\\Delta$} \\AxiomC{$\\Gamma',\\phi\\vdash\\Delta'$} \\RightLabel{$\\scriptsize\\;cut$} \\BinaryInfC{$\\Gamma,\\Gamma'\\vdash\\Delta,\\Delta'$} \\end{prooftree} $$ En déduction naturelle, cela correspond aux deux prémisses $\\Gamma\\vdash\\phi$ et $\\phi\\vdash\\psi$ desquels on déduit $\\Gamma\\vdash\\psi$.\nLa règle de coupure est souvent utilisée dans les preuves pour simplifier la déduction en introduisant des lemmes ou des théorèmes intermédiaires.\nÉtant donné qu\u0026rsquo;il n\u0026rsquo;y a plus de règles d\u0026rsquo;élimination, c\u0026rsquo;est la règle de coupure qui permet de rétablir la transitivité dans les déductions.\nL\u0026rsquo;affaiblissement à droite reflète l\u0026rsquo;idée que si on a une preuve que quelque chose est vrai à partir d\u0026rsquo;un ensemble de prémisses, ajouter une conclusion supplémentaire comme possibilité n\u0026rsquo;enlève rien à la validité de cette preuve.\nEn remplaçant $\\Delta$ par $\\emptyset$, on retrouve la règle d\u0026rsquo;absurdité intuitionniste.\nPreuve du tiers exclu\u0026nbsp;: $$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\neg_d$} \\UnaryInfC{$\\vdash\\phi,\\neg\\phi$} \\RightLabel{$\\scriptsize\\;\\lor_d$} \\UnaryInfC{$\\vdash\\phi\\lor\\neg\\phi$} \\end{prooftree} $$ Preuve de $\\vdash\\phi\\rightarrow\\neg\\neg\\phi$\u0026nbsp;: $$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\neg_g$} \\UnaryInfC{$\\phi,\\neg\\phi\\vdash$} \\RightLabel{$\\scriptsize\\;\\neg_d$} \\UnaryInfC{$\\phi\\vdash\\neg\\neg\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow_d$} \\UnaryInfC{$\\vdash\\phi\\rightarrow\\neg\\neg\\phi$} \\end{prooftree} $$ Preuve de l'élimination des doubles négations\u0026nbsp;: $$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\neg_d$} \\UnaryInfC{$\\vdash\\phi,\\neg\\phi$} \\RightLabel{$\\scriptsize\\;\\neg_g$} \\UnaryInfC{$\\neg\\neg\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow_g$} \\UnaryInfC{$\\vdash\\neg\\neg\\phi\\rightarrow\\phi$} \\end{prooftree} $$ Les deux formules précédentes se ressemblent fortement mais seule la première est prouvable en logique intuitionniste.\nLes deux preuves en calcul des séquents sont quasiment les mêmes, mais l\u0026rsquo;ordre d\u0026rsquo;introduction des négations est inversé (gauche-droite pour la première et droite-gauche pour la deuxième).\nLa logique intuitionniste est la version du calcul des séquents dont les règles sont restreintes aux séquents avec au plus une formule à droite, la contraction à droite étant considérée comme implicite.\nL\u0026rsquo;idée est d\u0026rsquo;obliger une preuve à indiquer spécifiquement la conclusion dérivée, sans ambiguïté.\nEt la négation est interprétée comme l\u0026rsquo;absence de preuve puisque puisque c\u0026rsquo;est maintenant seulement $\\phi\\vdash\\bot$ qui donne $\\vdash\\neg\\phi$. Cela rend par conséquent l\u0026rsquo;utilisation de la négation à droite impossible dans la deuxième preuve alors qu\u0026rsquo;il n\u0026rsquo;y a pas de problème dans la première.\nLa logique minimale est la version du calcul des séquents sans la règle de l\u0026rsquo;affaiblissement à droite et dont les règles sont restreintes aux séquents avec au plus une formule à droite (la contraction à droite n\u0026rsquo;est ici pas considérée comme implicite).\nPreuve de $\\vdash\\neg\\neg(\\neg\\neg\\phi\\rightarrow\\phi)$\u0026nbsp;: Chaque nœud de l\u0026rsquo;arbre de preuve ne contient pas plus d\u0026rsquo;une formule à droite du symbole du séquent, par contre la règle d\u0026rsquo;affaiblissement à droite est utilisée, ce qui situe cette preuve dans le cadre de la logique intuitionniste mais pas de la logique minimale.\nLa règle de coupure correspond, en déduction naturelle, à l\u0026rsquo;introduction de l\u0026rsquo;implication suivie aussitôt de son élimination :\n$$ \\begin{prooftree} \\AxiomC{$\\Gamma,\\phi\\vdash\\psi$} \\RightLabel{$\\scriptsize\\;\\rightarrow_i$} \\UnaryInfC{$\\Gamma\\vdash\\phi\\rightarrow\\psi$} \\AxiomC{$\\Gamma'\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow_e$} \\BinaryInfC{$\\Gamma,\\Gamma'\\vdash\\psi$} \\end{prooftree} $$ Cette règle est essentielle pour tenir compte de la manière dont nous raisonnons puisqu\u0026rsquo;il arrive très souvent que l\u0026rsquo;on utilise les conclusions d\u0026rsquo;un raisonnements antérieur comme hypothèses pour de nouvelles démonstrations (hypothèses que nous faisons disparaître sur la base du fait que nous les avons démontrées).\nMais si notre objectif est d\u0026rsquo;automatiser les preuves, de les algorithmiser, les règles de coupure deviennent un handicap.\nÉlimination des coupures\nS\u0026rsquo;il existe, en calcul des séquents classique (respectivement intuitionniste), une preuve du séquent $\\Gamma\\vdash\\Delta$, alors il existe une preuve en calcul des séquents classique (respectivement intuitionniste) de ce séquent sans utilisation de la règle de coupure.\nTout séquent est donc prouvable par utilisation des seuls axiomes, règles logiques et règles structurelles. La démonstration du théorème consiste alors justement à remplacer dans une preuve donnée chaque utilisation de la règle de coupure par d\u0026rsquo;autres règles.\nCorollaire : le séquent \u0026ldquo;$\\vdash$\u0026rdquo; n\u0026rsquo;est pas prouvable en logique classique.\n\u0026ldquo;$\\vdash$\u0026rdquo; signifie que le vrai entraîne le faux. Nous voilà plutôt soulagé d\u0026rsquo;être dans l\u0026rsquo;incapacité de prouver ce résultat qui ferait s\u0026rsquo;effondrer tout l\u0026rsquo;édifice logique patiemment construit. Si $\\vdash$ était prouvable, alors par affaiblissement, tout séquent $\\vdash\\Delta$ le serait également\u0026hellip;\nS\u0026rsquo;il existait une preuve du séquent $\\vdash$, il en existerait aussi une sans utilisation de la règle de coupure. Or toutes les autres règles introduisent une formule soit à droite, soit à gauche, soit des deux côtés à la fois. La seule règle permettant de faire disparaître une formule est la règle de contraction, mais elle ne la fait pas disparaître complètement puisqu\u0026rsquo;elle se contente d\u0026rsquo;en faire disparaître les occurences. On ne pourra donc jamais aboutir à \u0026ldquo;$\\vdash$\u0026rdquo; sans la règle de coupure et donc d\u0026rsquo;après le théorème d\u0026rsquo;élimination des coupures, on ne peut pas démontrer $\\vdash$ en calcul des séquents. Youpi.\nPropriété de la sous-formule\nSi le séquent $\\Gamma\\vdash\\Delta$ est prouvable en logique classique (respectivement intuitionniste), alors il existe une preuve en logique classique (respectivement intuitionniste) de ce séquent dans laquelle n\u0026rsquo;apparaîssent que des séquents constitués de sous-formules des formules de $\\Gamma$ et de $\\Delta$.\nIl existe une preuve sans coupure de $\\Gamma\\vdash\\Delta$. Or on peut vérifier règle par règle, par induction sur la hauteur de cette preuve sans coupure, qu\u0026rsquo;elle peut ne faire apparaître que des sous-formules de $\\Gamma$ et de $\\Delta$.\ninfo\nUne conséquence de la propriété de la sous-formule est qu\u0026rsquo;une preuve d\u0026rsquo;une disjonction en logique intuitionniste passe nécessairement par une preuve d\u0026rsquo;un des termes de la disjonction : $\\vdash_{\\!i}\\phi\\lor\\psi$ si et seulement si ($\\vdash_{\\!i}\\phi$ ou $\\vdash_{\\!i}\\psi$). Par exemple, prouver une instance du tiers exclu $\\Gamma\\vdash_{\\!i}\\phi\\lor\\neg\\phi$ en logique intuitionniste suppose qu\u0026rsquo;on a été capable soit de prouver $\\Gamma\\vdash_{\\!i}\\phi$, soit de prouver $\\Gamma\\vdash_{\\!i}\\neg\\phi$, et ce n\u0026rsquo;est pas le cas en logique classique.\nLa conséquence majeure de la propriété de la sous-formule est de permettre (tout particulièrement en logique intuitionniste) une recherche de preuve automatique. La production mécanique de preuve comme par exemple la preuve de correction de programmes informatiques devient en effet possible une fois qu\u0026rsquo;on se débarrasse du caractère abstrait des preuves par coupures. Cela donne des preuves longues mais simples ne faisant appel qu\u0026rsquo;aux sous-formules des formules du séquent qu\u0026rsquo;il s\u0026rsquo;agit de prouver.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/relatres/",
	"title": "Relativité restreinte",
	"tags": [],
	"description": "",
	"content": " Relativité restreinte Vision géométrique Série de vidéos sur la relativité restreinte avec un angle plutôt géométrique.\nSource : le génial petit livre \u0026ldquo;Very special relativity - An illustrated guide\u0026rdquo; de Sander Bais.\nDiagrammes d\u0026rsquo;espace-\u0026ldquo;temps propre\u0026rdquo; FloatHeadPhysics communique bien dans la vidéo suivante la joie de la découverte d\u0026rsquo;un modèle explicatif simple.\nIci, elle vient de ce merveilleux petit livre illustré : Relativity visualized de Lewis Caroll Epstein.\nnote\nNéanmoins, si les diagrammes utilisant le temps propre permettent de bonnes intuitions, ils ne peuvent pas réellement se substituer aux diagrammes de Minkowski comme outil de travail.\nCe ne sont même pas vraiment des diagrammes d\u0026rsquo;espace-temps puisque le temps propre n\u0026rsquo;est pas une coordonnée globale (il dépend du référentiel).\nMathématiquement, $\\tau=\\int \\sqrt{g_{\\mu\\nu}dx^\\mu dx^\\nu}$ (avec une signature ($+$,$-$,$-$,$-$)) et on ne peut donc pas écrire $d\\tau = f_\\mu(x)dx^\\mu$. Cela signifie que $\\tau$ n\u0026rsquo;est pas une différentielle exacte et par suite qu\u0026rsquo;une intégrale du temps propre dépend du chemin choisi.\nRésultat, la notion d\u0026rsquo;évènement perd son sens ! Dans un diagramme de Minkowski, chaque point est un évènement unique $(t,x)$ et c\u0026rsquo;est quand même un peu ce qu\u0026rsquo;on attend d\u0026rsquo;une description de l\u0026rsquo;espace-temps\u0026hellip; Mais ce n\u0026rsquo;est plus le cas pour les diagrammes d\u0026rsquo;Epstein ! Les retrouvailles des jumeaux de Langevin ont lieu en deux points distincts par exemple.\nMasse du photon Paradoxe des jumeaux Champ magnétique Il n\u0026rsquo;y a pas de champ magnétique, seulement un champ électrique et du mouvement.\nEn cela, le champ magnétique est l\u0026rsquo;illustration la plus spectaculaire de la relativité restreinte puisque le moindre aimant est une manifestation de la théorie d\u0026rsquo;Einstein !\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/geometrie/geo3/",
	"title": "Aires et Volumes",
	"tags": [],
	"description": "",
	"content": " Aires et volumes "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/meca/hamilton/",
	"title": "Bypass vers Hamilton",
	"tags": [],
	"description": "",
	"content": " De Newton à Hamilton "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/",
	"title": "Physique",
	"tags": [],
	"description": "",
	"content": " Physique Mécanique Électromagnétisme Optique Relativité restreinte Relativité générale Mécanique quantique Théorie quantique des champs Thermodynamique et Physique statistique Chaos Curiosités "
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/proba/",
	"title": "Probabilités",
	"tags": [],
	"description": "",
	"content": " Probabilités Loi Binomiale D\u0026rsquo;une épreuve de Bernoulli à la loi binomiale :\nLe petit programme ci-dessous permet de déterminer l\u0026rsquo;intervalle de fluctuation pour un seuil donné.\nEt on pourra vérifier l\u0026rsquo;intervalle graphiquement avec l\u0026rsquo;applet suivant.\nLoi normale La célèbre gaussienne (ou courbe en cloche) :\nOn peut à nouveau tirer profit de l\u0026rsquo;applet géogebra qui précède pour obtenir des intervalles de fluctuation.\nDensité de probabilités Nouvelle énigme permettant de parler des densités de probabilité :\nQuelle est la distance moyenne entre deux points pris au hasard sur un segment ?\nOn peut vérifier la solution statistiquement avec un petit programme simplissime :\nProbabilités conditionnelles et Bayes Les probabilités conditionnelles au tribunal, chez le médecin et dans un vieux jeu télévisé américain :\nChaînes de Markov Le jeu de Penney a d\u0026rsquo;étonnantes propriétés. Les chaînes de Markov aident à le démystifier :\nCuriosités et énigmes Spaghetti et inégalité triangulaire Une petite énigme à base d\u0026rsquo;inégalité triangulaire :\nQuelle est la probabilité de pouvoir faire un triangle avec les 3 bouts obtenus en cassant aléatoirement un spaghetti en deux endroits ?\nAnniversaires simultanés Quelle est la probabilité que deux élèves d\u0026rsquo;une classe est un anniversaire le même jour ?\nLorsqu\u0026rsquo;on n\u0026rsquo;a jamais fait le calcul ou entendu parler du résultat, notre intuition nous amène généralement à soupçonner une probabilité bien plus petite qu\u0026rsquo;elle ne l\u0026rsquo;est.\nC\u0026rsquo;est d\u0026rsquo;ailleurs assez fréquent que notre intuition soit aux fraises lorsqu\u0026rsquo;il s\u0026rsquo;agit d\u0026rsquo;estimer une probabilité ou expliquer des statistiques (voir le paradoxe de Simpson).\nParadoxe des deux enfants Le paradoxe des deux enfants repose grandement sur la formulation et provoque encore parfois des débats passionnés.\nParadoxe de Cover Où comment un tirage aléatoire permet de gagner de l\u0026rsquo;information !\nOn vous présente deux papiers pliés où sont écrits deux nombres choisis aléatoirement $a$ et $b$.\nVous choisissez un des deux papiers et l\u0026rsquo;ouvrez pour découvrir le nombre $X$ (soit $a$, soit $b$). Puis on vous demande lequel des deux papiers contient le plus grand nombre.\nCela semble du 50-50, et pourtant\u0026hellip; Si vous tirez un nombre aléatoire $Z$ selon une distribution continue sur $\\mathbb{R}$, vous pouvez augmenter vos chances ! Il suffit de comparer $Z$ à $X$ : si $Z\u0026gt;X$ vous choisissez $Y$ comme plus grand nombre (l\u0026rsquo;autre papier) et si $Z\u0026lt;X$, vous choisissez $X$.\nDans le code ci-dessous, on tire deux nombres aléatoires $a$ et $b$ selon une distribution normale centrée sur 0 et d\u0026rsquo;écart-type 100, puis on tire au sort le nombre $X$. Petite différence par rapport à ce qui précède, on se sert d\u0026rsquo;une fonction logistique pour \u0026ldquo;envoyer\u0026rdquo; $X$ entre 0 et 1 puis on le compare à un $Z$ tiré uniformément entre 0 et 1.\nOn voit qu\u0026rsquo;on obtient alors un taux de succès autour de 75% !\nPar quel miracle ?\nDéjà, comme $Z$ est issue d\u0026rsquo;une loi continues, on n\u0026rsquo;aura jamais $Z=a$ ou $Z=b$ et pour que la méthode permette de deviner correctement, il faut que :\n$X=\\max(a,b)$ (une chance sur deux) et $Z\u0026lt;X$ $X=\\min(a,b)$ (une chance sur deux) et $Z\u0026gt;X$ La probabilité de succès est donc :\nD'où $P_\\text { succ }=\\frac{1}{2}P(Z\u003c\\max (a, b))+\\frac{1}{2}[1-P(Z\u003c\\min (a, b))]=\\frac{1}{2}+\\frac{1}{2}[P(Z\u003c\\max (a, b))-P(Z \u003c \\min (a, b))]=\\frac{1}{2}+\\tfrac12\\color{#970E53}P\\left(\\min(a,b) \u003c Z\u003c\\max(a,b)\\right)$\nPuisque $Z$ suit une loi continue ${\\color{#970E53}P\\left(\\min(a,b) \u003c Z\u003c\\max(a,b)\\right)}\u003e0$. On se retrouve donc bien avec une probabilité supérieure à $\\tfrac12$. Dans une démonstration similaire (qu\u0026rsquo;on retrouve dans cet article de John Baez retraçant l\u0026rsquo;origine du paradoxe), Greg Egan utilise une fonction $f:\\mathbb{R}\\rightarrow ]0,1[$ strictement croissante (si $x\u0026lt;y$, $f(x)\u0026lt;f(y)$) pour ramener $X$ dans $]0,1[$. C\u0026rsquo;est en suivant cette méthode qu\u0026rsquo;on a utilisé dans le code la fonction logistique $f(x)=\\frac{\\mathrm{e}^x}{\\mathrm{e}^x +1}$.\nLe nombre aléatoire $Z$ est alors choisi uniformément entre $0$ et $1$ et on compare $Z$ à $f(X)$ plutôt qu\u0026rsquo;à $X$.\nSupposons $a\u0026gt;b$.\nLa probabilité de deviner correctement devient :\nOn obtient ainsi $P_\\text { succ }=\\frac{1}{2}+\\frac{1}{2}(f(a)-f(b))\u003e\\frac{1}{2}$ puisque $f(a)\u003ef(b)$ par croissance stricte de $f$. Les deux démonstrations sont équivalentes. Suffit de poser $f(x)=P(Z\u0026lt;x)$ (bien strictement croissante) pour s\u0026rsquo;en convaincre. $P_\\text { succ }$ devient alors $\\frac{1}{2}+\\frac{1}{2}(P(Z\u0026lt;\\max(a,b))-P(Z\u0026lt;\\min(a,b))$.\nDans le code ci-dessus, les nombres $a$ et $b$ sont volontairement très éloignés de 0 ($\\sigma=100$). La fonction logistique donne alors soit $\\approx 0$ (nombre $\\ll 0$), soit $\\approx 1$ (nombre $\\gg 0$). Donc $f(a)-f(b)$ va donner $1$ lorsque $a$ et $b$ sont de signes différents (50% des cas) et $0$ dans les autres cas. Ça nous donne une espérance de succès de $\\tfrac12+\\tfrac12\\tfrac12=\\tfrac34$.\nEn diminuant $\\sigma$, on diminue le pourcentage de succès (on retrouve que l\u0026rsquo;efficacité de la méthode est d\u0026rsquo;autant plus grande que $Z$ a de chances d\u0026rsquo;être entre $a$ et $b$).\nSi $a$, $b$ et $Z$ sont tirés uniformément dans $[0,1]$, on obtient une espérance de succès de $\\tfrac23$. En effet, on a alors $P_\\text { succ }(a,b)=\\tfrac12 + \\tfrac12|a-b|$. Or comme on l\u0026rsquo;a vu plus haut, $\\mathbb{E}[|a-b|]=\\tfrac13$, donc $\\mathbb{E}[P_\\text{succ}(a,b)]=\\tfrac12+\\tfrac12\\tfrac13=\\tfrac23$.\nCela finit par sérieusement perdre de sa magie\u0026hellip; Si la loi d\u0026rsquo;où est tiré $Z$ est adaptée aux valeurs de $a$ et $b$, tout va pour le mieux, mais dès que ce n\u0026rsquo;est plus le cas, on se retrouve bêtement autour de 50%\u0026hellip; Il suffit par exemple d\u0026rsquo;un $\\mu\\gg\\sigma$ dans loi normale du code. La fonction logistique va envoyer tout le monde sur $\\approx 1$ et on n\u0026rsquo;aura quasi jamais $Z\u0026gt;f(a)$.\nCertes, le hasard permet d\u0026rsquo;augmenter ses chances, mais tant que la distribution des nombres sur les papiers et celle du nombre aléatoire tiré ne se correspondent pas, l\u0026rsquo;avantage obtenu sera infinitésimal\u0026hellip; Le tirage n\u0026rsquo;apporte donc en soit aucune information importante. Celle-ci viendrait plutôt du choix adéquat de la distribution de $Z$ puisqu\u0026rsquo;il impliquerait de connaître celle de $a$ et $b$ !\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/qed-light/",
	"title": "QED light",
	"tags": [],
	"description": "",
	"content": " Réflexion vitreuse Pourquoi lors d’une réflexion sur un milieu transparent (eau, verre, etc.), la lumière semble-t-elle se réfléchir uniquement sur la surface, alors que la transparence même du milieu assure que la lumière pénètre bien à l’intérieur ? Qu’a donc de si particulier la surface ? Rien assurément. Mais POURQUOI ALORS crie le curieux désespéré face au Grand Téton.\nphoto : Chascar (Parc national du Grand Teton près du Jackson Lake Lodge)\nC’est là qu’un Feynman capée de sa déconcertante facilité débarque : «c’est une histoire de petites flèches pauvre créature limitée». Et il développa ces histoires de flèches dans une série de conférences tout public consacrées à l’électrodynamique quantique (QED the strange theory of light and matter).\nL’électrodynamique quantique, hmm\u0026hellip; D’après Wikipedia, cette théorie s’occupe des interactions entre électrons, donc de la lumière. On lit aussi que la QED s’inscrit dans un cadre conceptuel plus large : la théorie quantique des champs (TQC). Là, on a peur : la réponse serait-elle aussi inaccessible que le Grand Téton ? Mais puisque l’irritant génie te dit que ce n’est qu’une histoire de flèches\u0026hellip;\nTentons de colporter Sa parole.\nLong préalable : des flèches qui tournent La quantique nous apprend que l’observation d’un évènement est probabiliste : on a une certaine probabilité d’observé un photon à un endroit donné, et la probabilité complémentaire de ne pas l’observer. La TQC ajoute que cette probabilité finale résulte d’une interférence entre de multiples ondelettes de probabilité qui se propagent partout dans l’espace sur un champ mystérieux.\nPrécisons un peu : quelque soit l’évènement, la probabilité de le détecter s’obtient par superposition (ou somme) de toutes les probabilités attachées à chaque réalisation possible de l’évènement (même la plus saugrenue !). Techniquement, on somme des amplitudes de probabilités qui se propagent en oscillant au cours du temps le long de chaque chemin de réalisation considéré.\nMais sommer des trucs qui oscillent n’est pas tout à fait aussi simple que de sommer des carottes. Cela revient en fait à sommer des flèches qui tournent.\nRemarque : une flèche qui tourne peut être modélisée par un nombre complexe $\\mathrm{e}^{\\mathrm{i}t}$ (les complexes permettent de représenter un vecteur, une flèche, en un seul nombre, la partie imaginaire servant de 2e axe, et $\\mathrm{e}^{\\mathrm{i}t}$ représente une flèche qui fait un tour par seconde). Grâce aux complexes, on somme presque des ondes comme des carottes.\nAjoutons que le nombre de tours par seconde que font ces flèches (la fréquence des ondes) est proportionnel à l’énergie (plus précisément à l’hamiltonien $H$, ce qui donne une amplitude complexe en $\\mathrm{e}^{\\mathrm{i}Ht}$).\nN’importe quel processus, comme aller d’un point A à un point B, peut se faire d’une infinité de façons. La TQC assure que toutes ces possibilités sont à prendre en compte et à sommer, y compris la promenade du photon au jardin des tuileries avant de revenir en B. Dans un monde très simplifié où le photon n’interagit avec personne lors de son parcours (énergie constante et uniforme), chacune de ces possibilités a la même probabilitéréflexion de se réaliser, et par conséquent, chacune correspond à une flèche de même taille (très très petite). Même taille d’accord, mais pas même orientation. Chaque flèche aura tourné d’un angle proportionnel au temps du parcours !\nEt cette rotation qui n’a l’air de rien est primordiale. En effet, on peut sommer 1000 flèches de même longueur et obtenir une flèche de longueur nulle, il suffit que, mises bout à bout, la dernière flèche pointe sur le pied de la première.\nOn ajoute donc des petites flèches les unes aux autres, une flèche par possibilité, et la probabilité final d’observer l’évènement va dépendre de l’angle dont les flèches tournent d’un chemin à l’autre (le chemin étant une possibilité de réalisation). Rq : cette somme sur l’ensemble des chemins correspond aux intégrales de chemin de Feynman.\nMais voyons où peuvent nous mener ces flèches qui tournent\u0026hellip;\nPropagation rectiligne de la lumière Reprenons l’histoire du photon allant d’un point A à un point B et dessinons tous les chemins possibles. Répétons-le, la longueur du chemin est proportionnelle à l’angle de rotation de la flèche correspondante. En sommant toutes les flèches, on se rend vite compte que seules certaines contributions sont «constructives» dans le sens où elles seules permettent d’augmenter la longueur de la flèche finale. Et elles correspondent à des parcours rectilignes ou quasi rectilignes.\nEn effet, près de la ligne droite, les différents chemins sont quasiment aussi longs et sont donc parcourus en des temps très proches. Par conséquent, les flèches tournent peu et pointent à peu près dans la même direction. Leur somme donne donc une flèche plus grande ! C’est une situation d’interférence entre ondes en phase. Au contraire, dès qu’on s’écarte de la ligne droite, ça tourne beaucoup et la somme ne donne plus rien (les ondes sont déphasées, les interférences destructives).\nOn redécouvre là le principe de Fermat (et tous les autres principes variationnels qui se résument au principe de moindre action puisque c’est elle, $H\\times t$, qui fait la phase de nos ondes) ! Mais la lumière ne «cherche» pas à minimiser son temps de parcours, c’est seulement que près de ce chemin particulier, la quasi stationnarité des phases augmente la probabilité de présence. Un extrémum dans la variation de parcours correspond à des flèches variant peu d’angles pour des parcours proches et donc à une accumulation d’amplitudes de probabilité dans cette zone. Et à l’inverse, si on s’écarte de cet extrémum (puits ou col) les variations sont grandes et la somme devient vite destructive (les flèches tournent en rond).\nSi le milieu de propagation des photons est inhomogène, faisant varier spatialement la vitesse de la lumière (on verra plus bas que cette variation est artificielle), l’extrémisation des temps de trajet ne donne alors plus une ligne droite ! D’où la réfraction (et les mirages).\nMais réhomogénéisons le milieu et revenons sur un point très important : le parcours en ligne droite ne représente pas à lui seul la contribution la plus probable. Il l’est autant que n’importe quel autre parcours (y compris le passage par les Tuileries). C’est l’accumulation de parcours proches variant peu qui augmente la probabilité ! Mais que peut-il alors bien se passer si on empêche les ondes de probabilité d’aller fureter aux alentours ?\nDiffraction Envoyons un photon depuis A et comparons la probabilité qu’il arrive en B ou en C. On a compris l’histoire : dans le 1er cas, il faut sommer tous les chemins possibles entra A et B, et dans le 2e, entre A et C. D’un chemin à l’autre, il y a plus de variation entre A et C, donc les flèches tournent plus et les ondes de probabilité ont donc plus vite fait de se détruire.\nRésultat : une probabilité de détection beaucoup plus faible en C qu’en B (négligeable même).\nMais si on resserre le passage\u0026hellip;\nMoins de chemins sont maintenant accessibles et les ondes de probabilité n’ont donc plus trop le loisir d’interférer destructivement. Tous les lieux de détection tendent alors à se ressembler et la probabilité d’être détecté en C n’est ainsi plus du tout négligeable ! La lumière est diffractée.\nOn peut aussi jouer les vicieux et obturer certains passages de manière à ce que les flèches arrivent toute en phase en un point où, sans cela, la probabilité de détection aurait été très faible.\nSans bouchons, c’est pas terrible :\nAvec bouchons, oulala la grosse flèche :\nOn vient de fabriquer un réseau et prouver du même coup que les probas fouinent bien partout ; un endroit plongé dans le noir peut être éclaboussé de lumière en cachant une partie de cette lumière ! Il faut se débrouiller néanmoins pour la cacher astucieusement afin d’augmenter la probabilité de présence en cet endroit. Au départ, les probas étaient bien venues voir C mais elles trouvaient l’endroit pas terrible. Après disposition des petits caches, C devient tout à fait fréquentable\u0026hellip;\nMais revenons au Grand Téton.\nRalentissement de la lumière dans le verre Il nous manque un ingrédient pour parler de réflexion : la diffusion d’un photon par un atome (la lumière réfléchie n’est pas la lumière incidente, c’est de la lumière toute neuve, crachée par un atome éclairé).\nIl va y a voir une certaine probabilité d’être diffusé, une nouvelle flèche\u0026hellip; Sa taille donne la proba de diffusion dans la direction qui nous intéresse, chose qu’on ne sait pas calculer dans le cas du verre ou de l’eau mais qu’on peut déduire expérimentalement comme on le verra plus loin.\nOn a sa taille (enfin non mais on fait comme si), il manque encore la direction :\ndans le cas d’un milieu transparent, la proba de diffusion fait un quart de tour par rapport à la proba de passer sans encombre (+90°). Effectivement, la flèche finale (être et ne pas être diffusée) ne doit pas se trouvée réduite (dans l’approximation où le milieu est parfaitement transparent et donc n’absorbe pas, ça semble logique), ni agrandie (car ça serait bizarre). Seule possibilité restante : l’ajout d’une petite flèche à 90° qui fait seulement tourner la probabilité de ne pas être diffusé.\nEn vrai, bien sûr, le milieu absorbe un peu. Il suffit, pour le retranscrire en flèches, de courber à peu moins de 90° la flèche de la diffusion, ce qui réduira la taille de la flèche finale.\nFaisons traverser une certaine épaisseur de verre à la lumière et regardons la probabilité que le récepteur en détecte. On simplifie en supposant que la lumière ne se dirige que dans une direction.\nUn photon détecté peut être passé à travers le verre sans avoir été diffusé (grosse flèche bleue) ou avoir été émis par un atome après diffusion (petite flèche verte). Chaque atome sur le trajet est susceptible d’avoir émis un photon détecté après avoir été atteint par le photon incident (différentes flèches vertes, toutes alignées car le trajet total est toujours aussi long). On somme tout ça et qu’obtient-on approximativement ? Une grosse flèche de passage direct qui a tourné un peu (flèche rouge).\nTrès bien, mais n’y a-t-il pas autre chose qui aurait pu donner une flèche semblable ? Si, si, un trajet sans verre mais un poil plus long ! On peut donc faire semblant et dire que le temps de parcours (et donc l’angle de rotation de la flèche) est rallongé par le verre, puisque tout se passe comme si. D’où la vilaine et néanmoins très pratique assertion «le verre ralentit la lumière» (alors que les photons continuent tous à se déplacer à c bien sûr).\nRéflexion vitreuse Ayé, il est temps d’essayer de répondre à la question du départ\u0026hellip; Il suffit de déplacer le récepteur au niveau de l’émetteur (pour rester avec une seule direction) et on regarde ce qui se passe avec les flèches :\nLa lumière qui retourne au récepteur a nécessairement été diffusée (flèche verte) et suivant la profondeur de l’atome diffusant, le parcours total est plus ou moins long, et donc, d’une possibilité de diffusion à l’autre, les flèches vertes tournent (la probabilité d’être détecté après diffusion sur l’atome 4 est déphasée par rapport à celle correspondant à l’atome 3). Mises bout à bout, les probabilités de diffusion, de même taille et un peu déphasées, tournent en rond. Le résultat final peut alors se résumer à la somme de deux rayons de cercle (flèche bleue + flèche rouge). Suivant l’épaisseur de la vitre traversée, la probabilité de détecter un photon réfléchi varie donc entre 0 et le diamètre du cercle dessiné par les probabilités. Expérimentalement, cette oscillation de la réflexion en fonction d l’épaisseur est bien observée et on la trouve comprise entre 0 et 16%. Mais cela suppose une épaisseur de vitre parfaitement déterminée (à une fraction de longueur d’onde près). Dans un cas plus ordinaire, la flèche rouge s’affole et fait pleins de tours, son orientation devient aléatoire. La probabilité d’être détecté se réduit à la flèche bleue. On mesure donc facilement le rayon du cercle puisqu’il correspond à la probabilité de réflexion moyenne sur une surface vitrée (8%).\nOn remarque que la flèche verte n°1, correspondant à la probabilité de diffusion sur un atome de la surface (le tout premier), fait un angle de -90° par rapport à la flèche bleue. La flèche bleue a par conséquent un angle de rotation opposé (+π) à la flèche qui représenterait la probabilité de faire le même trajet aller-retour, mais sans avoir été diffusé (si c’était possible), rebond du photon sur la face avant quoi. Par ailleurs, la flèche rouge a, elle, une orientation correspondante à un trajet long comme l’aller-retour jusqu’à la face postérieure (la diffusion sur le petit dernier, n°6, fait bien un angle d’environ +90° avec elle).\nOn peut maintenant simplifier notre description. La réflexion sur une vitre se résume à l’interférence entre deux trajets possibles pour le photon :\nrebond sur la face avant + déphasage de 180° avec une probabilité de 8% (flèche bleue), rebond sur la face arrière avec aussi une probabilité de 8% (flèche rouge). Et si la vitre est ordinaire (épaisseur aléatoire sur une taille caractéristique d’une fraction de longueur d’onde), tout se résume à un rebond sur la face avant\u0026nbsp;! Et l’agacement nerveux provoqué par les deux Grands Tétons s’apaisent enfin (les quelques secondes où on réussi à étouffer le désespérant bourdonnement des millions de questions qui essaiment derrière ces flèches).\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/relatge/",
	"title": "Relativité générale",
	"tags": [],
	"description": "",
	"content": " Relativité générale Le plus impressionnant livre sur la relativité générale est sans contexte à mes yeux \u0026ldquo;A Journey into Gravity and Spacetime\u0026rdquo; du génial John Archibald Wheeler. Il réussit le tour de force d\u0026rsquo;expliquer la relativité générale quasiment sans mathématiques tout en n\u0026rsquo;esquivant rien de la profondeur conceptuelle de la théorie, au contraire.\nLe livre démarre par un long poème dont les derniers vers dévoilent la notion au cœur de son explication, relation fondamentale de la topologie algébrique qui peut sembler de prime abord assez cryptique : le bord d\u0026rsquo;un bord est vide.\nHelp me to make this account\nA radiant testimony\nTo the wonderful simplicity\nOf the principle that the boundary of a boundary is zero,\nHeart of Einstein\u0026rsquo;s great 1915,\nBattle-tested, and still standard Geometric account of your action, oh Gravity.\nWheeler est aussi l\u0026rsquo;un des 3 auteurs (les deux autres sont ses anciens thésards Kip Thorne et Charles Misner) de la bible de la relat, alias le \u0026ldquo;gros livre noir\u0026rdquo; ou MTW, bien sûr beaucoup plus touffu et mathématique que son \u0026ldquo;voyage dans la gravité et l\u0026rsquo;espace-temps\u0026rdquo;, mais enrobé d\u0026rsquo;une narration, là aussi, assez singulière.\nVidéo sur les ondes gravitationnelles et sur leur détection par le clignotement des pulsars.\nJe m\u0026rsquo;étais lancé, il y a longtemps, dans la rédaction de notes \u0026ldquo;améliorées\u0026rdquo; après visionnage de ces vidéos de Léonard Susskind sur la relativité générale.\nSusskind est un immense physicien théoricien et les vidéos sont des captations de ses cours donnés dans le cadre du Stanford Continuing Studies Program (sorte de cours du soir pour enthousiastes).\nLa partie 1 introduit le principe d\u0026rsquo;équivalence et les outils mathématiques nécessaires (tenseurs, métrique, dérivée covariante). La partie 2 se concentre sur la courbure. La partie 3 est le plat de résistance. Elle présente l\u0026rsquo;équation d\u0026rsquo;Einstein et détaille aussi les ondes gravitationnelles. La partie 4, moins aboutie, présente Schwarzschild et les trous noirs. La partie 5 se concentre sur les 3 tests historiques de la relativité générale. "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/quantique/3ndimensions/",
	"title": "3N dimensions",
	"tags": [],
	"description": "",
	"content": " Une ontologie à 3N dimensions Ontologie de la fonction d\u0026rsquo;onde Ney, A. « Three arguments for wave function realism », European Journal for Philosophy of Science, vol. 13, art. 50 (24 octobre 2023).\nDonner une réalité à la fonction d\u0026rsquo;onde $\\psi$, c\u0026rsquo;est accepter que notre espace 3D ne soit qu\u0026rsquo;une projection de l\u0026rsquo;espace à 3N dimensions dans lequel $\\psi$ vit.\nPourquoi 3N dimensions ? En mécanique quantique, la fonction d\u0026rsquo;onde est construite comme un champ sur un espace de $3N$ dimensions où $N$ est le nombre de particules.\nPourquoi s\u0026rsquo;embêter avec ces dimensions supplémentaires ?\nÀ cause des intrications.\nSupposons que l\u0026rsquo;on ait deux particules préparées dans un état singulet de spin $|\\uparrow\\downarrow\\rangle-|\\downarrow\\uparrow\\rangle$. Les particules sont ensuite envoyées dans des directions opposées vers deux Stern-Gerlach qui vont les dévier vers le haut ou vers le bas en fonction de la valeur de leur spin selon l’axe $z$.\nOn appelle $\\text{A}$, $\\text{B}$, $\\text{C}$ et $\\text{D}$ les 4 localisations possibles après les déflections.\nComme les particules deviennent intriquées avec les appareils de mesure, elles se trouvent au final dans un état intriqué non pas seulement par rapport à leurs spins mais aussi par rapport à leurs positions :\n$$\\psi=\\frac{1}{\\sqrt{2}}\\left[\\left|\\uparrow_z, A\\right\\rangle_1\\left|\\downarrow_z, D\\right\\rangle_2+\\left|\\downarrow_z, B\\right\\rangle_1\\left|\\uparrow_z, C\\right\\rangle_2\\right]$$ À 3-D, on ne peut pas interpréter cet état quantique comme un seul champ si on veut qu\u0026rsquo;il attribue des propriétés intrinsèques et séparables à chaque particule. Il faut donc deux champs dont les composantes correspondront aux différentes positions.\nSupposons maintenant que les deux particules ne soient pas dans un état intriqué mais dans un état produit $|\\uparrow_x\\rangle_1\\otimes |\\uparrow_x\\rangle_2$. Après les Stern-Gerlach, on a :\n$$\\psi^{\\prime}=\\frac{1}{2}\\left[\\left|\\uparrow_z, A\\right\\rangle_1\\left|\\uparrow_z, C\\right\\rangle_2+\\left|\\uparrow_z, A\\right\\rangle_1\\left|\\downarrow_z, D\\right\\rangle_2+\\left|\\downarrow_z, B\\right\\rangle_1\\left|\\uparrow_z, C\\right\\rangle_2+\\left|\\downarrow_z, B\\right\\rangle_1\\left|\\downarrow_z, D\\right\\rangle_2\\right]$$ On peut essayer à nouveau de décrire cet état par deux champs à 3-D.\nMais comment alors peut-on distinguer les deux états quantiques ? Les deux champs obtenus ont bien, chacun, la même densité sur $\\text{A}$, $\\text{B}$, $\\text{C}$ et $\\text{D}$ ; en 3-D on ne voit donc aucune différence.\nPar contre, si on interprète ces états quantiques non plus comme deux champs à 3-D mais un champ à 6-D, on capture la corrélation entre les états de $\\psi$ et donc la distinction avec $\\psi^\\prime$.\nIl suffit pour cela de considérer 4 points dans l\u0026rsquo;espace à 6-D désignés de manière transparente par $\\text{AC}$, $\\text{AD}$, $\\text{BC}$ et $\\text{BD}$ (le point $\\text{AC}$ dans l\u0026rsquo;espace 6-D des configurations correspond à la situation dans l\u0026rsquo;espace 3-D où la particule 1 est localisée en $\\text{A}$ et la particule 2 est localisée en $\\text{C}$).\nDans cet espace à 3N-D, $\\psi$ décrit un champ avec seulement des composantes en $\\text{AD}$ et $\\text{BC}$. $\\psi^\\prime$, lui, décrit un champ avec des composantes aux 4 points $\\text{AC}$, $\\text{AD}$, $\\text{BC}$, $\\text{BD}$.\nLe schéma bi-champ efface la corrélation “si et seulement si” entre la localisation de 1 et celle de 2.\nSéparabilité et localité Vouloir décrire la fonction d\u0026rsquo;onde dans un espace impose d\u0026rsquo;accepter la non séparabilité et/ou la non localité de la théorie.\nUne métaphysique est séparable si les parties (R1 et R2) permettent de déduire le tout (R1 $\\cup$ R2) .\nLa localité signifie que deux régions non causalement reliées ne peuvent pas s\u0026rsquo;influencer.\nUne motivation pour une théorie séparable et locale est l\u0026rsquo;obtention d\u0026rsquo;une ontologie simple, propre, parcimonieuse (rasoir d\u0026rsquo;Ockham) :\nles faits concernant une entité ne dépendent pas des faits concernant une autre, et un objet ne peut pas agir là où il n\u0026rsquo;est pas.\nDans l\u0026rsquo;ontologie de la fonction d\u0026rsquo;onde, la physique est bien séparable et locale (mais dans l’espace à 3N-D, pas dans l’espace 3-D). En effet :\nL’unique objet ontique est un champ $\\psi$ sur l’espace des configurations $\\mathbb R^{3N}$. L’équation de Schrödinger est une équation aux dérivées partielles locales dans cet espace : $\\partial_t\\psi(q,t)=\\hat H(q)\\psi(q,t)$. Un « point » $q=(\\mathbf r_1,\\mathbf r_2,\\dots)$ est par construction une région élémentaire ; donner $\\psi$ sur deux sous-régions disjointes $Q_1,Q_2\\subset\\mathbb R^{3N}$ suffit à fixer $\\psi$ sur leur union. Revenons à l\u0026rsquo;état singulet avec les deux stern-Gerlach.\nLa fonction d\u0026rsquo;onde sur les deux points $q_{\\text{AD}}=\\text{AD}$ et $q_\\text{BC}=\\text{BC}$ dans $\\mathbb R^{6}$ s\u0026rsquo;écrit $\\psi(q)=\\tfrac1{\\sqrt2}\\bigl[\\delta(q-q_\\text{AD})-\\delta(q-q_\\text{BC})\\bigr]$.\nRestreindre $\\psi$ à la petite boule $Q_\\text{AD}$ autour de $q_{\\text{AD}}$ et à $Q_\\text{BC}$ autour de $q_\\text{BC}$ suffit pour connaître tout $\\psi$. Cela montre la séparabilité.\nÀ 3D par contre, comme on l\u0026rsquo;a vu, connaître $|\\phi_1|^2$ sur les points $\\text{A}$ et $\\text{B}$ et $|\\phi_2|^2$ sur $\\text{C}$ et $\\text{D}$ ne détermine pas la corrélation « $\\text{A} \\leftrightarrow \\text{D}, \\text{B} \\leftrightarrow \\text{C} $ ». L’état intriqué et l’état produit restent indistinguables. La théorie n’est donc pas séparable si l’espace 3D est fondamental.\nL’évolution de $\\psi$ en $Q_{\\text{AD}}$ dépend seulement de $\\psi$ dans $Q_{\\text{AD}}$ (via $\\hat H$ évalué en $q_{\\text{AD}}$). Cela montre la localité.\nUn petit déplacement $(\\delta\\mathbf r_1,\\delta\\mathbf r_2)$ permet de rester dans la boule $Q_\\text{AD}$ dans $\\mathbb R^6$ ($\\text{AD}\\rightarrow \\text{AD}^\\prime$ proche de $\\text{AD}$), mais le même mouvement déplacerait deux particules simultanément dans des régions de l\u0026rsquo;espace éloignées à 3D : $\\text{A}\\rightarrow \\text{A}^\\prime$ et $\\text{D}\\rightarrow \\text{D}^\\prime$. On perdrait la localité.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/geometrie/geo4/",
	"title": "Dimensions",
	"tags": [],
	"description": "",
	"content": " Curse of dimensionality L\u0026rsquo;expression Curse of dimensionality (Fléau de la dimension dans sa traduction française) vient de l\u0026rsquo;américain Richard Bellman alors qu\u0026rsquo;il considérait des problèmes en programmation dynamique (qu\u0026rsquo;il inventa en 1953).\nMais qu\u0026rsquo;est-ce donc que ce \u0026ldquo;fléau\u0026rdquo; ?\nLe volume d\u0026rsquo;un cube de côté $a$ dans un espace à $n$ dimensions vaut $a^n$. Jusque là, ça va.\nPour la boule, de rayon $R$, c\u0026rsquo;est plus compliqué\u0026hellip;\nà 1D, la boule se réduit à un segment de longueur $R$. à 2D, c\u0026rsquo;est un disque de \u0026ldquo;volume\u0026rdquo; $\\pi R^2$. Ce qu\u0026rsquo;on appelle aire dans notre monde 3D est bien le volume (l\u0026rsquo;espace contenu à l\u0026rsquo;intérieur) d\u0026rsquo;un monde 2D. à 3D, le volume de la boule vaut $\\frac{4}{3}\\pi R^3$. Pour des boules de rayon 1, ça nous donne les volumes suivant : $1$ à 1D, $\\pi$ à 2D, $\\frac{4\\pi}{3}$ à 3D. Est-ce que cela continue d\u0026rsquo;augmenter pour les dimensions supérieures ?\nAussi bizarre que cela puisse paraître, on va voir que le volume culmine en dimension 5 puis s\u0026rsquo;effondre jusqu\u0026rsquo;à presque s\u0026rsquo;évanouir !\nLa petite énigme suivante va nous familiariser avec ces bizarreries.\nPetite énigme On prend un $n$-cube (cube de dimension $n$) de côté $4$ et on y place à l\u0026rsquo;intérieur autant de $n$-boules de rayon $1$ que l\u0026rsquo;on peut.\nQuel sera le rayon de la plus grande $n$-boule possible que l\u0026rsquo;on pourra placer au centre de la boite, dans l\u0026rsquo;espace laissé libre par les autres boules ?\nOn peut décomposer la boite en $2^n$ sous-boites de côté $2$ contenant chacune une boule de côté $1$.\nà 1D, il ne reste aucune place pour une sphère au milieu : le $1$-cube de côté $4$ est un segment de longueur $4$ dans lequel on peut placer deux $1$-boules de rayon $1$ qui sont des segments de longueur $2$. à 2D, 4 $2$-boules (disques) sont placés dans le $2$-cube (carré). Pythagore nous dit que la distance entre le centre de chaque boule et le centre de la boite vaut $\\sqrt{R^2+R^2}=R\\sqrt{2}$. Par conséquent, le rayon de la plus grande $2$-boule (disque) que l\u0026rsquo;on peut placer au centre est de $R\\sqrt{2}-R\\Rightarrow\\sqrt{2}-1$. à 3D, on place 8 $3$-boules dans le $3$-cube. On peut faire le même raisonnement qu\u0026rsquo;à 2D pour trouver la distance entre le centre des $3$-boules et celui de la boite. C\u0026rsquo;est toujours Pythagore qui nous aide (du moins sa généralisation à $n$ dimensions) et on obtient comme plus grand rayon possible pour la $3$-boule du milieu : $\\sqrt{R^2+R^2+R^2}-R = R\\sqrt{3}-R \\Rightarrow \\sqrt{3}-1$. On peut sans problème généraliser à $n$ dimensions. Le rayon de la sphère du milieu vaudra $\\sqrt{n}-1$. Facile !\nMais regardons maintenant ce que cela signifie.\nÀ 4D, la sphère du milieu est aussi grande que les 16 autres ! À 9D, le rayon de la sphère centrale vaut 2 !!! Elle est deux fois plus grande que les 512 voisines et elle touche les parois de la boite. À partir de 10D, la boule centrale déborderait de la boite\u0026hellip; Sorcellerie !\nVolume d\u0026rsquo;une $n$-boule Le volume $V_n$ d\u0026rsquo;une $n$-boule de rayon $R$ peut se déterminer grâce à la formule de récurrence suivante $\\displaystyle V_n(R)=\\frac{2\\pi R^2}{n} V_{n-2}(R)$\nEn prenant $V_0(R) =1$ (à partir de la formule du cube $a^n$) et $V_1(R)=2R$, on peut déterminer le volume en toute dimension.\nDémonstration de la formule de récurrence\u0026nbsp;: Le volume du $n$-boule est par définition $\\displaystyle V_n(R) = \\underset{x_1^2+x_2^2+\\cdots+x_n^2≤R^2}{\\int\\cdots\\int} dx_1 dx_2\\cdots dx_n$.\nSéparons l'intégration en deux\u0026nbsp;: $\\displaystyle V_n(R) = \\underset{x_1^2+x_2^2≤R^2}{\\iint}\\left(\\underset{x_3^2+\\cdots+x_n^2≤R^2-x_1^2-x_2^2}{\\int\\cdots\\int} dx_3\\cdots dx_n\\right)dx_1dx_2 = \\underset{x_1^2+x_2^2≤R^2}{\\iint}V_{n-2}\\left(\\sqrt{R^2-x_1^2-x_2^2}\\right)dx_1dx_2$\nPassons en coordonnées polaires pour les deux premières dimensions en posant $x_1 = r\\cos\\theta$ et $x_2 = r\\sin\\theta$.\nOn a alors $x_1^2+x_2^2=r^2$ et $dx_1dx_2 = r dr d\\theta$.\n$V_n(R)$ devient\u0026nbsp;:\n$\\displaystyle V_n(R) = \\int_0^{2\\pi}\\int_0^R V_{n-2}\\left(\\sqrt{R^2-r^2}\\right)rdrd\\theta$\nOr par symétrie et analyse dimensionnelle $V_n(a\\times R) = a^nV_n(R)$, et ici, on a $V_{n-2}\\left(\\sqrt{R^2-r^2}\\right)=V_{n-2}\\left(\\left(\\sqrt{1-\\frac{r^2}{R^2}}\\right)R\\right)=\\left(1-\\frac{r^2}{R^2}\\right)^{\\frac{n-2}{2}}V_{n-2}(R)$\nFinalement $V_n$ se réduit à\u0026nbsp;:\n$ \\begin{aligned} V_n(R) \u0026= V_{n-2}(R)\\int_0^{2\\pi}\\int_0^R \\left(1-\\frac{r^2}{R^2}\\right)^{\\frac{n-2}{2}}rdrd\\theta\\\\\\\\ \u0026 = 2\\pi V_{n-2}(R) \\int_0^R \\left(1-\\frac{r^2}{R^2}\\right)^{\\frac{n-2}{2}}rdr\\\\\\\\ \u0026 = 2\\pi V_{n-2}(R) \\left[-\\frac{R^2}{2}\\frac{2}{n}\\left(1-\\frac{r^2}{R^2}\\right)^{\\frac{n}{2}}\\right]_{r=0}^{r=R}\\\\\\\\ \u0026 = \\frac{2\\pi R^2}{n} V_{n-2}(R) \\end{aligned} $ On va ne s\u0026rsquo;intéresser ici qu\u0026rsquo;aux $n$-boules de rayon un. Le petit programme suivant nous permet d\u0026rsquo;obtenir leurs volumes :\nLe volume culmine en dimension 5 puis ne fait que diminuer. Qu\u0026rsquo;a de particulier la dimension 5 ? Pas grand chose\u0026hellip;\nRegardons ce que nous dit la formule de récurrence :\nsi $n$ est pair :\n$\\displaystyle V_n(R) = \\frac{2\\pi R}{n}\\times\\frac{2\\pi R}{n-2}\\times\\cdots\\times \\frac{2\\pi R}{4}\\times\\frac{2\\pi R}{2}\\times V_0(R)$, avec $V_0(R) = 1$\nDonc $\\displaystyle V_n(R) = \\frac{2^{n/2}\\pi^{n/2}R^n}{n\\cdot (n-2)\\cdots 4 \\cdot 2} = \\frac{\\pi^{n/2}R^n}{\\left(\\frac{n}{2}\\right)!}$ si $n$ est impair :\n$\\displaystyle V_n(R) = \\frac{2\\pi R}{n}\\times\\frac{2\\pi R}{n-2}\\times\\cdots\\times \\frac{2\\pi R}{3}\\times\\frac{2\\pi R}{1}\\times V_1(R)$, avec$V_1(R)=2R$\nDonc $\\displaystyle V_n(R) = \\frac{2^{\\frac{n+1}{2}}\\pi^{\\frac{n-1}{2}}R^n}{n\\cdot (n-2)\\cdots 3\\cdot 1} $\nL\u0026rsquo;ensemble des deux formules peuvent se combiner en une seule utilisant la fonction gamma $\\Gamma$ :\n$\\displaystyle V_n(R) = \\frac{\\pi^{\\frac{n}{2}}R^n}{\\Gamma\\left(\\frac{n}{2}+1\\right)} $ En traçant l\u0026rsquo;évolution du volume pour différents rayons, on constate que le maximum change de dimension.\nLa présence d\u0026rsquo;un maximum s\u0026rsquo;explique par la course entre la tortue numérateur, à base de puissances, et le lièvre dénominateur fait de factorielles. Si les puissances peuvent démarrer plus vite, la factorielle finira toujours fatalement par dominer (dans les deux cas, on a un produit de $n$ facteurs mais pour la factorielle, la taille des facteurs augmente en fonction de $n$). Et c\u0026rsquo;est bien ça le plus intéressant : cet évanouissement du volume de l\u0026rsquo;hyperboule pour des grandes dimensions.\nEssayons de mieux comprendre son origine.\nBoule dans boite Plaçons maintenant la $n$-boule de rayon 1 dans un $n$-cube de côté 2 et calculons la proportion de volume occupé par la boule (il suffit de diviser par $2^n$ le volume de la boule).\nLe rapport des deux volumes s\u0026rsquo;écrabouille très très vite. Il faut une échelle logarithmique pour y voir plus clair.\nÀ 11 dimensions, la boule ne représente plus qu\u0026rsquo;un millième du volume de la boite et c\u0026rsquo;est environ un millionième en dimension 17, soit pas grand chose\u0026hellip; Alors qu\u0026rsquo;il s\u0026rsquo;agit bien à chaque fois de la plus grande sphère possible calée dans la boite ! Si on augmentait son rayon d\u0026rsquo;un poil, elle déborderait.\nRaison de cette apparente étrangeté : la différence entre le volume du cube et celui de la sphère est à chercher dans les coins !\nOr plus la dimension augmente, plus le cube a de coins\u0026hellip; En effet, un $n$-cube possède $2^n$ coins. Ajoutez à cela que les diagonales deviennent de plus en plus grandes ($\\sqrt{n}$), et on commense à se convaincre que le volume de la sphère va vite devenir négligeable\u0026hellip;\nConcentration sur les bords En tirant des points au hasard dans un hypercube, on va vérifier que plus la dimension augmente, plus le volume se concentre sur les bords.\nDans le petit programme suivant, on tire un million de points au hasard dans un $n$-cube de côté 1 et on calcule la proportion qui se trouve à une distance inférieure à 0,1 d\u0026rsquo;un bord. C\u0026rsquo;est facile, il suffit de checker si c\u0026rsquo;est le cas pour chaque tirage correspondant à la coordonnée sur chaque axe. Dès qu\u0026rsquo;une coordonnée est inférieure à 0,1 ou supérieure à 0,9 (on suppose ici que les coordonnées des sommets du cube sont des séquences de 0 et de 1), on compte le point.\nLa raison de cette concentration devient évidente à la lecture du programme ; pour qu\u0026rsquo;un point soit près d\u0026rsquo;un bords, il suffit qu\u0026rsquo;une de ses coordonnées soit près d\u0026rsquo;un bord. Or si on augmente le nombre de dimensions, on augmente par la même le nombre de coordonnées et donc le nombre de tirages. Plus la dimension augmente moins ça devient possible qu\u0026rsquo;aucune des coordonnées ne tombe près d\u0026rsquo;un bord.\nCela permet d\u0026rsquo;ailleurs d\u0026rsquo;expliquer que le volume de l\u0026rsquo;hypersphère de rayon 1 se ratatine : ses points doivent être presque tous loin des bords. En effet, la sphère touche les bords qu\u0026rsquo;au centre des faces or un hypercube de dimension $n$ compte seulement $2n$ faces (hypercubes de dimension $n-1$) pour $2^n$ sommets !\nThe \u0026ldquo;Average man\u0026rdquo; Ce résultat a une conséquence très importante en science des données : augmenter le nombre de variables descriptives revient à augmenter le nombre de dimensions et donc à renforcer dans le même mouvement la probabilité d\u0026rsquo;obtenir des valeurs hors normes. Si ces variables décrivent différents aspects d\u0026rsquo;un individu, on en arrive à la conclusion que l\u0026rsquo;individu moyen n\u0026rsquo;existe pas puisqu\u0026rsquo;il y aura presque toujours au moins une de ces variables qui sortira des standards.\nL\u0026rsquo;U.S. Air Force l\u0026rsquo;a découvert au prix fort à la fin des années 40. Beaucoup de pilotes se crashaient (jusqu\u0026rsquo;à 17 le même jour !), mais personne ne savait trop pourquoi\u0026hellip; Jusqu\u0026rsquo;à ce qu\u0026rsquo;un jeune scientifique qui venait d\u0026rsquo;être engagé par l\u0026rsquo;armée, Gilbert Daniels, se résolut à mesurer tous les pilotes.\nLes cockpits et les casques avaient tout spécialement été dessinés pour s\u0026rsquo;adapter parfaitement au pilote moyen\u0026hellip; sans suspecter alors que le pilote moyen était tout à fait extraordinaire ! Ses mensurations furent obtenues au moyen d\u0026rsquo;une large campagne statistique menée sur des milliers de jeunes hommes. Mais lorsque Daniels prit les mesures d\u0026rsquo;un peu plus de 4000 pilotes sur 10 critères différents (circonférence de la poitrine, longueur d\u0026rsquo;entre-jambes, etc.) et qu\u0026rsquo;il inventoria ceux dont l\u0026rsquo;ensemble des caractéristiques s\u0026rsquo;étalait autour de la moyenne (à 30% près), il n\u0026rsquo;en trouva\u0026hellip; aucun ! Le pilote moyen n\u0026rsquo;existe pas. Résultat, chaque pilote avait au moins un truc qui cloche par rapport au design du cockpit, ce qui rendait plus hasardeuse, semble-t-il, la maîtrise de l\u0026rsquo;avion.\nDaniels reporta ses résultats dans cette étude intitulée The \u0026ldquo;Average man\u0026rdquo;?. Cela fit changer son fusil d\u0026rsquo;épaule à l\u0026rsquo;armée américaine qui se mit à adapter l\u0026rsquo;avion au pilote plutôt que l\u0026rsquo;inverse.\nDistance entre les points Une autre bizarrerie apparaît lorsqu\u0026rsquo;on regarde la distance séparant deux points pris au hasard dans un hypercube.\nLes histogrammes suivant représentent la répartition des distances obtenues entre chaque paire pour 1000 points tirés au hasard uniformément dans l\u0026rsquo;hypercube. Les pointillés indiquent les distances minimales (0) et maximales ($\\sqrt{n}$) possibles pour ces distances.\nEncore de la magie noire : en grande dimension, la distance entre deux points est à peu près toujours la même !!\nCe n\u0026rsquo;est finalement pas si dur à intuiter. Tirer un point au hasard dans l\u0026rsquo;hypercube de dimension $n$ revient à choisir au hasard $n$ coordonnées pour construire un vecteur $\\vec{v_1}=x_1 \\vec{e_1}+x_2\\vec{e_2}+\\cdots + x_n\\vec{e_n}$. Et on fait pareil avec un 2e point : $\\vec{v_2}=x_1^\\prime \\vec{e_1}+x^\\prime_2\\vec{e_2}+\\cdots + x^\\prime_n\\vec{e_n}$.\nPour $n$ grand, la probabilité que ces deux vecteurs soient orthogonaux (ce qui signifie que leurs directions sont indépendantes) devient énorme. En effet, il suffit qu\u0026rsquo;une des coordonnées soit quasiment nulle pour qu\u0026rsquo;il y ait orthogonalité et en multipliant les tirages, ça va finir par arriver.\nEn supposant pour simplifier que chacun des points se ballade à peu près au milieu d\u0026rsquo;une demi-diagonale de l\u0026rsquo;hypercube (puisqu\u0026rsquo;il est bourré de diagonales en grande dimension), on se retrouve avec deux vecteurs orthogonaux éloignés de $\\frac{\\sqrt{n}}{4}$ du centre. Leur distance sera donc approximativement $\\sqrt{\\left(\\frac{\\sqrt{n}}{4}\\right)^2+\\left(\\frac{\\sqrt{n}}{4}\\right)^2}=\\frac{1}{2\\sqrt{2}}\\sqrt{n}\\approx 0,35\\sqrt{n}$. La distance entre deux points augmenterait ainsi comme la racine carrée du nombre de dimensions.\nEn ajustant la courbe obtenue en traçant la distance moyenne par une fonction $a\\sqrt{n}$, l\u0026rsquo;accord est plutôt très bon ! On obtient $a=0,41$. Pour le coup, ce n\u0026rsquo;est pas tout à fait nos $0,35$ mais pour un modèle ultra simplifié, on n\u0026rsquo;est pas si loin\u0026hellip;\nOn constate par contre que la variabilité, incarnée par l\u0026rsquo;écart-type, n\u0026rsquo;est pas modifiée ! C\u0026rsquo;est ce qui explique que la variabilité relative devient, elle, de plus en plus faible quand le nombre de dimensions augmente.\nLa raison de cette conservation de l\u0026rsquo;écart-type est à chercher du côté de son évolution inversement proportionnelle à la racine carrée du nombre d\u0026rsquo;échantillons.\nOr ici, le nombre d\u0026rsquo;échantillons correspond à la dimension. Par conséquent l\u0026rsquo;augmentation de la variabilité provoquée par l\u0026rsquo;augmentation des distances ($\\propto\\sqrt{n}$) est compensée par la diminution de cette même variabilité du fait de l\u0026rsquo;augmentation du nombre de tirages ($\\propto 1/\\sqrt{n}$) .\nOn comprend mieux pourquoi un algorithme comme KNN qui vise à prédire une valeur ou catégorie en regardant celle des plus proches voisins galère à grande dimension ; tous les voisins sont à peu près à la même distance\u0026hellip;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/",
	"title": "Gifted Amateur",
	"tags": [],
	"description": "",
	"content": " Notes de lecture Notes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Les auteurs nous tiennent assez fermement par la main pour nous aider à traverser sans se perdre une forêt peu accueillante où s'enchevêtrent mathématiques et formalismes abscons. Ils tentent de motiver au maximum les raisons de chaque nouveau pas dans cette épreuve en nous faisant miroiter la clairière au panorama immense sur l'ensemble de la physique qui se trouve de l'autre côté.\nPartie\u0026nbsp;0\u0026nbsp;: Lagrangien et principe de moindre action Partie\u0026nbsp;1\u0026nbsp;: Les oscillateurs harmoniques Représentation en nombre d'occupation Seconde quantification Partie\u0026nbsp;2\u0026nbsp;: Théorie classique des champs Klein-Gordon Quelques Lagrangiens de champs classiques Partie\u0026nbsp;3\u0026nbsp;: Passage du temps Transformations continues Théorème de Noether Partie\u0026nbsp;4\u0026nbsp;: Quantification d'un champ scalaire réel Quantification d'un champ scalaire complexe Quantification d'un champ à plusieurs composantes et symétries internes Partie\u0026nbsp;5\u0026nbsp;: Champs de jauge et théorie de jauge Symétries discrètes Partie\u0026nbsp;6\u0026nbsp;: Propagateurs et fonctions de Green Propagateurs et champs Notations Les points de $\\mathbb{R}^3$ sont conventionnellement notés en gras pour les distinguer des points de l'espace-temps $\\mathbb{R}^4$. Ainsi, le point $x=(x_0,x_1,x_2,x_3)$ désigne le point de l'espace $\\boldsymbol{x}=(x_1,x_2,x_3)$ à l'instant $t=x^0/c$ et donc $x=(x^0,\\boldsymbol{x})$. Toujours par convention, les indices désignés par des lettres grecques courent de 0 à 3 alors que les lettres latines vont de 1 à 3. On suivra aussi le plus souvent la convention de sommation d'Einstein qui consiste à sommer sur toutes les valeurs possibles les indices répétés en positions hautes et basses. On introduit la matrice diagonale 4×4 $\\eta_{\\mu\\nu}$ avec $\\eta_{00}=1$ et $\\eta_{ii}=-1$ (ce qui signifie donc que $\\eta_{11}=\\eta_{22}=\\eta_{33}=-1)$.\nLa forme bilinéaire de Lorentz (généralisation du produit scalaire dans l'espace-temps) s'écrit alors\u0026nbsp;: $(x,y)=\\eta_{\\mu\\nu}x^\\mu y^\\nu=\\sum_{0≤\\mu,\\nu≤3}\\eta_{\\mu\\nu}x^\\mu y^\\nu = x^0y^0-x^1 y^1 - x^2 y^2 -x^3 y^3$. On note $\\mathbb{R}^{1,3}$ l'espace $\\mathbb{R}^4$ muni de la forme de Lorentz pour rappeler la signature choisie $(+,-,-,-)$ et on le nomme espace de Minkowski. "
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique-c/",
	"title": "Logique",
	"tags": [],
	"description": "",
	"content": " Logique Algèbre de Boole L\u0026rsquo;algèbre de Boole permet d\u0026rsquo;algébriser la logique.\nLogique des propositions L\u0026rsquo;implication $A\\rightarrow B$ signifie que si $A$ est vraie alors $B$ est vraie.\n$A$ $B$ $A\\rightarrow B$ 0 0 1 0 1 1 1 0 0 1 1 1 L\u0026rsquo;implication réciproque s\u0026rsquo;écrit $B\\rightarrow A$. On peut la lire : $B$ est vraie seulement si $A$ est vraie.\n$A$ $B$ $B\\rightarrow A$ 0 0 1 0 1 0 1 0 1 1 1 1 S\u0026rsquo;il y a conjonction entre l\u0026rsquo;implication ($B$ si $A$) et sa réciproque ($B$ seulement si $A$), ce qui s\u0026rsquo;écrit $(A\\rightarrow B) \\land (B\\rightarrow A)$, alors les propositions $A$ et $B$ sont dites matériellement équivalentes $A \\leftrightarrow B$.\n$A$ $B$ $A\\leftrightarrow B$ 0 0 1 0 1 0 1 0 0 1 1 1 On peut exprimer l\u0026rsquo;équivalence entre deux propositions de plusieurs manières :\n$A$ si et seulement si (ssi) $B$ ; pour que $A$, il faut et il suffit que $B$ ; une condition nécessaire et suffisante pour $A$ est $B$. L\u0026rsquo;implication $A\\rightarrow B$ est logiquement équivalente à sa contraposée $\\neg B\\rightarrow \\neg A$, ce qui signifie que leurs tables de vérité sont les mêmes.\n$A$ $B$ $\\neg B\\rightarrow \\neg A$ 0 0 1 0 1 1 1 0 0 1 1 1 On note $A\\rightarrow B \\equiv \\neg B\\rightarrow \\neg A$.\nL\u0026rsquo;équivalence logique entre deux propositions revient à dire que l\u0026rsquo;équivalence matérielle entre les deux propositions est une tautologie (toujours vraie). L\u0026rsquo;équivalence matérielle est un opérateur, c\u0026rsquo;est un élément du langage de la logique des propositions. L\u0026rsquo;équivalence logique ne fait pas partie de ce langage mais d\u0026rsquo;un méta-langage.\nIl ne faut pas confondre la contraposée avec la négation de l\u0026rsquo;antécédent $\\neg A\\rightarrow \\neg B$ qui n\u0026rsquo;est pas logiquement équivalente à l\u0026rsquo;implication.\n$A$ $B$ $\\neg A\\rightarrow \\neg B$ 0 0 1 0 1 0 1 0 1 1 1 1 La négation de l\u0026rsquo;antécédant est équivalent à la réciproque de la contraposée. L\u0026rsquo;utiliser en pensant qu\u0026rsquo;il est équivalent à l\u0026rsquo;implication est un raisonnement fallacieux ou sophisme.\nUne proposition $F$ est une conséquence logique d\u0026rsquo;un ensemble de formules $\\Gamma$ si lorsque les $\\Gamma$ sont vraies, alors $F$ est vraie. On note $\\Gamma\\models F$.\nCela revient à dire que $\\Gamma\\rightarrow F$ est une tautologie.\nComme $\\equiv$, $\\models$ est un élément du méta-langage.\nExemples :\n$A\\models A\\lor B$\n$A\\land B \\models B$\nModus ponens :\n$A \\land (A\\rightarrow B) \\models B$\nS\u0026rsquo;il a plu ($A$) alors le sol est mouillé ($B$)\nIl a plu ($A$)\nAlors le sol est mouillé ($B$)\n$A$ $B$ $A\\rightarrow B$ $A \\land (A\\rightarrow B) $ $A \\land (A\\rightarrow B) \\rightarrow B$ 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 Modus tollens :\n$\\neg B \\land (A\\rightarrow B) \\models \\neg A$ S\u0026rsquo;il a plu ($A$) alors le sol est mouillé ($B$)\nLe sol n\u0026rsquo;est pas mouillé ($\\neg B$)\nAlors il n\u0026rsquo;a pas plu ($\\neg A$)\n$A$ $B$ $A\\rightarrow B$ $\\neg B \\land (A\\rightarrow B) $ $\\neg B \\land (A\\rightarrow B) \\rightarrow \\neg A$ 0 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 Syllogisme hypothétique :\n$(A\\rightarrow B) \\land (B\\rightarrow C) \\models A\\rightarrow C$\nSi je bois ($A$), je ne peux pas conduire ($B$).\nSi je ne peux pas conduire ($B$), je dois appeler un taxi ($C$).\nPar conséquent, si je bois ($A$), alors je dois appeler un taxi ($C$).\nCe syllogisme incarne le principe de transitivité de l\u0026rsquo;implication.\n$A$ $B$ $C$ $A\\rightarrow B$ $B\\rightarrow C$ $A\\rightarrow C$ $(A\\rightarrow B) \\land (B\\rightarrow C)$ $(A\\rightarrow B) \\land (B\\rightarrow C) \\rightarrow (A\\rightarrow C)$ 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 Syllogisme disjonctif :\n$(A\\lor B) \\land (\\neg A) \\models B$\nC\u0026rsquo;est soit bleu ($A$), soit rouge ($B$)\nCe n\u0026rsquo;est pas bleu ($\\neg A$)\nAlors c\u0026rsquo;est rouge ($B$)\n$A$ $B$ $A\\lor B$ $(A\\lor B)\\land (\\neg A) $ $(A\\lor B)\\land (\\neg A) \\rightarrow B$ 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 Deux raisonnements fallacieux sont cousins du modus ponens et du modus tollens :\nla négation de l\u0026rsquo;antécédent :\n$\\neg A \\land (A\\rightarrow B) \\not\\models \\neg B$ S\u0026rsquo;il a plu ($A$) alors le sol est mouillé ($B$)\nIl n\u0026rsquo;a pas plu ($\\neg A$)\nAlors le sol n\u0026rsquo;est pas mouillé ($\\neg B$)\nC\u0026rsquo;est faux, le sol peut être mouillé malgré l\u0026rsquo;absence de pluie.\n$A$ $B$ $A\\rightarrow B$ $\\neg A \\land (A\\rightarrow B) $ $\\neg A \\land (A\\rightarrow B) \\rightarrow \\neg B$ 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 La table de vérité montre que $\\neg A \\land (A\\rightarrow B) \\rightarrow \\neg B$ n\u0026rsquo;est pas une tautologie, ce qui prouve qu\u0026rsquo;il n\u0026rsquo;y a pas conséquence logique.\nl\u0026rsquo;affirmation du conséquent :\n$B \\land (A\\rightarrow B) \\not\\models A$\nS\u0026rsquo;il a plu ($A$) alors le sol est mouillé ($B$)\nLe sol est mouillé ($B$)\nAlors il a plu ($A$)\nC\u0026rsquo;est faux. Le sol peut être mouillé pour d\u0026rsquo;autres raisons.C\u0026rsquo;est une confusion entre la possibilité et la nécessité.\n$A$ $B$ $A\\rightarrow B$ $B \\land (A\\rightarrow B) $ $B \\land (A\\rightarrow B) \\rightarrow A$ 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 Logique des prédicats Tous les hommes sont mortels ($A$)\nSocrate est un homme ($B$)\nDonc Socrate est mortel ($C$)\nCe syllogisme s\u0026rsquo;écrit en logique des propositions $A\\land B \\models C$, ce qui n\u0026rsquo;est pas valide\u0026hellip;\nLa logique des propositions vit dans un monde constitué seulement de faits. On va dépasser ça grâce à la logique des prédicats qui peuple le monde d\u0026rsquo;objets (chiens, théories, chiffres, Socrate, etc.), de relations (rouge, premier, plus grand que, être mortel, etc.) et de fonctions (ajouter 1, longueur, nom du frère, etc.).\nL\u0026rsquo;upgrade de la logique des propositions en logique des prédicats ou logique du 1er ordre passe ainsi par l\u0026rsquo;introduction des notions de variables, de constantes, de fonctions, de prédicats, et de quantificateurs.\nLes variables ($x$, $y$, $z$, etc.) représentent les objets dont on parle. Les constantes ($a$, $b$, $c$, etc.) sont des valeurs particulières des objets (ici Socrate). Variables et constantes sont des termes du langage. les fonctions ($f$, $g$, $h$, etc.) permettent de fabriquer de nouveaux termes à partir d\u0026rsquo;anciens. Le nombre de terme auquels la fonction s\u0026rsquo;applique s\u0026rsquo;appelle sont arité. Une constante est une fonction d\u0026rsquo;arité 0. les prédicats ($P$, $Q$, $R$, etc.) rendent compte des relation entre les termes (ici \u0026ldquo;être un homme\u0026rdquo; et \u0026ldquo;être mortel\u0026rdquo;). L\u0026rsquo;arité d\u0026rsquo;un prédicat est à nouveau le nombre de termes qu\u0026rsquo;il prend en argument. les quantificateurs sont d\u0026rsquo;une part le quantificateur universel $\\foralll$ qui sinfie (\u0026ldquo;pour tout\u0026rdquo;) et le quantificateur existentiel $\\exists$ qui signifie (\u0026ldquo;il existe\u0026rdquo;). Les constantes et les variables sont issues d’un même ensemble appelé le domaine.\nUne formule atomique est de la forme $P(t_1,\\ldots,t_n)$ où $P$ est une prédicat d\u0026rsquo;arité $n$ et $t_1,\\ldots,t_n$ sont des termes.\nOn construit des formules complexes en combinant des formules atomiques grâce à des connecteurs.\nRevenons à l\u0026rsquo;exemple de départ.\nAppelons $x$ la variable homme, $s$ la constante Socrate, $P$ le prédicat \u0026ldquo;être un homme\u0026rdquo; et $Q$ le prédicat \u0026ldquo;être mortel\u0026rdquo;.\nL\u0026rsquo;énoncé devient :\n$(\\forall x P(x)\\rightarrow Q(x))\\land P(s)\\models Q(s)$\nCarré d\u0026rsquo;Aristote En combinant deux prédicats $P$ et $Q$, leur négation et les deux quantificateurs, on peut construire 4 propositions donts les relations sont représentées dans le carré d\u0026rsquo;Aristote.\nDeux propositions sont contradictoires si elles ne peuvent être ni vraies ni fausses en même temps.\nSont donc contradictoire deux propositions dont l\u0026rsquo;une est la négation de l\u0026rsquo;autre.\nPour mieux le voir, on va réécrire les propositions universelles sous des formes logiquement équivalentes utilisant le quantificateur existentiel.\nIl est ainsi équivalent de dire $\\forall x(P(x)\\rightarrow Q(x))$ (tout $P$ est $Q$) et $\\neg\\exists x (P(x)\\land\\neg Q(x))$ (il n\u0026rsquo;existe pas de $P$ qui soit non $Q$). On retrouve bien ainsi que A est la négation de O.\nDe même, $\\forall x(P(x)\\rightarrow \\neg Q(x)$ (\u0026ldquo;aucun $P$ n\u0026rsquo;est $Q$\u0026rdquo; ou plus clairement \u0026ldquo;tous les $P$ sont non $Q$\u0026rdquo;) est équivalent à $\\neg\\exists x (P(x)\\land Q(x))$ (\u0026ldquo;il n\u0026rsquo;existe pas de $P$ qui soit $Q$\u0026rdquo;). Donc E nie bien I.\nDeux propositions sont contraires ne peuvent pas être vraies en même temps (comme les contradictoires) mais elles peuvent être fausses en même temps par contre. Pour une proposition donnée, on peut trouver plusieurs propositions contraires. Le fait qu\u0026rsquo;une propositions soit fausse n\u0026rsquo;entraîne pas qu\u0026rsquo;une de ses propositions contraires soit vraie.\nDes propositions subcontraires peuvent être vraies en même temps mais pas fausses en même temps.\nDes propositions subalternes s\u0026rsquo;opposent par la quantité. Si la proposition universelle est vraie, alors la proposition particulière est vraie aussi.\nAparté sur les syllogismes Aristote s\u0026rsquo;est amusé à définir et répertorier tous les syllogismes possibles.\nUn syllogisme est un raisonnement mettant en œuvre trois propositions : les deux premières sont les prémisses et elles conduisent à une conclusion.\nLa 1re prémisse est la majeure. C\u0026rsquo;est une des propositions universelles A ou I. La 2e prémisse est la mineure. Elle peut être d\u0026rsquo;une des 4 formes A, E, I ou O, mais elle doit avoir un terme commun avec la majeure. La conclusion peut être d\u0026rsquo;une des 4 formes, mais elle doit avoir un terme commun avec la majeure. De toutes les possibilités, Aristote a isolé 24 syllogismes valides dont 4 ont un statut particulier puisqu\u0026rsquo;ils peuvent générer les autres. Au Moyen-Âge, les scolastiques ont fabriqué des moyens mnémotechniques à base des lettres correspondant aux formes des propositions. Les 4 stars sont alors :\nBarbara (pour AAA) dont l\u0026rsquo;exemple type est celui du début avec Socrate (où il faut remplacer \u0026ldquo;Socrate est un homme\u0026rdquo; et \u0026ldquo;Socrate est mortel\u0026rdquo; par \u0026ldquo;tous les Socrates sont des hommes\u0026rdquo; et \u0026ldquo;tous les Socrates sont mortels\u0026rdquo;.\nTout P est Q\nTout R est P\nDonc tout R est Q\nCelarent (pour EAE) :\nAucun P n\u0026rsquo;est Q\nTout R est P\nDonc aucun R n\u0026rsquo;est Q\nDarii (pour AII) :\nTout P est Q\nQuelque R est P\ndonc quelque R est Q\nFerio (pour EIO) :\nAucun P n\u0026rsquo;est Q\nQuelque R est P\ndonc quelque R n\u0026rsquo;est pas Q\nLes syllogismes, c\u0026rsquo;est mignons, mais ils n\u0026rsquo;ont à peu près jamais été utilisés par les mathématiciens pour leurs démonstrations\u0026hellip;\nVariables libres et liées On dit qu’une occurrence de la variable $x$ dans une formule $F$ est liée si un quantificateur porte sur elle.\nSi aucun quantificateur ne porte sur l\u0026rsquo;occurence de la variable, on parle d’occurrence libre. Une variable est libre si toutes ses occurrences sont libres.\nUne variable est liée si elle a au moins une occurrence liée. On parle aussi de variable muette dans ce dernier cas.\nExemple :\nDans $\\forall x (P(x)\\rightarrow Q(x,y))$, $x$ est lié et $y$ est libre.\nUne formule sans variable libre est dit close (ou fermée).\nUne théorie est un ensemble de formules closes.\nAspect sémantique Interprétations et modèles Les interprétations visent à donner une valeur de vérité aux formules. Pour cela, l\u0026rsquo;interprétation $I$ d\u0026rsquo;une formule $F$ sur un domaine $D$ pioche dans $D$ pour affecter des valeurs aux constantes et associe à chaque symbole de prédicat une relation sur le domaine (de la bonne arité) et à chaque symbole de fonction une fonction sur le domaine (toujours de la bonne arité).\nUne formule atomique $P(t_1,\\ldots,t_n)$ est vraie dans une interprétation $I$ sur le domaine $D$ si et seulement si les éléments du domaine qui sont l\u0026rsquo;interprétation des termes $t_1,\\ldots,t_n$ sont dans la relation correspondant à l\u0026rsquo;interprétation du prédicat $P$.\nLa valeur de vérité d\u0026rsquo;une formule complexe est calculée à partir des valeurs de vérités des atomes dont elle est composée en suivant les règles de la logique des propositions.\nExemples :\nInterprétons $F = \\forall x P(x)\\rightarrow Q(x)$ sur le domaine $\\{a,b,c\\}$.\n$x$ $P_I(x)$ $Q_I(x)$ $a$ 1 1 $b$ 0 1 $c$ 0 0 Dans cette interprétation, $F$ est vraie car $P_I(x)\\rightarrow Q_I(x)$ est bien vraie pour toutes les valeurs de $x$.\nMais il suffit d\u0026rsquo;interpréter $P(x)$ un peu différemment pour que $F$ devienne fausse :\n$x$ $P_{I\u0026rsquo;}(x)$ $Q_{I\u0026rsquo;}(x)$ $a$ 1 1 $b$ 0 1 $c$ 1 0 De manière moins arbitraire, on cherche en général une interprétation qui a du sens.\nLa formule $F = \\forall x\\forall y (P(x,a)\\land P(y,x)\\rightarrow \\neg P(y,a)$ peut vouloir dire \u0026ldquo;les amis des amis de $a$ ne sont pas amis de $a$\u0026rdquo; en interprétant $P$ comme une relation d\u0026rsquo;amité.\nConsidérons l\u0026rsquo;interprétation suivante : $D=\\{\\text{Anna}, \\text{Bob}, \\text{Joe}, \\text{Clovis}, \\text{Aline}\\}$, $a_I=\\text{Anna}$, $P_I=\\{ (\\text{Anna},\\text{Joe}), (\\text{Anna},\\text{Bob}), (\\text{Joe},\\text{Clovis}), (\\text{Bob},\\text{Aline}) \\}$.\nAnna est amie avec Joe et Bob, Joe est ami avec Clovis, Bob est ami avec Aline, et ni Clovis, ni Aline ne sont amis avec Anna. Donc la formule est vraie dans cette interprétation.\nOn aurait aussi pu interpréter $P$ comme une relation de supériorité.\nImaginons donc maintenant $I\u0026rsquo;$ donnée par : $D\u0026rsquo;=\\{1,5,12,52\\}$, $a_{I\u0026rsquo;}=5$ et $P_{I\u0026rsquo;}(x,y)=1$ si $x\u0026gt;y$.\nOn voit maintenant que $F$ est forcément fausse dans cette interprétation puisque la relation $\u0026gt;$ est transitive.\nUne interprétation d\u0026rsquo;une formule sur un domaine est un modèle de cette formule si la formule est vraie pour cette interprétation.\nC\u0026rsquo;est le cas de $I$ pour nos exemples, mais pas de $I\u0026rsquo;$.\nUne formule est valide (tautologie) si elle est vraie quelle que soit l\u0026rsquo;interprétation (si toute interprétation est un modèle).\nAucune des deux formules en exemple n\u0026rsquo;est valide.\nExemple de formule valide : $\\forall x P(x) \\rightarrow \\exists x P(x)$ (si une propriété est vraie pour tout $x$, alors elle est vraie pour au moins un $x$).\nUne formule est satisfaisable (ou consistante) s\u0026rsquo;il elle possède un modèle.\nRq : une formule peut donc être invalide et consistante.\nÉquivalence et conséquence Deux formules $F$ et $F\u0026rsquo;$ sont sémantiquement équivalentes si pour toute interprétation $I$, $F_I=F_I\u0026rsquo;$ (mêmes tables de vérité).\nOn note $F\\equiv F\u0026rsquo;$.\nExemple : $\\neg\\neg P(x) \\equiv P(x)$\nUn ensemble de formules $\\{F_1,\\ldots,F_n\\}$ satisfait une formule $F$ si tout modèle de $\\{F_1,\\ldots,F_n\\}$ est aussi modèle de $F$.\nOn dit que $F$ est conséquence sémantique de la théorie$T = \\{F_1,\\ldots,F_n\\}$ et on note $\\{F_1,\\ldots,F_n\\} \\models F$.\nExemple : $P(a)\\models \\exists x P(x)$\nSi $F$ est une formule valide, on note $\\models F$ (il y a une quantification universelle implicite sur toutes les représentations).\n$\\{F_1,\\ldots,F_n\\} \\models F$ équivaut à :\n$\\models (F_1\\land\\ldots\\land F_n)\\rightarrow F$ $\\models \\neg(F_1\\land\\ldots\\land F_n)\\lor F$ si les formules sont closes, $ F_1\\land\\ldots\\land F_n \\land \\neg F$ est insatisfaisable (preuve par l\u0026rsquo;absurde). Aspect syntaxique S\u0026rsquo;assurer de la validité d\u0026rsquo;une formule suppose de s\u0026rsquo;assurer qu\u0026rsquo;elle est vraie pour un nombre d\u0026rsquo;interprétations potentiellement infini. Et même lorsqu\u0026rsquo;il est fini, l\u0026rsquo;attribution\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/quantique/",
	"title": "Mécanique quantique",
	"tags": [],
	"description": "",
	"content": " Mécanique quantique Notions fondamentales Un jeu d\u0026rsquo;axiomes Information quantique Sphère de Bloch Pairs de Qubits Non-clonage quantique Téléportation quantique Paradoxe EPR l’état GHZ à 3 photons peut s\u0026rsquo;écrire plus simplement que la forme présentée dans la vidéo. De base, il est donné par : $$|\\mathrm{GHZ}\\rangle=\\frac 1{\\sqrt 2} \\left(|\\uparrow_v\\rangle|\\uparrow_v\\rangle|\\uparrow_v\\rangle + |\\downarrow_v\\rangle|\\downarrow_v\\rangle|\\downarrow_v\\rangle\\right)$$ Et c\u0026rsquo;est une fois qu\u0026rsquo;on écrit chacun des états de spin dans la base h (avec $|\\uparrow_v\\rangle = \\tfrac 1{\\sqrt 2} \\left(|\\uparrow_h\\rangle+|\\downarrow_h\\rangle\\right)$ et $|\\downarrow_v⟩ = \\tfrac 1{\\sqrt2} \\left(|\\uparrow_h\\rangle-|\\downarrow_h\\rangle\\right)$) qu\u0026rsquo;on obtient la forme donnée permettant de dérouler la démonstration.\nThéorie-jouet de Spekkens Interprétations Le monde de l\u0026rsquo;interprétation de la mécanique quantique est un grand cirque disparate. Pénétrer sous le chapiteau, c\u0026rsquo;est tenter de trouver une solution au problème de la mesure, donner un sens aux probabilités, etc.\nParmi les différentes familles d\u0026rsquo;interprétation, on trouve :\nla théorie des mondes multiples (Everett, Deutsch), la théorie de l'onde pilote (Bohm, Dürr), la théorie GRW de l'effondrement spontané (Ghirardi, Rimini, Weber), l'interprétation relationnelle (Rovelli), théories stochastiques réalistes (Barandes), etc. Une interprétation fonctionnelle décrit l’apparition de résultats uniques et la statistique qu\u0026rsquo;ils suivent . On peut la compléter par un choix ontologique (particules, densités, onde, etc.) qui précise la nature de ces résultats. Que considère-t-on comme fondamental ? Qu\u0026rsquo;est-ce qui existe ? Certaines interprétations vont guider une ontologie donnée mais pas nécessairement. Et tout choix ontologique semble passer par un marchandage conceptuel, des compromis à digérer. Décréter la réalité de tel ou tel objet impose en effet d\u0026rsquo;accepter certaines implications.\nQuelques exemples :\nLes réalistes de la fonction d'onde (ψ-ontistes) choisissent la fonction d'onde de Schrödinger comme description fondamentale de la réalité. En contrepartie, ils doivent accepter un espace à 3N dimensions, reléguant notre espace 3D au rang d'illusion émergente\u0026nbsp;; Les réalistes de l'espace-temps attribuent un état quantique par région d’espace-temps qui devient l'élément central. On perd alors la séparabilité (les états de régions qui se recouvrent ne se factorisent pas)\u0026nbsp;; Les partisans du holisme relationnel font de l'intrication quantique (corrélations non locales) un fait premier plutôt qu’un problème à expliquer. Ils abandonnent clairement la séparabilité\u0026nbsp;; L'ontologie primitive met, elle, en avant les objets matériels localisés résultants des expériences (les \"local beables\" de Bell) pour rattacher la théorie aux \"choses qui laissent des traces\". Ils perdent la localité. Epistémique vs. ontique Distinction selon Harrigan et Speckens Attraits de la vision épistémique Théorème PBR et nécessité de l\u0026rsquo;ontologie Une ontologie à 3N dimensions ER = EPR "
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/stat/",
	"title": "Statistiques",
	"tags": [],
	"description": "",
	"content": " Statistiques tip\nLes probabilités sont des calculs a priori, visant à prédire les issues d’évènements n’ayant pas encore eu lieu, alors que les statistiques sont des calculs a posteriori qui analysent les issues d’évènements déjà réalisés.\nLes différentes moyennes Monte Carlo Dans la duxième partie de la vidéo, on cherche à répondre à la grande question suivante : comment s\u0026rsquo;assurer d\u0026rsquo;avoir suffisamment d\u0026rsquo;échantillons pour obtenir des statistiques fiables ? Ou plus prosaïquement : à partir de combien est-ce suffisant ? On utilise d\u0026rsquo;abord les propriétés de la loi normale puis on construit un estimateur à partir de l\u0026rsquo;inégalité de Bienaymé-Tchebychev.\nParadoxe de Simpson Ce paradoxe a fait des dégâts lors de la campagne de vaccination pour le Covid. Les chiffres de la vidéo sont proches des chiffres communiqués par l\u0026rsquo;état d\u0026rsquo;Israël qui était un des premiers pays à pouvoir vacciner.\nL\u0026rsquo;article qui a servi en partie pour la vidéo.\nTest d\u0026rsquo;hypothèse Les statistiques au service de la prise de décision en science.\nAnalyse en composante principale L\u0026rsquo;analyse en composante principale permet de faire parler les données.\nUn autre exemple + tuto avec des données INSEE départementales p-hacking Extrait traduit du livre \u0026#39;Everything is predictable\u0026#39; de Tom Chivers\nPeut-être que l’exemple le plus célèbre est celui du scientifique de l\u0026rsquo;alimentation Brian Wansink, une star de l’Université Cornell qui a reçu des millions de dollars de financement du gouvernement fédéral américain sous l’administration Obama. Il a publié beaucoup d’études sur notre comportement alimentaire, notamment une sur la façon dont les hommes mangent plus en compagnie de femmes (probablement pour les impressionner) ; une autre sur comment donner des noms plus “attractifs” aux légumes (appeler les carottes “carottes à vision X-ray,” par exemple) permet que les enfants d’école primaire en mangent deux fois plus. Puis, en 2016, il a commis l’erreur de publier un billet de blog intitulé \u0026ldquo;The Grad Student Who Never Said \u0026lsquo;No.\u0026rsquo;\u0026rdquo;. L’étudiante en question était une doctorante turque. Lorsqu’elle est arrivée à Cornell, Wansink “lui a donné un ensemble de données d’une étude autofinancée, sans résultats probants” - une étude qui examinait le comportement alimentaire dans un buffet italien à volonté pendant un mois. Selon ses propres termes, il lui a dit : “Cela nous a coûté beaucoup de temps et d\u0026rsquo;argent à collecter. Il doit y avoir quelque chose ici que nous pouvons sauver parce que cet ensemble de données est cool (riche et unique).” L’étudiante s’est donc mise au travail et a découpé l’ensemble de données de nombreuses façons différentes. Et, inévitablement, elle a trouvé de nombreuses corrélations p \u0026lt; 0,05 - suffisamment pour qu\u0026rsquo;elle et Wansink publient cinq articles (y compris l’article “les hommes mangent trop pour impressionner les femmes”). Cela a éveillé la suspicion de certains scientifiques et journalistes scientifiques, et ils ont commencé à passer au crible les autres recherches de Wansink. De plus, Stephanie Lee, une journaliste scientifique de BuzzFeed, a mis la main sur ses emails, dans lesquels - il s’est avéré - il demanda à sa doctorante de découper les données en “mâles, femelles, mangeurs de déjeuner, mangeurs de dîner, personnes mangeant seules, personnes mangeant en groupe de 2, personnes mangeant en groupe de 2+, personnes commandant de l’alcool, personnes commandant des boissons non alcoolisées, personnes restant près du buffet, personnes s’asseyant loin, etc.” pour “essorer le jeu de données jusqu\u0026rsquo;à en extraire tous les résultats significatifs possibles” et faire en sorte que cela “devienne viral à grande échelle.” En conséquence, dix-huit des articles de Wansink ont été rétractés ; sept ont reçu des \u0026ldquo;expressions of concern” que les revues ajoutent aux études qu’elles ne pensent pas pouvoir être totalement fiables, mais qu’elles ne sont pas prêtes à rétracter complètement ; et quinze ont été corrigés. Wansink, entre-temps, a démissionné de Cornell en 2019, après que l’université a conclu qu’il avait commis une faute scientifique et lui a interdit l’enseignement et la recherche. C\u0026rsquo;est un exemple particulièrement frappant, mais d’une certaine manière Wansink a eu la malchance d’être détruit publiquement pour quelque chose qui était presque une pratique standard. Le p-hacking se produit tout le temps, de manière beaucoup moins dramatique - et beaucoup de scientifiques n’ont absolument aucune idée qu’ils font quelque chose de mal. Daryl Bem, dans un chapitre d\u0026rsquo;un livre de 1987 écrit comme un guide pour aider les étudiants à publier leurs recherches, a écrit qu’il “y a deux articles que vous pouvez écrire : l’article que vous aviez prévu d’écrire lorsque vous avez conçu votre étude ; l’article qui a le plus de sens maintenant que vous avez vu les résultats. La bonne réponse est le second.” Il a appelé les chercheurs à “analyser les sexes séparément, créer de nouveaux index composites… réorganiser les données pour les mettre en relief de manière plus audacieuse… Les données peuvent être suffisamment solides pour justifier de recentrer votre article autour des nouvelles découvertes et de subordonner ou même d’ignorer vos hypothèses originales.\u0026rdquo;\nComme on l\u0026rsquo;a vu sur les espaces de données à grande dimension , plus on a d\u0026rsquo;axes (de catégories, d\u0026rsquo;entrées,\u0026hellip;), plus on a de chance d\u0026rsquo;obtenir un résultat hors norme sur au moins l\u0026rsquo;un d\u0026rsquo;eux.\nCe chouette site met le phénomène à profit pour débusquer des corrélations amusantes (avec pour chacune des p-values inférieures à 1%\u0026hellip;).\nÉcart-type expérimental (estimateur de l\u0026rsquo;écart-type non biaisé) L\u0026rsquo;écart-type pour une variable quantitative dont les relevés $(x_1,\\ldots,x_N)$ sont pris sur la population complète (avec pour moyenne $\\mu$) est donné par la racine carrée de la variance :\n$$\\sigma = \\sqrt{V}=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N (x_i-\\mu)^2}$$\nC\u0026rsquo;est donc la racine-carrée des carrés des écarts à la moyenne divisé par $N$. On obtient bien ainsi une moyenne des écarts à la moyenne en empèchant qu\u0026rsquo;un écart en-dessous ne compense une écart au-dessus.\nMais si les relevés ne concernent qu\u0026rsquo;un échantillon $n\u0026lt;N$ de la population et non la population complète (ce qui est le cas pour une série de mesures expérimentales), on passe des probas aux stats et on a alors besoin d\u0026rsquo;un estimateur de l\u0026rsquo;écart-type ($\\sigma\\rightarrow S$).\nL\u0026rsquo;estimateur de l\u0026rsquo;écart-type non biaisé (aussi appelé écart-type expérimental) est alors donné par :\n$$S_{n-1} =\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar{x})^2}$$ note\nL\u0026rsquo;estimateur non biaisé de l\u0026rsquo;écart-type est parfois appelé écart-type expérimental et noté $\\sigma_{exp}$. C\u0026rsquo;est particulièrement le cas dans l\u0026rsquo;enseignement de la physique au lycée.\nPourquoi \u0026ldquo;non biaisé\u0026rdquo; ? Et pourquoi passer de \u0026ldquo;$n$\u0026rdquo; à \u0026ldquo;$n-1$\u0026quot; ? Notre problème est qu\u0026rsquo;on ne connaît plus la valeur vraie de la moyenne $\\mu$ mais seulement son estimation $m=\\frac{1}{n}\\sum_{i=1}^n x_i$.\nEt en substituant $m$ à $\\mu$ dans la variance, on biaise son espérance\u0026hellip;\nEn effet :\n$$ \\begin{aligned} E\\left(\\frac{1}{n}\\sum_{i=1}^n (x_i-m)^2\\right) \u0026= E \\left( \\frac{1}{n}\\sum_{i=1}^n x_i^2 - m^2 \\right) \u0026\u0026\\text{car } -2 \\frac{1}{n}\\sum_{i=1}^n x_i\\times m = -2 m^2\\\\ \u0026 = \\frac{1}{n}\\sum_{i=1}^n E(x_i^2)-E(m^2)\\\\ \u0026 = E(x^2)-E(m^2) \u0026\u0026\\text{car }E(x_i)=E(x)\\text{ ne dépend pas de }i\\\\ \u0026= \\left(V(x)+E(x)^2\\right)-\\left(V(m)+E(m)^2\\right)\u0026\u0026\\text{car }V(x)=E(x^2)-E(x)^2\\\\ \u0026= \\left(V(x)+E(x)^2\\right)-\\left(\\frac{1}{n}V(x)+E(x)^2\\right)\u0026\u0026\\text{car }V(m)=\\frac{1}{n}V(x)\\\\ \u0026= \\frac{n-1}{n}V(x) \\end{aligned} $$ Par conséquent, on multiplie par $\\frac{n}{n-1}$ pour se débarrasser du biais\u0026hellip;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/",
	"title": "Théorie quantique des champs",
	"tags": [],
	"description": "",
	"content": " Théorie quantique des champs \"Particules et ondes ne sont que les excitations d'un champ quantique s'étendant dans l'espace temps.\" La théorie quantique des champs (TQC) est considéré à ce jour comme la meilleure (à défaut d\u0026rsquo;être l\u0026rsquo;ultime) théorie explicative de l\u0026rsquo;univers qui nous entoure.\nC\u0026rsquo;est à la fois la théorie testée avec la plus grande précision 😎, mais aussi celle qui a produit le plus grand écart entre théorie et expérience 😱 (catastrophe du vide).\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell.\nVieux texte qui reprend des idées développées dans le petit livre grand public de Feynman sur l\u0026rsquo;électrodynamique quantique QED: The Strange Theory of Light and Matter.\nChouette article de Quanta Magazine qui résume et illustre le modèle standard des particules dont la structure repose sur la théorie quantique des champs ; l\u0026rsquo;état actuel de notre compréhension du monde. "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/quantique/erepr/",
	"title": "ER = EPR",
	"tags": [],
	"description": "",
	"content": " ER = EPR Le paradoxe EPR est depuis quelques temps au cœur d\u0026rsquo;avancées prometteuses vers le graal de la physique théorique : une théorie de la gravité quantique. En effet, la conjecture ER = EPR proposée en 2013 par Léonard Susskind et Joan Maldacena connecterait les particules intriquées par des trou de vers (pont d’Einstein-Rosen)1.\nC\u0026rsquo;est la géométrie même de l\u0026rsquo;espace-temps qui serait déterminée par l\u0026rsquo;intrication quantique.\nCes articles de Quanta Magazine permettent de creuser ces questions au centre de la physique théorique aujourd\u0026rsquo;hui :\nsur le paradoxe des trous noirs qui a tout lancé ER=EPR Un peu fou qu\u0026rsquo;Einstein soit présent dans les deux acronymes\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/geometrie/geo5/",
	"title": "Poursuites",
	"tags": [],
	"description": "",
	"content": " Poursuites note\nCette page est très inspirée du livre Chases and Escapes The Mathematics of Pursuit and Evasion de Paul Nahin.\nParadoxe d\u0026rsquo;Achille et de la tortue Dans La Physique, Aristote rapporte les célèbres paradoxes de Zénon d\u0026rsquo;Élée dont celui d\u0026rsquo;Achille et la tortue. :\nCelui qui court le plus lentement ne sera jamais rattrapé par le plus rapide. Car le poursuivant doit d\u0026rsquo;abord atteindre le point d\u0026rsquo;où est parti le poursuivi, si bien que le plus lent doit toujours avoir une longueur d\u0026rsquo;avance.\nImaginons qu\u0026rsquo;Achille aille deux fois plus vite que la tortue et que la tortue ait une distance d\u0026rsquo;avance $d$ au départ. Il faut un temps $T$ à Achille pour parcourir la distance $d$, mais pendant ce temps $T$, la tortue a continué d\u0026rsquo;avancer et a parcouru une distance $d/2$. Il faudra alors $T/2$ à Achille pour parcourir cette distance supplémentaire, mais pendant ce temps, la tortue a avancé de $d/4$, etc.\nLe temps mis par Achille pour rattraper la tortue vaut donc :\n$\\displaystyle T+\\frac{T}{2}+\\frac{T}{4}+\\frac{T}{8}+\\cdots = \\sum_{n=0}^{\\infty}\\frac{T}{2^n}$\nLe but de Zénon d\u0026rsquo;Élée était de prouver l\u0026rsquo;impossibilité même du mouvement en piégeant le concept avec ces histoires d\u0026rsquo;infini.\nRéussir à imaginer que la somme d\u0026rsquo;un nombre infini de termes positifs puisse donner un résultat fini constitua une montagne mathématique qu\u0026rsquo;on mis très longtemps à gravir.\nAjourd\u0026rsquo;hui, on sait que ce type de série géométrique converge si sa raison est comprise dans $]-1;1[$. Et dans notre exemple, comme l\u0026rsquo;illustre l\u0026rsquo;animation ci-dessous, la série converge pour donner $2T$.\nCourbe de poursuite Ok, on peut rattraper la tortue, mais comment faire en pratique si on devait programmer Achille en supposant que les deux démarrent dans les poitions notées ci-dessous ?\nLe plus rapidement : calculer la direction de la ligne droite à suivre permettant d\u0026rsquo;intercepter la tortue. Le plus simple : toujours orienter son vecteur vitesse (la direction de son mouvement) vers la cible. Dans le deuxième cas, on obtient une famille de courbes particulières appelées courbes de poursuite.\nExtrait d\u0026rsquo;un manuel de l\u0026rsquo;US Air Force distribué aux mitrailleurs :\nComme l\u0026rsquo;explique le manuel, si un poursuivant se dirige en permanence vers sa cible (son vecteur vitesse étant selon la ligne joignant le poursuivant à la cible) alors sa trajectoire sera une courbe de poursuite.\nSi on voulait déterminer analytiquement l\u0026rsquo;équation de cette courbe, on poserait que :\nla pente $p(x)$ de la trajectoire du chasseur est donnée par $\\displaystyle p(x)=\\frac{dy}{dx}=\\frac{y-V_{cible}t}{x-x_0}$ d\u0026rsquo;après le schéma ci-dessous. et le chasseur parcours en un temps $t$ le morceau de trajectoire compris enntre $0$ et $x$ : $\\displaystyle V_{chasseur} t = \\int_0^x \\sqrt{1+\\left(\\frac{dy}{dx\u0026rsquo;}\\right)^2}dx\u0026rsquo; = \\int_0^x \\sqrt{1+p(x\u0026rsquo;)^2}dx\u0026rsquo; $ En résolvant pour $t$, on obtient $\\displaystyle \\frac{1}{V_{chasseur}}\\int_0^x\\sqrt{1+p(x\u0026rsquo;)^2}dx\u0026rsquo; = \\frac{y}{V_{cible}}-p(x)\\frac{(x-x_0)}{V_{cible}}$\nAprès quelques étapes, cela s\u0026rsquo;intègre en $\\displaystyle p(x)=\\frac{dy}{dx}=\\frac{1}{2}\\left[\\left(1-\\frac{x}{x_0}\\right)^{-n}-\\left(1-\\frac{x}{x_0}\\right)^{n}\\right]$ en posant $\\displaystyle n = \\frac{V_{cible}}{V_{chasseur}}$ qui s\u0026rsquo;intègre à son tour en $\\displaystyle y(x)=\\frac{n}{1-n^2}x_0 + \\frac{1}{2}(x_0-x)\\times\\left[\\frac{(1-x/x_0)^n}{1+n}-\\frac{(1-x/x_0)^{-n}}{1-n}\\right]$.\nLa cible est rattrapée au point $(x_0\\,;y(x_0))=\\left(x_0\\,;\\frac{n}{(1-n^2)}x_0\\right)$. Elle a alors parcouru une distance $\\displaystyle \\frac{n}{(1-n^2)}x_0$ et le chasseur a parcouru $\\displaystyle \\frac{V_{chasseur}}{V_{cible}}$ fois plus.\nC\u0026rsquo;est finalement beaucoup plus simple à programmer qu\u0026rsquo;à calculer. Et si la cible se met à remuer, cela devient même mission quasi impossible d\u0026rsquo;obtenir une formule fermée pour la courbe\u0026hellip;\nDans ce premier petit programme, on retrouve la courbe d\u0026rsquo;allure caractéristique.\nEt dans ce deuxième programme, on prend le point de vue du passager de l'avion cible pour confirmer la mise-en-garde du manuel\u0026nbsp;; si le chasseur semble glisser dans le ciel tout en grossissant, ça n'est pas bon signe...\nInterception Arrêtons maintenant de poursuivre bêtement et essayons d\u0026rsquo;intercepter.\nSupposons que le chasseur est en $A$ à $t=0$ et que la cible est en $B$ au même instant et supposons aussi qu\u0026rsquo;ils ont chacun un mouvement rectiligne uniforme (à $v_{chasseur}$ et $v_{cible}$). À quelle condition le chasseur intercepte-t-il la cible au point $I$ ?\nIl faut que le ratio des distances et des vitesses soient les mêmes : $\\displaystyle \\frac{BI}{AI}=\\frac{v_{cible}}{v_{chasseur}}=n$.\nLes points $I$ solutions doivent donc vérifier : $\\displaystyle \\frac{\\sqrt{(x-p)^2+y^2}}{\\sqrt{(x-m)^2+y^2}}=n$.\nCela donne l\u0026rsquo;équation d\u0026rsquo;un cercle, le cercle d\u0026rsquo;Apollonius : $\\displaystyle \\left[x-\\frac{n^2 m-p}{n^2-1}\\right]^2 + y^2 = \\left[\\frac{n(p-m)}{1-n^2}\\right]^2$.\nSon centre est sur l\u0026rsquo;axe horizontal $\\displaystyle \\left(\\frac{n^2m-p}{n^2-1};0\\right)$ et son rayon vaut $\\displaystyle \\frac{n(p-m)}{|1-n^2|}$.\nPour déterminer la direction à prendre pour le chasseur, il suffit de regarder où la direction de la cible coupe le cercle d\u0026rsquo;Apollonius.\nOn remarque que même si la cible va plus vite que le chasseur $n\u0026gt;1$, il peut y avoir interseption (et pour deux directions différentes du chasseur). Mais pour cela, l\u0026rsquo;angle $\\theta$ de la direction de la cible ne doit pas être supérieur à $\\displaystyle \\sin^{-1}\\left(\\frac{v_{chasseur}}{v_{cible}}\\right)$.\nEn effet, la situation limite correspond à une direction de la cible tangente au cercle d\u0026rsquo;Apollonius (voir dessin ci-dessous).\nOn a bien $\\displaystyle \\sin\\theta = \\frac{CI}{CB} = \\frac{\\frac{n(p-m)}{n^2-1}}{p-\\frac{n^2m-p}{n^2-1}}=\\frac{1}{n}$\nManœuvre d\u0026rsquo;évitement de l\u0026rsquo;Enola Gay Juste après avoir lâcher sa terrible bombe sur Hiroshima, l\u0026rsquo;Enola Gay a manœuvré pour tenter d\u0026rsquo;éviter au maximum le souffle de l\u0026rsquo;explosion atomique.\nLe B-29 avait une vitesse de $\\pu{529 km/h}$ par rapport au sol au moment du largage de la bombe. Sachant que l\u0026rsquo;explosion a eu lieu après une chute de $\\pu{19700 ft}$, soit $\\pu{9,053 km}$, on peut en déduire le délai entre le largage et l\u0026rsquo;explosion : dans l\u0026rsquo;hypothèse d\u0026rsquo;une chute libre (poids comme seule force), la distance verticale parcourue est donnée par $h=\\frac{1}{2}gt^2$, d\u0026rsquo;où $t=\\sqrt{\\frac{2h}{g}}\\approx\\pu{43 s}$. C\u0026rsquo;est en accord avec la valeur généralement admise. Le pilote a donc environ 43 secondes pour s\u0026rsquo;éloigner le plus possible de la zone d\u0026rsquo;impact.\nPendant ces 43 secondes, la bombe continue d\u0026rsquo;avancer horizontalement à la vitesse de l\u0026rsquo;avion au moment du largage, ce qui donne une distance parcourue de $\\pu{6,3 km}$. Le pilote parle plutôt d\u0026rsquo;une distance horizontale de 3,5 mi, soit $\\pu{5,6 km}$. Vu la forme de la bombe, cela paraît logique que son mouvement horizontal soit plus impacté par les frottements que son mouvement vertical.\nAu moment du largage, le pilote va évidemment chercher à séloigner le plus possible de la future explosion. Pour cela il entreprend immédiatement un virage à pleine vitesse (350 mph par rapport à l\u0026rsquo;air, soit $\\pu{563 km/h}$) en braquant ses ailes à un angle $\\alpha = 60^\\circ$.\nSi le B-29 tourne, c\u0026rsquo;est grâce à l\u0026rsquo;inclinaison de la force de portance $\\vec{L}$, perpendiculaire aux ailles. On va supposer que la trajectoire pendant le virage est circulaire de rayon $R$ et que la vitesse $v$ de l\u0026rsquo;avion par rapport à l\u0026rsquo;air reste constante. L\u0026rsquo;accélération est alors horizontale et purement centripète et vaut $\\displaystyle a = \\frac{v^2}{R}$.\nLe PFD (2e loi de Newton) appliqué à l\u0026rsquo;avion nous dit que : $\\displaystyle\\vec{P}+\\vec{L} = m\\vec{a}$.\nEn projetant verticalement, on obtient : $\\displaystyle-mg+L_y = 0$. On retrouve que la composante verticale de la portance compense le poids. Cela donne : $\\displaystyle L\\times\\cos\\alpha = mg$.\nEt la projection horizontale donne : $\\displaystyle -L_x = -ma$, et donc $\\displaystyle L\\sin\\alpha = ma$.\nPar conséquent : $\\displaystyle ma=\\frac{mg}{\\cos\\alpha}\\sin\\alpha \\Rightarrow a = g\\tan\\alpha$.\nComme $\\displaystyle\\tan(60^\\circ) = \\sqrt{3}$, on se retrouve avec une accélération horizontale de l\u0026rsquo;avion valant $\\sqrt{3}g$.\nLes passagers de l\u0026rsquo;avion ressentent, eux, une accélération oblique de $2g$ (accélération du champ de pesanteur à laquelle s\u0026rsquo;ajoute l\u0026rsquo;accélération perpendiculaire due à l\u0026rsquo;avion).\nEt puisque $\\displaystyle a=\\frac{v^2}{R}$, on obtient : $\\displaystyle R =\\frac{v^2}{g \\tan\\alpha} \\approx \\pu{1,4 km}$. Un petit coucou ou un gros porteur feront le même virage du moment que vitesse et inclinaison des ailes sont les mêmes !\nIl nous reste à trouver quand quitter le virage. $(AH)$ et $(HB)$ sont tangentes au cercle formé et $AH=AB$. Les triangles $HBC$ et $HAC$ sont donc identiques impliquant l\u0026rsquo;égalité des angles $\\widehat{HCB}$ et $\\widehat{HCA}$ ($\\beta$ sur le schéma). Le bombardier quitte le virage au bout d\u0026rsquo;une déviation de $2\\beta$ où $\\beta = \\arctan\\left(\\frac{AH}{R}\\right)=\\arctan(5,6/1,4)$. Cela donne une déviation totale $2\\beta$ d\u0026rsquo;environ $152^\\circ$.\nLa distance parcourue sur le cercle vaut alors $\\frac{2\\beta}{360}\\times 2\\pi R \\approx \\pu{3,7 km}$ et le temps pour la parcourir vaut $3,7/545\\times 3600 = \\pu{24 s}$. Pour la vitesse de l\u0026rsquo;avion, on fait une moyenne entre la vitesse de l\u0026rsquo;avion par rapport au sol au moment du largage et sa vitesse par rapport à l\u0026rsquo;air. Il reste alors $43-24=\\pu{19 s}$ pour s\u0026rsquo;éloigner à tout berzingue avant que la bombe n\u0026rsquo;explose. Le bombardier sera alors à une distance $d=5,6+545\\times 19/3600 \\approx \\pu{8,5 km}$.\nAu bout de combien de temps sera-t-il touché par le souffle de l\u0026rsquo;explosion, et à quelle distance de l\u0026rsquo;impact sera-t-il alors ? Supposons une onde de choc se déplaçant à $v_c$ d\u0026rsquo;environ $\\pu{1300 km/h}$ (un peu plus rapide que le son) et appelons $t_i$ le temps au bout duquel le souffle rattrape le bombardier (de vitesse $v_b = \\pu{545 km/h}$). N\u0026rsquo;oublions pas que le bombardier est à la hauteur $h$ par rapport au point d\u0026rsquo;impact ($\\approx\\pu{9,1 km}$) au-dessus du point d\u0026rsquo;impact. Pythagore donne alors : $(v_c\\times t_i)^2 = (d+v_b\\times t_i)^2+h^2$. La solution positive de ce trinôme du second degré en $t_i$ est $\\pu{0,0144 h}$ soit $\\pu{52 s}$ environ. Le pilote parle plutôt d\u0026rsquo;un délai de $\\pu{45 s}$ après la détonation et d\u0026rsquo;une distance de $\\pu{11,5 mi}$ au point d\u0026rsquo;impact. Si notre résultat surestime le délai, il donne une distance à l\u0026rsquo;impact de $0,0144\\times 1300 \\approx \\pu{18,7 km} = \\pu{11,6 mi}$. L\u0026rsquo;accord n\u0026rsquo;est pas mauvais.\nPoursuite cyclique $n$ tortues sont disposées sur les sommets d\u0026rsquo;un polygone régulier à $n$ côtés. Au top départ, chacune se met à poursuivre à vitesse constante sa voisine dans le sens trigonométrique jusqu\u0026rsquo;à ce qu\u0026rsquo;elles se rejoignent toutes au centre.\nOn remarque que le polygone formé par les tortues est à tout moment une réduction et rotation du polygone de départ.\nUtilisons les coordonnées polaires $(r(t);\\theta(t))$ pour décrire une des tortues dont la position initiale servira d\u0026rsquo;origine des angles $(r(0)=r;\\theta(0)=0).$\n$\\displaystyle v_r = \\frac{dr}{dt} = -v\\sin\\left(\\frac{\\pi}{n}\\right)$\n$\\displaystyle v_\\theta = r\\frac{d\\theta}{dt} = v\\cos\\left(\\frac{\\pi}{n}\\right)$\nEn jouant un peu avec ces deux expressions, on obtient :\n$\\displaystyle \\frac{v}{r}\\cos\\left(\\frac{\\pi}{n}\\right) = \\frac{d\\theta}{dt} = \\frac{d\\theta}{dr} \\cdot\\frac{dr}{dt} = -\\frac{d\\theta}{dr}v\\sin\\left(\\frac{\\pi}{n}\\right)$\nOn peut simplifier par $v$, ce qui donne :\n$\\displaystyle \\frac{dr}{d\\theta} = -r\\frac{\\sin(\\pi/n)}{\\cos(\\pi/n)} = -r\\tan\\left(\\frac{\\pi}{n}\\right)$\nEt finalement :\n$\\displaystyle \\frac{dr}{r} = - \\tan\\left(\\frac{\\pi}{n}\\right) d\\theta$\nCe qui s\u0026rsquo;intègre en une belle spirale logarithmique :\n$\\displaystyle r(\\theta) = r(0)\\exp\\left(-\\theta\\tan\\left(\\frac{\\pi}{n}\\right)\\right)$\nCherchons maintenant le temps mis par les tortues pour rejoindre le centre du polygone :\n$\\displaystyle T=\\int_0^T dt = \\int_{r(0)}^0 \\frac{dr}{(dr/dt)}=\\int_{r(0)}^0\\frac{dr}{-v\\sin(\\pi/n)}$\nD\u0026rsquo;où $\\displaystyle T = \\frac{1}{v}\\int_0^{r(0)}\\frac{dr}{\\sin(\\pi/n)} = \\frac{r(0)}{v \\sin(\\pi/n)}$\nEt la distance parcourue vaut donc $\\displaystyle D = \\frac{r(0)}{ \\sin(\\pi/n)}$.\nDans le cas d\u0026rsquo;un carré, les expressions deviennnent encore plus simples :\n$\\displaystyle r(\\theta) = r(0)\\exp\\left(-\\theta \\right)$\nEt $\\displaystyle D = r(0)\\frac{\\sqrt{2}}{2} = a$, en appelant $a$ le côté du carré. Donc le long de leur spirale, les tortues parcourent au final la longueur d\u0026rsquo;un côté du carré initial.\nOn peut expliquer ça simplement par le fait que la vitesse relative entre deux tortues restent en permanence identique à $v$ puisque leurs vecteurs vitesse sont à tout moment orthogonaux.\nOn peut s\u0026rsquo;amuser à combiner les poursuites pour faire des jolies figures\u0026hellip; Ci-dessous, on assemblé 6 poursuites à 3 tortues pour former un hexagone.\nMurmurations Comme le montre cette très chouette vidéo de fouloscopie, nos histoires de poursuite participent à l\u0026rsquo;explication des comportements collectifs fascinants des bancs de possions ou des murmurations d\u0026rsquo;étourneaux. Un poisson (ou oiseau) ne ferait en fait que poursuivre les quelques individus immédiatement dans son champ de vision.\nLe code suivant utilise les 6 premiers voisins et permet de voyager un peu dans le diagramme de phase.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/jeux/",
	"title": "Théorie des jeux",
	"tags": [],
	"description": "",
	"content": " Un peu de théorie des jeux note\nCette page est très inspirée du livre Chases and Escapes The Mathematics of Pursuit and Evasion de Paul Nahin.\nAttaque - Défense Un petit problème mathématique qui fleure bon la Guerre Froide :\nDeux bombardiers bleus sont en mission. L\u0026rsquo;un transporte une bombe H et l\u0026rsquo;autre des équipements pour brouiller les radars. Leur plan de vol est tel qu\u0026rsquo;un des deux bombardiers est mieux protégé par les mitrailleuses de l\u0026rsquo;autre. Pour éviter qu\u0026rsquo;un avion de chasse rouge abatte l\u0026rsquo;avion bleu transportant la bombe, que vaut-il mieux : le placer dans la position la mieux ou la moins bien protégée ?\nLa réponse semble de prime abord évidente et pourtant\u0026hellip;\nListons les stratégies possibles pour les Bleus et les Rouges :\nB1 : placer la bombe dans le bombardier le mieux protégé B2 : placer la bombe dans le bombardier le moins bien protégé R1 : attaquer le bombardier le mieux protégé R2 : attaquer le bombardier le moins bien protégé La matrice de gains ci-dessus peut s\u0026rsquo;interpréter ainsi : les chances de survie du bombardier le mieux protégé sont de 80% s\u0026rsquo;il est attaqué (et de 100% s\u0026rsquo;il ne l\u0026rsquo;est pas\u0026hellip;) alors que le moins bien protégé n\u0026rsquo;a que 60% de chance de s\u0026rsquo;en sortir (et 100% si on le laisse tranquille).\nOn a imaginé ainsi un \u0026ldquo;jeu\u0026rdquo; à somme nulle (les gains d\u0026rsquo;un joueur sont les pertes de l\u0026rsquo;autre). Dans la nomenclature de la théorie des jeux, il s\u0026rsquo;agit d\u0026rsquo;un jeu injuste puisque toutes les entrée sont positives $\\Rightarrow$ bleu repart forcément \u0026ldquo;gagnant\u0026rdquo; (même si ça n\u0026rsquo;a pas vraiment de sens ici).\nLa stratégie optimale pour deux joueurs rationnels va consister à minimiser le \u0026ldquo;score\u0026rdquo; maximum que l\u0026rsquo;autre joueur peut faire en suivant ainsi un principe du moindre mal. En effet, chaque joueur sait que l\u0026rsquo;autre va chercher à maximiser ses gains et va donc s\u0026rsquo;évertuer à rendre cette maximisation la plus faible possible.\nSi une stratégie unique permet d\u0026rsquo;arriver à cette fin, sans que le joueur soit jamais motivé à changer de stratégie lors d\u0026rsquo;un prochain tour, on dit qu\u0026rsquo;il joue une stratégie pure.\nC\u0026rsquo;est le cas dans l\u0026rsquo;exemple suivant :\nBleu est incité à choisir la stratégie B1 pour maximiser ses gains minimums réalisables : dans le pire des cas, il recevra 5 alors qu\u0026rsquo;avec la stratégie B2, il recevrait au pire 3\u0026hellip;\nSi Rouge joue sa première stratégie, il perd au pire 6 alors qu\u0026rsquo;avec la seconde, il perd au pire 5. Donc Rouge va joueur la seconde stratégie.\nEn résumé, Bleu choisi la ligne contenant le plus grand des gains minimums et Rouge choisit la colonne contenant le plus petit gain maximum possible.\nLe résultat de la partie est ici un gain de 5 pour Bleu.\nLorsque, comme dans cet exemple, le gain minimal sur la ligne comportant le plus grand minimum possible est égal au gain maximal sur la colonne contenant le plus petit maximum possible, on dit que le jeu est stable. Aucun des joueurs ne sera inciter à changer de stratégie. Le jeu possède alors un point col (ou point-selle en référence à une selle de cheval). On a en effet, un minimum local dans une direction et un maximum local dans l\u0026rsquo;autre.\nChangeons de matrice de gain :\nPour maximiser le gain mimal, Bleu choisit la stratégie B2 ($4\u0026gt;3$) alors que pour minimiser le gain maximal pour Bleu, Rouge choisit la stratégie R1 ($5\u0026lt;6$). On remarque que le plus grand gain minimal sur une rangée ($4$) est maintenant différent du plus petit gain maximal sur une colonne ($5$). La conséquence est que maintenant, chacun des deux joueurs est incité à changer sa stratégie. Si Rouge passe de la stratégie R1 à R2 par exemple, il réduit le gain de Bleu de $5$ à $4$ (en imaginant que Bleu ait bien choisi la stratégie B2). Mais alors, Bleu est tenté de passer de la stratégie B2 à B1 pour passer d\u0026rsquo;un gain de $4$ à un un gain de $6$. Et ainsi de suite\u0026hellip;\nJouer une stratégie pure est donc optimal pour aucun des joueurs. Le mieux qu\u0026rsquo;ils puissent faire est jouer une stratégie mixte composée d\u0026rsquo;un mélange aléatoire (pour que l\u0026rsquo;autre joueur ne puisse pas anticiper) des stratégies 1 et 2.\nCe fut l\u0026rsquo;un des tours de force de John von Neumann de démontrer en 1928 qu\u0026rsquo;il existe pour tout jeu à deux opposants à somme nulle une stratégie mixte optimale pour les deux joueurs telle qu\u0026rsquo;ils puissent espérer le même gain moyen $V$, $V$ étant le meilleur possible. C\u0026rsquo;est le théorème du minimax.\nDébusquons cette stratégie optimale pour le dernier exemple. On va supposer que Bleu joue la stratégie B1 avec une probabilité $p$ et B2 avec donc une probabilité $1-p$. Et Rouge joue R1 avec la probabilité $q$ et R2 avec la probabilité $1-q$.\nSi bleu joue B1, il gagne $3q + 6(1-q)$. Et s\u0026rsquo;il joue B2, il gagne $5q+4(1-q)$. L\u0026rsquo;espérance de gain de Bleu est donc $V(p,q)=p[3q+6(1-q)]+(1-p)[5q+4(1-q)]=4+2p+q-4pq$.\nDu point de vue de Rouge, on obtient une espérance de gain valant $q[3p+5(1-p)]+(1-q)[6p+4(1-p)]=4+2p+q-4pq$. On obtient donc à nouveau $V(q,p)$.\nTracer $V(p,q)$ fait apparaître une selle de cheval. La stratégie optimale pour Bleu et Rouge va donc consister à placer leurs stratégies sur le point col ; le point minimax.\nBleu veut choisir $p$ tel que $\\displaystyle \\frac{\\partial V}{\\partial q} = 0$. Cela donne $1-4p=0$ et donc $p=1/4$. La distribution de probabilité optimale pour Bleu est donc $(1/4\\,;3/4)$.\nRouge veut choisir $q$ tel que $\\displaystyle \\frac{\\partial V}{\\partial p} = 0$. Cela donne $2-4q=0$ et donc $q=1/2$. La distribution de probabilité optimale pour Rouge est donc $(1/2\\,;1/2)$.\nLe gain pour les deux au point minimax est alors $V(p=1/4,q=1/2) = 4,5$.\nRevenons maintenant à notre problème de guerre froide.\nIl n\u0026rsquo;y a clairement pas de point col pour des stratégies pures puisque le plus grand minimum pour les lignes est $80$ alors que le plus petit maximum pour les colonnes est $100$. Il faut donc jouer des stratégies mixtes.\nSupposons, comme dans l\u0026rsquo;exemple précédent, que la stratégie mixte de Bleu soit $(p\\,;1-p)$ et que celle de rouge soit $(q\\,;1-q)$.\nPour Bleu : $V(p,q)= p[80q+100(1-q)]+(1-p)[100q+60(1-q)]$ Pour Rouge : $V(p,q)=q[80p+100(1-p)]+(1-q)[100p+60(1-p)]$ Dans les deux cas, $V(p,q)=60+40p+40q-60pq$. On obtient bien à nouveau une selle de cheval. Cherchons son point col.\nLe point minimax est tel que $\\displaystyle \\frac{\\partial V}{\\partial p} = \\frac{\\partial V}{\\partial q} = 0$. Cela donne $40-60q=0$ et $40-60p=0$, d\u0026rsquo;où $p=q=2/3$. Les stratégies mixtes optimales pour Bleu et Rouge ont le même distribution de probabilités : $(2/3\\,;1/3)$.\nContrairement à l\u0026rsquo;intuition, Bleu doit donc placer la bombe dans l\u0026rsquo;avion le moins protégé dans 1/3 des cas. Et toujours dans 1/3 des cas, Rouge doit attaquer ce bombardier (le moins protégé).\nLa valeur du gain au point minimax vaut $\\displaystyle V\\left(\\frac{1}{3},\\frac{2}{3}\\right)=60+40\\left(\\frac{2}{3}\\right)+40\\left(\\frac{2}{3}\\right)-60\\left(\\frac{2}{3}\\right)\\left(\\frac{2}{3}\\right)=\\frac{260}{3}\\approx 86,7$. Le porteur de la bombe va donc survivre avec une probabilité de $86,7\\%$ qui est supérieure aux $80\\%$ correspondant à la stratégie évidente, mais finalement naïve, de toujours placer la bombe dans le bombardier le mieux protégé.\nEt que se passe-t-il si Bleu applique la stratégie mais pas Rouge ?\nOn a alors $\\displaystyle V\\left(\\frac{1}{3},\\frac{2}{3}\\right) = 60+40\\left(\\frac{2}{3}\\right)+40q-60\\left(\\frac{2}{3}\\right)q=\\frac{260}{3}+40q-40q=\\frac{260}{3}$. Le résultat est indépendant de $q$ ! Quelle que soit la stratégie de Rouge, le gain est assuré par Bleu. Et à l\u0026rsquo;inverse, si Rouge applique la stratégie mais pas Bleu, on obtient à nouveau le même résultat, independant de $p$.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/phistat/",
	"title": "Thermodynamique",
	"tags": [],
	"description": "",
	"content": " Thermodynamique et physique statistique Physique statistique Petite série de vidéo sur la physique statistique, de la définition de la température à l\u0026rsquo;entropie en passant par le facteur de Boltzmann et la loi de Planck :\nEntropie note\nPW Atkins a écrit un livre merveilleux sur l\u0026rsquo;entropie : The Second Law.\nIl fait partie de la vieille collection Scientific American Library qui est bourrée de pépites.\nL\u0026rsquo;entropie est une notion passionnante qui ramifie dans différents champs.\nEn théorie de l\u0026rsquo;information, Claude Shannon a appelé entropie la mesure de la quantité d\u0026rsquo;information contenu dans un message. Ramenée à la physique, l\u0026rsquo;entropie de Shannon mesure le nombre de questions binaires (dont la réponse est oui ou non) auxquelles il faudrait répondre pour spécifier le microétat du système pour un macroétat donné.\nCherchant une réponse au paradoxe de Maxwell, Leó Szilárd a formulé en 1929 une équivalence entre l\u0026rsquo;énergie et l\u0026rsquo;information grâce à une version simplifiée du démon de Maxwell. Il en a déduit l\u0026rsquo;énergie nécessaire au démon pour faire varier l\u0026rsquo;information d\u0026rsquo;un bit. Landauer en a fait un principe (en 1961) qui stipule que l\u0026rsquo;énergie dissipée lors de l\u0026rsquo;effacement d\u0026rsquo;un bit d\u0026rsquo;information vaut au minimum $k_BT \\ln(2)$.\nLien entre l\u0026rsquo;entropie de Shannon et l\u0026rsquo;entropie thermodynamique dans le cas d\u0026rsquo;un gaz parfait à l\u0026rsquo;équilibre (composé de $N$ particules) dans une enceinte de volume $V$ à la température $T$ :\nL\u0026rsquo;information sur une particule consiste à connaître sa position et sa vitesse. L\u0026rsquo;additivité de l\u0026rsquo;entropie permet de distinguer ces deux contributions.\nAppelons $H_p(V)$ l\u0026rsquo;entropie de Shannon liée la position d\u0026rsquo;une particule du gaz.\nAprès avoir doublé le volume de l\u0026rsquo;enceinte, on peut revenir à la situtation de départ (en terme d\u0026rsquo;information) en posant à la particule la question binaire \u0026ldquo;es-tu dans l\u0026rsquo;enceinte de gauche ou celle de droite ?\u0026rdquo;. On a par conséquent $H_p(2V) = H_p(V) + 1$ (il faut 1 bit d\u0026rsquo;information en plus). La seule fonction ayant cette propriété est le logarithme en base deux : $\\log_2$. D\u0026rsquo;où $H_p(V) = \\log_2(V)+cste$.\nÀ l\u0026rsquo;équilibre thermique, l\u0026rsquo;énergie cinétique pour chacun des degrés de liberté possibles (3 pour une particule monatomique, et 5 ou 7 pour une particule diatomique en ajoutant les rotations et vibrations) est proportionnelle à la température. Mais notre information porte sur la vitesse. La vitesse pour chaque réservoir d\u0026rsquo;énergie cinétique est donc proportionnelle à la racine carrée de la température. En quadruplant la température, on double donc la vitesse dans chacun des réservoirs. Il faudra alors autant de questions binaires que de réservoirs (3 pour une particule monoatomique) pour revenir à la situation informationnelle de départ.\nEn appelant $H_v(T)$ l\u0026rsquo;entropie de Shannon liée à la vitesse d\u0026rsquo;une particule, on a : $H_v(4T) = H_v(T)+3$. La fonction ayant cette propriété est $3\\log_4$ ou $\\frac{3}{2}\\log_2$.\nD\u0026rsquo;où $\\displaystyle H_v(T)=\\frac{3}{2}\\log_2(T) + cste$.\nL\u0026rsquo;entropie de Shannon totale de la particule vaut donc $\\displaystyle H(V,T)=H_v(V)+H_p(T) = \\frac{3}{2}\\log_2(T)+\\log_2(V)+cste$\nEt pour $N$ particules ? Si les particules sont supposées indiscernables, multiplier $H$ par $N$ nous ferait tomber dans le paradoxe de Gibbs. Comme toutes les permutations de particules amènent à la même quantité d\u0026rsquo;infomation, il faut retirer $\\log_2(N!)$ à l\u0026rsquo;entropie obtenue : $\\displaystyle H_{gaz}(V,T) = N H(V,T) - \\log_2(N!) \\approx \\frac{3N}{2}\\log_2(T) + N\\log_2(V) + Ncste - N\\log_2(N) + N$.\nEn utilisant le principe de Landauer, on peut convertir ces bits en énergie par unité de température en multipliant par $k_B \\ln_2$. On passe alors de $H$ à $S$, l\u0026rsquo;entropie thermodynamique :\n$\\displaystyle S(V,T) = \\frac{3Nk_B}{2}\\ln(T) + Nk_B\\ln(V) - Nk_B\\ln(N)+CNk_B$ (où $C$ est une constante).\nSource : vidéo de Lê Nguyên Hoang sur l\u0026rsquo;entropie\nInformation et thermodynamique dans la physique théorique contemporaine On observe depuis quelques temps une convergence de la recherche en informatique et en physique théorique sur les questions de l\u0026rsquo;entropie et de la complexité.\nJohn Wheeler est un des premiers à militer pour qu\u0026rsquo;on s\u0026rsquo;oriente vers une explication de l\u0026rsquo;univers basée sur son contenu en information, le bit élémentaire, ce qu\u0026rsquo;il résume par la formule \u0026ldquo;it from bit\u0026rdquo; (Wheeler a un certain talent pour les formules, en plus du reste\u0026hellip;).\nLe fait qu\u0026rsquo;un trou noir ait une entropie proportionnelle à son aire et non à sa masse, comme l\u0026rsquo;ont montré Stephen Hawking et Jacob Bekenstein, semble donner corps à cette idée que le bit est l\u0026rsquo;atome de notre compréhension de l\u0026rsquo;univers. Tout horizon se présenterait en effet comme une sorte d\u0026rsquo;écran sur lequel s\u0026rsquo;écrit l\u0026rsquo;information qu\u0026rsquo;il contient. C\u0026rsquo;est le principe holographique.\nEt plus récemment, Leonard Suskind a jeté un nouveau pont entre physique et information en liant le volume d\u0026rsquo;un trou noir et sa complexité.\nThermodynamique Fluide supercritique On place un échantillon d\u0026rsquo;eau dans un récipient hermétique à 25 °C. On fait le vide d\u0026rsquo;air, et on laisse l\u0026rsquo;équilibre de vaporisation-condensation s\u0026rsquo;établir. On obtient un mélange d\u0026rsquo;eau liquide et de vapeur d\u0026rsquo;eau à une pression de 0,03 atm. Une frontière distincte entre le liquide plus dense et le gaz moins dense est clairement observable. En augmentant la température, la pression de la vapeur d\u0026rsquo;eau augmente, comme décrit par la courbe liquide-gaz du diagramme de phase de l\u0026rsquo;eau, et un équilibre diphasique entre les phases liquide et gazeuse persiste. À une température de 374 °C, la pression de vapeur a atteint 218 atm, et toute augmentation supplémentaire de la température entraîne la disparition de la frontière entre les phases liquide et vapeur. Toute l\u0026rsquo;eau dans le récipient est maintenant présente dans une phase unique dont les propriétés physiques sont intermédiaires entre celles des états gazeux et liquide, le fluide supercritique. Au-dessus de sa température critique, un gaz ne peut pas être liquéfié, quelle que soit la pression appliquée. La pression requise pour liquéfier un gaz à sa température critique est appelée pression critique.\nEn contournant le point critique, on peut passer de l\u0026rsquo;état liquide à l\u0026rsquo;état gazeux (et inversement) continument, sans seuil. On parle alors de transition de phase du deuxième ordre. Au moment du contournement du point critique (lorsque $P\u0026gt;P_C$ et $T\u0026gt;T_C$), on dit que la matière est dans un nouvel état (un 5e état avec les solides, liquides, gaz et plasmas), le fluide supercritique.\nCet état possède à la fois des propriétés d\u0026rsquo;un gaz et d\u0026rsquo;un liquide ; comme un gaz, il occupe tout l\u0026rsquo;espace disponible, possède une grande diffusivité mais pas de tension superficielle, et comme un liquide, il a une grand pouvoir de dissolution de solutés non volatils. Il peut donc pénétrer plus efficacement les petites ouvertures d\u0026rsquo;un mélange solide et en extraire les composants solubles. Ces propriétés font des fluides supercritiques des solvants extrêmement utiles pour un large éventail d\u0026rsquo;applications. Par exemple, le dioxyde de carbone supercritique est devenu un solvant très populaire dans l\u0026rsquo;industrie alimentaire, étant utilisé pour décaféiner le café, éliminer les graisses des chips et extraire les composés aromatiques et les fragrances des huiles d\u0026rsquo;agrumes. Il est non toxique et relativement peu coûteux. Après utilisation, le $\\ce{CO2}$ peut être facilement récupéré en réduisant la pression et en collectant le gaz résultant.\nOn comprend assez bien les propriétés du fluide superfluide aux vues des conditions pour y parvenir. La grande température rend l\u0026rsquo;agitation cinétique supérieure aux liaisons intermoléculaires, empêchant l\u0026rsquo;agglomération, mais la grande pression donne malgré tout au fluide une densité de liquide. Les molécules sont très proches mais ne \u0026ldquo;collent\u0026rdquo; pas.\nEn agitant un extincteur par une journée fraiche, on sent et entend le liquide remuer à l\u0026rsquo;intérieur. Mais si on répète l\u0026rsquo;expérience par une journée très chaude, on ne sent plus aucun liquide à l\u0026rsquo;intérieur de l\u0026rsquo;extincteur. Où est-il passé ?\nÀ une température supérieure à la température critique du $\\ce{CO2}$ (31 °C), aucune pression n\u0026rsquo;est plus suffisante pour le liquéfier.\nOn voit dans la vidéo que la proximité du point critique peut se traduire (lors du passage de superfluide à gaz+liquide) par un phénomène spectaculaire : le fluide devient \u0026ldquo;brouillardeux\u0026rdquo;. On appelle ce phénomène l\u0026rsquo;opalescence critique. Lorsque la température ou la pression d\u0026rsquo;un superfluide passe sous sa valeur critique, on assiste en tout point du fluide à une hésitation de la matière entre liquide et gaz provoquant des fluctuations de densité de toutes tailles. Ces fluctuations de densité entraînent des fluctuations d\u0026rsquo;indice optique et les plus petites fluctuations (échelles de taille de la longueur d\u0026rsquo;onde de la lumière visible) vont entraîner la forte diffusion qui caractérise le phénomène (un peu comme les micro-gouttes d\u0026rsquo;huile dans le lait ou le pastis allongé).\nTechniquement, la transition de phase du deuxième ordre que subit alors le superfluide est caractérisée par la divergence de la longueur de corrélation $\\xi$ qui traduit la distance sur laquelle une perturbation locale peut influencer le système. Près du point critique, les fluctuations deviennent corrélées sur des distances de plus en plus grandes. Plus précisément, la longueur de corrélation évolue alors en loi de puissance : $\\xi=|T-T_C|^\\nu$ ($\\nu$ est appelé exposant critique). Au point critique, la longueur de corrélation devient théoriquement infinie ; il y a des fluctuations (ici de densité) à toutes les échelles.\nLes transitions du premier ordre sont des transitions avec sauts alors que celles du deuxième ordre sont des transitions avec divergences.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/chaos/",
	"title": "Chaos",
	"tags": [],
	"description": "",
	"content": " Chaos Au confin des maths et de la physique, une des plus belle chose qui soit : le chaos (avec son cheptel de fractales et d\u0026rsquo;attracteurs étranges). Le chaos n\u0026rsquo;a pas grand chose à voir avec son image \u0026ldquo;grand public\u0026rdquo; puisqu\u0026rsquo;il est finalement très organisé. Mais timide, il le cache bien\u0026hellip;\nL\u0026rsquo;intégration du système d\u0026rsquo;équations différentielles suivant donne le fameux attracteur de Lorenz avec sa forme caractéristique de papillon : $$\\begin{cases}x\u0026rsquo;=\\sigma(y-x)\\\\y\u0026rsquo;=\\rho x-y-xz\\\\z\u0026rsquo;=xy-\\beta \\end{cases}$$ Avec les valeurs de paramètres suivantes : $$\\begin{cases}\\sigma = 3\\\\\\rho = 26.5\\\\\\beta = 1\\end{cases}$$ et le point de départ $(x_0;y_0;z_0)=(0;1;1,05)$, cela donne :\nCet article relate l\u0026rsquo;histore du chaos en tant qu\u0026rsquo;objet de recherche et insiste en particulier sur le rôle fondamental de deux programmeuses : Ellen Fetter et Margaret Hamilton.\nLe livre Nonlinear Dynamics and Chaos de Steven Strogatz introduit le domaine de manière passionnante.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/jeux/",
	"title": "Curiosités",
	"tags": [],
	"description": "",
	"content": " Curiosités La toupie tippe-top La toupie tippe-top se redresse semblant braver les lois physiques. C\u0026rsquo;est le couple des forces de frottement qui est responsable de cette bizarrerie.\nWolfgang Pauli et Niels Bohr, deux immenses physiciens au cœur de la révolution quantique, regardant une toupie tippe-top se relever.\nComme le centre de masse s\u0026rsquo;est élevé, on a envie de s\u0026rsquo;inquiéter pour la conservation de l\u0026rsquo;énergie Mais il ne faut pas ! Elle est solide\u0026hellip; La toupie tourne moins vite à la fin qu\u0026rsquo;au début. Une partie de l\u0026rsquo;énergie de rotation est partie dans les frottements et une autre dans le retournement.\nD\u0026rsquo;où vient la force qui amène la toupie à se retourner ? Pauli et Bohr se sont sans doute posés la question moins d\u0026rsquo;un quart de seconde mais chacun son rythme\u0026hellip;\nComme c\u0026rsquo;est une toupie, on pense rapidement à un mouvement de précession. La précession est un mouvement de rotation lent (par rapport à la rotation propre) du centre de masse du solide. Or elle naît quand un couple est appliqué sur le solide et cette rotation se fait autour de la direction de la force provoquant le couple.\nFormidable vidéo de VSauce expliquant les gyroscopes :\nPremière candidat pour l\u0026rsquo;origine du couple redressant la toupie : le poids. Bof\u0026hellip; Le moment du poids n\u0026rsquo;a pas la bonne direction puisqu\u0026rsquo;il entraîne la toupie dans un mouvement de rotation autour d\u0026rsquo;un axe vertical et non horizontal (et contrairement à une toupie classique, le centre de masse est très bas et donc le bras de levier très court, ce qui donne un couple de faible intensité).\nIl nous faudrait une précession autour d\u0026rsquo;un axe horizontal pour faire pivoter la toupie et pour ça, on a besoin d\u0026rsquo;un moment d\u0026rsquo;axe vertical et donc finalement une force elle-même horizontale !\nSon origine devient claire si l\u0026rsquo;on s\u0026rsquo;attarde sur la grande différence entre cette toupie et les toupies classiques (outre la position du centre de masse) : elle n\u0026rsquo;a pas de pointe ! Le point de contact peut se balader le long de la partie arrondie de la toupie, ce qui le désaligne avec l\u0026rsquo;axe de rotation principal.\nDès que la toupie bascule un peu, le point de contact $I$ s\u0026rsquo;éloigne du sommet $H$ et la toupie se met à précesser (sous l\u0026rsquo;action du poids) autour d\u0026rsquo;un axe vertical. Cette précession entraîne le point de contact $I$ dans un mouvement circulaire (en première approximation) et comme la toupie tourne trop vite autour de son axe de symétrie pour pouvoir rouler, elle glisse. Donc le point de contact glisse sur la surface et subit des forces de frottement de glissement $\\vec{f}$. On a trouvé notre force horizontale ! Le moment de la force de frottement crée la précession qui va pivoter notre toupie.\nPlus la toupie penche, plus la rotation autour de l\u0026rsquo;axe vertical s\u0026rsquo;accélère tandis que celle autour de l\u0026rsquo;axe de symétrie ralentit. C\u0026rsquo;est la conservation du moment cinétique qui l\u0026rsquo;impose. Le couple de redressement devient, lui, de plus en plus faible au fur et à mesure de la diminution de la rotation propre de la toupie.\nLorsque la toupie est horizontale, la rotation de la toupie autour de son axe de symétrie s\u0026rsquo;arrête et donc le couple de redressement disparaît momentanément. Mais entraînée par son élan, la toupie ne reste pas horizontale longtemps\u0026hellip;\nUne fois la position horizontale dépassée, la rotation propre de la toupie redémarre, mais elle est inversée par rapport à celle de départ (c\u0026rsquo;est toujours la conservation du moment cinétique qui se manifeste). Et l\u0026rsquo;échange entre les deux rotations (celle autour de l\u0026rsquo;axe vertical et celle autour de l\u0026rsquo;axe de symétrie de la toupie) continue, mais au bénéfice maintenant de la rotation propre qui se fait de plus en plus rapide au fur et à mesure du basculement. Le moment lié aux forces de frottement reprend lui aussi du poil de la bête et va se renforcer avec la vitesse de rotation propre. Et surtout, il est dans le même sens que précédemment puisque l\u0026rsquo;inversion de la rotation propre de la toupie est compensée par l\u0026rsquo;inversion du sens des forces de frottement ! Il continue donc de redresser la toupie.\nVient ensuite le contact entre le pied et le sol. Le bras de levier du moment des forces de frottement augmente d\u0026rsquo;un coup donnant un petit \u0026ldquo;kick\u0026rdquo; à la remontée. Il faut néanmoins une réserve d\u0026rsquo;énergie suffisante, les frottements ayant dispersé dans le support une partie non négligeable de l\u0026rsquo;énergie initiale et l\u0026rsquo;élévation du centre de masse ayant lui-aussi aussi puisé dans la réserve cinétique.\nUne fois redressée, les frottements (sur les bords du pied) continuent à stabiliser la position de la toupie, le centre de masse etant ramené en position verticale.\nnote\nL'inversion du sens de rotation propre de la toupie entre le début et la fin illustre joliment la conservation du moment cinétique. L\u0026rsquo;anagyre L\u0026rsquo;anagyre (ou rattleback en anglais) refuse obstinément de tourner dans le sens qu\u0026rsquo;il n\u0026rsquo;aime pas. Si par malheur on l\u0026rsquo;y envoie, il freine, rouspète dans un bruit de crécelle (rattle en anglais) puis se met à tourner dans le bon sens, le seul, le vrai.\nEn effet, son Créateur lui a choisi un sens en le rendant asymétrique.\nSupposons qu\u0026rsquo;un anagyre ait été fabriqué à partir d\u0026rsquo;un demi-ellipsoïde auquel on a ajouté de la masse en deux endroits symétriques par rapport à son centre. Son axe principal d\u0026rsquo;inertie est alors décalé par rapport à ses axes de symétrie géométrique (cf. dessin ci-dessous).\nLa conséquence du décalage de l\u0026rsquo;axe d\u0026rsquo;inertie est le couplage entre les différentes rotations de l\u0026rsquo;anagyre. C\u0026rsquo;est très visible lorsqu\u0026rsquo;on le fait osciller d\u0026rsquo;avant en arrière (autour de $\\vec{j}$) puisqu\u0026rsquo;apparaît immédiatement une oscillation de gauche à droite (autour de $\\vec{i}$) et une rotation dans le sens des aiguilles d\u0026rsquo;une montre (autour de $\\vec{k}$).\nImaginons dans un premier temps que les deux masses soient sur l\u0026rsquo;axe de symétrie ($\\vec{i}$) et regardons leur mouvement si l\u0026rsquo;anagyre oscille dans le sens de la longueur (autour de l\u0026rsquo;axe $\\vec{j}$).\nLa 2e loi de Newton appliquée à la masse $i$ nous dit que $\\vec{F}_{\\text{anagyre sur masse i}} = m_{\\text{masse i}}\\left(\\vec{a}_{\\text{masse i}}-\\vec{g}\\right)$. Quand $\\vec{a}$ et $\\vec{g}$ sont dans le même sens, la force de l\u0026rsquo;anagyre sur la masse est moins grande. Donc $F$ est plus grande lorsque la masse est au plus bas. De plus, la 3e loi de Newton nous dit que $\\vec{F}_{\\text{masse i sur anagyre}} = -\\vec{F}_{\\text{anagyre sur masse i}} $. On se retrouve donc avec la situation schématisée ci-dessous :\nMaintenant si on redécale les deux masses par rapport à l\u0026rsquo;axe de symétrie dans la direction de $\\vec{j}$, on comprend mieux comment la rotation selon $\\vec{j}$ se couple avec celle selon $\\vec{i}$. La masse la plus basse exerce sur l\u0026rsquo;anagyre une force plus grande que la plus haute, ce qui provoque un moment auour de $\\vec{i}$.\nFinalement, lorsque l\u0026rsquo;anagyre oscille d\u0026rsquo;avant en arrière, la partie surélevée va chuter \u0026ldquo;en se tournant\u0026rdquo; du côté de sa masse ajoutée. Imaginons que c\u0026rsquo;est la partie avant qui redescend en entraînant une rotation autour de $\\vec{i}$ dans le sens négatif (cf. vidéo ci-dessous).\nThere should have been a video here but your browser does not seem to support it. Au point de contact, l\u0026rsquo;anagyre exerce alors sur le sol une force selon $-\\vec{j}$ et réciproquement, le sol exerce une force selon $+\\vec{j}$ sur l\u0026rsquo;anagyre. Lorsque c\u0026rsquo;est la partie arrière qui redescend, la rotation selon $\\vec{i}$ se fait dans le sens positif, ce qui donne au final une force du sol selon $-\\vec{j}$. Le sol exerce donc un couple sur l\u0026rsquo;anagyre le faisant tourner autour de $\\vec{k}$ dans le sens positif (anti-horaire).\nLorsqu\u0026rsquo;on envoie l\u0026rsquo;anagyre dans le sens impie (horaire ici), la moindre petite oscillation selon $\\vec{i}$ ou $\\vec{j}$ va être amplifiée car elle entraîne, comme on l\u0026rsquo;a vu, un mouvement de rotation dans l\u0026rsquo;autre sens qui va freiner l\u0026rsquo;anagyre et le cambrer à la manière d\u0026rsquo;un freinage sur la roue avant d\u0026rsquo;un vélo augmentant ainsi l\u0026rsquo;amplitude des oscillations. L\u0026rsquo;augmentation de l\u0026rsquo;amplitude augmente à son tour la force du sol sur l\u0026rsquo;anagyre qui augmente le cambrage\u0026hellip; C\u0026rsquo;est une rétroaction positive. La rotation autour de $\\vec{k}$ finit par être stoppée et les importantes oscillations autour de $\\vec{i}$ et $\\vec{j}$ relancent ensuite la rotation autour de $\\vec{k}$ dans l\u0026rsquo;autre sens.\nThere should have been a video here but your browser does not seem to support it. Un deuxième type d\u0026rsquo;anagyre est plus simple à comprendre car sa forme elle-même est asymétrique. En effet, la ligne de crête de sa partie basse dessine un S.\nLors d\u0026rsquo;une oscillation d\u0026rsquo;avant en arrière, l\u0026rsquo;anagyre va reposer préférentiellement sur sa partie la plus plate ce qui le fait basculer autour de $\\vec{i}$. Le sol s\u0026rsquo;oppose à ce mouvement faisant tourner l\u0026rsquo;anagyre autour de $\\vec{k}$.\nL\u0026rsquo;oiseau buveur L\u0026rsquo;oiseau buveur estt une star (on le retrouve dans Alien ou les Simpsons par exemple).\nEt c\u0026rsquo;est aussi un bon exercice de thermodynamique (qui a été donné au concours de Centrale il y a un bail).\nExplication par Sixty Symbols en utilisant une caméra thermique :\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/",
	"title": "Science en vrac",
	"tags": [],
	"description": "",
	"content": " En vrac Logique Informatique Mathématiques Physique Pour toute question : "
},
{
	"uri": "https://sciencesilencieuse.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://sciencesilencieuse.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]