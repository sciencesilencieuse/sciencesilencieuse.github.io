[
{
	"uri": "https://sciencesilencieuse.github.io/maths/algebre/",
	"title": "Alg√®bre",
	"tags": [],
	"description": "",
	"content": " Alg√®bre √âquations Jusque l√†, rien de bien folichon\u0026hellip; Mais si on lib√®re la variable de nos polyn√¥mes en lui permettant de se balader dans l\u0026rsquo;espace des complexes, tout devient plus sympa. D√©j√†, plus d\u0026rsquo;histoires de 0, 1 ou 2 solutions pour un polyn√¥me de degr√© 2\u0026hellip; En complexe, un polyn√¥me de degr√© 2 a toujours 2 solutions. C\u0026rsquo;est quand m√™me plus propre comme √ßa. Et d\u0026rsquo;ailleurs, un polyn√¥me de degr√© n a toujours n racines et peut donc se factoriser en n polyn√¥mes de degr√© 1. C\u0026rsquo;est le th√©or√®me fondamental de l\u0026rsquo;alg√®bre.\nnote\nJolie s√©rie de vid√©os sur les complexes.\nIdentit√©s remarquables Alg√®bre de Boole L\u0026rsquo;alg√®bre de Boole permet d\u0026rsquo;alg√©briser la logique. C\u0026rsquo;est le langage naturelle des circuits √©lectroniques √† la base de l\u0026rsquo;informatique.\nAlg√®bre lin√©aire L\u0026rsquo;alg√®bre lin√©aire est un outil fondamental pour la physique et en particulier pour la physique quantique dont c\u0026rsquo;est la base m√™me.\nApplication de l\u0026rsquo;alg√®bre lin√©aire (et de la g√©om√©trie projective) pour d√©celer et corriger le plus efficacement possible les erreurs dans un message.\nCombinatoire / graphes tip\nCours sur les graphes donn√© √† des √©l√®ves de TSI1 en informatique.\nPetit jeu sur un graphe invent√© par Hamilton. Le but est de trouver un chemin hamiltonien consistant √† ne visiter qu\u0026rsquo;une seule fois chaque sommet avant de retourner au sommet initial. Il appela le jeu \u0026ldquo;The Icosian Game\u0026rdquo; car les 20 sommets forment un icosa√®dre r√©gulier. Si le challenge n\u0026rsquo;est pas suffisant, ajoutez √† la ville de d√©part un, deux ou trois sommets avant de commencer √† chercher le chemin.\nTh√©orie des groupes info\nLa th√©orie des groupes d√©coupe et structure les repr√©sentations physiques que l‚Äôon se fait du monde. Elle prescrit le choix m√™me des grandeurs physique. Son importance est ph√©nom√©nale.\nQuelques pages d√©di√©es ici.\nMultiplication de polyn√¥mes et FFT "
},
{
	"uri": "https://sciencesilencieuse.github.io/info/algorithmique/",
	"title": "Algorithmique",
	"tags": [],
	"description": "",
	"content": " Algorithmique G√©n√©ralit√©s La notion de complexit√© permet de classer des algorithmes r√©alisant la m√™me t√¢che comme dans la vid√©o suivante. C\u0026rsquo;est finalement une sorte d\u0026rsquo;√©chelle de raffinement¬†; descendre d\u0026rsquo;un barreau la complexit√© temporelle ou spatiale demande le plus souvent de pousser la r√©flexion plus loin voire de repenser enti√®rement le probl√®me. Et c\u0026rsquo;est ainsi qu\u0026rsquo;une recherche de doublons dans une liste se transforme en la chasse de cycles sur un graphe dans l\u0026rsquo;ultime mouture pour atteindre le Graal ($\\mathcal{O}(n)$ en temps et $\\mathcal{O}(1)$ en espace sans mutation).\nLa notion est aussi au centre d\u0026rsquo;une des plus importantes conjectures des math√©matiques¬†: $\\mathrm{P}\\stackrel{?}{=}\\mathrm{NP}$.\nGrossi√®rement, la conjecture revient √† se demander si l\u0026rsquo;ensemble $\\mathrm{NP}$ des probl√®mes dont les solutions sont facilement v√©rifiables (en temps polynomial) est le m√™me que l\u0026rsquo;ensemble $\\mathrm{P}$ des probl√®mes faciles (qu\u0026rsquo;on peut r√©soudre en temps polynomial).\nPrenons l\u0026rsquo;exemple classique du voyageur de commerce.\nLe probl√®me (dans sa version d√©cisionnelle) demande s\u0026rsquo;il existe un trajet de longueur inf√©rieure √† une valeur $K$ passant une et une seule fois par un ensemble de villes. Pour valider une solution, il suffit donc de v√©rifier que chaque ville est bien l√† une et une seule fois, calculer la distance et la comparer √† $K$. Cela se fait bien s√ªr en temps polynomial rendant le probl√®me $\\mathrm{NP}$.\nPar contre, aucun algorithme connu √† ce jour ne sait aboutir √† la solution en un temps mieux qu\u0026rsquo;exponentiel\u0026hellip;\nOr si $\\mathrm{P=NP}$, cela voudrait dire qu\u0026rsquo;un algorithme polynomial existe bel et bien.\nLa grande majorit√© des chercheurs se rangent dans le camp pessimiste $\\mathrm{P‚â†NP}$.\nLe meilleur algorithme (non quantique) permettant de r√©soudre de mani√®re exacte le probl√®me du voyageur de commerce, l\u0026rsquo;algorithme de Held-Karp, utilise la programmation dynamique dont la logique est pr√©sent√©e dans la vid√©o suivante.\nUne impl√©mentation de l\u0026rsquo;algorithme de Held-Karp est donn√©e ci-dessous. Elle utilise la technique du masquage pour parcourir tous les sous-ensembles de villes visit√©es possibles et pour pouvoir tester rapidement l\u0026rsquo;appartenance ou non d\u0026rsquo;une ville √† ce sous-ensemble.\nCode def tsp_chemin(distance, depart=0): \u0026#34;\u0026#34;\u0026#34; R√©sout le probl√®me du voyageur de commerce (TSP) en utilisant une programmation dynamique bas√©e sur les masques binaires. Cette fonction calcule, pour chaque sous-ensemble de villes et pour chaque ville terminale, le co√ªt minimal pour parcourir l\u0026#39;ensemble des villes sp√©cifi√© par le masque et terminer dans cette ville. Elle permet √©galement de reconstruire le chemin optimal en stockant, pour chaque √©tat, la ville pr√©c√©dente. Param√®tres ---------- distance : list[list[float]] Matrice des distances entre les villes, o√π distance[i][j] repr√©sente la distance allant de la ville i √† la ville j. depart : int, optionnel Indice de la ville de d√©part (par d√©faut 0). Retourne -------- tuple Un tuple (dp, parent) contenant : - dp : list[list[float]] Tableau √† deux dimensions de taille (2^n x n) o√π dp[masque][j] est le co√ªt minimum pour visiter l\u0026#39;ensemble de villes indiqu√© par le masque (repr√©sent√© en binaire) et terminer √† la ville j. - parent : list[list[int]] Tableau √† deux dimensions de taille (2^n x n) permettant de reconstruire le chemin optimal. Pour chaque √©tat (masque, j), parent[masque][j] stocke l\u0026#39;indice de la ville qui pr√©c√®de j dans le chemin optimal. Description ----------- L\u0026#39;algorithme initialise une table dp avec des valeurs infinies, sauf pour l\u0026#39;√©tat correspondant au d√©part. Il parcourt ensuite tous les masques possibles (correspondant aux 2^n sous-ensembles de villes) et, pour chaque ville d√©j√† visit√©e, il tente de mettre √† jour le co√ªt minimal pour atteindre chaque ville non encore visit√©e. La structure parent est utilis√©e pour enregistrer les transitions effectu√©es, ce qui permet ult√©rieurement de reconstruire le chemin optimal. Exemple d\u0026#39;utilisation --------------------- \u0026gt;\u0026gt;\u0026gt; distance = [ ... [0, 10, 15, 20], ... [10, 0, 35, 25], ... [15, 35, 0, 30], ... [20, 25, 30, 0] ... ] \u0026gt;\u0026gt;\u0026gt; dp, parent = tsp_chemin(distance, depart=0) \u0026#34;\u0026#34;\u0026#34; n = len(distance) INF = float(\u0026#39;inf\u0026#39;) dp = [[INF] * n for _ in range(1 \u0026lt;\u0026lt; n)] parent = [[-1] * n for _ in range(1 \u0026lt;\u0026lt; n)] dp[1 \u0026lt;\u0026lt; depart][depart] = 0 for masque in range(1 \u0026lt;\u0026lt; n): # 2^n possibilit√©s (chaque ville est visit√©e ou non visit√©e) for i in range(n): if masque \u0026amp; (1 \u0026lt;\u0026lt; i): # si la ville i est visit√©e for j in range(n): if not (masque \u0026amp; (1 \u0026lt;\u0026lt; j)): # si la ville j n\u0026#39;est pas encore visit√©e nv_masque = masque | (1 \u0026lt;\u0026lt; j) if dp[nv_masque][j] \u0026gt; dp[masque][i] + distance[i][j]: dp[nv_masque][j] = dp[masque][i] + distance[i][j] parent[nv_masque][j] = i return dp, parent Comme √† chaque fois en programmation dynamique, la matrice dp contient toutes les informations n√©cessaires et pour reconstruire le plus petit chemin et obtenir son co√ªt, on peut utiliser le code suivant¬†:\nCode def reconstr_chemin(parent, masque, dernier): chemin = [] while dernier != -1: chemin.append(dernier) temp = parent[masque][dernier] masque = masque \u0026amp; ~(1 \u0026lt;\u0026lt; dernier) dernier = temp chemin.reverse() return chemin dp, parent = tsp(distance, depart=0) masque_final = (1 \u0026lt;\u0026lt; len(distance)) - 1 for i in range(len(distance)): co√ªt_total = dp[masque_final][i] + distance[i][0] if co√ªt_total \u0026lt; co√ªt_min: co√ªt_min = co√ªt_total chemin_min = reconstr_chemin(parent, masque_final, i) chemin_min.append(0) # Fermer le cycle Il est bas√©e sur la relation de r√©currence suivante : $C(S,i) = \\min_{j \\in S,\\ j \\neq i} \\left\\{ C\\left(S \\setminus \\{i\\}, j\\right) + \\mathrm{d}(j,i) \\right\\}$ o√π $C(S,i)$ repr√©sente la plus petite distance pour parcourir chaque ville de l\u0026rsquo;ensemble $S$ et terminer dans la ville $i$ et $d(i,j)$ est la distance entre la ville $i$ et la ville $j$.\nSa complexit√© temporelle est en $\\mathcal{O(n^22^n)}$. En effet, tous les sous-ensembles de villes sont inspect√©s $\\mathcal{O}(2^n)$ et pour chacun, on parcourt toutes les paires ville de d√©part - ville d\u0026rsquo;arriv√©e possibles $\\mathcal{O}(n)\\times\\mathcal{O}(n)$ sans jamais faire autre chose que des op√©rations en temps constant $\\mathcal{O}(1)$ pour chaque paire.\nUn algorithme de programmation dynamique ne se laisse pas approcher facilement, particuli√®rement dans sa forme non-r√©cursive avec sa grosse matrice myst√©rieuse. Mais c\u0026rsquo;est plut√¥t rare que la forme it√©rative d\u0026rsquo;un algorithme vole la vedette √† sa forme r√©cursive tant la r√©cursivit√© sait insuffler un air de magie aux algorithmes en permettant √† de tout petits codes de r√©aliser des t√¢ches √©tonnamment complexes.\nFonction centrale du programme ci-dessus¬†:\ndef Hanoi(n, depart, cible, inter): if n == 1: deplacement(depart,cible) else : Hanoi(n-1,depart,inter,cible) Hanoi(1,depart,cible,inter) Hanoi(n-1,inter,cible,depart) Hanoi(6, 0, 2,1) Quelques algorithmes remarquables Les tris sont omnipr√©sents en informatique et Tim Roughgarden (auteur des g√©niaux Algorithms illuminated) en parle m√™me comme de la ‚Äúm√®re de tous les probl√®mes algorithmiques‚Äù.\nCi-dessous, vous pourrez comparer l\u0026rsquo;efficacit√© de diff√©rents tris par comparaison en les voyant se d√©battre courageusement avec les donn√©es.\nQuicksort Un code Python possible pour la version d√©taill√©e dans la vid√©o¬†:\nCode from random import randint def partition(L, g, d): p = L[g] i = g+1 for j in range(g+1,d+1): if L[j] \u0026lt; p: permute(L,i,j) i += 1 permute(L,g,i-1) return i-1 def tri_rapide_gd(L, g, d): if g \u0026lt; d: pivot = randint(g,d) permute(L,pivot,g) pivot = partition(L, g, d) tri_rapide(L, g, pivot-1) tri_rapide(L, pivot+1, d) return L def tri_rapide(L): g = 0 d = len(L)-1 tri_rapide_gd(L, g, d) La danse hongroise ci-dessous impl√©mente une version de l\u0026rsquo;algorithme sans hasard¬†:\nDans cette version, le premier √©l√©ment est syst√©matiquement choisi comme pivot (chapeau noir) et la partition marche un peu diff√©remment¬†:\nLe chapeau rouge i est d'abord donn√© au dernier √©l√©ment de la partition puis compar√© au pivot p. Si le chapeau rouge est plus petit que le pivot, les deux √©l√©ments √©changent leur place et le chapeau rouge est donn√© au plus proche voisin du c√¥t√© du pivot. Sinon le pivot reste √† sa place et le chapeau rouge est l√† aussi donn√© au voisin imm√©diat le plus proche du pivot. Lorsque le pivot r√©cup√®re le chapeau rouge, il est bien plac√©. On coupe ensuite la liste en deux sur le pivot et on recommence r√©cursivement. Un code Python possible pour cette variante¬†:\nCode def partition(L): p = 0 # pivot (chapeau noir) i = len(L)-1 # chapeau rouge while i != p: if (L[p]-L[i])*(p-i) \u0026lt; 0: L[p],L[i] = L[i],L[p] p,i = i,p i -= (i-p)//abs(i-p) return p def tri_rapide(L): if len(L) \u0026lt;= 1: return L else: p = partition(L) L[:p] = tri_rapide(L[:p]) L[p+1:] = tri_rapide(L[p+1:]) return L Un tirage non al√©atoire du pivot rend l\u0026rsquo;algorithme moins robuste sur certaines entr√©es. Avec le choix syst√©matique du premier √©l√©ment (par exemple), on se retrouve avec une complexit√© en $O(n^2)$ sur des donn√©es d√©j√† tri√©es ou presque. Le tirage al√©atoire rend extr√™mement peu probable un sc√©nario \u0026ldquo;d√©favorable\u0026rdquo;.\nSi en plus de choisir le premier √©l√©ment comme pivot, on ne s\u0026rsquo;occupe m√™me plus d\u0026rsquo;avoir un tri en place (c\u0026rsquo;est-√†-dire qu\u0026rsquo;on s\u0026rsquo;autorise des copies de la liste de d√©part), Quicksort peut s\u0026rsquo;√©crire de mani√®re tr√®s compacte¬†:\ndef triRapide(elements): if len(elements) \u0026lt;= 1: return elements else: pivot = elements[0] plusPetit = triRapide([e for e in elements[1:] if e \u0026lt;= pivot]) plusGrand = triRapide([e for e in elements[1:] if e \u0026gt; pivot]) return plusPetit + [pivot] + plusGrand Mais attention, l\u0026rsquo;√©l√©gance de ce code cache sa l√¢chet√© puisqu\u0026rsquo;en ne rangeant plus en place, il esquive la principale difficult√© d\u0026rsquo;un algorithme de tri. S\u0026rsquo;interdire la cr√©ation de nouvelles listes oblige en effet √† g√©rer astucieusement les permutations au sein de la liste de d√©part. Et si cela donne souvent des codes plus verbeux, il n\u0026rsquo;en sont pas moins beaucoup plus fins.\nAlgorithmes gloutons Les algorithmes gloutons sont tr√®s faciles √† comprendre et √† impl√©menter. Si on devait leur trouver un d√©faut, elle est √† chercher du c√¥t√© de leur correction, pas toujours simple √† d√©montrer\u0026hellip;\nSont pr√©sent√©s ci-dessous deux algorithmes gloutons tr√®s similaires qui permettent de r√©soudre des probl√®mes d\u0026rsquo;optimisation sur des graphes¬†:\nDijkstra cherche le plus court chemin entre deux n≈ìuds. Prim construit l\u0026rsquo;arbre couvrant de poids minimal. Parcours d\u0026rsquo;un graphe Pour inventorier les sommets d\u0026rsquo;un graphe, deux strat√©gies sont possibles :\nle parcours en largeur (BFS), et le parcours en profondeur (DFS). Le code est identique entre les deux algorithmes √† la structure de donn√©e utilis√©e pr√®s¬†! On utilise une file (queue en anglais) pour le parcours en largeur et une pile pour le parcours en profondeur.\nAvec sa logique FIFO (premier arriv√©, premier parti, comme dans une file d\u0026rsquo;attente), BFS explore le graphe de proche en proche.\nfrom collections import deque # deque est une file √† double extr√©mit√©s (double-ended queue) avec la primitive popleft() pour retirer √† gauche comme une file et pop() pour retirer √† droite comme une pile. def BFS(G,depart): \u0026#34;\u0026#34;\u0026#34; graphe G(S,A) repr√©sent√© par une liste d\u0026#39;adjacence impl√©ment√©e par un dictionnaire et depart un sommet de S \u0026#34;\u0026#34;\u0026#34; file = deque() file.append(depart) Vus = {s : False for s in G} Sommets = [] while file: # tant que la file n\u0026#39;est pas vide sommet = file.popleft() # m√©thode de la classe deque permettant de d√©filer if not Vus[sommet]: file += G[sommet] Vus[sommet] = True Sommets.append(sommet) return Sommets Avec sa logique LIFO (dernier, arriv√©e, premier parti, comme pour une pile d\u0026rsquo;assiette pendant la vaisselle), DFS s\u0026rsquo;enfonce dans une branche du graphe jusqu\u0026rsquo;√† son extr√©mit√© puis revient √† la derni√®re jonction rencontr√©e et explore une nouvelle branche, et ainsi de suite.\ndef DFS(G,depart): pile = deque() pile.append(depart) Vus = {s : False for s in G} Sommets = [] while pile: sommet = pile.pop() if not Vus[sommet]: pile += G[sommet] Vus[sommet] = True Sommets.append(sommet) return Sommets Comme la r√©cursivit√© utilise naturellement une pile via la pile d\u0026rsquo;ex√©cution, il y a une √©criture r√©cursive naturelle de la recherche en profondeur.\ndef DFS(G,depart,Vus,Sommets=[]): Sommets.append(depart) Vus[depart] = True for sommet in G[depart]: if not Vus[sommet]: DFS(G,sommet,Vus,Sommets) return Sommets On peut voir Dijkstra comme un parcours de graphe o√π la structure de donn√©e n\u0026rsquo;est plus une file ou une pile mais une file de priorit√© (qui donne en $\\mathcal{O}(1)$ l\u0026rsquo;√©l√©ment stock√© de plus grand score).\nPour impl√©menter une file de priorit√©, on peut utiliser un tas (heap), arbre binaire presque ordonn√© dont la racine est l\u0026rsquo;√©l√©ment prioritaire.\nUne impl√©mentation possible de Dijkstra¬†:\nCode import heapq def Dijkstra(G, depart): \u0026#34;\u0026#34;\u0026#34; Calcule les plus courts chemins dans un graphe pond√©r√© √† partir d\u0026#39;un sommet de d√©part. Param√®tres : - G : dict, repr√©sentation du graphe o√π les cl√©s sont les sommets et les valeurs sont des dictionnaires {voisin: poids}. - depart : le sommet de d√©part. Retourne : - predecesseurs : dict, permettant de reconstruire les plus courts chemins. - distances : dict, distances minimales depuis le sommet de d√©part. \u0026#34;\u0026#34;\u0026#34; if depart not in G: raise ValueError(\u0026#34;Le sommet de d√©part n\u0026#39;est pas pr√©sent dans le graphe\u0026#34;) distances = {sommet: float(\u0026#39;infinity\u0026#39;) for sommet in G} predecesseurs = {sommet: None for sommet in G} distances[depart] = 0 tas = [(0, depart)] while tas: distance_actuelle, sommet_actuel = heapq.heappop(tas) # On ignore l\u0026#39;entr√©e obsol√®te si un meilleur chemin a √©t√© trouv√© if distance_actuelle \u0026gt; distances[sommet_actuel]: continue for voisin, poids in G[sommet_actuel].items(): nouvelle_distance = distance_actuelle + poids if nouvelle_distance \u0026lt; distances[voisin]: distances[voisin] = nouvelle_distance predecesseurs[voisin] = sommet_actuel heapq.heappush(tas, (nouvelle_distance, voisin)) return predecesseurs, distances Algorithme d\u0026rsquo;Euclide Une des plus vieux algorithmes ($\\approx$-300).\nLa suite de Fibonacci est un peu la tarte √† la cr√®me du math√©maticien de s√©rie t√©l√©vis√©e avec ses spirales dans les coquillages\u0026hellip; Mais c\u0026rsquo;est bien elle qui montre le bout de son nez l√† o√π on ne l\u0026rsquo;attendait pas vraiment dans cette vid√©o sur la complexit√© de l\u0026rsquo;algorithme d\u0026rsquo;Euclide.\nOn peut utiliser le pulv√©risateur (algorithme d\u0026rsquo;Euclide √©tendu) ci-dessous. Le code est corrig√© par rapport √† celui de la vid√©o qui pr√©sente une erreur dans la cas de base (on doit retourner b, 0, 1 comme Euclide bien s√ªr et non a,0,1).\nEt le petit programme qui donne la d√©composition en fraction continue¬†:\nAlgorithme X Polyominos et Sudoku ont le droit √† leur algorithme au nom aguicheur¬†: X.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique1/",
	"title": "Calcul des propositions",
	"tags": [],
	"description": "",
	"content": " info\nNotes de lecture du livre La logique pas √† pas de Jacques Duparc que je paraphrase all√©grement.\nCalcul des propositions Syntaxe S√©mantique Preuve Le calcul des propositions (ou calcul propositionnel ou logique des propositions) permet de mod√©liser des raisonnements simples. Son pouvoir expressif est assez limit√© mais c\u0026rsquo;est sur ses fondations que se construisent les logiques plus √©volu√©es.\nSyntaxe Langage Le langage du calcul des propositions est constitu√© de variables propositionnelles $P$, $Q$, $R$,..., de connecteurs logiques $\\neg, \\lor, \\land, \\rightarrow, \\leftrightarrow$ et de parenth√®ses. On d√©signa par $VAR=\\Set{P,Q,R\\dots}$ l\u0026rsquo;ensemble des variables propositionnelles, potentiellement infini.\nLe connectecteur logique $\\neg$ est unaire (ou d'arit√© 1) puisqu'il transforme √† lui seul une formule en une autre formule. C'est le symbole de n√©gation et il se dit non. Les autres connecteurs logiques sont binaires (d'arit√© 2) puisqu'ils lient deux formules en une nouvelle. $\\lor$ est le symbole de disjonction (\"ou\") $\\land$ est le symbole de conjonction (\"et\") $\\rightarrow$ est le symbole d'implication (\"mplique\") $\\leftrightarrow$ est le symbole de double implication ou d'√©quivalence (\"si et seulement si\" ou \"√©quivaut √†\") Le langage du calcul des propositions est alors l\u0026rsquo;ensemble suivant¬†: $\\mathcal{L} = VAR\\cup\\Set{\\neg,\\lor,\\land,\\rightarrow,\\leftrightarrow,(,)}$\nFormules On peut repr√©senter les formules du calcul propositionnel par des arbres dont les feuilles sont des variables propositionnelles et les n≈ìuds sont des connecteurs.\nL'ensemble $\\mathcal{F}$ des formules du calcul propositionnel est le plus petit ensemble d'arbres qui\ncontient chaque arbre r√©duit √† sa racine qui est une variable propositionnelle. chaque fois qu'il contient des formules $\\phi$ et $\\psi$ contient √©galement les formules suivantes\u0026nbsp;: La hauteur d\u0026rsquo;une formule est la longueur de sa plus longue branche.\ntip\nC\u0026rsquo;est une d√©finition par r√©currence (ou inductive)¬†: on a d\u0026rsquo;abord d√©fini les feuilles, cas de base de hauteur 0, puis on a donn√© la recette pour passer d\u0026rsquo;un arbre de hauteur $n$ √† un arbre de hauteur $n+1$.\nUne sous formule d\u0026rsquo;une formule $\\phi$ est un sous-arbre de $\\phi$ dont l\u0026rsquo;un des n≈ìuds de $\\phi$ est la racine.\nLa formule de hauteur 5 ci-dessus contient¬†:\n5 feuilles, sous-formules de hauteur 0 (en vert), 3 sous-formules de hauteur 1 (en rouge), 2 sous-formules de hauteur 2 (en bleu), 1 sous-formule de hauteur 3 (en rose), 1 sous-formule de hauteur 4 (en jaune). Lorsqu\u0026rsquo;une sous-formule appara√Æt plusieurs fois dans une formule, on dit qu\u0026rsquo;elle a plusieurs occurences.\nLin√©arisation d\u0026rsquo;une formule La lin√©arisation d\u0026rsquo;une formule $\\theta$ peut s\u0026rsquo;obtenir par induction¬†:\nune feuille $\\color{#007100}P$ se lin√©arise en $\\color{#007100}P$¬†: $\\theta=\\color{#007100}P$ les lin√©arisations des sous-formules suivantes sont donn√©es respectivement par $\\theta=\\left(\\color{#B51700}{\\neg}\\color{#007100}{\\phi}\\color{#000} \\right)$, $\\theta=\\left(\\color{#007100}{\\phi} \\color{#B51700}{\\lor} \\color{#007100}{\\psi} \\color{#000} \\right)$, $\\theta=\\left( \\color{#007100}{\\phi} \\color{#B51700}{\\land} \\color{#007100}{\\psi} \\color{#000} \\right)$, $\\theta=\\left( \\color{#007100}{\\phi} \\color{#B51700}{\\rightarrow} \\color{#007100}{\\psi} \\color{#000} \\right)$, $\\theta=\\left( \\color{#007100}{\\phi} \\color{#B51700}{\\leftrightarrow} \\color{#007100} \\psi \\color{#000} \\right)$. Exemple :\nLa lin√©arisation de cette formule $\\phi$ donne¬†:\n$$\\displaystyle \\phi = ( \\color{#B51700}{\\neg}\\color{#000} ( \\color{#007100}{P} \\color{#B51700}{\\lor} \\color{#007100}{R}\\color{#000} ) ) \\color{#B51700}{\\land} \\color{#000} (\\color{#007100}{P} \\color{#B51700}{\\lor}\\color{#000} ( ( \\color{#B51700}{\\neg} \\color{#007100}{R}\\color{#000} ) \\color{#B51700}{\\rightarrow} \\color{#000}( ( \\color{#B51700}{\\neg} \\color{#007100}{Q}\\color{#000} ) \\color{#B51700}{\\leftrightarrow} \\color{#007100}{P} \\color{#000} ) )) $$\nnote\nLa syntaxe, c\u0026rsquo;est l\u0026rsquo;articulation des symboles. La s√©mantique, c\u0026rsquo;est ce que √ßa raconte.\nSuite : la s√©mantique "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/quantique/fondations/",
	"title": "Fondations",
	"tags": [],
	"description": "",
	"content": " Fondations de la m√©canique quantique Source¬†: Knee, G. C. ¬´ Isolation of the Conceptual Ingredients of Quantum Theory by Toy Theory Comparison ¬ª, m√©moire de Master of Science, Imperial College London, soutenu le 20 septembre 2010 üåê\nUn jeu d\u0026rsquo;axiomes L\u0026rsquo;interpr√©tation de la m√©canique quantique qui sert encore de r√©f√©rence √† l\u0026rsquo;heure actuelle, l\u0026rsquo;√©cole de Copenhague, repose sur un jeu d\u0026rsquo;axiomes1¬†:\nAxiome 1¬†: √âtat quantique\n√Ä un instant fix√©, l‚Äô√©tat d‚Äôun syst√®me quantique est un vecteur $\\lvert \\psi\\rangle$ dans un espace de Hilbert $\\mathcal{H}$.\nAxiome 2¬†: √âvolution unitaire\nL‚Äô√©volution d‚Äôun syst√®me quantique est toujours unitaire, c‚Äôest-√†-dire d√©crite par un op√©rateur unitaire $\\hat U$ (tel que $\\hat U^\\dagger = \\hat U^{-1}$), parfois appel√© ¬´ porte ¬ª, agissant selon $\\lvert \\psi\\rangle \\;\\longmapsto\\;\\hat U \\lvert \\psi\\rangle$.\nEn particulier, l‚Äô√©volution dans le temps ob√©it √† l‚Äô√©quation de Schr√∂dinger $$\\mathrm{i}\\,\\frac{\\partial}{\\partial t}\\lvert \\psi\\rangle \\;=\\; \\hat H \\lvert \\psi\\rangle ,$$ o√π $\\hat H$ est l‚Äôop√©rateur Hamiltonien.\nAxiome 3¬†: R√®gle de Born\nUne mesure sur un syst√®me quantique est d√©crite par un op√©rateur hermitien $\\hat A$ (i.e. $\\hat A = \\hat A^\\dagger$), appel√© \u0026ldquo;observable\u0026rdquo;.\nCette observable poss√®de une d√©composition spectrale $$\\hat A \\;=\\; \\sum_i \\lambda_i \\,\\lvert e_i\\rangle\\langle e_i\\rvert$$ o√π les valeurs propres $\\lambda_i$ sont les r√©sultats possibles de la mesure et les projecteurs $\\lvert e_i\\rangle\\langle e_i\\rvert$ en sont les op√©rateurs associ√©s.\nSi l‚Äôon mesure l‚Äô√©tat $\\lvert \\psi\\rangle$, la probabilit√© d‚Äôobtenir le r√©sultat $i$ est $$p(i) \\;=\\; \\langle \\psi \\rvert e_i\\rangle\\langle e_i \\lvert \\psi\\rangle$$\nAxiome 4¬†: Postulat de r√©duction du paquet d\u0026rsquo;onde\nApr√®s la mesure, l‚Äô√©tat du syst√®me s‚Äôeffondre sur l‚Äô√©tat propre associ√© au r√©sultat obtenu. Si l‚Äôissue $i$ est observ√©e, l‚Äô√©tat devient $$\\frac{\\lvert e_i\\rangle\\langle e_i \\lvert \\psi\\rangle}{\\sqrt{p(i)}}$$\nAxiome 5¬†: Relation de compl√©tude\nLes projecteurs satisfont la relation $$\\sum_i \\lvert e_i\\rangle\\langle e_i\\rvert \\;=\\; \\mathbb{I}$$ o√π $\\mathbb{I}$ est l‚Äôop√©rateur identit√© sur $\\mathcal{H}$.\nInformation quantique Le bit classique Un bit est la plus petite unit√© d‚Äôinformation¬†: il distingue entre deux possibilit√©s (le r√©sultat d‚Äôun pile ou face, l‚Äô√©tat marche/arr√™t d‚Äôune lampe, le niveau de tension haut/bas dans un circuit, etc.). On les note habituellement ¬´ z√©ro logique ¬ª et ¬´ un logique ¬ª : $0, 1$.\nPour repr√©senter plus d‚Äôinformation, on consid√®re une cha√Æne de bits¬†: avec $n$ bits, on a $2^n$ possibilit√©s (par exemple, pour 3 bits : $000,001, \\dots,111$).\nLe qubit L‚Äôanalogue quantique du bit est le qubit. C‚Äôest le syst√®me quantique non trivial le plus simple. √Ä l‚Äôissue d‚Äôune mesure, il distingue √©galement deux possibilit√©s, correspondant aux √©l√©ments d‚Äôune base orthonormale de l‚Äôespace de Hilbert √† deux dimensions $\\mathcal{H}_2$. On note $\\lvert0\\rangle$ (par exemple spin haut) et $\\lvert1\\rangle$ (spin bas) les deux vecteurs de base. On peut les r√©aliser physiquement de plein de fa√ßons¬†: le premier et deuxi√®me niveau d\u0026rsquo;√©nergie d\u0026rsquo;un atome d\u0026rsquo;hydrog√®ne, le spin haut et le spin bas d\u0026rsquo;une particule de spin $1/2$, la polarisation horizontale et verticale d\u0026rsquo;un photon.\nLa grande diff√©rence avec un bit classique est qu\u0026rsquo;entre deux mesures, le qubit existe comme une superposition des deux possibilit√©s¬†: $$\\lvert\\psi\\rangle = \\alpha\\,\\lvert0\\rangle + \\beta\\,\\lvert1\\rangle$$ avec $\\alpha,\\beta\\in\\mathbb{C}$ et $\\lvert\\alpha\\rvert^2 + \\lvert\\beta\\rvert^2 = 1$.\nUne autre diff√©rence importante est qu\u0026rsquo;on peut tout √† fait mesurer le qubit dans une direction qui ne distingue pas $|0\\rangle$ de $|1\\rangle$.\nOn identifie deux √©tats qui ne diff√®rent que par une phase globale √©tant donn√© qu\u0026rsquo;ils produisent des pr√©dictions physiques identiques. Cela permet de sp√©cifier un qubit par un poids relatif et une phase relative. En ne consid√©rant que des poids √©gaux dans la superposition, on peut √©crire¬†: $$|\\psi\\rangle = \\frac{|0\\rangle+\\mathrm{e}^{\\mathrm{i}\\theta} |1\\rangle}{\\sqrt{2}}$$\nLes cas particuliers $\\theta=0$ et $\\theta=\\pi$ donnent respectivement les deux √©tats suivants¬†: $$|+\\rangle = \\frac{|0\\rangle + |1\\rangle}{\\sqrt{2}}$$ $$|-\\rangle = \\frac{|0\\rangle - |1\\rangle}{\\sqrt{2}}$$\nIl est possible de r√©aliser une mesure distinguant $|+\\rangle$ de $|-\\rangle$ en choisissant l‚Äôobservable $$\\hat X \\;=\\; |+\\rangle\\langle+| \\;-\\; |-\\rangle\\langle-| \\tag{12}$$\nContrairement au cas classique, le r√©sultat d\u0026rsquo;une mesure n\u0026rsquo;est pas toujours enti√®rement d√©termin√© par la mesure elle-m√™me et l\u0026rsquo;√©tat $|\\psi\\rangle$.\nL‚Äôinformation contenue dans un syst√®me binaire classique est quantifi√©e par un bit, c‚Äôest-√†-dire la quantit√© n√©cessaire pour sp√©cifier compl√®tement l‚Äôun des deux √©tats possibles. En revanche, pour un √©tat quantique, les coefficients complexes $\\alpha$ et $\\beta$ peuvent varier de fa√ßon continue, de sorte qu‚Äôil faudrait une quantit√© infinie d‚Äôinformation classique pour d√©crire exactement un qubit.\nMatrice densit√© On d√©finit la matrice densit√© $\\rho_\\psi$ associ√©e √† un √©tat pur $\\lvert\\psi\\rangle$ par¬†: $$ \\rho_\\psi = \\lvert\\psi\\rangle\\langle\\psi\\rvert$$\nSi $\\lvert\\psi\\rangle = \\alpha,\\lvert0\\rangle + \\beta,\\lvert1\\rangle$, alors $$\\rho_\\psi = \\begin{pmatrix} |\\alpha|^2 \u0026amp; \\alpha^*\\beta\\\\ \\beta^* \\alpha \u0026amp; |\\beta|^2 \\end{pmatrix}$$\nLa condition de normalisation $\\langle\\psi|\\psi\\rangle=1$ entra√Æne que la trace de la matrice densit√© vaut 1 ($\\mathrm{Tr}\\,\\rho_\\psi = 1$).\nFid√©lit√© Pour deux √©tats purs $\\lvert\\psi\\rangle$ et$ \\lvert\\chi\\rangle$, la fid√©lit√© est d√©finie par $F = \\bigl|\\langle\\psi|\\chi\\rangle\\bigr|^2$.\nOu pour deux √©tats dont les matrices de densit√© sont $\\rho$ et $\\sigma$, $F = \\mathrm{Tr}\\,\\sqrt{\\rho}\\,\\sqrt{\\sigma}$.\nOrthogonalit√© Deux √©tats $\\lvert\\psi\\rangle$ et $\\lvert\\chi\\rangle$ sont dits orthogonaux si $\\langle\\psi|\\chi\\rangle = 0$. Les deux √©tats ont alors une fid√©lit√© nulle.\n√âtat pur et √©tat mixte Un √©tat pur est un vecteur dans l‚Äôespace de Hilbert. Toute combinaison lin√©aire normalis√©e des vecteurs de base est un √©tat pur¬†: $$|\\psi\\rangle = \\sum_i\\alpha_i|i\\rangle$$ Les $\\alpha_i\\in\\mathbb{C}$ sont appel√©s poids.\nToute combinaison lin√©aire d\u0026rsquo;√©tats purs est encore un √©tat pur.\nUn √©tat mixte est une combinaison convexe de matrices densit√©¬†: $$ \\rho = \\sum_i p_i\\,\\rho_i $$ o√π chaque $\\rho_i$ repr√©sente un √©tat pur, et les $p_i$ sont des probabilit√©s telles que $\\sum_i p_i=1$.\nIl existe de nombreuses d√©compositions convexes d‚Äôun √©tat mixte : par exemple, l‚Äô√©tat mixte maximal $$ \\rho \\;=\\;\\tfrac12\\bigl(|0\\rangle\\langle0| + |1\\rangle\\langle1|\\bigr) \\;=\\;\\tfrac12\\bigl(|+\\rangle\\langle+| + |-\\rangle\\langle-|\\bigr) \\;=\\;\\tfrac{\\mathbb{I}}2 $$ Un √©tat mixte ne peut pas s‚Äô√©crire comme une somme d‚Äô√©tats de base √† poids complexes.\nLa diff√©rence s√©mantique entre un √©tat ¬´ pur ¬ª et un √©tat ¬´ mixte ¬ª tient au fait que l‚Äôincertitude pour le premier est une incertitude purement quantique, tandis que pour le second une part d‚Äôincertitude classique est introduite. Un √©tat mixte appara√Æt si l‚Äôon pr√©pare un √©tat quantique en fonction du r√©sultat d‚Äôun pile ou face classique, en assignant un certain √©tat pour pile et un autre pour face. On peut aussi interpr√©ter cette incertitude classique comme une forme d‚Äôerreur exp√©rimentale.\nSph√®re de Bloch Il existe une repr√©sentation g√©om√©trique √©l√©gante des qubits, qui facilite grandement la visualisation des op√©rations de mesure, d‚Äô√©volution et de similarit√© des √©tats dans l‚Äôespace de Hilbert.\nDans la repr√©sentation par matrice densit√© d‚Äôun √©tat pur, on compte √† premi√®re vue deux coefficients complexes, soit quatre nombres r√©els. La condition de normalisation (trace = 1) en √©limine un. Il reste donc trois degr√©s de libert√© r√©els pour un √©tat mixte. Et comme on a vu que deux √©tats purs ne se diff√©rentiant que par une phase globale sont en fait identiques, il ne reste que deux degr√©s de libert√© pour un √©tat pur2. Cela permet de repr√©senter les √©tats purs sur la surface d‚Äôune sph√®re et les √©tats mixtes dans son int√©rieur.\nOn d√©finit le vecteur de Bloch $\\vec{r} = [r_x,\\,r_y,\\,r_z]$ associ√© √† un √©tat $\\rho$ par $$ \\rho = \\frac12\\bigl(\\mathbb{I} + \\vec{r}\\!\\cdot\\!\\vec{\\sigma}\\bigr) =\\frac12 \\begin{pmatrix} 1 + r_z \u0026amp; r_x - \\mathrm{i}\\,r_y \\\\ r_x + \\mathrm{i}\\,r_y \u0026amp; 1 - r_z \\end{pmatrix} $$ o√π $\\boldsymbol{\\sigma} = [\\hat X,\\,\\hat Y,\\,\\hat Z]$ est le triplet de matrices de Pauli.\nGr√¢ce √† ce vecteur, tout qubit peut √™tre visualis√© comme un point de la sph√®re de Bloch¬†:\nles √©tats purs correspondent aux points de la surface\u0026nbsp;; les √©tats mixtes se situent √† l‚Äôint√©rieur\u0026nbsp;; deux √©tats orthogonaux apparaissent comme des points diam√©tralement oppos√©s. Par construction, les vecteurs propres des matrices de Pauli occupent trois paires de points antipodaux sur la sph√®re¬†: $$\\hat X|+\\rangle = |+\\rangle,\\quad \\hat X|-\\rangle = -|-\\rangle$$ $$\\hat Y|{+}\\mathrm{i}\\rangle = |{+}\\mathrm{i}\\rangle=\\frac{|0\\rangle+\\mathrm{i}|1\\rangle}{\\sqrt{2}},\\quad \\hat Y|{-}\\mathrm{i}\\rangle = -|{-}\\mathrm{i}\\rangle=\\frac{|0\\rangle-\\mathrm{i}|1\\rangle}{\\sqrt{2}}$$ $$\\hat Z|0\\rangle = |0\\rangle,\\quad \\hat Z|1\\rangle = -|1\\rangle$$\nChaque paire de vecteurs propres est une base de l\u0026rsquo;espace de Hilbert qui d√©finit un axe orthogonal dans la sph√®re de Bloch.\nAttention, √ßa ne veut pas dire que ces trois bases sont orthogonales (√ßa n\u0026rsquo;aurait pas de sens puisque chacune est compl√®te)¬†; elles sont mutuellement non biais√©es3 (c\u0026rsquo;est √ßa que traduit g√©om√©triquement les axes orthogonaux de la sph√®re de Bloch).\nPar combinaison convexe, on fait passer les √©tats purs situ√©s sur la surface de la sph√®re de Bloch vers des √©tats mixtes √† l‚Äôint√©rieur de celle‚Äëci. L‚Äô√©tat maximalement mixte, $\\mathbb{I}/2$, se situe au centre de la sph√®re de Bloch.\nPaire de Qubits Lorsqu‚Äôil faut d√©crire plusieurs syst√®mes quantiques simultan√©ment, on a recours au produit tensoriel‚ÄØ$\\otimes$. Supposons, par exemple, qu‚Äôon souhaite mod√©liser √† la fois un qubit situ√© √† La Rochelle et un autre √† Paris‚ÄØ; on √©crit alors $$ |\\psi\\rangle_{\\text{La Rochelle}}\\;\\otimes\\;|\\phi\\rangle_{\\text{Paris}} $$ o√π $|\\psi\\rangle$ d√©signe l‚Äô√©tat du premier qubit et $|\\phi\\rangle$ celui du second. Une fois qu‚Äôun ordre conventionnel est adopt√©, il est d‚Äôusage de supprimer les indices g√©ographiques (et m√™me de fusionner les deux kets en un seul)‚ÄØ: $$ |\\psi\\rangle_{\\text{La Rochelle}}\\otimes|\\phi\\rangle_{\\text{Paris}} \\;=\\; |\\psi\\rangle\\otimes|\\phi\\rangle \\;=\\; |\\psi\\phi\\rangle $$\nCette expression est appel√©e un √©tat produit¬†; elle vit dans l‚Äôespace de Hilbert tensoriel $H_1\\otimes H_2$. Ainsi, si $|\\psi\\rangle\\in H_1$ poss√®de la base $\\{|L\\rangle,|R\\rangle\\}$ et $|\\phi\\rangle\\in H_2$ la base $\\{|P\\rangle,|S\\rangle\\}$, alors $|\\psi\\rangle\\otimes|\\phi\\rangle$ appartient √† $H_1\\otimes H_2$ et la base combin√©e devient $\\{|LP\\rangle,|LS\\rangle,|RP\\rangle,|RS\\rangle\\}$.\nDe m√™me qu‚Äôun qubit unique peut passer par une porte (unitaire), un √©tat produit peut √™tre soumis √† l‚Äôaction d‚Äôun op√©rateur tensoriel $$ \\bigl(\\hat A_{\\text{La Rochelle}}\\otimes\\hat B_{\\text{Paris}}\\bigr)\\, |\\psi\\rangle\\otimes|\\phi\\rangle $$ Si l‚Äôon prend $\\hat A=\\mathbb{I}$, on agit uniquement sur le qubit de Paris‚ÄØ; si c‚Äôest $\\hat B=\\mathbb{I}$, on agit seulement sur celui de La Rochelle.\nLa base computationnelle $\\{|0\\rangle,|1\\rangle\\}$ d‚Äôun qubit se prolonge naturellement en une base de l‚Äôespace bipartite¬†: $\\{|00\\rangle,\\;|01\\rangle,\\;|10\\rangle,\\;|11\\rangle\\}$. Un √©tat g√©n√©ral de deux qubits est donc une combinaison lin√©aire de ces quatre kets.\nAu cours de l‚Äô√©volution, un tel √©tat bipartite peut devenir un √©tat intriqu√© comme, par exemple, l\u0026rsquo;√©tat de Bell¬†: $$ |{\\rm Bell}\\rangle \\frac{1}{\\sqrt{2}}\\bigl(|00\\rangle+|11\\rangle\\bigr) $$ On remarque que l\u0026rsquo;√©tat de Bell ne contient pas les ¬´‚ÄØtermes crois√©s‚ÄØ¬ª $|01\\rangle$ et $|10\\rangle$ (leurs poids sont nuls). Il est donc impossible de l‚Äô√©crire comme produit de deux qubits s√©par√©s¬†! Les √©tats intriqu√©s comme celui‚Äëci jouent un r√¥le central dans la plupart des protocoles de l‚Äôinformation quantique.\nNon-clonage quantique Maintenant qu‚Äôun espace produit bipartite a √©t√© introduit, on peut l‚Äôutiliser pour illustrer certains traits particuli√®rement surprenants de la m√©canique quantique. Supposons que l‚Äôon veuille copier ‚Äì‚ÄØcloner‚ÄØ‚Äì un √©tat quantique. Un cloneur universel op√®re simultan√©ment sur un qubit de donn√©es $|\\psi\\rangle$ et sur un qubit ancillaire initialis√© dans un √©tat fixe (par exemple l‚Äô√©tat ¬´‚ÄØvierge‚ÄØ¬ª |0\\rangle), de sorte que l‚Äô√©tat final soit le produit de deux exemplaires du qubit de donn√©es¬†:\n$$U\\,|\\psi\\rangle|0\\rangle \\;=\\; |\\psi\\rangle|\\psi\\rangle$$\net, par le m√™me proc√©d√©, pour tout autre √©tat $|\\phi\\rangle$¬†:\n$$ U\\,|\\phi\\rangle|0\\rangle \\;=\\; |\\phi\\rangle|\\phi\\rangle $$\nEn prenant le produit scalaire des deux relations pr√©c√©dentes, on obtient¬†:\n$$ \\langle\\psi|\\phi\\rangle \\;=\\; \\bigl(\\langle\\psi|\\phi\\rangle\\bigr)^{2} $$\n√©quation qui n‚Äôadmet de solution que si les √©tats sont identiques ($\\langle\\psi|\\phi\\rangle = 1$) ou orthogonaux ($\\langle\\psi|\\phi\\rangle = 0$). Il en d√©coule qu‚Äôun cloneur universel ‚Äì capable de copier n‚Äôimporte quel √©tat ‚Äì est impossible.\nEn d‚Äôautres termes, la seule proc√©dure qui conserve les produits scalaires (et donc la structure de l‚Äôespace de Hilbert) ne peut cloner qu‚Äôun √©tat unique ou, au mieux, un ensemble d‚Äô√©tats orthogonaux. Le point crucial est qu‚Äôun processus de clonage universel violerait l‚Äô√©volution unitaire prescrite par l‚ÄôAxiome‚ÄØ2¬†: il ne pr√©serverait pas les produits scalaires et ne pourrait donc √™tre repr√©sent√© par un op√©rateur unitaire.\nPour cloner un √©tat, l‚Äôappareil devrait acc√©der √† toute l‚Äôinformation qui le caract√©rise sans le perturber. Les √©tats classiques satisfont cette condition¬†: ils peuvent √™tre enti√®rement d√©termin√©s par une quantit√© finie d‚Äôinformations et restent intacts lors d‚Äôune mesure, de sorte qu‚Äôil suffit de les identifier puis de pr√©parer un second syst√®me identique. En revanche, un √©tat quantique ne peut √™tre d√©termin√© qu‚Äôau prix d‚Äôun nombre infini de mesures et, surtout, le processus de mesure le d√©truit. Cette impossibilit√© fondamentale d‚Äôextraire toutes les informations sans perturber le syst√®me explique pourquoi un √©tat quantique arbitraire ne peut jamais √™tre clon√©.\nT√©l√©portation quantique D\u0026rsquo;une source √† l\u0026rsquo;autre, l\u0026rsquo;ordre et le nombre d\u0026rsquo;axiomes varient.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLa normalisation $|\\alpha|^2+|\\beta|^2=1$ contraint de vivre √† la surface d\u0026rsquo;une sph√®re $S^3$ dans $\\mathbb{R}^4$. Multiplier par une phase globale $\\mathrm{e}^{\\mathrm{i}\\gamma}$ fait parcourir un cercle $S_1$ dans cette sph√®re. Puisqu\u0026rsquo;on identifie tous ces points, on quotiente $S^3$ par $S^1$. Et $S^3/S^1\\simeq S^2$, la sph√®re ordinaire.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nDeux bases $\\{e_i\\}$ et $\\{f_i\\}$ sont mutuellement non biais√©es si, pour tout $i,j$¬†: $\\bigl|\\langle e_i | f_j\\rangle\\bigr|^2 \\;=\\;\\text{cste}$.\nPour des bases orthonormales, on peut d√©montrer que la constante vaut $\\frac 1 d$ o√π $d=\\dim\\mathcal H$ gr√¢ce √† la relation suivante $\\langle e_i|\\Bigl(\\sum_j |f_j\\rangle\\langle f_j|\\Bigr)|e_i\\rangle \\;=\\;\\sum_j \\bigl|\\langle f_j|e_i\\rangle\\bigr|^2 \\;=\\;\\langle e_i|\\mathbb I|e_i\\rangle\\;=\\;1$ o√π on a utilis√© la compl√©tude de la base $\\{f_i\\}$, $\\sum_j |f_j\\rangle\\langle f_j| = \\mathbb I$, et la normalit√© des √©l√©ments $\\{e_i\\}$. Si tous les termes $\\bigl|\\langle f_j|e_i\\rangle\\bigr|^2$ sont en plus √©gaux (d√©finition de bases non biais√©es), chacun vaut forc√©ment $\\frac1d$ (puisqu\u0026rsquo;il y a autant de vecteurs dans chaque base que de dimensions).\nLa cons√©quence g√©om√©trique est que des √©tats mutuellement non biais√©s sont √©quidistances sur la sph√®re de Bloch puisque les angles entre eux sont identiques.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/geometrie/",
	"title": "G√©om√©trie",
	"tags": [],
	"description": "",
	"content": " G√©om√©trie La g√©om√©trie est avec l\u0026rsquo;arithm√©tique une des plus vieilles branches des math√©matiques (l\u0026rsquo;arithm√©tique pour tenir les comptes et la g√©om√©trie pour mesurer les terres).\nG√©om√©trie et physique sont intimement li√©es. Le d√©veloppement de la g√©om√©trie a en partie √©t√© pouss√© par les questionnements des astronomes (sur la position des astres, leur mouvement, leur taille, leur distance, etc.) et des ing√©nieurs (pour construire des trucs qui tiennent debout). Et une explication g√©om√©trique des lois fondamentales de l\u0026rsquo;univers attire toujours certains chercheurs, plus de 2000 ans apr√®s Platon.\nTriangle : Pythagore, Thal√®s, droite d\u0026rsquo;Euler Cercle et triangle : trigonom√©trie, angle inscrit Aires et Volumes Curse of dimensionality Poursuites "
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/algebre/groupes/",
	"title": "Groupes",
	"tags": [],
	"description": "",
	"content": " Miettes de th√©orie des groupes pour physicien Trois sources majoritairement plagi√©es pour les pages qui suivent¬†:\nle pr√©cieux livre de Wu-Ki Tung ¬´Group Theory in Physics¬ª le fabuleux cours en ligne de Bertrand Delamotte : ¬´Un soup√ßon de th√©orie des groupes: groupe des rotations et groupe de Poincar√©¬ª les tr√®s clairs et fournis cours en ligne de Jean-Bernard Zuber : ¬´Invariances en physique et th√©orie des groupes¬ª et ¬´Introduction √† la th√©orie des groupes et de leurs repr√©sentations¬ª La th√©orie des groupes d√©coupe et structure les repr√©sentations physiques que l‚Äôon se fait du monde. Elle prescrit le choix m√™me des grandeurs physique. Son importance est ph√©nom√©nale.\nMais il faut abandonner tout espoir de trouver ici rigueur math√©matique ou exhaustivit√©. Vous serez en √©change combl√©s de graves approximations, voir errances, √©maill√©es d‚Äô¬´inclart√©s¬ª. Je tente seulement de faire appara√Ætre, en paraphrasant souvent mal mes source, les angles de ce qu‚Äôil a √©t√© pour moi important de comprendre. Les deux premiers chapitres d√©crivent les r√©sultats g√©n√©raux obtenus sur les groupes discrets. Leur r√©daction s‚Äôappuie grandement sur le Wu-Ki Tung, tr√®s tr√®s grandement\u0026hellip;\nLes deux premiers chapitres d√©crivent les r√©sultats g√©n√©raux obtenus sur les groupes discrets. Leur r√©daction s‚Äôappuie grandement sur le Wu-Ki Tung, tr√®s tr√®s grandement\u0026hellip;\nLes groupes discrets Leurs repr√©sentations "
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/algebre/groupes/groupe1/",
	"title": "Groupes discrets",
	"tags": [],
	"description": "",
	"content": " Les groupes Retour sommaire\nUn groupe est un club priv√©, un entre-soi monarchique d‚Äô√©l√©ments se reproduisant entre eux\u0026hellip;\nG√©n√©ralit√©s D√©finition d\u0026rsquo;un groupe Comme son nom l‚Äôindique, un groupe d√©signe un ensemble d‚Äô√©l√©ments. Mais pour que cet ensemble soit promu groupe, on doit le munir d‚Äôune loi de composition interne (un truc qui dit comment les √©l√©ments jouent entre eux et rien qu‚Äôentre eux). Quand on compose un √©l√©ment du groupe avec un autre √©l√©ment du groupe, on obtient encore un √©l√©ment du groupe. C‚Äôest surtout √ßa un groupe¬†!\nTechniquement, un groupe $G=\\{a,b,c\\}$ est bien un groupe si\u0026nbsp;: $\\forall (a,b) \\in G^2,\\ a\\cdot b \\in G$ La loi de composition interne est associative, c‚Äôest-√†-dire\n$ (a\\cdot b)\\cdot c = a\\cdot (b\\cdot c) $ $G$ contient un √©l√©ment neutre $e$ tel que\n$\\forall a \\in G,\\ a\\cdot e = a$ Pour chaque $a \\in G,$ il existe un √©l√©ment sym√©trique $a^{-1}$ tel que $a\\cdot a^{-1} = e$ Remarque : on dira indiff√©remment \u0026ldquo;loi de composition interne\u0026rdquo; ou \u0026ldquo;loi de multiplication interne\u0026rdquo;.\nExemple de groupe ultra simple\u0026nbsp;: $\\boldsymbol{C_2} = \\{1,-1\\}$ avec la multiplication ordinaire comme loi de composition interne. En effet, $1\\times1,\\; 1\\times(-1),\\; (-1)\\times(-1)$ font tous partie de $\\boldsymbol{C_2}$ puisque le r√©sultat est toujours 1 ou -1. La permutation entre deux √©l√©ments associ√©e √† l‚Äôidentit√© forme un groupe en tout point similaire, c‚Äôest aussi $\\boldsymbol{C_2}$. De la m√™me fa√ßon, l‚Äôidentit√© et l‚Äôinversion spatiale (parit√©), i.e. $x \\mapsto -x,$ forment encore $\\boldsymbol{C_2}$. Un m√™me groupe peut donc √™tre d√©crit diff√©remment ! Groupes et sym√©tries Groupes et sym√©tries sont fortement li√©s, ce qui explique l‚Äôimportance de leur √©tude en physique, √©tant donn√© que les sym√©tries sont au centre de notre compr√©hension physique du monde.\nImaginons un syst√®me $\\mathcal{S}$ laiss√© invariant par deux sym√©tries diff√©rentes $A$ et $B$. La loi de composition interne $A\\cdot B$ correspond, dans le cas des sym√©tries, √† l‚Äôapplication successive de $A$ et de $B$ sur le syst√®me. Et l‚Äôapplication successive des sym√©tries sur le syst√®me continue n√©cessairement √† le laisser invariant, donc $A\\cdot B$ est aussi une sym√©trie du syst√®me. La loi de composition interne √©tant interpr√©t√©e comme une succession d‚Äôop√©rations de sym√©trie, l‚Äôassociativit√© en d√©coule : on aura forc√©ment $A\\cdot (B\\cdot C) = (A\\cdot B)\\cdot C$ (en gardant l‚Äôordre).\nEnfin, laisser $\\mathcal{S}$ tel quel constitue l‚Äô√©l√©ment neutre de toutes les sym√©tries et chaque action d‚Äôune sym√©trie peut √™tre invers√©e.\nLes op√©rations de sym√©trie sur un syst√®me forment donc toujours un groupe !\nVocabulaire L‚Äôordre ou le cardinal d‚Äôun groupe est son nombre d‚Äô√©l√©ments. Un groupe est dit ab√©lien lorsque la loi de composition est commutative, c‚Äôest-√†-dire $ab = ba, \\forall a,b \\in G$. $\\boldsymbol{C_2}$ est √† l‚Äô√©vidence ab√©lien.\nExemple : Le plus petit groupe non ab√©lien est le groupe des sym√©tries du triangle √©quilat√©ral, appel√© groupe di√©dral $\\boldsymbol{D_3}$. Il y a 6 transformations qui laissent le triangle invariant : l‚Äôidentit√© ($e$), les rotations de $\\frac{2\\pi}{3}$ et $\\frac{4\\pi}{3}$ (appel√©es $R_1$ et $R_2$), et les r√©flexions par rapport aux hauteurs (not√©es $S_A, S_B, S_C$). Elles sont toutes inversibles et la table de multiplication entre ces transformations finit de prouver qu‚Äôil s‚Äôagit bien d‚Äôun groupe.\nLe caract√®re non ab√©lien est √©vident si on compare, par exemple, $R_1 S_A$ (qui am√®ne $A$ en $C$, $B$ en $B$ et $C$ en $A$, en appliquant les transformations de la droite vers la gauche) et $S_A R_1$ (qui am√®ne $A$ en $B$, $B$ en $A$ et $C$ en $C$).\nLa table de multiplication d‚Äôun groupe permet de savoir comment chacun des √©l√©ments interagit et permet donc de le d√©crire enti√®rement. Pour $\\boldsymbol{D_3}$, la table de multiplication est la suivante¬†:\nDeux grandes dynasties de groupe se partagent le royaume¬†:\nles groupes cycliques qui peuvent d√©composer tous les groupes ab√©liens. les groupes sym√©triques auxquels peuvent se rapporter tout groupe d‚Äôordre fini. Groupes cycliques et groupes sym√©triques Groupe cyclique Le groupe cyclique $\\boldsymbol{C_n}$ (dont on a d√©j√† c√¥toy√© un membre avec $\\boldsymbol{C_2}$) a la structure g√©n√©rale $\\{e,a,a^2,\\ldots,a^{n-1};a^n=e\\}$ avec $n$ un entier positif quelconque. On le note aussi $\u0026lt;a\u0026gt;$.\na g√©n√©rant tous les √©l√©ments du groupe est appel√©\u0026hellip; g√©n√©rateur du groupe.\nOn appelle p√©riode ou ordre d‚Äôun √©l√©ment $a$ d‚Äôun groupe le plus petit entier positif $m$ tel que $a^m=e$. Si $m$ n‚Äôexiste pas, $a$ est dit d‚Äôordre infini. Le g√©n√©rateur $a$ de $\\boldsymbol{C_n}$ est donc, par d√©finition, de p√©riode (ou d‚Äôordre) $n$.\nL‚Äôordre d‚Äôun groupe cyclique co√Øncide avec l‚Äôordre de son g√©n√©rateur (ce qui explique l‚Äôutilisation du termes ordre au lieu de p√©riode pour un √©l√©ment).\nTous les groupes cycliques sont ab√©liens (car $a^i a^j=a^j a^i=a^{i+j}$).\nLes racines n-i√®me de l‚Äôunit√© $\\{\\exp{^{i2\\pi/n}}\\}$ munies de la r√®gle usuelle de multiplication sont l‚Äôexemple concret le plus direct d‚Äôun groupe cyclique. Les lignes et colonnes de la table de multiplication de ces groupes sont en permutation circulaire (telle que l‚Äôordre reste le m√™me) les unes par rapport aux autres, d‚Äôo√π son nom.\nTable de multiplication de $\\boldsymbol{C_2}$\u0026nbsp;: Table de multiplication de $\\boldsymbol{C_3}$\u0026nbsp;: Groupe sym√©trique Le groupe sym√©trique $\\boldsymbol{S_n}$ est form√© de toutes les permutations possibles entre $n$ √©l√©ments diff√©rents et est donc d‚Äôordre $n!$.\nOn peut repr√©senter de mani√®re g√©n√©rale les permutations de $n$ √©l√©ments sur deux lignes¬†:\n$p=\\left(\\begin{array}{ccccc} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; \\cdots \u0026amp; n \\\\ p_1 \u0026amp; p_2 \u0026amp; p_3 \u0026amp; \\cdots \u0026amp; p_n \\end{array}\\right)$\nUne permutation suivie d‚Äôune seconde en forme bien s√ªr une troisi√®me, ce qui d√©finit la loi de composition du groupe. L‚Äôidentit√© correspond √† l‚Äôabsence de permutation¬†:\n$ e=\\left(\\begin{array}{lllll} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; \\cdots \u0026amp; n \\\\ 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; \\cdots \u0026amp; n \\end{array}\\right) $\nEt l‚Äôinverse de p est logiquement¬†:\n$ p^{-1}=\\left(\\begin{array}{ccccc} p_1 \u0026amp; p_2 \u0026amp; p_3 \u0026amp; \\cdots \u0026amp; p_n \\\\ 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; \\cdots \u0026amp; n \\end{array}\\right) $\nExemple : $\\boldsymbol{S_3}$ est d‚Äôordre $3!=6$. Les 6 permutations sont :\n$$ \\begin{aligned} \u0026amp; \\left(\\begin{array}{lll} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 2 \u0026amp; 3 \\end{array}\\right) \\\\ \u0026amp; \\left(\\begin{array}{lll} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 1 \u0026amp; 3 \\end{array}\\right),\\left(\\begin{array}{lll} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 1 \u0026amp; 3 \u0026amp; 2 \\end{array}\\right),\\left(\\begin{array}{lll} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 2 \u0026amp; 1 \\end{array}\\right) \\\\ \u0026amp; \\left(\\begin{array}{lll} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 2 \u0026amp; 3 \u0026amp; 1 \\end{array}\\right),\\left(\\begin{array}{lll} 1 \u0026amp; 2 \u0026amp; 3 \\\\ 3 \u0026amp; 1 \u0026amp; 2 \\end{array}\\right) \\end{aligned} $$\nLe premier √©l√©ment n‚Äôest autre que $e$ (rien n‚Äôa boug√©). Les trois √©l√©ments suivants sont obtenus en permutant deux √©l√©ments du trio et en laissant le troisi√®me tranquille. Les deux derniers √©l√©ments sont obtenus en permutant circulairement les trois √©l√©ments.\nPour une √©criture plus compacte, on d√©compose les permutations en 1-cycle (pas de permutations), 2-cycle (permutations 2 √† 2), 3-cycle (permutations circulaires √† 3 √©l√©ments), etc.\nCela donne pour les 6 permutations du groupe $\\boldsymbol{S_3}$\u0026nbsp;:\n$$\\begin{aligned} \u0026amp; e=(1)(2)(3) \\\\ \u0026amp; (12)(3),(1)(23),(2)(31), \\\\ \u0026amp; (123),(321) \\end{aligned} $$\nL‚Äôordre d‚Äô√©criture est indiff√©rent et il est d‚Äôusage d‚Äôomettre les 1-cycle ou √©l√©ments non permut√©s ($(12)(3)=(12)$).\nLorsqu‚Äôon multiplie deux permutations, on part de la permutation la plus √† droite et on regarde o√π arrive successivement chacun des √©l√©ments.\nExemple : $(23)\\cdot(13)=\\,?$\n$1\\rightarrow\\,?$ : $(13)$ est tel que $1\\rightarrow 3$ et $(23)$ est tel que $3\\rightarrow 2$, donc 1 est envoy√© sur 2 ($1\\rightarrow 2$) par le produit des permutations. $2\\rightarrow\\,?$ : $(13)$ est tel que $2\\rightarrow 2$ et $(23)$ est tel que $2\\rightarrow 3$, donc 2 est envoy√© sur 3 ($2\\rightarrow 3$) par le produit des permutations. $3\\rightarrow\\,?$ : $(13)$ est tel que $3\\rightarrow 1$ et $(23)$ est tel que $1\\rightarrow 1$, donc 3 est envoy√© sur 1 ($3\\rightarrow 1$) par le produit des permutations. On obtient bien la permutation cyclique $(123)$ dans laquelle $1\\rightarrow 2\\rightarrow 3\\rightarrow 1$. Donc $(23)\\cdot (13) = (123)$.\nAutre exemple : $(321)\\cdot (12) = (1)(23)(2) = (23)$\nSous-groupes, morphismes, classes et groupes quotients Sous-groupes Un sous-ensemble de $G$ qui forme un groupe avec la m√™me loi de multiplication est un sous-groupe de $G$. Tout √©l√©ment $a$ diff√©rent de $e$ d‚Äôun groupe $G$ d‚Äôordre fini forme un sous-groupe cyclique de $G$.\nPreuve : $a\\cdot a=a^2$ est dans $G$ par propri√©t√© des groupes et vaut soit $e$ soit un √©l√©ment diff√©rent de $a$ (car seul $a\\cdot e$ donne $a$).\nDe m√™me, si $a^2‚â†e$, alors $a^2\\cdot a=a^3$ vaut soit $e$, soit un √©l√©ment diff√©rent √† la fois de $a$ et $a^2$ puisqu‚Äôils sont tous deux diff√©rents de $e$. En continuant ainsi, on obtient un ensemble ${a,a^2,a^3,a^4,a^5,\\ldots}$ s‚Äôarr√™tant pour $a^p$ valant $e$ (et cela arrive n√©cessairement puisque le groupe $G$ est fini). On obtient alors un groupe cyclique d‚Äôordre $p$.\nMorphismes Un homomorphisme entre un groupe $G$ et un groupe $G^{\\prime}$ est une application envoyant les √©l√©ments de $G$ vers $G^{\\prime}$ tout en pr√©servant la loi de composition\u0026nbsp;: si $g_i \\in G \\mapsto g_i^{\\prime} \\in G^{\\prime}$ et $g_1g_2=g_3$, alors $g_1^{\\prime} g_2^{\\prime}=g_3^{\\prime}$.\nQuand l‚Äôapplication est bijective, un √©l√©ment pour un √©l√©ment, on parle d‚Äôisomorphisme (on le note symboliquement $G \\simeq G^{\\prime}$).\nSi on appelle $f$ l‚Äôhomomorphisme de $G$ vers $G^{\\prime}$, on a :\n$f(g_1)f(g_2) = f(g_1g_2)$ par d√©finition, $f(g)f(e) = f(g)$ donc $f(e) = e'$ (l‚Äô√©l√©ment neutre est pr√©serv√©), $f(g^{-1}) = f^{-1}(g)$ car $f(g^{-1})f(g) = f(e) = e'$. Cela montre que¬†:\nL‚Äôimage de l'homomorphisme $f$, not√©e $\\mathrm{Im}(f) \\equiv f(G) \\equiv \\{ f(g) ; g \\in G \\}$, est un sous-groupe de $G'$. Preuve : $e' \\in \\mathrm{Im}(f)$, Pour tout $g_1, g_2 \\in G$, $f(g_1)f(g_2) = f(g_1g_2) \\in \\mathrm{Im}(f)$, Pour tout $g \\in G$, $f(g^{-1}) = f(g)^{-1} \\in \\mathrm{Im}(f)$. Construisons la table de multiplication du groupe $\\boldsymbol{S_3}$ :\nOn remarque qu‚Äôen identifiant les deux permutations circulaires aux rotations de $\\boldsymbol{D_3}$ et les 2-cycles aux r√©flexions, on retrouve exactement la m√™me table de multiplication. Cela montre que $\\boldsymbol{S_3}$ et $\\boldsymbol{D_3}$ sont isomorphes ($\\boldsymbol{S_3} \\simeq \\boldsymbol{D_3}$).\nL‚Äôexemple pr√©c√©dent est g√©n√©ralisable\u0026nbsp;: Th√©or√®me de Cayley\u0026nbsp; Tout groupe $G$ d‚Äôordre $n$ est isomorphe √† un sous-groupe de $\\boldsymbol{S_n}$.\nPreuve : Les √©l√©ments de $G$ sont √©tiquet√©s $\\{g_i \\, ; i = 1, \\dots, n\\}$.\nPour un √©l√©ment $a \\in G$, l‚Äô√©l√©ment $a g_i$ est un √©l√©ment de $G$.\nOn peut tr√®s bien appeler $a_i$ l‚Äôindice entier qui d√©signe cet √©l√©ment¬†: $g_{a_i} \\equiv a g_i$, ce qui d√©termine une s√©quence de nombres entiers $\\left(a_1, \\cdots, a_n\\right)$.\nComme $a g_i=a g_k$ seulement pour $i=k$ (suffit de multiplier par $a^{-1}$ pour s‚Äôen assurer), tous les entiers $\\left\\{a_1, \\cdots, a_n\\right\\}$ sont diff√©rents. Ils forment donc une permutation de $(1,2, \\cdots, n)$.\nPlus simplement, on peut voir l‚Äôaction de $a$ sur l‚Äôensemble des $g\\in G$ comme une translation (√† gauche) des √©l√©ments du groupe ($ga$ serait la translation √† droite, diff√©rente par d√©faut).\nOn peut par cons√©quent envoyer $G$ sur $\\boldsymbol{S_n}$¬†:\n$ a \\in G \\longrightarrow p_a=\\left(\\begin{array}{cccc} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; n \\\\ a_1 \u0026amp; a_2 \u0026amp; \\cdots \u0026amp; a_n \\end{array}\\right) \\in \\boldsymbol{S_n} $\nSi $ab=c$ dans $G$, on a¬†:\n$\\begin{aligned} p_a p_b\u0026amp;=\\left(\\begin{array}{cccc} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; n \\\\ a_1 \u0026amp; a_2 \u0026amp; \\cdots \u0026amp; a_n \\end{array}\\right) \\cdot\\left(\\begin{array}{cccc} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; n \\\\ b_1 \u0026amp; b_2 \u0026amp; \\cdots \u0026amp; b_n \\end{array}\\right) \\\\ \u0026amp; =\\left(\\begin{array}{llll} b_1 \u0026amp; b_2 \u0026amp; \\cdots \u0026amp; b_n \\\\ a_{b_1} \u0026amp; a_{b_2} \u0026amp; \\cdots \u0026amp; a_{b_n} \\end{array}\\right) \\cdot\\left(\\begin{array}{llll} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; n \\\\ b_1 \u0026amp; b_2 \u0026amp; \\cdots \u0026amp; b_n \\end{array}\\right)=\\left(\\begin{array}{cccc} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; n \\\\ a_{b_1} \u0026amp; a_{b_2} \u0026amp; \\cdots \u0026amp; a_{b_n} \\end{array}\\right) \\end{aligned} $\nOr $g_{a_{b_i}}=a g_{b_i}=a\\left(b g_i\\right)=(a b) g_i=c g_i=g_{c_i}$, donc¬†:\n$p_a p_b=p_c=\\left(\\begin{array}{cccc} 1 \u0026amp; 2 \u0026amp; \\cdots \u0026amp; n \\\\ c_1 \u0026amp; c_2 \u0026amp; \\cdots \u0026amp; c_n \\end{array}\\right) \\in S_n$\nOn a finalement montr√© que l‚Äôapplication $a \\in G \\longrightarrow p_a \\in \\boldsymbol{S_n}$ pr√©serve la loi de composition interne, ie c‚Äôest un homomorphisme.\nEt comme on a commenc√© par dire que l‚Äôapplication envoyait un ant√©c√©dent sur une image unique, il s‚Äôagit d‚Äôun isomorphisme.\nL‚Äôensemble des $p_a$ (pour tous les $a$ de $G$) forme donc un sous-groupe de $\\boldsymbol{S_n}$ isomorphe √† $G$.\nClasses Deux √©l√©ments $a$ et $b$ de $G$ sont dit conjugu√©s s‚Äôil existe un troisi√®me √©l√©ment $p$ de $G$ tel que $b=pap^{-1}$. On √©crit $b\\sim a$ car il s‚Äôagit d‚Äôune relation d‚Äô√©quivalence. Une relation d‚Äô√©quivalence se doit d‚Äô√™tre sym√©trique ($a\\sim b \\Rightarrow b\\sim a$), r√©flexive ($a\\sim a$) et transitive (si on a $a\\sim b$ et $b\\sim c$ alors $a\\sim c$), ce qu‚Äôon v√©rifie bien avec la relation de conjugaison.\nDes √©l√©ments conjugu√©s les uns par rapport aux autres forment une classe de conjugaison. L‚Äôidentit√© (√©l√©ment neutre) forme une classe √† elle seule. Dans un groupe sym√©trique, les cycle d‚Äôune m√™me longueur appartienne √† une m√™me classe. Preuve : soit $p$ un cycle de longueur donn√©e et $q$ une permutation quelconque du m√™me groupe de sym√©trie alors :\n$ \\begin{aligned} q p q^{-1} \u0026amp; =\\left(q_i \\leftarrow i\\right)\\left(p_i \\leftarrow i\\right)\\left(i \\leftarrow q_i\\right) \\\\ \u0026amp; =\\left(q_i \\leftarrow i\\right)\\left(p_i \\leftarrow q_i\\right) \\\\ \u0026amp; =\\left(q_{p_i} \\leftarrow p_i\\right)\\left(p_i \\leftarrow q_i\\right) \\\\ \u0026amp; =\\left(q_{p_i} \\leftarrow q_i\\right) \\\\ \u0026amp; =q[p] \\end{aligned} $\nOn n‚Äôa fait qu‚Äô√©tiqueter diff√©remment la permutation $p$¬†:\n$ \\left(\\begin{array}{ccccc} 1 \u0026amp; 2 \u0026amp; 3 \u0026amp; \\cdots \u0026amp; n \\\\ p_1 \u0026amp; p_2 \u0026amp; p_3 \u0026amp; \\cdots \u0026amp; p_n \\end{array}\\right) $ devient $ \\left(\\begin{array}{ccccc} q_1 \u0026amp; q_2 \u0026amp; q_3 \u0026amp; \\cdots \u0026amp; q_n \\\\ p_{q_1} \u0026amp; p_{q_2} \u0026amp; p_{q_3} \u0026amp; \\cdots \u0026amp; p_{q_n} \\end{array}\\right) $.\nL‚Äôordre change mais pas ce que devient chacune des valeurs. La structure des cycles est donc n√©cessairement la m√™me.\nExemple : $\\boldsymbol{S_3}$ est donc constitu√© de 3 classes¬†:\n$e$ $\\{(12),(23),(13)\\}$ $\\{(123),(321)\\}$ Comme l‚Äôisomorphisme entre $\\boldsymbol{D_3}$ et $\\boldsymbol{S_3}$ l‚Äôimpose, ces classes trouves bien leurs correspondances dans $\\boldsymbol{S_3}$ puisqu‚Äôon peut v√©rifier que l‚Äôidentit√©, les deux rotations et les trois r√©flexions forment l√† encore trois classes de conjugaison. Chaque √©l√©ment d‚Äôun groupe appartient √† une et une seule classe puisque l‚Äôaction de conjugaison consiste en une translation √† gauche et une translation √† droite (composer tous les √©l√©ments du groupe par un m√™me √©l√©ment revient √† les d√©caler (ou permuter) tous de la m√™me fa√ßon) et donc associe un √©l√©ment conjugu√© diff√©rent √† chaque √©l√©ment diff√©rent du groupe (c‚Äôest une bijection).\nEt d‚Äôautre part, deux classes diff√©rentes sont disjointes par transitivit√© (si elle ne sont pas disjointes, elles co√Øncident). Par cons√©quent, l‚Äôunion de toutes les classes d‚Äôun groupe reforme le groupe, ou dit autrement, les classes forment une partition du groupe.\nUn sous-groupe $H$ de $G$ est dit invariant, ou normal, ou distingu√©, s‚Äôil est identique √† ses sous-groupes conjugu√©s ($H=g^{-1} H g$ pour $g\\in G$).\nRemarque¬†:\nun sous-groupe invariant est n√©cessairement une union de classes conjugu√©es dont l‚Äôidentit√© (un groupe doit la contenir).\nExemple : Le sous-groupe ${e, (123), (321)}$ de $\\boldsymbol{S_3}$ forme un sous-groupe invariant puisqu‚Äôil contient l‚Äôidentit√© et la classe enti√®re des 3-cycles. Tout √©l√©ment conjugu√© de cet ensemble appartient √† une de ces deux classes et se trouve donc dans l‚Äôensemble de d√©part.\nLes classes de conjugaison ne sont pas les seules √† savoir d√©couper un groupe. Leurs concurrentes : les classes lat√©rales dites √† gauche ou √† droite issues d‚Äôun sous-groupe donn√©.\nCette partition diff√®re de la pr√©c√©dente sur deux points : elle n‚Äôest pas n√©cessairement unique et les classes lat√©rales entrant dans la partition comportent chacune le m√™me nombre d‚Äô√©l√©ments.\nSoit $H=\\{h_1,h_2,\\ldots\\}$ un sous-groupe de $G$ et $p$ un √©l√©ment de $G$ (qui n‚Äôest pas dans $H$). Alors l‚Äôensemble $pH=\\{ph_1,ph_2,\\ldots\\}$ est appel√© classe √† gauche de $G$ suivant $H$.\nDe m√™me, $Hp$ est une classe √† droite suivant $H$.\nRemarques¬†:\nsi $p$ est dans $H$, on r√©cup√®re $H$ ($pH=H=Hp$) par d√©finition d‚Äôun sous-groupe. une classe √† gauche (ou √† droite), comme une classe tout court, n‚Äôest g√©n√©ralement pas un groupe (on doit contenir l‚Äôidentit√© pour en √™tre un¬†!). Deux classes √† gauche (ou √† droite) d‚Äôun m√™me sous-groupe soit co√Øncident compl√®tement, soit n‚Äôont aucun √©l√©ment en commun.\nPreuve : Soient $pH$ et $qH$ deux classes √† gauche et supposons qu‚Äôon ait, pour un certain $h_i$, et un certain $h_j$ pris dans $H$, $ph_i=qh_j$, donc $pH$ et $qH$ ont au moins un √©l√©ment en commun.\nComme $q^{-1}p=h_j h_i^{-1}$, $q^{-1}p$ est un √©l√©ment de $H$.\nPar cons√©quent, $q^{-1}pH=H$ (puisque $H$ est un sous-groupe donc un groupe).\nConclusion : $pH=qH$.\nPour ne pas √™tre dans ce cas, il ne faut aucun $h_i$ et $h_j$ tels que $ph_i=qh_j$, autrement dit, $pH$ et $qH$ doivent √™tre disjoints.\nEt chaque classe √† gauche (ou √† droite) suivant un sous-groupe $H$ a n√©cessairement autant d‚Äô√©l√©ments que $H$.\nOn en d√©duit que pour un sous-groupe $H$ d‚Äôordre $n_H$, l‚Äôensemble des classes √† gauche (ou √† droite) forme une partition des √©l√©ments de $H$ en ensembles disjoints de $n_H$ √©l√©ments chacun.\nExemple : Le sous-groupe $\\left\\{H_1: e,(123),(321)\\right\\}$ de $\\boldsymbol{S_3}$ poss√®de une classe √† gauche $\\{M:(12),(23),(31)\\}$ obtenue en multipliant √† gauche les √©l√©ments de $H_1$ par $(12)$, ou par $(23)$, ou encore par $(31)$.\nLe sous-groupe $\\left\\{H_2: e,(12)\\right\\}$ de $\\boldsymbol{S_3}$ poss√®de, lui, deux classes √† gauche¬†:\n$\\left\\{M_1:(23),(321)\\right\\}$ obtenue en multipliant √† gauche les √©l√©ments de $H_2$ soit par $(23)$, soit par $(321)$\u0026nbsp;; $\\left\\{M_2:(31),(123)\\right\\}$ obtenue en multipliant √† gauche les √©l√©ments de $H_2$ soit par $(31)$, soit par $(123)$. Le th√©or√®me de Lagrange en d√©coule¬†:\nl‚Äôordre d‚Äôun groupe fini doit √™tre un multiple entier de l‚Äôordre de n‚Äôimporte lequel de ses sous-groupes.\nCela entra√Æne que tout groupe d‚Äôordre premier est cyclique et donc ab√©lien. Plut√¥t joli, non¬†?\nPreuve : Soit $a$ un √©l√©ment de $G$ diff√©rent de $e$.\nAlors $a$ forme un sous-groupe cyclique de $G$ d‚Äôordre au moins 2.\nOr cet ordre doit diviser l‚Äôordre $G$.\nSeule solution, il vaut l‚Äôordre $G$, ce qui implique que $G$ soit cyclique et par cons√©quent ab√©lien.\nGroupes quotients Les classes √† gauche ou √† droite issues d‚Äôun sous-groupe invariant sont particuli√®rement simples et utiles. D√©j√†, classes √† gauche et classes √† droite co√Øncident ($pHp^{-1}=H$ implique $pH=Hp$). De plus, la partition obtenue est unique et une ¬´factorisation¬ª de $G$ bas√©e sur cette partition devient naturelle.\nL‚Äôensemble des classes issues d‚Äôun sous-groupe invariant $H$ d‚Äôun groupe $G$ a la propri√©t√© de former lui-m√™me un groupe, appel√© groupe quotient $G/H$, d‚Äôordre $n_G/n_H$.\nPreuve : La loi de composition interne entre deux classes lat√©rales $pH$ et $qH$ est d√©finie comme l‚Äôensemble des produits $ph_iqh_j=(pq)h_k$ avec $h_k=(q^{-1}h_iq)h_j$ appartenant bien √† $H$ (puisque $H$ est un sous-groupe invariant).\nPlus simplement\u0026nbsp;: $pHqH=pqH$. $H=eH$ joue le r√¥le de l‚Äô√©l√©ment neutre. $p^{-1}H$ est l‚Äôinverse de $pH$. $pH\\cdot(qH\\cdot rH)=(pH\\cdot qH)\\cdot rH=(pqr)H$ Exemple 1 : Consid√©rons $\\mathbb{Z}/2\\mathbb{Z}$ o√π $\\mathbb{Z}$ est l‚Äôensemble des entiers relatifs munis de l‚Äôaddition comme loi de composition interne. $2\\mathbb{Z}$ est donc l‚Äôensemble des entiers relatifs pairs. Et par cons√©quent, $\\mathbb{Z}/2\\mathbb{Z}$ est form√© de deux sous-groupes distincts¬†: les entiers pairs et les entiers impairs. Le groupe quotient $\\mathbb{Z}/2\\mathbb{Z}$ est donc isomorphe au groupe cyclique √† deux √©l√©ments $\\boldsymbol{C_2}$. Il correspond aussi √† l‚Äôensemble $\\{0,1\\}$ muni de l‚Äôaddition modulo 2.\nOn peut g√©n√©raliser en disant que $\\mathbb{Z}/2\\mathbb{Z}$ est isomorphe au groupe cyclique $\\boldsymbol{C_n}$ et correspond aussi √† l‚Äôensemble des restes dans la division euclidienne de $k$ par $n$, soit l‚Äôensemble $\\{0,1,\\ldots,n\\}$ muni de l‚Äôaddition modulo $n$.\nL‚Äôexemple pr√©c√©dent permet de mieux comprendre pourquoi $G/H$ se lit $G$ modulo $H$.\nExemple 2 : Dans le cas de $\\boldsymbol{S_3}$, $H=\\{e,(123),(321)\\}$ est un sous-groupe invariant.\n$G/H$ contient deux √©l√©ments : $H$ et $M=\\{(12),(23),(31)\\}$.\n$H$ est l‚Äôensemble des permutations paires √† 3 √©l√©ments (l‚Äôidentit√© correspond √† aucune permutation et les 3 cycles √† des permutations doubles). On appelle aussi $H$ $\\boldsymbol{A_3}$, groupe altern√© d‚Äôordre 3.\n$M$ est l‚Äôensemble des permutations impaires.\nOn voit facilement que la composition de deux permutations impaires ou paires donne une permutation paire alors qu‚Äôune composition mixte donne une permutation impaire¬†:\n$HM=MH=M$, $HH=H$ et $MM=H$.\nOn en d√©duit que $G/H$ est isomorphe √† $\\boldsymbol{C_2}$ (H est envoy√© sur l‚Äôidentit√© et $\\boldsymbol{M}$ correspond √† l‚Äôautre √©l√©ment).\nLe morphisme $f: G \\rightarrow G / H, g \\mapsto g H$ est appel√© morphisme canonique ou projection canonique.\nLe deuxi√®me exemple illustre un th√©or√®me qui va se r√©v√©ler bien utile mais d√©finissons d‚Äôabord le noyau d‚Äôun homomorphisme¬†:\nSoit f un homomorphisme de $G$ √† $G^{\\prime}$. On appelle noyau $K$ de cet homomorphisme l‚Äôensemble des √©l√©ments de $G$ qui sont envoy√© sur l‚Äô√©l√©ment neutre de $G‚Äô$ ($K=\\{g \\in G ; g \\stackrel{f}{\\longmapsto} e^{\\prime} \\in G^{\\prime}\\}$).\nTh√©or√®me d‚Äôisomorphisme (premier)¬†:\nSoit $f$ un homomorphisme de $G$ √† $G^{\\prime}$ de noyau $K$.\n$K$ forme alors un sous-groupe invariant de $G$.\nLe groupe quotient $G/K$ est isomorphe √† $G^{\\prime}$.\nAutrement dit, on rend $f$ injectif en quotientant $G$ par son noyau.\nCela se note symboliquement $G / K \\simeq G^{\\prime}$ ou encore $G / \\operatorname{Ker}(f) \\simeq f(G)$ o√π $\\operatorname{Ker}(f)$ d√©signe le noyau de $f$ et $f(G)$ est l‚Äôimage de $f$.\nPreuve :\nMontrons que $K$ est un sous-groupe\u0026nbsp;:\npour $a$ et $b$ dans $K$, $a \\cdot b \\xrightarrow{f} e^{\\prime} \\cdot e^{\\prime}=e^{\\prime}$, donc $a\\cdot b$ est aussi dans $K$. La pr√©servation de la loi de composition par l‚Äôhomomorphisme assure que pour $g \\xrightarrow{f} g^{\\prime}$, on a aussi $e \\xrightarrow{f} e^{\\prime}$ et $g^{-1} \\xrightarrow{f} g^{\\prime-1}$. D‚Äôo√π $e \\in K$, et si $a\\in K$, alors $a^{-1}$ est aussi dans $K$ (car $a^{-1} \\xrightarrow{f} e^{\\prime-1}=e^{\\prime}$). Montrons que $K$ est invariant\u0026nbsp;:\nprenons $a$ dans $K$ et $g$ dans $G$. $g a g^{-1} \\xrightarrow{f} g^{\\prime} e^{\\prime} g^{\\prime-1}=e^{\\prime}$. Donc $g a g^{-1} \\in K$ pour tout $g\\in G$. Montrons que $G/K$ est isomorphe √† $G^{\\prime}$\u0026nbsp;: Les √©l√©ments du groupe quotient $G/K$ sont les classes lat√©rales $pK$.\nConsid√©rons l‚Äôapplication envoyant les classes lat√©rales vers l‚Äôimage de $f$, $p K \\xrightarrow{\\rho} f(p)=p^{\\prime} \\in G^{\\prime}$.\n$\\rho$ est bien d√©finie\u0026nbsp;: si $pK=qK$ pour $p,q\\in K$, alors $q^{-1}p$ est aussi dans $K$ (puisque $K$ est un groupe) et donc comme $f$ est un homomorphisme\u0026nbsp;: $ \\begin{aligned} f\\left(q^{-1} p\\right) \u0026amp; =1 \\\\ \u0026amp; =f\\left(q^{-1}\\right) f(p) \\\\ \u0026amp; =f^{-1}(q) f(p) \\end{aligned} $\nEt finalement, $f(q)=f(p)$.\n$\\rho$ est bien un homomorphisme\u0026nbsp;: $ \\begin{aligned} \\rho(p K \\cdot q K) \u0026amp; =\\rho(p q K) \\\\ \u0026amp; =f(p q) \\\\ \u0026amp; =f(p) f(q) \\\\ \u0026amp; =\\rho(p K) \\rho(q K) \\end{aligned} $\nSi $\\rho(p K)=\\rho(q K)$ alors $\\rho\\left(q^{-1} p K\\right)=\\rho\\left(q^{-1} K \\cdot p K\\right)$ (le groupe quotient est un groupe), et par action de groupe de l‚Äôhomomorphisme, $\\rho\\left(q^{-1} K \\cdot p K\\right)=\\rho\\left(q^{-1} K\\right) \\rho(p K)=\\rho^{-1}(q K) \\rho(p K)=e^{\\prime}$, ce qui implique $q^{-1} p K=K$ ou $qK=pK$. L‚Äôapplication est bien bijective.\nLe dessin ci-dessus illustre le cas d‚Äôun groupe $G$ contenant 15 √©l√©ments avec un noyau $K$ en contenant 3. $G/K$ et $G^{\\prime}$ sont alors tous deux d‚Äôordre 5.\nExemple : on a vu dans un exemple pr√©c√©dent que l‚Äôhomomorphisme de $\\boldsymbol{S_3}$ sur $\\boldsymbol{C_2}$ devient un isomorphisme si on quotiente $\\boldsymbol{S_3}$ par le sous groupe invariant $H=\\{e,(123),(321)\\}$¬†: $\\boldsymbol{S_3} / H \\simeq \\boldsymbol{C_2}$. Or $H$ est bien le noyau $K$ de l‚Äôhomomorphisme comme le pr√©voit le th√©or√®me d‚Äôisomorphisme.\nImage et noyau nous disent beaucoup sur les homomorphismes.\nEn effet :\nPour un homomorphisme $f$ tel que $G \\xrightarrow{f} G^{\\prime}$¬†:\n$f$ est injectif si et seulement si $\\operatorname{Ker}(f)=e$ $f$ est surjectif si et seulement si $\\operatorname{Im}(f)=G^{\\prime}$ Produit direct de deux groupes Soit $H_1$ et $H_2$ deux sous-groupes du groupe $G$ avec les deux propri√©t√©s suivantes¬†:\nles √©l√©ments de $H_1$ commutent avec les √©l√©ments de $H_2$. chaque √©l√©ment de $g\\in G$ peut s‚Äô√©crire $g=h_1h_2$ avec $h_1\\in H_1$ et $h_2\\in H_2$. $G$ est alors le produit direct de $H_1$ et $H_2$\u0026nbsp;: $G=H_1 \\otimes H_2$. Exemple : D√©composons $\\boldsymbol{C_6}=\\{e=a^6, a, a^2, a^3, a^4, a^5\\}$ en $\\color{red} H_1=\\{e, a^3\\}$ et $\\color{blue} H_2=\\{e, a^2,a^4\\}$.\nComme $\\boldsymbol{C_6}$ est ab√©lien, le premier crit√®re est respect√©.\nEt on a¬†: $e = \\color{red}e\\color{blue}e$, $a = \\color{red}a^3\\color{blue}a^4$, $a^2 = \\color{red}e\\color{blue}a^2$, $a^3 = \\color{red}a^3\\color{blue}e$, $a^4 = \\color{red}e\\color{blue}a^4$, $a^5 = \\color{red}a^3\\color{blue}a^2$.\n$H_1 \\simeq C_2$ et $H_2 \\simeq C_3$ donc $C_6 \\simeq C_2 \\otimes C_3$.\nSi $G=H_1 \\otimes H_2$, alors $H_1$ et $H_2$ doivent √™tre invariants.\nPreuve : Pour $a_1\\in H_1$, $g a_1 g^{-1}=h_1 h_2 a_1\\left(h_1 h_2\\right)^{-1}=h_1 h_2 a_1 h_2^{-1} h_1^{-1}=h_1 a_1 h_1^{-1} \\in H_1$ (et on peut bien s√ªr faire pareil avec $a_2$ dans $H_2$).\nOn peut donc construire les groupes quotient $G/H_1$ et $G/H_2$. On montre alors que $G / H_1 \\simeq H_2$ et $G / H_2 \\simeq H_1$, ce qui √©claire un peu plus le terme de groupe quotient.\nChapitre suivant\u0026nbsp;: les repr√©sentations\nRetour sommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/bayes/",
	"title": "Inf√©rences bay√©siennes",
	"tags": [],
	"description": "",
	"content": " Inf√©rences bay√©siennes L\u0026rsquo;intelligence artificielle vise √† simuler l\u0026rsquo;intelligence humaine. Mais la compr√©hension du fonctionnement de notre cerveau en est elle-m√™me encore qu\u0026rsquo;√† ses pr√©mices.\nTel un programme informatique, le cerveau produit de nouvelles informations √† partir de l\u0026rsquo;information qu\u0026rsquo;il re√ßoit (les donn√©es). Un changement de paradigme a eu lieu r√©cemment dans la communaut√© scentifique concernant la modalit√© m√™me de ce traitement de l\u0026rsquo;information par le cerveau. Stanislas Dehaene, un psychologue sp√©cialis√© en neuropsychologie, r√©sume cette nouvelle fa√ßon d\u0026rsquo;apr√©hender l\u0026rsquo;information dans ses le√ßons au Coll√®ge de France¬†:\nUn vaste courant r√©cent des sciences cognitives s‚Äôappuie sur la th√©orie math√©matique de l‚Äôinf√©rence bay√©sienne pour mod√©liser une tr√®s grande diversit√© de ph√©nom√®nes psychologiques : perception, inf√©rence statistique, prise de d√©cision, apprentissage, traitement du langage\u0026hellip; La rapidit√© avec laquelle cette th√©orie envahit et unifie divers domaines de la cognition, la simplicit√© de ses fondements axiomatiques, et la profondeur de ses conclusions justifient de parler d‚Äôune v√©ritable ¬´ r√©volution bay√©sienne ¬ª en sciences cognitives.\nPour r√©sumer, la th√©orie bay√©sienne fournit un mod√®le math√©matique de la mani√®re optimale de mener un raisonnement plausible en pr√©sence d‚Äôincertitudes. D√®s la naissance, le b√©b√© semble dot√© de comp√©tences pour ce type de raisonnement probabiliste. L‚Äôinf√©rence bay√©sienne rend √©galement bien compte des processus de perception : √©tant donn√© des entr√©es ambig√ºes, le cerveau en reconstruit l‚Äôinterpr√©tation la plus probable. La r√®gle de Bayes indique comment combiner, de fa√ßon optimale, les a priori issus de notre √©volution ou de notre m√©moire avec les donn√©es re√ßues du monde ext√©rieur. En cela, elle offre une nouvelle vision de l‚Äôapprentissage qui d√©passe le dilemme classique entre th√©ories empiristes et nativistes. Enfin, de nombreuses d√©cisions humaines semblent r√©sulter d‚Äôune approximation de la r√®gle bay√©sienne d‚Äôaccumulation d‚Äô√©vidence, combin√©e √† une estimation de la valeur attendue des cons√©quences de nos choix. Dans la mesure o√π les principes de l‚Äôinf√©rence bay√©sienne sont ainsi partag√©s par de multiples domaines de la cognition, il se pourrait que l‚Äôarchitecture du cortex ait √©volu√© pour approximer ce type de calcul probabiliste √† grande vitesse, et de fa√ßon massivement parall√®le. L‚Äôalgorithme utilis√© pourrait expliquer non seulement l‚Äôorganisation du cortex en couches, mais aussi la mani√®re dont notre cerveau anticipe sur le monde ext√©rieur (codage pr√©dictif) et dont il r√©pond √† la nouveaut√© (propagation des signaux d‚Äôerreur).\nDigression sur le raisonnement avec une autre vid√©o d\u0026rsquo;Hygi√©ne Mentale.\nBiblioth√©caire vs agriculteur On trouve dans le livre \u0026ldquo;Syst√®me 1 / Syst√®me 2. Les deux vitesses de la pens√©e\u0026rdquo; du prix Nobel d\u0026rsquo;√©conomie Daniel Kahneman une exp√©rience illustrant ce que peut nous apporter une meilleur familiarit√© avec le bay√©sianisme.\nL\u0026rsquo;exp√©rience consistait √† pr√©senter √† un amphi le portrait suivant¬†:\nSteve est tr√®s timide et r√©serv√©, toujours pr√™t √† rendre service, mais sans vraiment s'int√©resser aux gens ou √† la r√©alit√©. Personnalit√© docile et m√©ticuleuse, il a besoin d'ordre et de structure, et se passionne pour les d√©tails. Puis on demande √† l\u0026rsquo;audience si Steve est plus susceptible d\u0026rsquo;√™tre biblioth√©caire ou agriculteur.\nUne large majorit√© r√©pond alors biblioth√©caire tant le portrait est proche du st√©r√©otype associ√© √† cette profession.\nEt c\u0026rsquo;√©tait aussi le cas de ChatGPT 3.5 au d√©part (rappelons qu\u0026rsquo;il ne s\u0026rsquo;agit pas d\u0026rsquo;une IA qui raisonne mais d\u0026rsquo;une IA produisant un discours visant √† satisfaire l\u0026rsquo;utilisateur)¬†:\nMais sachant qu\u0026rsquo;aux Etats-Unis, o√π l\u0026rsquo;√©tude a √©t√© men√©e, il y a au moins 20 fois plus d\u0026rsquo;agriculteurs que de biblioth√©caires, les chances que Steve soit biblioth√©caire sont minces.\nUn raisonnement bay√©sien nous aurait pr√©muni d\u0026rsquo;une conclusion trop h√¢tive sur la profession de Steve. Il consiste non pas √† d√©duire une probabilit√© d\u0026rsquo;une information donn√©e mais √† mettre √† jour une probabilit√© a priori √† partir de cette information.\nLa probabilit√© a priori de l\u0026rsquo;hypoth√®se $H$ est $\\textcolor{#0076BA}{P(H)}$.\nIci, l\u0026rsquo;hypoth√®se est que Steve est biblioth√©caire et la probabilit√© a priori vaut 4,8% $\\left(\\frac{1}{20+1}\\right)$.\nLa probabilit√© a priori de l\u0026rsquo;hypoth√®se contraire $\\overline{H}$ vaut $\\textcolor{#56C1FF}{P(\\overline{H})}=1-\\textcolor{#0076BA}{P(H)}$.\nIci, cela correspond √† un Steve agriculteur puisqu\u0026rsquo;on suppose qu\u0026rsquo;il est soit biblioth√©caire, soit agriculteur. La probabilit√© a priori d\u0026rsquo;un Steve agriculteur vaut 95,2%.\nLe portrait de Steve est l\u0026rsquo;information nouvelle apport√©e $I$, elle modifie la plausibilit√© que Steve soit biblioth√©caire tel un curseur qui va tirer la probabilit√© dans un sens ou dans l\u0026rsquo;autre. La probabilit√© de l\u0026rsquo;information $I$ est not√©e $\\textcolor{#1DB100}{P(I)}$.\nLa probabilit√© a posteriori correspond alors √† la probabilit√© de $H$ sachant $I$, qu\u0026rsquo;on note $\\textcolor{#CB297B}{P(H|I)}$ (que devient la probabilit√© de $H$ une fois qu\u0026rsquo;on a connaissance de l\u0026rsquo;information $I$).\nC\u0026rsquo;est le th√©or√®me de Bayes qui va nous permettre de mettre √† jour la probabilit√© a priori pour obtenir la probabilit√© a posteriori¬†:\n$ \\displaystyle \\textcolor{#CB297B}{P(H|I)}=\\frac{ \\textcolor{#FF644E}{P(I|H)}\\times \\textcolor{#0076BA}{P(H)}}{\\textcolor{#1DB100}{P(I)}}=\\frac{ \\textcolor{#FF644E}{P(I|H)}\\times \\textcolor{#0076BA}{P(H)}}{ \\textcolor{#FF644E}{P(I|H)}\\times \\textcolor{#0076BA}{P(H)} + \\textcolor{#00A89D}{P(I|\\overline{H})} \\times \\textcolor{#56C1FF}{P(\\overline{H})}} $ $\\textcolor{#FF644E}{P(I|H)}$ est la probabilit√© d\u0026rsquo;avoir $I$ si $H$ est vraie (probabilit√© que Steve corresponde √† la description s\u0026rsquo;il est biblioth√©caire).\n$\\textcolor{#00A89D}{P(I|\\overline{H})}$ est la probabilit√© d\u0026rsquo;avoir $I$ si $H$ est fausse (probabilit√© que Steve corresponde √† la description s\u0026rsquo;il est agriculteur).\nCes deux probabilit√©s ne sont pas connues, mais on peut les estimer¬†!\nL\u0026rsquo;illustration ci-dessous permet de se rendre compte que le th√©or√®me de Bayes est finalement assez trivial¬†: Et encore plus trivial avec le plus grand effort p√©dagogique fait ci-dessous¬†:\nOn peut utiliser la repr√©sentation graphique d√©crite dans la vid√©o d\u0026rsquo;introduction pour nous aider √† faire les calculs en fonction de nos estimations. C\u0026rsquo;est ce qui est fait dans l\u0026rsquo;appliquette Geogebra ci-dessous (cliquer pour l\u0026rsquo;ouvrir).\nEn supposant que le portrait corresponde √† 40% des biblioth√©caires et √† 5% des agriculteurs, que vaut la probabilit√© a posteriori que Steve soit biblioth√©caire ?\nLa probabilt√© que le portrait corresponde √† un biblioth√©caire devrait √™tre combien de fois sup√©rieure √† celle qu\u0026rsquo;il corresponde √† un agriculteur pour faire basculer la probabilit√© a posteriori que Steve soit biblioth√©caire au-del√† de 50% ? Comparez au ratio biblioth√©caires sur agriculteurs.\nMoralit√©, le portrait correspond probablement plus √† un agriculteur !\ninfo\nLes fr√©quentistes et les bay√©siens interpr√®tent les probabilit√©s diff√©remment.\nPour les fr√©quentistes, une probabilit√© est la limite vers laquelle tendrait une fr√©quence mesur√©e sur un √©chantillon lorsqu\u0026rsquo;on fait tendre la taille de l\u0026rsquo;√©chantillon vers l\u0026rsquo;infini.\nPour les bay√©siens, une probabilit√© mesure un degr√© de conviction qui est mis √† jour √† chaque nouvelle information obtenue.\nAutre c√©l√®bre exemple de portrait tir√© du m√™me livre de Kahneman (et toujours avec ChatGPT comme cobaye) :\nje pleure pic.twitter.com/d9qL7qcp2I\n\u0026mdash; Dr. Juliette (@FerryDanini) February 12, 2023 Le paradoxe des deux enfants Un couple a deux enfants. On nous informe que l\u0026rsquo;un des deux est une fille. Quelle est la probabilt√© que les deux enfants soient des filles¬†?\nUtilisez le th√©or√®me de Bayes pour r√©pondre en pr√©cisant d\u0026rsquo;abord $H$ et $I$, puis en d√©terminant les diff√©rentes probabilit√©s $P(H)$, $P(I|H)$ et $P(I)$.\nV√©rifier votre r√©sultat en compl√©tant le code python suivant afin qu\u0026rsquo;il simule une exp√©rience sur 100000 couples de deux enfants.\nUn couple a deux enfants. On apper√ßoit une fille dans le jardin. Quelle est la probabilt√© que les deux enfants soient des filles¬†?\nLa r√©ponse change-t-elle ?\nCorrection (cliquer pour afficher) Oui : dans l'hypoth√®se o√π il y a deux filles (1 chance sur 4), il y a 100% de chance de voir une fille dans le jardin. Et dans l'hypoth√®se o√π il n'y a pas deux filles ($\\bar{H}=3/4$), il y a 1 chance sur 3 de voir une fille dans le jardin (il faut √† la fois que les parents aient un gar√ßon et une fille, 2 chance sur 3, et il faut que cela soit la fille dans le jardin, 1 chance sur 2). On a donc $P(H|I)=\\frac{1\\times 1/4}{1\\times 1/4 + 1/3\\times 3/4}=\\frac{1}{2}$. Comment modifier le code pour qu\u0026rsquo;il corresponde √† cette situation¬†?\nCorrection (cliquer pour afficher) from random import randint vufillejardin = 0 deuxfilles = 0 n = 100000 enfants = [None,None] for i in range(n): enfants[0] = randint(0,1) enfants[1] = randint(0,1) if enfants[randint(0,1)] == 1: # on voit une fille dans le jardin (on tire au sort l'enfant) vufillejardin += 1 if enfants[0] == enfants[1]: # deux filles deuxfilles += 1 P = deuxfilles/vufillejardin*100 print(f\"Probabilit√© qe les parents aient deux filles sachant qu'on a vu une fille dans le jardin : {P:.1f}%\") note\nla probabilit√© a priori d\u0026rsquo;avoir deux filles vaut 1/4.\nL\u0026rsquo;information apport√©e tire cette probabilit√© vers le haut.\nElle tire plus dans le second cas que dans le premier, car l\u0026rsquo;information est plus pr√©cise ; un tirage a √©t√© fait.\nDans le premier cas, l\u0026rsquo;information suppl√©mentaire laisse trois possibilit√©s √©quiprobables (on sait seulement que le couple ne peut pas avoir deux gar√ßons), alors que dans le second, il n\u0026rsquo;y en a plus que deux (l\u0026rsquo;autre enfant est soit une fille, soit un gar√ßon)¬†!\nProbl√®me de Monty Hall Dans un ancien jeu t√©l√©vis√© am√©ricain, pr√©sent√© par Monty Hall, un candidat devait choisir une porte parmi trois. Derri√®re l\u0026rsquo;une d\u0026rsquo;elles se cache une voiture et derri√®re les deux autres une ch√®vre.\nApr√®s que le candidat ait indiqu√© son choix, Monty ouvre une des deux autres portes derri√®re laquelle il sait que se trouve une ch√®vre (s\u0026rsquo;il y a une ch√®vre derri√®re les deux portes non choisies, il en choisit une au hasard).\nIl demande ensuite au candidat s\u0026rsquo;il veut garder sa porte ou s\u0026rsquo;il veut choisir l\u0026rsquo;autre porte.\nDoit-il changer de porte ?\nSupposons que vous ayez pr√©alablement choisi la porte 1 et que Monty ouvre la porte 2. Montrez en utilisant le th√©or√®me de Bayes que le candidat a int√©r√™t √† changer de porte.\nPour raisonner, on va prendre pour hypoth√®se $H$ \u0026ldquo;il y a une voiture derri√®re la porte 3\u0026rdquo; et pour information $I$ : \u0026ldquo;Monty ouvre la porte 2\u0026rdquo;.\nL\u0026rsquo;argument de l\u0026rsquo;Apocalypse Le philosophe Nick Bolstrom pr√©sente sa version du \u0026ldquo;Doomsday Argument\u0026rdquo; √† peu pr√®s ainsi¬†:\n1re √©tape :\nImaginez un univers constitu√© de 100 boites habit√©es chacune par un humain.\nL\u0026rsquo;ext√©rieur des boites est peint en bleu pour 90 d\u0026rsquo;entre elles et en rouge pour les 10 autres.\nChaque personne conna√Æt la situation et on leur demande de deviner la couleur de leur boite.\nQue r√©pondez-vous ?\nCorrection (cliquer pour afficher) Si vous supposez qu'il y a 90% de chance que votre boite soit bleue, vous √™tes SSA (self-sampling assumption) dans la terminologie de Bostrom. Et si vous pensez plut√¥t qu'il n'y a que 50% de chance qu'elle soit bleue, vous √©chappez √† la conclusion du Doomsday. 2e √©tape :\nOn modifie un peu l\u0026rsquo;exp√©rience en rempla√ßant la couleur des boites par une num√©rotation entre 1 et 100 (le num√©ro est, l√† encore, peint √† l\u0026rsquo;ext√©rieur).\nMaintenant, un dieu bizarre lance une pi√®ce. Si √ßa tombe sur face, il cr√©e une personne dans chaque boite et si √ßa tombe sur pile, il ne cr√©e des personnes que dans les boites 1 √† 10.\nVous vous retrouvez dans une de ces boites et on vous demande s\u0026rsquo;il y a 10 ou 100 personnes dans l\u0026rsquo;univers.\nN\u0026rsquo;ayant pas d\u0026rsquo;information suppl√©mentaire, que r√©pondez-vous¬†?\nEt si on vous demande d\u0026rsquo;estimer la probabilit√© que le num√©ro de votre pi√®ce soit entre 1 et 10 selon chacune des deux possibilit√©s pour le pile ou face¬†?\nSupposons maintenant que vous sortiez de votre boite pour d√©couvrir que son num√©ro est le 7.\nOn vous demande alors d\u0026rsquo;estimer la probabilit√© que la pi√®ce soit tomb√©e sur pile maintenant que vous connaissez le num√©ro de votre boite.\nCorrection (cliquer pour afficher) $$ \\color{#006C65} \\begin{aligned} P(\\text{Pile}|7)\u0026amp;=\\frac{P(7|\\text{Pile})P(\\text{pile})}{P(7|\\text{Pile})P(\\text{pile})+P(7|\\text{face})P(\\text{face})}\\\\ \u0026amp;=\\frac{1/10\\times 1/2}{1/10\\times 1/2 + 1/100\\times 1/2}\\\\ \u0026amp;=\\frac{10}{11}\\\\ \u0026amp;=91\\% \\end{aligned} $$\n3e √©tape :\nOn transpose ces r√©sultats √† la situation actuelle sur Terre.\nPosons les deux hypoth√®ses rivales suivantes¬†:\napocalypes pr√©coce : l\u0026rsquo;humanit√© va s\u0026rsquo;√©teindre dans le prochain si√®cle et la quantit√© totale d\u0026rsquo;humain ayant exist√© sera d\u0026rsquo;environ 200 milliards. apocalypse tardive : l\u0026rsquo;humanit√© va survivre le prochain si√®cle et coloniser la galaxie. Le nombre total d\u0026rsquo;humain ayant exist√© s\u0026rsquo;√©l√®vera √† 200 mille milliards. Quelle probabilit√© a priori attribuez-vous √† chacun de ces sc√©narios¬†?\nVous n\u0026rsquo;√™tes pas loin d\u0026rsquo;√™tre l\u0026rsquo;humain n¬∞100 milliards (en terme d\u0026rsquo;ordre de naissance).\nComme le th√©or√®me de Bayes va-t-il faire glisser les probabilit√©s attribu√©es √† chacun des sc√©narios sachant cela (faire le parall√®le avec l\u0026rsquo;univers des boites num√©rot√©es).\nCommentaire (cliquer pour afficher) On peut √©chapper √† cette conclusion sinistre en rejetant SSA. On peut en effet consid√©rer qu'il y a plus de chance qu'il y ait 100 personnes que 10 car le fait que j'existe devient alors plus probable.\nSans conna√Ætre le num√©ro de la boite, on peut donc penser qu'il y a 10 fois plus de chances que dieu ait tir√© face. Cela r√©√©quilibre a posteriori les deux hypoth√®ses car maintenant, $P(\\text{Pile}|7)$ vaut 1/2 et de m√™me, les deux sc√©narios d'apocalypse retrouvent leurs probabilit√©s a priori. Nick Bostrom a aussi d√©velopp√© des arguments semblables sur la probabilit√© que l\u0026rsquo;on vive dans une simulation.\nBayes et cote Le raisonnement de la question 2 sugg√®re de r√©exprimer le th√©or√®me de Bayes en terme de cotes, ce qui va permettre de simplifier √† la fois son calcul et son interpr√©tation.\nLa cote d\u0026rsquo;un √©v√©nement (odds en anglais) est le ratio de la probabilit√© que l\u0026rsquo;√©v√©nement se produise par la probabilit√© qu\u0026rsquo;il ne se produise pas. On l\u0026rsquo;exprime en g√©n√©ral comme une paire de nombres (le num√©rateur et le d√©nominateur).\nPar exemple, si un √©v√®nement a une probabilit√© de 5% de se produire, il a donc aussi une probabilit√© de 95% de ne pas se produire et sa cote est alors de 5 contre 95 (ou 1 contre 19 qu\u0026rsquo;on peut aussi noter $1:19$).\nSi un pari consiste √† obtenir un 5 ou un 6 au d√©, que vaut alors sa cote¬†?\n√Ä quelle cote correspond une probabilit√© de 50%¬†?\nL\u0026rsquo;utilisation des cotes est tr√®s commune pour les paris sportifs en Angleterre.\nTh√©or√®me de Bayes exprim√© en termes de cote¬†:\nLa cote de $H$ sachant $I$ vaut la cote de $H$ multipli√©e par le facteur de Bayes $\\left(\\frac{P(I|H)}{P(I|\\overline{H})}\\right)$. Le facteur de Bayes mesure le m√©rite relatif des deux hypoth√®ses $H$ et $\\bar{H}$, le rapport de leurs vraisemblances.\nV√©rifions avec Steve :\nLa cote de $H$ correspond au ratio biblioth√©caires/agriculteurs (soit 1/20) et le facteur de Bayes correspond √† combien de fois le portrait $I$ correspond plus √† un biblioth√©caire qu\u0026rsquo;√† un agriculteur. Si le facteur de Bayes vaut 20, on trouve une cote de 1 pour la cote a posteriori, ce qui correspond bien √† une probabilit√© de 50% que Steve soit biblioth√©caire.\nReprenez la premi√®re questions du paradoxe des deux filles en utilisant les cotes.\nInf√©rence bay√©sienne et diagnostic m√©dical Le psychologue Gerd Gigerenzer pr√©sente le probl√®me suivant dans un s√©minaire de statistique √† des gyn√©cologues en activit√©¬†:\nUne femme de 50 ans sans sympt√¥me passe une mammographie de routine. L'examen se r√©v√®le positif. Alarm√©e, elle veut savoir avec quelle certitude cela implique qu'elle a un cancer du sein.\n√Ä part le r√©sultat du test, vous ne savez rien sur cette femme.\nLa pr√©valence des cancers du sein est de 1% chez les femmes de cet √¢ge.\nLa sensibilit√© du test est de 90%.\nEt sa sp√©cificit√© est de 91%.\nParmi les femmes dont le test est positif, combien sont atteintes d'un cancer du sein\u0026nbsp;?\nA : 9 sur 10 ; B : 8 sur 10 ; C : 1 sur 10 ; D : 1 sur 100 Un peu de vocabulaire :\nLa sensibilit√© d\u0026rsquo;un test mesure sa capacit√© √† donner un r√©sultat positif lorsqu\u0026rsquo;une hypoth√®se est v√©rifi√©e = capacit√© √† d√©tecter un maximum de malades (avoir le moins possible de faux n√©gatifs). La sp√©cificit√© d\u0026rsquo;un test mesure sa capacit√© √† donner une r√©sultat n√©gatif lorsque l\u0026rsquo;hypoth√®se n\u0026rsquo;est pas v√©rifi√©e = capacit√© √† ne d√©tecter que les malades (avoir le moins possible de faux positifs). En notant VP et FP les vrais et les faux positifs, et VN et FN les vrais et faux n√©gatifs, on a¬†:\nMalade Non malade Test positif VP FP Test n√©gatif FN VN sensibilit√© $=\\frac{VP}{VP+FN}$\nsp√©cificit√© $=\\frac{VN}{VN+FP}$\ninfo\nEn bon bay√©sien, il ne faut pas consid√©rer qu\u0026rsquo;un test d√©termine si on a une maladie, ni m√™me qu\u0026rsquo;il d√©termine les chances d\u0026rsquo;avoir une maladie.\nTout ce qu\u0026rsquo;il fait, c\u0026rsquo;est mettre √† jour les chances d\u0026rsquo;avoir une maladie¬†!\nQue vaut le facteur de Bayes dans cet exemple¬†?\nR√©ponse (cliquer pour afficher) $\\displaystyle \\frac{P(+|\\text{Cancer})}{P(+|\\overline{\\text{Cancer}})}=\\frac{\\text{probabilit√© de vrais positifs}}{\\text{probabilit√© de faux positifs}}=\\frac{\\text{sensibilit√©}}{\\text{1-sp√©cificit√©}} $ Le th√©or√®me de Bayes version cote devient donc¬†:\n$\\displaystyle cote(\\text{cancer}|+)=\\frac{\\text{sensibilit√©}}{\\text{1-sp√©cificit√©}}\\times cote(\\text{cancer}) $\nQuelle est la bonne r√©ponse (les cotes permettent de l\u0026rsquo;estimer facilement)¬†?\nPlus de la moiti√© des docteurs pr√©sents ont choisi la r√©ponse A, ce qui est tr√®s √† c√¥t√© de la plaque, et seulement 1 sur 5 ont choisi la bonne r√©ponse\u0026hellip;\nChangeons la pr√©valence √† 10 %¬†:\npr√©valence 10% sensibilit√© 90% sp√©cificit√© 91% Que devient la probabilit√© d\u0026rsquo;avoir un cancer en cas de test positif¬†?\nPassons-la maintenant √† 0,1 %¬†:\npr√©valence 0,1% sensibilit√© 90% sp√©cificit√© 91% Que vaut $P(cancer|+)$ maintenant¬†? Augmentons la sp√©cificit√© √† 99 % et reprenant une pr√©valence de 1 %.\npr√©valence 1% sensibilit√© 90% sp√©cificit√© 99% Que devient la probabilit√©¬†?\nEt si le test est n√©gatif ?\nReprendre les donn√©es de d√©part et trouvez la probabilit√© de ne pas avoir de cancer si on a √©t√© test√© n√©gatif.\nEt si on passe un second test n√©gatif¬†?\nMoustiques Trois maladies virales peuvent √™tre transmises par les moustiques¬†:\ndengue chikungunya zika Elles provoquent des sympt√¥mes qui peuvent √™tre assez proches, ce qui les rend difficiles √† diff√©rencier directement.\nIci on s‚Äôint√©resse √† la mise en place d‚Äôune aide statistique au diagnostic. Pour cela, on va s‚Äôappuyer sur des donn√©es obtenues chez des personnes dont le diagnostic a pu √™tre certifi√© par des examens biologiques. Pour simplifier, on supposera que ces caract√®res apparaissent ind√©pendamment chez les personnes infect√©es.\nsympt√¥mes Dengue Chikungunya Zika Fi√®vre 95% 75% 75% Courbatures 75% 95% 50% Douleurs oculaires 50% 25% 50% D√©ficit globules blancs 50% 50% 25% H√©morragie 25% 5% 5% √Ä partir de ces donn√©es, on veut d√©terminer les probabilit√©s de chaque maladie selon les sympt√¥mes pr√©sent√©s et dans des conditions diff√©rentes.\nLa forme du th√©or√®me de Bayes la plus pratique est dans ce cas¬†:\n$\\displaystyle P(\\text{maladie}|\\text{sympt√¥mes})=\\frac{P(\\text{sympt√¥mes}|\\text{maladie})\\times P(\\text{maladie})}{P(\\text{sympt√¥mes})} $\nVous √™tes internes au service de maladies infectieuses du CHU de Limoges et une personne se pr√©sente avec √† la fois de la fi√®vre, pas de courbatures et des douleurs oculaires. La personne revient d‚Äôun pays dans lequel aucune des trois maladies n‚Äôest √©pid√©mique. On consid√®re donc a priori que les trois maladies sont √©quiprobables.\nCalculer la probabilit√© que la personne pr√©sente ces 3 sympt√¥mes ensemble pour chacune des maladies $P(\\text{sympt√¥mes}|\\text{maladie})$.\nCalculer la probabilit√© d\u0026rsquo;avoir ces sympt√¥mes quelle que soit la maladie $P(\\text{sympt√¥mes})$ aide¬†: $P(\\text{sympt√¥mes})=\\sum_{\\text{maladie}}P(\\text{sympt√¥mes}|\\text{maladie})\\times P(\\text{maladie})$\nQuelles sont les probabilit√©s a posteriori de chaque maladie si cette personne pr√©sente ces sympt√¥mes¬†? Quel est selon vous le diagnostic le plus probable dans ce cas¬†?\nSi vous apprenez maintenant qu\u0026rsquo;en fait la personne revient d\u0026rsquo;un pays dans lequel s√©vit une √©pid√©mie de Chikungunya. A priori, il y a 90¬†% de chances qu‚Äôelle ait √©t√© infect√©e par le Chikungunya et 5¬†% par chacune des deux autres maladies. Quel est le diagnostic le plus probable dans ce cas¬†?\nspam\nUn des premiers programmes de filtrage bay√©sien du courrier √©lectronique √©tait le programme iFile de Jason Rennie, publi√© en 1996.\nLe principe, analogue √† celui du diagnostic m√©dical, repose sur le fait que les mots du dictionnaire ont des probabilit√©s diff√©rentes d‚Äôappara√Ætre dans les spams et dans les courriers l√©gitimes.\nLe filtre de d√©tection des spams ne conna√Æt pas √† l‚Äôavance les probabilit√©s d‚Äôapparition de ces mots, c‚Äôest pourquoi il lui faut une phase d‚Äôapprentissage pour les √©valuer. Cette phase d‚Äôapprentissage est analogue √† la phase de calibrage du test m√©dical √©tudi√© ci-dessus.\nL‚Äôapprentissage se fait √† partir de l‚Äôobservation du comportement des utilisateurs, qui doivent indiquer manuellement si un message est un spam ou non. Pour chaque mot de chaque message ¬´ appris ¬ª, le filtre ajustera les probabilit√©s de rencontrer ce mot dans un spam ou dans un courrier l√©gitime et le stockera dans sa base de donn√©es.\nOn note $P(M|S)$ la probabilit√© qu‚Äôun spam contienne le mot $M$ et $P(M|\\overline{S})$ la probabilit√© qu‚Äôun courrier l√©gitime contienne le mot $M$. Ces deux probabilit√©s sont estim√©es au cours de la phase d‚Äôapprentissage, tout comme la probabilit√© $P(S)$ qu‚Äôun message quelconque soit un spam (analogue √† la pr√©valence $P(\\text{maladie})$ dans le test m√©dical).\nUne fois ces valeurs d√©termin√©es, la formule de Bayes permet de calculer la probabilit√© qu‚Äôun message donn√© soit un spam sachant qu‚Äôil contient le mot $M$ selon la formule¬†:\n$\\displaystyle P(S|M)=\\frac{P(M\\cap S)}{P(M)}=\\frac{P(M|S)P(S)}{P(M|S)P(S)+P(M|\\bar{S})P(\\bar{S})}$\nCette probabilit√© est compar√©e √† un seuil ; si elle est sup√©rieure au seuil, le filtre classera ce message dans les spams.\nDans la r√©alit√©, on travaille non pas sur un seul mot $M$, mais sur un stock de mots, en faisant l\u0026rsquo;hypoth√®se na√Øve que les mots pr√©sents dans un message sont ind√©pendants les uns des autres. Cela est faux dans les langages naturels, o√π par exemple la probabilit√© de trouver un adjectif est influenc√©e par celle de trouver un nom. De plus, cette technique de filtrage, connue sous le nom de filtrage bay√©sien na√Øf, ne tient pas compte du sens des mots, alors qu‚Äôil a une incidence sur la pr√©sence simultan√©e de certains mots √† l\u0026rsquo;int√©rieur du message. Par exemple, la pr√©sence du mot ¬´ anniversaire ¬ª n‚Äôest pas ind√©pendante de celle du mot ¬´ joyeux ¬ª.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/",
	"title": "Logique",
	"tags": [],
	"description": "",
	"content": " Logique La logique est √† l\u0026rsquo;intersection de l\u0026rsquo;informatique, des math√©matiques et de la philosophie.\nCalcul des propositions Syntaxe S√©mantique Preuve Logique modale Syntaxe et s√©mantique Syst√®mes logiques Diff√©rentes logiques modales Le paradoxe de Berry rend non calculable la complexit√© de Kolmogorov. Et cette non calculabilit√© tue dans l\u0026rsquo;≈ìuf le r√™ve du jeune Ray Solomonoff d\u0026rsquo;un super algorithme capable de r√©soudre l\u0026rsquo;int√©gralit√© des probl√®mes scientifiques üò¢\nVoir une chaussure blanche participe-t-il √† prouver que tous les corbeaux sont noirs¬†?\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/meca/",
	"title": "M√©canique",
	"tags": [],
	"description": "",
	"content": " M√©canique √ânergie cin√©tique et travail Une approche historique sur l\u0026rsquo;√©mergence du concept d\u0026rsquo;√©nergie en m√©canique, de Descartes √† Einstein.\nPourquoi une √©nergie cin√©tique en $v^2$¬†? Trois d√©monstrations de la formule de l\u0026rsquo;√©nergie cin√©tique.\nJe trouve la deuxi√®me (celle de Johann Bernoulli) fabuleuse d\u0026rsquo;intuition g√©om√©trique (elle utilise l\u0026rsquo;escargot de Pythagore).\nLagrange et Hamilton Les objets de la m√©canique classique dans sa formulation newtonienne sont des vecteurs √† 3 dimensions qui √©voluent dans le bon vieil espace physique euclidien $\\mathbb{R}^3$.\nMais la fin du 18e et la premi√®re moiti√© du 19e si√®cle ont vu na√Ætre de nouvelles formulations de la m√©canique o√π l\u0026rsquo;ar√®ne m√™me dans laquelle s\u0026rsquo;√©battent les objets physiques √©tudi√©s est chamboul√©e. On peut litt√©ralement parler de nouvelles fa√ßons de voir.\nSouhaitant s\u0026rsquo;affranchir de la g√©om√©trie (quel souhait √©trange), Lagrange vise une approche purement analytique (il ventait l\u0026rsquo;absence de tout sch√©ma dans son manuel de m√©canique\u0026hellip;). Son id√©e centrale fut de basculer de l\u0026rsquo;√©tude de $N$ syst√®mes comme $N$ vecteurs dans $\\mathbb{R}^3$ √† un seul syst√®me composite, un seul vecteur, √©voluant dans $\\mathbb{R}^{3N}$. Toute configuration des $N$ syst√®mes devient un unique point dans l\u0026rsquo;espace $\\mathbb{R}^{3N}$ appel√© alors espace des configurations. Et l\u0026rsquo;√©volution de l\u0026rsquo;ensemble des $N$ objets dessine un chemin unique dans l\u0026rsquo;espace des configurations.\nHamilton souhaite, lui, retrouver la source g√©om√©trique de la m√©canique mais tout en conservant une approche analytique. L\u0026rsquo;ar√®ne d\u0026rsquo;Hamilton ne doit plus seulement d√©crire la position du syst√®me mais son √©tat physique entier, ce qui n√©cessite de savoir comment le syst√®me bouge. On a besoin pour cela de l\u0026rsquo;espace des phases dont chaque point informe √† la fois de la position et de la quantit√© de mouvement de chacun des $N$ objets du syst√®me. L\u0026rsquo;espace des phases est donc de dimension $2\\times 3N$¬†!\nPanorama du principe de moindre action De Newton √† Hamilton en sautant Lagrange Chute libre et paraboles √âtudier la chute libre, c\u0026rsquo;est jouer avec des paraboles.\nForce de Coriolis La force de Coriolis n\u0026rsquo;est pas une force mais traduit juste la gal√®re d\u0026rsquo;aller droit dans un r√©f√©rentiel tournant.\nComprendre cet effet permet d\u0026rsquo;expliquer les rotations des masses d\u0026rsquo;air cycloniques et anticycloniques ou encore d\u0026rsquo;en savoir plus sur ces troyens qui peuplent l\u0026rsquo;orbite de Jupiter.\nTunnels traversant la Terre Petits exercices de niveau licence/pr√©pa traitant de voyages autour ou √† travers la Terre et durant √† chaque fois 42 min (nombre geek par excellence depuis que Douglas Adams en a fait la r√©ponse √† toute chose dans Le Guide du voyageur galactique).\nForces de mar√©e De mani√®re \u0026ldquo;amusante\u0026rdquo;, le ratio des contributions de la Lune et du Soleil aux forces de mar√©e sur Terre est donn√© par le ratio de leurs masses volumiques respectives¬†:\n$$\\frac{F_\\text{mar√©ee,Lune}}{F_\\text{mar√©ee,Soleil}}\\approx\\frac{\\rho_\\text{Lune}}{\\rho_\\text{Soleil}}$$\nEn effet, les forces de mar√©e exerc√©es par un astre sur la Terre sont proportionnelles au gradient de son champ gravitationnel, donc √† $\\displaystyle\\frac{M_\\text{Astre}}{d_\\text{Astre-Terre}^3}$ .\nMais on sait aussi que la Lune et le Soleil sont √† peu pr√®s vu sous le m√™me diam√®tre apparent de 30\u0026rsquo; depuis la Terre (un indice parmi d\u0026rsquo;autres : les √©clipses solaires parfois annulaires, parfois totales). Par cons√©quent, les distances des deux astres sont dans le rapport de leurs rayons (Thal√®s)¬†:\n$$\\frac{d_\\text{Terre-Lune}}{d_\\text{Terre-Soleil}}\\approx\\frac{r_\\text{Lune}}{r_\\text{Soleil}}$$\nOn a donc bien au final¬†:\n$$ \\begin{aligned} \\frac{F_\\text{mar√©ee,Lune}}{F_\\text{mar√©ee,Soleil}} \u0026amp;=\\frac{M_\\text{Lune}}{d_\\text{Lune-Terre}^3}\\left/\\frac{M_\\text{Soleil}}{d_\\text{Soleil-Terre}^3}\\right.\\\\ \u0026amp;=\\frac{M_\\text{Lune}}{M_\\text{Soleil}}\\times \\frac{d_\\text{Soleil-Terre}^3}{d_\\text{Lune-Terre}^3}\\\\ \u0026amp;\\approx \\frac{M_\\text{Lune}}{M_\\text{Soleil}}\\times \\frac{r_\\text{Soleil}^3}{r_\\text{Lune}^3}\\\\ \u0026amp;=\\frac{M_\\text{Lune}}{r_\\text{Lune}^3}\\left/\\frac{M_\\text{Soleil}}{r_\\text{Soleil}^3}\\right.\\\\ \u0026amp;=\\frac{\\rho_\\text{Lune}}{\\rho_\\text{Soleil}} \\end{aligned} $$\nEt avec $\\rho_\\text{Lune} = \\pu{3,3 g*cm-3}$ et $\\rho_\\text{Soleil} = \\pu{1,4 g*cm-3}$, on retrouve que la contribution du Soleil aux forces de mar√©e repr√©sente environ 42% (comme par hasard) de celle de la Lune.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/algebre/groupes/groupe2/",
	"title": "Repr√©sentations",
	"tags": [],
	"description": "",
	"content": " Les repr√©sentations Retour sommaire\nLa physique semble soumise aux sym√©tries (jamais les solutions d\u0026rsquo;un probl√®me ne s\u0026rsquo;affranchissent des sym√©tries qui contraignent le syst√®me). Essence des sym√©tries, les groupes sont l\u0026rsquo;objet id√©al pour d√©coder ce lien ind√©fectible.\nMais on ne manipule pas \u0026ldquo;la physique\u0026rdquo;, seulement des objets math√©matiques sens√©s mod√©liser la r√©alit√© sous-jacente. Ces objets sont toujours les solutions d‚Äô√©quations diff√©rentielles ou int√©grales et vivent donc dans des espaces vectoriels. Le mariage entre physique et espaces vectoriels fut m√™me enti√®rement consomm√© √† l‚Äôav√®nement de la physique quantique puisqu‚Äôils constituent le cadre fondamental de la th√©orie.\nCela serait donc sympa que l‚Äôon r√©ussisse √† balancer nos sym√©tries dans ces espaces vectoriels\u0026hellip; Les repr√©sentations sont l√† pour √ßa.\nUne repr√©sentation, c‚Äôest la description d‚Äôun groupe dans un espace vectoriel.\nMais maintenant qu‚Äôon a un peu de vocabulaire, disons qu\u0026rsquo;une repr√©sentation d\u0026rsquo;un groupe est le r√©sultat d‚Äôun homomorphisme de ce groupe vers le groupe des op√©rateurs lin√©aires sur les espaces vectoriels (espace des √©tats physiques pour ce qui nous int√©resse). Ces op√©rateurs sont incarn√©s par des matrices d√®s qu‚Äôon a une base.\nRemarque\u0026nbsp;:\non peut v√©rifier que les matrices carr√©es d‚Äôun ordre donn√© forment bien des groupes vis-√†-vis de la loi de multiplication entre matrices si n√©anmoins elles ont le bon go√ªt d‚Äô√™tre inversibles¬†:\n√† toute matrice inversible d‚Äôordre n correspond bien s√ªr une matrice inverse, elle-m√™me inversible et d‚Äôordre $n$, matrice inversible d‚Äôordre $n$ $\\times$ matrice inversible d‚Äôordre $n$ $=$ matrice inversible d‚Äôordre $n$, la multiplication de matrices est bien associative, la matrice identit√© d‚Äôordre $n$ sert d‚Äô√©l√©ment neutre. D√©finition \u0026ldquo;technique\u0026rdquo; d\u0026rsquo;une repr√©sentation¬†:\nUne repr√©sentation est une application $g \\in G \\stackrel{U}{\\longmapsto} U(g)$ o√π $U(g)$ est un op√©rateur lin√©aire sur un espace vectoriel $V$, tel que\u0026nbsp;: $U\\left(g_1\\right) U\\left(g_1\\right)=U\\left(g_1 \\cdot g_2\\right)$\nEt sur une base orthonorm√©e de l‚Äôespace vectoriel (de dimension finie), l‚Äôop√©rateur $U$ est associ√© √† une matrice $D$¬†:\n$U(g)\\left|e_i\\right\\rangle=\\left|e_j\\right\\rangle D(g)^j_{\\,i}$\nExemple du groupe de sym√©trie de la mol√©cule d‚Äôammoniac $\\ce{NH3}$\u0026nbsp;: Appelons $\\mathrm{s}_1$, $\\mathrm{s}_2$, $\\mathrm{s}_3$, les orbitales 1s des atomes d‚Äôhydrog√®ne et $\\mathrm{s_N}$ l‚Äôorbitale 2s de l‚Äôatome d‚Äôazote. On d√©nombre 6 √©l√©ments de sym√©trie dans le groupe :\non peut tout laisser √† l‚Äôidentique (√©l√©ment neutre) , on peut permuter circulairement les orbitales $\\mathrm{s}_i$ des hydrog√®nes dans un sens ou dans l‚Äôautre\u0026nbsp;: $\\mathrm{s}_1 \\rightarrow \\mathrm{s}_2 \\rightarrow \\mathrm{s}_3 \\rightarrow \\mathrm{s}_1$\u0026nbsp;, $\\mathrm{s}_1 \\rightarrow \\mathrm{s}_3 \\rightarrow \\mathrm{s}_2 \\rightarrow \\mathrm{s}_1$, on peut permuter deux √† deux les orbitales 1s, √ßa fait 3 possibilit√©s. On v√©rifie √† nouveau l\u0026rsquo;isomorphisme entre $\\boldsymbol{S_3}$ et $\\boldsymbol{D_3}$ puisque si la mol√©cule de $\\ce{NH3}$ a les sym√©tries de $\\boldsymbol{S_3}$, elle a bien aussi celles du triangle √©quilat√©ral, groupe $\\boldsymbol{D_3}$ (m√™mes transformations invariantes¬†: l‚Äôidentit√©, les 2 rotations $2\\pi/3$ et $-2\\pi/3$, et les 3 r√©flexions par rapport aux hauteurs).\nL‚Äôatome d‚Äôazote est, lui, toujours laiss√© invariant.\nOn peut partir de la table de multiplication de ce groupe (voir chapitre pr√©c√©dent) pour construire certaines repr√©sentations.\nRepr√©sentations de dimension 1\u0026nbsp;: La table est respect√©e par une premi√®re repr√©sentation de dimension 1 plut√¥t triviale consistant √† associer le nombre 1 √† chaque √©l√©ment du groupe. Une autre repr√©sentation de dimension 1 un peu plus int√©ressante consiste √† repr√©senter les rotations par des $1$ et les r√©flexions par des $-1$.\nUne rotation suivie d‚Äôune rotation donne bien une autre rotation ($1\\times 1=1$), de m√™me qu‚Äôune r√©flexion suivi d‚Äôune autre r√©flexion ($(-1)\\times (-1)=1$), alors que les compositions crois√©es correspondent effectivement toutes √† des r√©flexions ($1\\times(-1)=-1$). En munissant le plan d‚Äôun rep√®re orthonorm√©, on peut aussi √©crire les matrices 2√ó2 de chacune des transformations et obtenir ainsi une repr√©sentation de dimension 2\u0026nbsp;: l‚Äôidentit√© est alors repr√©sent√©e par\u0026nbsp;: $$ \\left(\\begin{array}{ll} 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \\end{array}\\right) $$\nles deux permutations circulaires (ou rotation de $\\pm2\\pi/3$) par\u0026nbsp;: $$ \\left(\\begin{array}{cc} \\cos (2 \\pi / 3) \u0026amp; -\\sin (2 \\pi / 3) \\\\ \\sin (2 \\pi / 3) \u0026amp; \\cos (2 \\pi / 3) \\end{array}\\right), \\left(\\begin{array}{cc} \\cos (4 \\pi / 3) \u0026amp; -\\sin (4 \\pi / 3) \\\\ \\sin (4 \\pi / 3) \u0026amp; \\cos (4 \\pi / 3) \\end{array}\\right) $$\net les 3 permutation deux √† deux (r√©flexions) par\u0026nbsp;: $$ \\left(\\begin{array}{cc} 1 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \\end{array}\\right), \\left(\\begin{array}{cc} \\cos (2 \\pi / 3) \u0026amp; \\sin (2 \\pi / 3) \\\\ \\sin (2 \\pi / 3) \u0026amp; -\\cos (2 \\pi / 3) \\end{array}\\right), \\left(\\begin{array}{cc} \\cos (4 \\pi / 3) \u0026amp; \\sin (4 \\pi / 3) \\\\ \\sin (4 \\pi / 3) \u0026amp; -\\cos (4 \\pi / 3) \\end{array}\\right) $$\nOn aurait aussi pu simplement partir de la base form√©e des 4 orbitales ($\\mathrm{s_N}$, $\\mathrm{s_1}$, $\\mathrm{s_2}$, $\\mathrm{s_3}$) et regarder ce qu‚Äôil advient de chacune. On obtient alors une repr√©sentation de dimension 4 de ce groupe de transformations\u0026nbsp;: l‚Äôidentit√© s'√©crit ainsi\u0026nbsp;:\n$$ \\left(\\begin{array}{llll} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right) $$\nles deux permutations circulaires (ou rotations)\u0026nbsp;: $$ \\left(\\begin{array}{llll} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{array}\\right) , \\left(\\begin{array}{llll} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right) $$\net les 3 permutation deux √† deux (r√©flexions)\u0026nbsp;: $$ \\left(\\begin{array}{llll} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\end{array}\\right) , \\left(\\begin{array}{llll} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right) , \\left(\\begin{array}{llll} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right) $$\nCet exemple illustre le bazar que le monde des repr√©sentations peut vite devenir\u0026hellip;\nOn va voir comment ranger tout √ßa en regroupant les repr√©sentations par classe et surtout en d√©composant les repr√©sentations d‚Äôun groupe en repr√©sentations irr√©ductibles, atomes de la th√©orie.\nR√©ductibilit√© et irr√©ductibilit√© Pour la plupart des groupes susceptibles d‚Äôint√©resser un physicien, les diverses fa√ßon de les repr√©senter sont limit√©es et peuvent √™tre r√©pertori√©es. Cela structure fortement l‚Äôespace vectoriel dans lequel joue le syst√®me physique.\nAfin de classer les repr√©sentations, il faut d‚Äôabord √™tre s√ªr qu‚Äôelles sont bien diff√©rentes. Pour cela, il faut v√©rifier qu‚Äôelles n‚Äôappartiennent pas √† la m√™me classe d‚Äô√©quivalence¬†:\nSoit $U(G)$ est une repr√©sentation du groupe $G$ sur l‚Äôespace vectoriel $V$ et $S$ n‚Äôimporte quelle op√©rateur inversible sur $V$.\nAlors la repr√©sentation $U^{\\prime}(G)$ telle que $U^{\\prime}(G)=S U(G) S^{-1}$ (les matrices associ√©es sont alors semblables) forme aussi une repr√©sentation de $G$ sur $V$, de m√™me dimension. On dit que $U(G)$ et $U^{\\prime}(G)$ sont √©quivalentes (on note alors $U \\sim U^{\\prime}$).\nEt l‚Äôensemble des repr√©sentations √©quivalentes forme une classe d‚Äô√©quivalence. Il suffit de conna√Ætre un √©l√©ment de chaque classe puisqu‚Äôon peut g√©n√©rer tous les autres √† partir de celui-ci.\nOn n‚Äôa fait l√† qu‚Äôimporter la notion de classes des groupes aux repr√©sentations.\nPour r√©pertorier les diff√©rentes repr√©sentation d‚Äôun groupe, on se concentre donc sur les repr√©sentations non √©quivalentes.\nEt pour s‚Äôassurer que deux repr√©sentations sont √©quivalentes ou non, il faut une grandeur variant d‚Äôune classe √† l‚Äôautre mais pas √† l‚Äôint√©rieur d‚Äôune classe. On appelle une telle quantit√© un invariant de similitude (puisqu‚Äôil est identique pour deux matrices semblables). La trace en est un (√ßa lui donne d‚Äôailleurs son nom¬†: ind√©pendante d‚Äôun changement de base, la trace caract√©rise la matrice)¬†!\nMais chez les repr√©sentations, le vocabulaire s‚Äôenrichit¬†:\nLe caract√®re $\\chi(G)$ d‚Äôun √©l√©ment de $G$ dans une repr√©sentation $U(g)$ est d√©fini comme $\\chi(g)=\\operatorname{Tr} U(g)$. Tous les √©l√©ments du groupe d‚Äôune m√™me classe ont le m√™me caract√®re.\nLe caract√®re caract√©rise donc une classe.\nLes diff√©rentes repr√©sentations √©quivalentes sont autant de doublons √† balayer mais une autre redondance pollue aussi l‚Äôanalyse¬†: une repr√©sentation donn√©e peut √™tre d√©crite comme la somme de ses sous-parties.\nSupposons que l‚Äôon ait deux repr√©sentations $U_1(G)$ et $U_2(G)$ dans deux espaces orthogonaux $V_1$ et $V_2$. On peut alors construire une nouvelle repr√©sentation dans l‚Äôespace somme directe de $V_1$ et $V_2$¬†: $V_1 \\oplus V_2$. La repr√©sentations est alors dite somme directe des repr√©sentations¬†: $U(G)=U_1(G) \\oplus U_2(G)$. Chacun des deux sous-espaces reste invariant sous l‚Äôaction de $U$ par construction (on dit plut√¥t qu‚Äôils sont laiss√©s stables).\nC‚Äôest l‚Äôop√©ration inverse qui va nous int√©resser¬†: quand une repr√©sentation donn√©e peut √™tre d√©compos√©e en sous-repr√©sentations laissant stables certains sous-espaces. La repr√©sentation est alors dites r√©ductibles.\nPr√©cisons le vocabulaire¬†:\nUn sous-espace $V_1$ de $V$ est dit stable par l‚Äôaction de $U(G)$ si pour tout $g\\in G$, et pour tout $x \\in V_1$, $U(g) x \\in V_1$.\nUne repr√©sentation $U(G)$ sur $V$ est dite irr√©ductible s‚Äôil n‚Äôy a pas dans $V$ de sous-espace laiss√© stable par l‚Äôaction de $U(G)$.\nSi un tel sous-espace invariant existe et si le sous-espace orthogonal est aussi invariant, alors la repr√©sentation est dite compl√®tement r√©ductible.\nExemple de repr√©sentation r√©ductible non compl√®tement r√©ductible\u0026nbsp;: c‚Äôest le cas des repr√©sentations du groupe des translations √† une dimension¬†:\n$D(a)=\\left(\\begin{array}{ll} 1 \u0026amp; a \\\\ 0 \u0026amp; 1 \\end{array}\\right)$\nElle laisse invariant tout vecteur $(x,0)$ mais n‚Äôa pas de sous-espace suppl√©mentaire invariant (tentons par exemple $(0,1)$ comme sous-espace compl√©mentaire, on se retrouve avec $D(a)(0,1)=\\binom{a}{1}$ qui n\u0026rsquo;appartient pas au sous-espace $\\{(0,1)\\}$ (sauf pour $a=0$).\nRepr√©sentation unitaire Une repr√©sentation unitaire $U(g)$ est d√©finie sur un espace vectoriel muni d‚Äôun produit scalaire (donnant une norme d√©finie positive). Un tel espace est dit pr√©hilbertien.\nLa repr√©sentation unitaire doit respecter $U^{\\dagger} U=1$ (o√π $U^\\dagger$ est l‚Äôop√©rateur adjoint de $U$). Elle pr√©serve les longueurs, les angles et le produit scalaire, et est donc naturellement associ√©e aux transformations de sym√©trie (d‚Äôo√π son int√©r√™t).\nTechniquement, une repr√©sentation $U(g)$ sur un espace pr√©hilbertien $V$ est unitaire si pour tout $g\\in G$, $\\langle U(g) x \\mid U(g) y\\rangle=\\langle x \\mid y\\rangle$ pour tout $x,y \\in V$, avec $\\langle x \\mid y\\rangle$ d√©signant le produit scalaire entre les vecteurs $|x\\rangle$ et $|y\\rangle$.\nOn peut retrouver le lien entre repr√©sentation unitaire et op√©ration de sym√©trie en partant d‚Äôun √©tat physique quelconque¬†:\nSoit $|\\psi\\rangle$ un ¬´vecteur d‚Äô√©tat¬ª d‚Äôun syst√®me sur un espace vectoriel d‚Äô√©tats physiques. Une op√©ration de sym√©trie transforme $|\\psi\\rangle$ en $|\\psi^\\prime\\rangle$. Les deux ensembles de vecteurs $\\{|\\psi\\rangle\\}$ et $\\{|\\psi^\\prime\\rangle\\}$ doivent fournir des descriptions √©quivalentes du syst√®me physique ce qui implique que l‚Äôop√©rateur de sym√©trie soit lin√©aire.\nDe plus, toute observable physique doit rester invariante sous la transformation or ces observable sont toujours exprim√©es sous la forme de produits scalaires du type $\\langle\\phi \\mid \\psi\\rangle$. Et des transformations lin√©aires qui pr√©servent le produit scalaire sont induites par des op√©rateurs unitaires¬†!\nLes repr√©sentations unitaires ont une propri√©t√© remarquable qui va beaucoup nous occuper¬†:\nSi une repr√©sentation unitaire est r√©ductible alors elle est compl√®tement r√©ductible.\nPreuve\u0026nbsp;: Soit $U(G)$ une repr√©sentation unitaire r√©ductible sur $G$ et soit $V_1$ un sous-espace stable par l‚Äôaction de $U(G)$, et $V_2$ le compl√©ment orthogonal √† $V_1$. Il faut montrer que $V_2$ est lui aussi stable par l‚Äôaction de $U(G)$.\nQuels que soient $x\\in V_1$ et $y\\in V_2$,\n$$ \\begin{aligned} \\langle x \\mid U(g) y\\rangle \u0026amp; =\\left\\langle U\\left(g^{-1}\\right) x \\mid U\\left(g^{-1}\\right) U(g) y\\right\\rangle \\\\ \u0026amp; =\\left\\langle U\\left(g^{-1}\\right) x \\mid U^{-1}(g) U(g) y\\right\\rangle \\\\ \u0026amp; =\\left\\langle U\\left(g^{-1}\\right) x \\mid y\\right\\rangle=0 \\end{aligned} $$\ncar $U\\left(g^{-1}\\right) x \\in V_1$ comme $U(g)(x)$.\nPar cons√©quent $U(g)y$ appartient √† l‚Äôespace orthogonal √† $V_1$, c‚Äôest-√†-dire $V_2$, et ce pour tout $g\\in G$.\nConclusion, l‚Äôespace $V_2$ est stable sous l‚Äôaction de $G$.\nDonc une repr√©sentation unitaire pourra toujours s‚Äô√©crire comme la somme directe de ses repr√©sentations irr√©ductibles et non √©quivalentes (des repr√©sentations √©quivalentes vivent dans le m√™me sous-espace)¬†:\n$$ \\begin{aligned} U(G)\u0026amp;=\\underbrace{U^1(G) \\oplus \\cdots \\oplus U^1(G)}_{n_1\\text{ termes}} \\oplus \\underbrace{U^2(G) \\oplus \\cdots \\oplus U^2(G)}_{n_2\\text{ termes}} \\oplus \\cdots\\\\ \u0026amp;=\\sum_{\\mu \\oplus} n_\\mu U^\\mu(g) \\end{aligned} $$\no√π $n_\\mu$ est le nombre de fois que la repr√©sentation irr√©ductible $\\mu$ appara√Æt dans la d√©composition.\nTout ce qui va suivre d√©coule de cette d√©composition\u0026hellip;\nAvec le bon choix de base, les matrices de la repr√©sentation $U(g)$ appara√Ætront donc diagonales par bloc.\n$$ D(g)=\\left(\\begin{array}{cccc} D^1(g) \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; D^2(g) \u0026amp; \\cdots \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; D^k(g) \\end{array}\\right) $$\nPour tout $g,g^\\prime \\in G$,\n$$ D(g) D(g^{\\prime})=\\left(\\begin{array}{cccc} D^1(g) D^1(g^{\\prime}) \u0026amp; 0 \u0026amp; \\cdots \u0026amp; 0 \\\\ 0 \u0026amp; D^2(g) D^2(g^{\\prime}) \u0026amp; \\cdots \u0026amp; \\vdots \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ 0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; D^k(g) D^k(g^{\\prime}) \\end{array}\\right) $$\nLe non croisement des termes montre que $D(G)$ ne contient pas de nouvelles informations par rapport √† l‚Äôensemble des $D_i(G)$ justifiant que l‚Äôon puisse parler de redondance entre une repr√©sentation et l‚Äôensemble de ses repr√©sentations irr√©ductibles.\nCette d√©composition n‚Äôest a priori possible que pour des repr√©sentations unitaires mais c‚Äôest finalement peu restreignant car outre le fait que les groupes de sym√©trie qui int√©ressent le physicien sont naturellement associ√©s √† des transformations unitaires, toute repr√©sentation d‚Äôun groupe fini ou compact (√©l√©ments variant sur un espace compact) est √©quivalente √† une repr√©sentation unitaire.\nEn effet, toute repr√©sentation d‚Äôun groupe fini sur un espace dot√© d‚Äôun produit scalaire est √©quivalent √† une repr√©sentation unitaire.\nPreuve\u0026nbsp;: √Ä partir du produit scalaire $\\langle x \\mid y\\rangle$ d√©fini sur l‚Äôespace vectoriel $V$, on en construit un nouveau, toujours sur $V$, en op√©rant une sorte de moyennage sur l‚Äôaction du groupe $G$ (cette astuce de passage par la moyenne va beaucoup nous servir)¬†:\n$(x, y) \\equiv \\sum_g\\langle D(g) x \\mid D(g) y\\rangle$\n$(.\\,,.)$ a bien les propri√©t√©s d‚Äôun produit scalaire¬†: bilin√©aire, sym√©trique, positif, d√©fini.\nPassons maintenant de la base orthonorm√©e $\\{e_i\\}$ adapt√©e √† l‚Äôancien produit scalaire, telle que $\\left\\langle e_i \\mid e_j\\right\\rangle=\\delta_{i j}$ √† une base $\\{f_i\\}$ adapt√©e au nouveau, telle que $\\left(f_i, f_j\\right)=\\delta_{i j}$. Soit $S$ la matrice de changement de base¬†: $f_i=S_i^{\\,j} e_j$.\nOn a ainsi¬†: $(x, y)=\\langle S x \\mid S y\\rangle$ pour tout $x$ et $y$ dans $V$.\nLa repr√©sentation $U(g)=S D(g) S^{-1}$, √©quivalente √† $D(g)$, est alors unitaire.\nEn effet¬†:\n$\\begin{aligned} \\langle U(g) x \\mid U(g) y\\rangle \u0026 =\\left\\langle S D(g) S^{-1} x \\mid S D(g) S^{-1} y\\right\\rangle \\\\ \u0026 =\\left(D(g) S^{-1} x, D(g) S^{-1} y\\right) \\\\ \u0026 =\\sum_{g^{\\prime}}\\left\\langle D\\left(g^{\\prime}\\right) D(g) S^{-1} x \\mid D\\left(g^{\\prime}\\right) D(g) S^{-1} y\\right\\rangle \\\\ \u0026 =\\sum_{g^{\\prime \\prime}}\\left\\langle D\\left(g^{\\prime \\prime}\\right) S^{-1} x \\mid D\\left(g^{\\prime \\prime}\\right) S^{-1} y\\right\\rangle \\\\ \u0026 =\\left(S^{-1} x, S^{-1} y\\right) \\\\ \u0026 =\\langle x \\mid y\\rangle \\end{aligned}$ La preuve pr√©c√©dente est g√©n√©ralisable aux groupes compacts en rempla√ßant la somme dans la moyenne par une int√©grale mais √ßa devient un peu plus technique.\nSavoir qu‚Äôune repr√©sentation est r√©ductible nous fait une belle jambe tant qu‚Äôon ne sait pas distinguer une repr√©sentation d√©j√† r√©duite d‚Äôune repr√©sentation pouvant l‚Äô√™tre. Car si la matrice associ√©e n‚Äôa pas le bon go√ªt d‚Äô√™tre d‚Äôores et d√©j√† diagonale par bloc, comment fait-on¬†? Et quand s‚Äôarr√™te-t-on d‚Äôessayer de r√©duire¬†?\nUne propri√©t√© capitale de la th√©orie des repr√©sentations va nous donner les cl√©s pour op√©rer cette diagonalisation par bloc jusqu‚Äôau bout¬†: les repr√©sentations irr√©ductibles froment une base orthonorm√©e de l‚Äôespace des repr√©sentations du groupe.\nConditions d‚Äôorthonormalit√© et de compl√©tude pour les repr√©sentations irr√©ductibles On obtient de haute lutte les relations suivantes¬†:\nUne condition d‚Äôorthonormalit√© entre les repr√©sentations irr√©ductibles (le ¬´produit scalaire¬ª entre deux repr√©sentations irr√©ductibles identiques vaut 1, c‚Äôest ce qu‚Äôon nomme normalit√©, et est nul dans les autres cas, c‚Äôest l‚Äôorthogonalit√©)\u0026nbsp;: $\\displaystyle \\frac{n_\\mu}{n_G} \\sum_g D_\\mu^{\\dagger}(g)^k_i \\, D^\\nu(g)^j_l=\\delta^\\nu_\\mu \\, \\delta^j_i \\,\\delta^k_l$\nEt une conditions de compl√©tude (le mot a beau sonn√© vilain, il traduit que le pavage de l‚Äôespace des repr√©sentations par les morceaux irr√©ductibles est complet)\u0026nbsp;: $\\displaystyle \\sum_{\\mu, l, k} \\frac{n_\\mu}{n_G} D^\\mu(g)^l_k \\, D_\\mu^{\\dagger}\\left(g^{\\prime}\\right)^k_l=\\delta^g_{g^{\\prime}}$\nLe caract√®re complet est d√©montr√© par la tr√®s importante ultime relation\u0026nbsp;: $\\displaystyle\\sum_\\mu n_\\mu^2=n_G$\nLes $\\mu$ et $\\nu$ √©tiquettent les repr√©sentations irr√©ductibles non √©quivalentes de $G$, $n_\\mu$ est la dimension de chacune de ces repr√©sentations, et $n_G$ est le nombre d‚Äô√©l√©ments dans le groupe $G$.\nLes conditions d‚Äôorthonormalit√© sont √©lev√©es au rang de Great Orthogonality Theorem (GOT) chez les anglo-saxons.\nOn peut les r√©√©crire en notant √† la mani√®re quantique $\\langle g \\mid \\nu, j, l\\rangle=\\sqrt{\\frac{n_\\nu}{n_G}} D^\\nu(g)^j_l$ pour insister sur la nature ¬´vectorielle¬ª de ces relations¬†:\n$$\\displaystyle\\sum_g\\langle\\mu, i, k \\mid g\\rangle\\langle g \\mid \\nu, j, l\\rangle=\\delta^\\nu_\\mu \\,\\delta^j_i \\, \\delta^k_l$$\nCette forme permet d\u0026rsquo;appr√©hender plus facilement son interpr√©tation g√©om√©trique. Il faut s‚Äôimaginer un espace vectoriel complexe √† $n_G$ dimensions o√π chaque axe correspond √† un √©l√©ment du groupe.\nChaque $D^\\mu(g)^j_i$ peut donc √™tre vues comme un ¬´vecteur¬ª √† $n_G$ composantes (avec $g$ parcourant $G$), tous orthogonaux entre eux. Le premier de ces vecteurs serait par exemple¬†:\n$ \\left(D^1(e)^1_1, D^1\\left(g_1\\right)^1_1, D^1\\left(g_2\\right)^1_1, \\ldots, D^1\\left(g_{n_G}\\right)^1_1\\right) $\nOn peut aussi r√©√©crire √† la mani√®re quantique la relation de compl√©tude¬†:\n$$\\sum_{\\mu, l, k}\\langle g \\mid \\mu, l, k\\rangle\\left\\langle\\mu, l, k \\mid g^{\\prime}\\right\\rangle=\\delta^g_{g^{\\prime}}$$\nnote\nOn s‚Äôapplique en m√©canique quantique √† v√©rifier des relations du m√™me type sur les vecteurs de base de l‚Äôespace vectoriel des √©tats (c‚Äôest d‚Äôailleurs une des motivations pour la notation compacte en bra-ket adopt√©e ici). Mais au terme ¬´compl√©tude¬ª, les quanticiens pr√©f√®rent la mieux tourn√©e ¬´relation de fermeture¬ª dont le contenu est le m√™me¬†; il s‚Äôagit de prouver que l‚Äôespace ainsi d√©compos√© est complet, c‚Äôest-√†-dire que tout √©tat peut se d√©composer sur les vecteurs de base.\nLa d√©monstration de l‚Äôorthonormalit√© s‚Äôappuie sur le lemme de Schur qu‚Äôon va d√©tailler tout de suite (les relations pr√©c√©dentes sont d‚Äôailleurs parfois appel√©es relations d‚Äôorthogonalit√© de Schur) et la compl√©tude repose sur les repr√©sentations r√©guli√®res qu‚Äôon d√©crira plus loin.\nOp√©rateur d\u0026rsquo;entrelacement Avant d‚Äôarriver au lemme de Schur, il nous faut introduire un nouvel op√©rateur qui g√©n√©ralise la notion d‚Äôhomomorphisme d‚Äôune repr√©sentation √† une autre.\nPrenons une repr√©sentation $U_1(G)$ d‚Äôun groupe $G$ dans un espace vectoriel $V_1$ envoyant un vecteur $v_1$ quelconque vers un vecteur $v^\\prime_1$ pour un certain √©l√©ment $g$ de $G$ et une repr√©sentation $U_2(G)$ du m√™me groupe $G$ dans un espace $V_2$ envoyant un vecteur $v_2$ vers un vecteur $v^\\prime_2$ toujours pour le m√™me √©l√©ment $g$. On aimerait qu‚Äôun homomorphisme $A$ de l‚Äôune √† l‚Äôautre de ces repr√©sentations conserve leurs actions en associant parall√®lement les vecteurs modifi√©s pour chaque $g$.\nPour √™tre plus clair : si $A$ envoie $v_1$ sur $v_2$, on voudrait alors du m√™me coup que $v^\\prime_1$ soit envoy√© sur $v^\\prime_2$.\nOn aurait ainsi $v_2^{\\prime}=U_2(g) v_2=U_2(g) A v_1=A v_1^{\\prime}=A U_1(g) v_1$.\nFormalisons¬†: Soient $U_1(G) \\in V_1$ et $U_2(G) \\in V_2$ deux repr√©sentations. L‚Äôop√©rateur d‚Äôentrelacement $A$ (ou homomorphisme de repr√©sentations) est une application lin√©aire de $V_1$ sur $V_2$ tel que $A U_1(g)=U_2(g) A$ pour tout $g\\in G$.\nOn dit que $A$ entrelace $U_1$ et $U_2$ ce qui est plut√¥t mignon.\nOn dit aussi que $A$ est une application G-√©quivariante, sobriquet assez parlant bien que moins po√©tique.\nEt pour les esprits sch√©matiques, l‚Äôop√©rateur d‚Äôentrelacement peut aussi √™tre d√©finit tel que le diagramme suivant commute pour tout $g\\in G$, le chemin rouge et le chemin bleu arrivant au m√™me endroit¬†:\nL‚Äôensemble de tous les op√©rateurs d‚Äôentrelacement de $U_1$ √† $U_2$ est not√© $\\operatorname{Hom}_G\\left(U_1, U_2\\right)$.\nLemme de Schur\u2028Lemme de Schur\u0026nbsp;: Soit $U_1(G)$ et $U_2(G)$ deux repr√©sentations irr√©ductible d‚Äôun groupe $G$ sur des espaces vectoriels $V_1$ et $V_2$ et $A \\in \\operatorname{Hom}_G\\left(U_1, U_2\\right)$, un op√©rateur d‚Äôentrelacement.\nOn a alors : soit $A=0$, soit $V_1$ et $V_2$ sont isomorphes et les repr√©sentations $U_1$ et $U_2$ sont √©quivalentes.\nPreuve\u0026nbsp;: Si $A=0$, il n‚Äôy a rien √† d√©montrer. Supposons alors $A‚â†0$.\nMontrons que $A$ est surjectif\u0026nbsp;:\ncomme $A‚â†0$, il existe au moins deux vecteurs $v_2$ et $v_1$ respectivement de $V_2$ et $V_1$ tels que $v_2=A v_1$.\nEt pour tout $g\\in G$, on a\u0026nbsp;: $U_2(g) v_2=U_2(g) A v_1=A\\left(U_1(g) v_1\\right)$ qui appartient √† $A V_1$.\nPar cons√©quent, $A V_1$ est un sous-espace stable non nul de $V_2$ par rapport √† $U_2$ et comme $U_2$ est irr√©ductible, on doit avoir $AV_1 = V_2$ . Montrons que $A$ est injectif\u0026nbsp;: soit maintenant $W$ le noyau de $A$. $A$ est une application lin√©aire, or le noyau d‚Äôune application lin√©aire change un tantinet de ceux c√¥toy√©s jusqu‚Äôici puisque l‚Äô√©l√©ment neutre n‚Äôest plus l‚Äôidentit√© mais le vecteur nul (la composition interne consid√©r√©e est l‚Äôaddition). Le noyau √©tant d√©finit comme l‚Äôensemble aboutissant √† l‚Äô√©l√©ment neutre, on a $W=\\left\\{v_1 \\in V_1 ; A v_1=0\\right\\}$.\nPrenons un √©l√©ment $v_1$ de ce noyau. Alors $A\\left(U_1(g) v_1\\right)=U_2(g) A v_1=U_2(g) 0=0$, donc $U_1(g) v_1$ appartient √† $W$ et ce, pour tout $g$. Ce qui implique que $W$ est un sous-espace invariant de $V_1$ pour $U_1$, or comme $U_1$ est irr√©ductible, soit $W=V_1$ mais alors $A=0$, soit $W=\\{0\\}$ (injection). $A$ est donc une application bijective entre $V_1$ et $V_2$, ce qui implique que ces deux espaces soient isomorphes.\nD‚Äôautre part, comme le noyau de $A$ est le vecteur nul, $A$ est inversible et de $U_2(g) A v_1=A\\left(U_1(g) v_1\\right)$, on d√©duit $U_2(g)=A U_1(g) A^{-1}$ pour tout $g\\in G$.\nLes deux repr√©sentations sont bien √©quivalentes.\nOn tire un int√©ressant corollaire du lemme de Schur¬†:\nSoit $U(G)$ une repr√©sentation irr√©ductible et de dimension finie de $G$ dans l‚Äôespace vectoriel $V$ complexe. Alors, un homomorphisme de la repr√©sentation de $U$ sur elle-m√™me (automorphisme appartenant √† $\\operatorname{Hom}_G(U, U)$) est un multiple de la matrice identit√© $E$.\nPreuve\u0026nbsp;: soit $A$ un op√©rateur de cet homomorphisme et soit une valeur propre $Œª \\in \\mathbb{C}$ de $A$ (comme $A$ est inversible et de dim finie sur un espace vectoriel complexe, il y en a forc√©ment au moins une). $A- ŒªE$ appartient aussi √† l‚Äôautomorphisme puisque $E$ en fait partie et qu‚Äôon joue avec des applications lin√©aires (la loi de composition de groupe, si on peut encore l‚Äôappeler ainsi, est justement la combinaison lin√©aire d‚Äô√©l√©ments).\nMais par d√©finition d‚Äôune valeur propre, $A- ŒªE$ n‚Äôest pas inversible donc d‚Äôapr√®s le lemme, il ne peut s‚Äôagir que de l‚Äô√©l√©ment nul (le lemme dit : soit isomorphisme soit z√©ro).\nRemarque¬†:\nC‚Äôest la premi√®re fois qu‚Äôon s‚Äôimpose de travailler sur un espace vectoriel complexe. Lui-seul permet d‚Äôaffirmer qu‚Äôun op√©rateur lin√©aire sur un espace vectoriel de dimension finie poss√®de une valeur propre. C‚Äôest une cons√©quence de l‚Äôexistence d‚Äôune racine pour tout polyn√¥me de degr√© ‚â• 1 dans $\\mathbb{C}$ (le th√©or√®me fondamental de l\u0026rsquo;alg√®bre dit bien qu\u0026rsquo;un polyn√¥me de degr√© $n$ a $n$ racines). En particulier, le polyn√¥me caract√©ristique de l‚Äôop√©rateur a donc une racine.\nEt cela marche aussi pour un espace vectoriel complexe de dimension infinie mais cl√¥t, ce qui permet d‚Äô√©tendre la propri√©t√© aux cas qu‚Äôon rencontrera dans les chapitres suivants.\nNotons que l‚Äôautomorphisme d‚Äôune repr√©sentations commute, par d√©finition, avec tous les √©l√©ments de cette repr√©sentation.\nPar cons√©quent, la propri√©t√© peut se reformuler ainsi¬†:\nSoit $U(G)$ une repr√©sentation irr√©ductible d‚Äôun groupe $G$ dans un espace vectoriel $V$.\nUn op√©rateur $A$ de $V$ commutant avec tous les op√©rateurs ${U(g), g \\in G}$ est un multiple de l‚Äôidentit√©.\nOn utilise cette propri√©t√© pour d√©busquer les op√©rateurs de Casimir comme $\\overrightarrow{J^2}$ (d‚ÄôHenrik Casimir, physicien hollandais).\nEt on en d√©duit aussi (corollaire du corollaire)¬†:\nToute repr√©sentation irr√©ductible d‚Äôun groupe ab√©lien est de dimension 1.\nPreuve\u0026nbsp;: Soit $U(G)$ une repr√©sentation irr√©ductible du groupe ab√©lien $G$. Soit $p$ un √©l√©ment de $G$. Comme $G$ est ab√©lien, on a $U(p) U(g)=U(g) U(p)$ pour tout $g\\in G$.\nD‚Äôapr√®s le lemme de Schur, on a alors $U(p)=\\lambda_p E$. Et cela marche pour tout les $p\\in G$. Par cons√©quent, la repr√©sentation $U(G)$ est √©quivalente √† la repr√©sentation unidimensionnelle $p \\rightarrow \\lambda_p \\in \\mathbb{C}$ pour tous les $p\\in G$.\nEn physique, ces consid√©rations g√©n√©ralisent un r√©sultat bien connu¬†: des op√©rateurs qui commutent poss√®dent un jeu complet de vecteurs propres communs (cf. la recherche d‚Äôun ¬´ECOC¬ª, ensemble complet d‚Äôobservables qui commutent, en m√©canique quantique).\nPr√©cisons cela dans le cas de l‚ÄôHamiltonien.\nLa dynamique d‚Äôun syst√®me physique est d√©termin√©e par un op√©rateur appel√© l‚ÄôHamiltonien $H$ et cet op√©rateur doit, par d√©finition, rester invariant sous les op√©rations de sym√©trie laissant invariant le syst√®me physique lui-m√™me.\nMath√©matiquement, cela revient √† dire que $H$ commute avec les op√©rateurs unitaires de la sym√©trie consid√©r√©e. Et par cons√©quent, dans une repr√©sentation irr√©ductible donn√©e du groupe de sym√©trie, l‚ÄôHamiltonien a pour repr√©sentation un multiple de la matrice identit√©.\nEn d‚Äôautres mots, tous les vecteurs de la repr√©sentation irr√©ductible sont des vecteurs propres de l‚ÄôHamiltonien pour la m√™me valeur propre.\nCette valeur propre de l‚ÄôHamiltonien n‚Äôest autre que l‚Äô√©nergie du syst√®me et donc l‚Äôensemble des √©tats sym√©triques qu‚Äôon obtient (les vecteurs de la repr√©sentation irr√©ductible) sont √† √©nergie fix√©e.\nD√©monstration de l\u0026rsquo;orthonormalit√© On est maintenant arm√© pour prouver l‚Äôorthonormalit√© des repr√©sentations irr√©ductibles¬†:\nOn veut prouver $ \\frac{n_\\mu}{n_G} \\sum_g D_\\mu^{\\dagger}(g)^k_i \\, D^\\nu(g)^j_l=\\delta^\\nu_\\mu \\, \\delta^j_i \\,\\delta^k_l$. Soit $X$ une matrice $n_\\mu \\times n_\\nu$ quelconque √† partir de laquelle on forme¬†:\n$M_X=\\sum_g D_\\mu^{\\dagger}(g) X D^\\nu(g)$ o√π $D_\\mu(g)$ (resp. $D_\\nu(g)$) est une matrice de la repr√©sentation irr√©ductible unitaires de $G$ d‚Äôordre $\\mu$ (resp. $\\nu$). L\u0026rsquo;unitarit√© implique $D_\\mu^{\\dagger}(g)=D_\\mu^{-1}(g)$.\nEn d√©coule¬†:\n$$ \\begin{aligned} D_\\mu^{-1}(p) M_X D^\\nu(p) \u0026 =D_\\mu^{-1}(p)\\left[\\sum_g D_\\mu^{\\dagger}(g) X D^\\nu(g)\\right] D^\\nu(p) \\\\ \u0026 =\\sum_g\\left[D_\\mu^{-1}(p) D_\\mu^{-1}(g)\\right] X\\left[D^\\nu(g) D^\\nu(p)\\right] \\\\ \u0026 =\\sum_{h=p g} D_\\mu^{-1}(h) X D^\\nu(h) \\\\ \u0026 =M_X \\end{aligned} $$ pour tout $p\\in G$.\nD‚Äôapr√®s le lemme de Schur, soit $\\mu‚â†\\nu$ (les deux repr√©sentations ne sont pas √©quivalentes) et alors $M_X = 0$, soit $\\mu=\\nu$ et $M_X=c_X E$ avec $c_X$ une constante (car on est dans le cas de l‚Äôautomorphisme).\nChoisissons $X$ parmi les $n_\\mu \\times n_\\nu$ matrices $X_l^k\\left(k=1, \\cdots, n_\\nu ; l=1, \\cdots, n_\\mu\\right)$ dont les √©l√©ments sont d√©finis par¬†: $\\left(X_l^k\\right)^i_j=\\delta_j^k \\delta_l^i$.\nPrenons un exemple pour fixer les id√©es, avec $n_\\nu=4$ et $n_\\mu=3$, $X^1_2$ s‚Äô√©crit¬†:\n$X_2^1=\\left(\\begin{array}{llll} 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0\\\\ \\end{array}\\right)$\nUn seul √©l√©ment de chaque $X^k_l$ est non nul (et vaut 1), celui se trouvant colonne $k$, ligne $l$.\nOn a alors¬†:\n$\\left(M_l^k\\right)^i_j=\\sum_g D_\\mu^{\\dagger}(g)^i_m\\left(X_l^k\\right)^m_n D^\\nu(g)^n_j=\\sum_g D_\\mu^{\\dagger}(g)^i_l D^\\nu(g)^k_j$ qui doit √™tre nul dans le cas $\\mu‚â†\\nu$, expliquant le terme $\\delta^\\mu_\\nu$ dans la relation √† d√©montrer.\nEt dans le cas $\\mu=\\nu$, $\\left(M_l^k\\right)^i_j=c_l^k \\,\\delta_j^i$ o√π les $c_k^l$ sont des constantes.\nOn d√©termine la valeur de ces constantes en prenant la trace de chacun des membres de l‚Äô√©quation ($i=j$)¬†:\npour le membre de gauche, on obtient $n_\\mu c_l^k$ et pour le membre de droite $\\sum_g\\left[D_\\mu^{\\dagger}(g) D^\\mu(g)\\right]_l^k=n_G \\delta_l^k$.\nFinalement, $c_l^k=\\left(n_G / n_\\mu\\right) \\delta_l^k$.\nRemarque¬†:\nl‚Äôorthonormalit√© entre repr√©sentations irr√©ductibles in√©quivalentes est insuffisante pour obtenir la relation $\\Sigma_\\mu n_\\mu^2=n_G$.\nEn effet, en consid√©rant les $D^\\mu(g)_j^i$, $g \\in G$ comme un ensemble de vecteurs orthogonaux √©tiquet√©s par les $(\\mu,i,j)$, on se retrouve avec $\\Sigma_\\mu n_\\mu^2$ ¬´vecteurs¬ª dans cet ensemble puisque les $(i,j)$ prennent $n_\\mu^2$ valeurs diff√©rentes et chacun de ces vecteurs est form√© de $n_G$ composantes. Or le nombre de vecteurs mutuellement orthogonaux (donc lin√©airement ind√©pendants) doit √™tre au mieux √©gal √† la dimension de l‚Äôespace vectoriel, ici $n_G$.\nOn a donc seulement $\\sum_\\mu n_\\mu^2 \\leq n_G$.\nCela rend n√©anmoins d√©j√† possible la qu√™te principale de la th√©orie des repr√©sentations : le recensement de toutes les repr√©sentations irr√©ductibles non √©quivalentes d‚Äôun groupe donn√©.\nBien qu‚Äôanticipant la preuve, on sait d√©j√† que l‚Äôin√©galit√© est en r√©alit√© toujours satur√©e. Les ¬´vecteurs¬ª √©tiquet√©s par $(\\mu,i,j)$ et form√©s de la collection ordonn√©e de $n_G$ $D^\\mu(g)_j^i$, forment donc bien un ensemble complet en plus d‚Äô√™tre orthogonal.\nApplications de l\u0026rsquo;orthonormalit√© Voyons maintenant comment utiliser l‚Äôorthogonalit√© pour construire de nouvelles repr√©sentations irr√©ductibles.\nExemple\u0026nbsp;: $\\mathrm{C}_2$, groupe le plus simple, a, comme tout les groupes, une repr√©sentations √©videntes¬†: l‚Äôidentit√© $d_1$ d√©finit comme $(e, a) \\xrightarrow{d_1}(1,1)$ (notation signifiant $d_1(e)=1$ et $d_1(a)=1$). La notation r√©duite prend tout son sens avec la relation d‚Äôorthogonalit√© o√π les repr√©sentations sont vues comme des vecteurs). Si une deuxi√®me repr√©sentation non √©quivalente $d_2$ est aussi regard√© comme un vecteur a deux composantes, alors il doit √™tre orthogonal √† $(1,1)$. $(1,-1)$ est la seule possibilit√© √† la fois orthogonale et normalisable. Donc le seul candidat pour une seconde repr√©sentation irr√©ductible est $(e, a) \\xrightarrow{d_2}(1,-1)$.\nOn ne peut trouver d‚Äôautres vecteurs orthogonaux, on a donc l‚Äôensemble des repr√©sentation irr√©ductibles de $\\mathrm{C}_2$.\nPartir de repr√©sentations simples comme l‚Äôidentit√© est une des astuces.\nPour faire progresser l‚Äôinvestigation sur des groupes plus gros, on va avoir recours √† une autre astuce li√©e aux groupes quotients mais cela va nous amener √† digresser un peu\u0026hellip;\nLes groupes quotients fournissent de nouvelles repr√©sentations et permettent en outre de rendre fid√®le une repr√©sentation d√©g√©n√©r√©e.\nRepr√©sentations d‚Äôun groupe quotient¬†:\nSi un groupe $G$ a un sous-groupe invariant $H$ non trivial, alors toute repr√©sentation du groupe quotient $K=G/H$ est aussi une repr√©sentation de $G$, mais cette repr√©sentation de $G$ est d√©g√©n√©r√©e.\n√Ä l‚Äôinverse, si $U(G)$ est une repr√©sentation d√©g√©n√©r√©e de $G$, alors $G$ contient au moins un sous-groupe invariant $H$ tel que $U(G)$ d√©finisse une repr√©sentation fid√®le (non d√©g√©n√©r√©e) du groupe quotient $G/H$.\nExemple\u0026nbsp;: On a d√©j√† vu que $\\mathrm{S_3}$ a un sous-groupe invariant $H=\\{e,(123),(321)\\}$. Le groupe quotient est isomorphe √† $\\mathrm{C_2}=\\{e,a\\}$. Or $\\mathrm{C_2} $ a une repr√©sentation assez simple $\\{(e, a) \\rightarrow(1,-1)\\}$ (on reviendra plus loin sur ce type de notation et la m√©thode pour trouver une repr√©sentation autrement que par t√¢tonnement).\nCela induit une repr√©sentation unidimensionnelle de $\\mathrm{S_3}$ associant $1$ aux √©l√©ments $\\{e,(123),(321)\\}$ et $-1$ √† $\\{(12),(23),(31)\\}$. C‚Äôest effectivement une des possibilit√©s trouv√©es dans l‚Äôexemple pr√©c√©dent en jouant avec la table de multiplication du groupe. Cette repr√©sentation, d√©g√©n√©r√©e pour $\\mathrm{S_3}$, est bien une repr√©sentation fid√®le pour $S_3 / H \\simeq C_2$.\nRemarque¬†:\nles repr√©sentations d√©g√©n√©r√©es et la mani√®re de les rendre fid√®le (ces choix s√©mantiques traduisent-ils une croisade morale de certains math√©maticiens¬†?) reviendront sur le devant de la sc√®ne quand on parlera des groupes compacts (comme les rotations) et des topologies associ√©es.\nCe nouvel outil peut faciliter grandement la recherche de repr√©sentations irr√©ductibles comme on va le voir dans l‚Äôexemple suivant.\nExemple\u0026nbsp;: consid√©rons le groupe dih√©dral $\\mathrm{D_2}$ dont la table de multiplication est¬†:\nLe groupe correspond aux sym√©tries de la figure suivante : Les 4 √©l√©ments de sym√©trie sont l‚Äôidentit√©, les deux r√©flexions $(13)$ et $(24)$ et la rotation d‚Äôangle œÄ. Associ√©s deux √† deux, √ßa redonne un des 4. Et comme ils commutent tous, le groupe est ab√©lien. Par cons√©quent, on s‚Äôattend √† des repr√©sentations unidimensionnelle.\nLa premi√®re de ces repr√©sentations est la triviale identit√©\u0026nbsp;: $(e, a, b, c) \\xrightarrow{d_1}(1,1,1,1)$. Maintenant, l‚Äôastuce : les √©l√©ments $\\{e,a\\}$ forment un sous-groupe invariant et le groupe quotient $\\{\\{e, a\\},\\{b, c\\}\\}$ est isomorphe √† $\\mathrm{C_2}$ (il n‚Äôy a qu‚Äôun groupe √† deux √©l√©ments) dont on connait d√©j√† les deux repr√©sentations irr√©ductibles. Les deux repr√©sentations de $\\mathrm{C_2}$ induisent deux repr√©sentations d√©g√©n√©r√©es sur $\\mathrm{D_2}$. La premi√®re est la perp√©tuelle repr√©sentation identit√©, la deuxi√®me associe $-1$ √† $b$ et $c$¬†: $(e, a, b, c) \\xrightarrow{d_2}(1,1,-1,-1)$.\nOn peut ensuite tenir le m√™me raisonnement en partant du sous-groupe invariant $\\{e,b\\}$ qui nous am√®ne la troisi√®me repr√©sentation¬†: $(e, a, b, c) \\xrightarrow{d_3}(1,-1,1,-1)$.\nEnfin, le sous-groupe invariant $\\{e,c\\}$ nous donne la quatri√®me et derni√®re¬†: $(e, a, b, c) \\xrightarrow{d_4}(1,-1,-1,1)$.\nOn v√©rifie bien que les 4 ¬´vecteur¬ª sont orthonornaux comme il se doit et qu‚Äôaucun autre ne peut exister par la m√™me condition.\nRemarques¬†:\nLa relation $\\Sigma_\\mu n_\\mu^2=n_G$ (que l‚Äôon d√©montrera plus loin) nous assure que pour tout groupe ab√©lien (repr√©sentation unidimensionnelle $\\Leftrightarrow n_\\mu=1$), il y a autant de repr√©sentation que d‚Äô√©l√©ments ($n_G$) et les vecteurs form√©s par les repr√©sentations (√©tiquet√©s par $\\mu$) forment alors un ensemble orthogonal complet.\nLes repr√©sentations de $\\mathrm{C_2}$ et $\\mathrm{D_2}$ d√©busqu√©es dans les exemples pr√©c√©dents illustrent bien ce point.\nPlus besoin dans ce cas de guillemets au mot vecteur, il ne s‚Äôagit plus d‚Äôune collection pachydermique de $n_G$ objets de dimension $n_\\mu^2$ mais bien d‚Äôune collection ordonn√©e de $n_G$ nombres qu‚Äôon d√©signe plus volontiers ainsi (ou des tenseurs de rang 1 si on veut fanfaronner), m√™me si techniquement, les gros objets form√©s par une repr√©sentation quelconque peuvent tout autant revendiquer l‚Äôappellation. On s‚Äôint√©ressera dans les chapitres suivants √† des groupes infinis et par chance, en d√©pit de quelques ajustements cosm√©tiques (les int√©grales remplaceront les sommes discr√®tes), la condition d‚Äôorthonormalit√© des repr√©sentations irr√©ductibles tient le choc.\nPour le groupe infini le plus simple, ab√©lien et √† une dimension, elle se confond avec le th√©or√®me de Fourier des fonctions p√©riodiques et aiguise alors notre vision des choses ; la condition d‚Äôorthonormalit√© est en fin de compte une puissante extension du th√©or√®me de Fourier. Comme on l‚Äôa d√©j√† √©voqu√©, les matrices des repr√©sentations d√©pendent de la base. Pour les repr√©sentations 1D des groupes ab√©liens, pas de soucis, mais chez les groupes plus complexes, les dimensions suppl√©mentaires n‚Äôapporteront que tracas et confusion.\nGr√¢ce aux caract√®res, ind√©pendants de la base, on pourrait maintenir un traitement √©quivalent aux douillettes repr√©sentations 1D quelle que soit la dimension. Transposer les relations d‚Äôorthonormalit√© chez les caract√®res, bien plus engageants, semble donc une entreprise judicieuse.\nConditions d‚Äôorthonormalit√© et de compl√©tude pour les caract√®res irr√©ductibles Rappelons √† toute fin utile que les caract√®res d‚Äôune repr√©sentation $U(G)$ sont les traces des op√©rateurs $U(g)$. Ils sont ind√©pendant du choix de la base dans l‚Äôespace des repr√©sentations. Tous les √©l√©ments d‚Äôun groupe appartenant √† une m√™me classe ont par cons√©quent le m√™me caract√®re dans une repr√©sentation donn√©e.\nSoit $U^\\mu(G)$ une repr√©sentation irr√©ductible de $G$.\nAlors la somme des $U^\\mu(g)$ sur l‚Äôensemble des √©l√©ments d‚Äôune classe donn√©e vaut¬†:\n$\\sum_{h \\in \\zeta_i} U^\\mu(h)=\\frac{n_i}{n_\\mu} \\chi_i^\\mu E$\no√π $\\zeta_i$ est la classe $i$, $E$ l‚Äôop√©rateur identit√©, $n_\\mu$ la dimension de la repr√©sentation et $n_i$ le nombre d‚Äô√©l√©ments dans la classe $i$.\nPreuve\u0026nbsp;: Notons $A_i$ le membre de gauche de l‚Äô√©quation. On a alors $U^\\mu(g) A_i U^\\mu(g)^{-1}=A_i$ puisque le produit ne fait que r√©arranger l‚Äôordre de la sommation (en utilisant le fait que si $h \\in \\zeta_i$, alors $g h g^{-1} \\in \\zeta_i$ pour tout $g\\in G$). Donc $A_i$ commute avec toutes les repr√©sentations, ce qui, d‚Äôapr√®s le lemme de Schur, impose d‚Äô√™tre proportionnel √† l‚Äôop√©rateur identit√©¬†: $A_i=c_i E$. On trouve $c_i$ en √©valuant la trace des deux membres de l‚Äô√©quation. √Ä gauche, √ßa donne $n_i \\chi_i^\\mu$, et √† droite $c_i n_\\mu$.\n√áa va nous permettre de d√©montrer les relations d‚Äôorthonormalit√© et de compl√©tude sur le groupe des caract√®res. Mais d‚Äôabord, √©non√ßons-les\u0026hellip;\nLes caract√®res de repr√©sentations irr√©ductibles non √©quivalentes d‚Äôun groupe $G$ satisfont les relations suivantes¬†:\u2028$$ \\sum_i \\frac{n_i}{n_G} \\chi_\\mu^{\\dagger i} \\chi_i^\\nu=\\delta_\\mu^\\nu \\quad \\text { orthonormalit√© } $$\n$$ \\frac{n_i}{n_G} \\sum_\\mu \\chi_i^\\mu \\chi_\\mu^{\\dagger j}=\\delta_i^j \\quad \\text { compl√©tude } $$\no√π par convention $\\chi_\\mu^{\\dagger i}=\\left(\\chi_i^\\mu\\right)^*$\nLes $i$ courent sur les diff√©rentes classes du groupe et les $\\mu$ sur les diff√©rentes repr√©sentations irr√©ductibles non √©quivalentes. Preuve\u0026nbsp;: On part de la condition d‚Äôorthonormalit√© des repr√©sentations irr√©ductibles en imposant $i=k$ et $k=l$ pour obtenir les traces.\n√Ä gauche, on obtient¬†:\n$$ \\left(n_\\mu / n_G\\right) \\sum_g \\chi_\\mu^{\\dagger}(g) \\chi^\\nu(g)=\\left(n_\\mu / n_G\\right) \\sum_i n_i \\chi_\\mu^{\\dagger i}(g) \\chi_i^\\nu(g) $$\no√π on finit par sommer sur les classes (√† l‚Äôint√©rieur desquelles le caract√®re est invariant) plut√¥t que sur les √©l√©ments du groupe. Et √† droite, on obtient $n_\\mu \\delta_\\mu^\\nu$.\nPartons maintenant de la relation de compl√©tude entre repr√©sentations irr√©ductibles¬†:\n$$ \\sum_{\\mu, l, k} \\frac{n_\\mu}{n_G} D^\\mu(g)_k^l D_\\mu^{\\dagger}\\left(g^{\\prime}\\right)^k_l=\\delta_{g g^{\\prime}} $$\net sommons les $g$ parmi les √©l√©ments de la classe $\\zeta_i$, et les $g^\\prime$ parmi les √©l√©ments de la classe $\\zeta_j$¬†:\n$$ \\sum_{\\mu, l, k} \\frac{n_\\mu}{n_G} \\sum_{g \\in \\zeta_i} D^\\mu(g)_k^l \\sum_{g^{\\prime} \\in \\zeta_j} D_\\mu^{\\dagger}\\left(g^{\\prime}\\right)^k_l=\\sum_{g \\in \\zeta_i} \\sum_{g^{\\prime} \\in \\zeta_j} \\delta_{g g^{\\prime}} $$\n√Ä droite, √ßa nous donne $n_j \\delta_i^j$ et √† gauche, on utilise la relation d√©montr√©e un peu plus haut $\\sum_{h \\in \\zeta_i} U^\\mu(h)=\\frac{n_i}{n_\\mu} \\chi_i^\\mu E$. On obtient¬†:\n$$ \\sum_{\\mu, l, k} \\frac{n_\\mu}{n_G} \\frac{n_i}{n_\\mu} \\chi_i^\\mu E_k^l \\frac{n_j}{n_\\mu} \\chi_\\mu^{\\dagger j} E_l^k=\\sum_{\\mu, l, k} \\frac{n_i n_j}{n_\\mu n_G} \\chi_i^\\mu \\chi_\\mu^{\\dagger j} E_k^l E_l^k $$\nOr $\\sum_{l, k} E^l_k E^k_l=\\sum_l E_l^l=\\operatorname{Tr} E=n_\\mu$. D\u0026rsquo;o√π¬†:\n$$ \\sum_{\\mu, l, k} \\frac{n_\\mu}{n_G} \\frac{n_i}{n_\\mu} \\chi_i^\\mu E_k^l \\frac{n_j}{n_\\mu} \\chi_\\mu^{\\dagger j} E_l^k=\\sum_\\mu \\frac{n_i n_j}{n_G} \\chi_i^\\mu \\chi_\\mu^{\\dagger j} $$\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc0/",
	"title": "TQC-0",
	"tags": [],
	"description": "",
	"content": " Th√©orie quantique des champs \u0026ndash; Partie 0 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell.\nLagrangien et principe de moindre action La d√©termination de la trajectoire $x(t)$ d\u0026rsquo;une particule entre A et B par int√©gration de la 2e loi de Newton, $F=m\\ddot{x}$, ne cadre pas avec les enseignements de la m√©canique quantique. Car si on peut bien mesurer la particule en A √† $t=0$ et en B √† $t=\\tau$, un voile imp√©n√©trable nous emp√®che de savoir pr√©cis√©ment ce qui s\u0026rsquo;est pass√© entre les deux. Une √©quation diff√©rentielle ne semble donc pas un point de d√©part prometteur pour calculer $x(t)$\u0026hellip;\nConcentrons-nous sur les variations de l\u0026rsquo;√©nergie cin√©tique $T$ et l\u0026rsquo;√©nergie potentielle $V$ le long de la trajectoire. L\u0026rsquo;√©nergie totale $E=T+V$ doit √™tre une constante du mouvement de la particule mais l\u0026rsquo;√©quilibre entre les deux types d\u0026rsquo;√©nergie peut varier.\nNotons $\\displaystyle\\bar{T}=\\frac{1}{\\tau}\\int_0^\\tau\\frac{1}{2}m[\\dot{x}(t)]^2 dt$ l\u0026rsquo;√©nergie cin√©tique moyenne sur la trajectoire et $\\displaystyle\\bar{V}=\\frac{1}{\\tau}\\int_0^\\tau V[x(t)] dt$, l\u0026rsquo;√©nergie potentielle moyenne. On a bien s√ªr $E=\\bar{E}=\\bar{T}+\\bar{V}$.\nMath√©matiquement, $\\bar{T}$ et $\\bar{V}$ sont des fonctionnelles. L√† o√π des fonctions se nourrissent de nombres pour produire des nombres, les fonctionnelles produisent des nombres √† partir de fonctions ($\\bar{T}$ produit un nombre √† partir de $\\dot{x}(t)$ et $\\bar{V}$ √† partir de $x(t)$).\nLa d√©riv√©e fonctionnelle de $F[f(x)]$ nous dit comment le nombre produit par la fonctionnelle varie lorsqu\u0026rsquo;on change l√©g√®rement la fonction $f(x)$ qu\u0026rsquo;elle machouille¬†:\n$$ \\frac{\\delta F}{\\delta f(x)}=\\lim_{\\epsilon\\to 0}\\frac{F[f(x')+\\epsilon\\delta(x-x')]-F[f(x')]}{\\epsilon} $$ D√©riv√©es fonctionnelles utilis√©es dans la suite\u0026nbsp;: $F[f]=\\int_a^b g[f(x)]\\mathrm{d}x$ avec $g'=\\mathrm{d}g/\\mathrm{d}f$ $$ \\begin{aligned} \\frac{\\delta F[f]}{\\delta f(x_0)}\u0026=\\lim_{\\epsilon \\to 0}\\frac{1}{\\epsilon}\\left[\\int g[f(x)+\\epsilon \\delta(x-x_0)]dx-\\int g[f(x)]\\mathrm{d}x\\right]\\\\\\\\ \u0026=\\lim_{\\epsilon \\to 0}\\frac{1}{\\epsilon}\\left[\\int (g[f(x)]+\\epsilon\\delta(x-x_0)g'[f(x)])\\mathrm{d}x - \\int g[f(x)]\\mathrm{d}x\\right]\\\\\\\\ \u0026=\\int \\delta(x-x_0) g'[f(x)] \\mathrm{d}x\\\\\\\\ \u0026= g'[f(x_0)] \\end{aligned} $$ On d√©termine ainsi que $\\displaystyle \\frac{\\delta \\bar{V}[x]}{\\delta x(t)}=\\frac{1}{\\tau}V\u0026rsquo;[x(t)]$.\n$F[f]=\\int g(f\u0026rsquo;)\\mathrm{d}y$ avec $f\u0026rsquo;=\\mathrm{d}f/\\mathrm{d}y$\n$$\\frac{\\delta F[f]}{\\delta f(x)}=\\lim_{\\epsilon \\to 0}\\frac{1}{\\epsilon}\\left[\\int dy \\,g\\!\\left( \\frac{\\partial}{\\partial y}[f(y)+\\epsilon \\delta(y-x)]\\right) - \\int dy \\,g\\!\\left(\\frac{\\partial f}{\\partial y}\\right)\\right]$$ Par un d√©veloppement de Taylor au premier ordre, on obtient¬†:\n$$g\\left(\\frac{\\partial}{\\partial y}[f(y)+\\epsilon\\delta(y-x)]\\right)=g(f'+\\epsilon\\delta'(y-x))\\approx g(f')+\\epsilon\\delta'(y-x)\\frac{\\mathrm{d}g(f')}{\\mathrm{d} f'}$$ On aboutit √†¬†:\n$$\\frac{\\delta F[f]}{\\delta f(x)}=\\int \\mathrm{d}y \\delta'(y-x)\\frac{\\mathrm{d}g(f')}{\\mathrm{d} f'}$$ On utilise alors une int√©gration par partie\u0026nbsp;: $$\\int \\mathrm{d}y\\, \\delta'(y-x)\\frac{\\mathrm{d}g(f')}{\\mathrm{d} f'}=\\left[\\delta(y-x)\\frac{\\mathrm{d}g(f')}{\\mathrm{d} f'}\\right]-\\int \\mathrm{d}y \\,\\delta(y-x)\\frac{\\mathrm{d}}{\\mathrm{d} y}\\left(\\frac{\\mathrm{d}g(f')}{\\mathrm{d}f'}\\right)$$ Si $x$ se balade sur l'intervalle d'int√©gration, le terme entre crochets est de mesure nulle et on obtient finalement\u0026nbsp;: $$\\frac{\\delta F[f]}{\\delta f(x)}=-\\frac{\\mathrm{d}}{\\mathrm{d} x}\\left(\\frac{\\mathrm{d}g(f\u0026rsquo;)}{\\mathrm{d}f\u0026rsquo;}\\right)$$\nOn d√©termine ainsi que $\\displaystyle \\frac{\\delta \\bar{T}[x]}{\\delta x(t)}=-\\frac{m\\ddot{x}}{\\tau}$.\nD\u0026rsquo;autre part, si on a $F[\\phi]=\\int\\left(\\frac{\\partial\\phi}{\\partial y}\\right)^2\\mathrm{d}y$, on obtient $\\displaystyle \\frac{\\delta F[\\phi]}{\\delta\\phi(x)}=-2\\frac{\\partial^2\\phi}{\\partial x^2}$.\nR√©sultat qui se g√©n√©ralise √† 3 dimensions pour donner un truc plut√¥t utile¬†:\nSi $\\displaystyle I=\\int(\\nabla\\phi)^2\\mathrm{d}^3 x$, alors $\\displaystyle\\frac{\\delta I}{\\delta\\phi}=-2\\nabla^2\\phi$.\nOn peut maintenant v√©rifier comment $\\bar{T}$ et $\\bar{V}$ varient lorsque la trajectoire $x(t)$ est un peu modifi√©e¬†:\n$$ \\frac{\\delta \\bar{V}[x]}{\\delta x(t)}=\\frac{V'[x(t)]}{\\tau}\\qquad \\qquad\\frac{\\delta \\bar{T}[x]}{\\delta x(t)}=-\\frac{m\\ddot{x}}{\\tau} $$ Or d\u0026rsquo;apr√®s la 2e loi de Newton, la trajectoire classique correspond √† $m\\ddot{x}=-dV/dx$ et d\u0026rsquo;apr√®s ce qui pr√©c√®de, cela entra√Æne que $\\displaystyle \\frac{\\delta \\bar{V}[x]}{\\delta x(t)}=\\frac{\\delta \\bar{T}[x]}{\\delta x(t)}$.\nOn en conclut que pour une l√©g√®re d√©viation autour de la trajectoire classique, l\u0026rsquo;√©nergie potentielle moyenne et l\u0026rsquo;√©nergie cin√©tique moyenne vont varier ensemble (g√©n√©ralement augmenter) et de la m√™me valeur¬†!\nCela peut se r√©√©crire $\\displaystyle \\frac{\\delta }{\\delta x(t)}(\\bar{T}[x]- \\bar{V}[x])=0$.\nLa diff√©rence entre les deux √©nergies moyennes est stationnaire pr√®s de la trajectoire classique.\nOn d√©cide alors de donner un nom √† cette diff√©rence entre √©nergie cin√©tique et potentielle¬†: le lagrangien $L$.\n$$ L=T-V $$ L\u0026rsquo;int√©grale du lagrangien sur le temps d√©finit l\u0026rsquo;action $S$.\n$$ S=\\int_0^\\tau L \\mathrm{d}t $$ Ces deux nouvelles grandeurs vont nous permettre de r√©√©crire de mani√®re synth√©tique ce qu\u0026rsquo;on a d√©couvert jusque-l√†.\n$\\displaystyle S=\\int_0^\\tau(T-V)\\mathrm{d}t = \\tau(\\bar{T}[x]-\\bar{V}[x])$, et donc\n$$ \\frac{\\delta S}{\\delta x(t)} = 0 $$ C\u0026rsquo;est le principe de moindre action de Hamilton. √âquations d\u0026rsquo;Euler-Lagrange¬†:\nLe lagrangien $L$ peut s\u0026rsquo;√©crire comme une fonction √† la fois de la position et de la vitesse $L(x(t),\\dot{x}(t))$. Le principe de moindre action donne alors¬†:\n$$ \\frac{\\delta L}{\\delta x(t)}-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\delta L}{\\delta\\dot{x}(t)} = 0 $$ D√©monstration\u0026nbsp;: $$ \\begin{aligned} \\frac{\\partial S}{\\partial x(t)} \u0026 = \\int \\mathrm{d}u\\left[\\frac{\\partial L}{\\partial x(u)}\\frac{\\partial x(u)}{\\partial x(t)}+\\frac{\\partial L}{\\partial \\dot{x}(u)}\\frac{\\partial \\dot{x}(u)}{\\partial x(t)}\\right]\\\\\\\\ \u0026 = \\int \\mathrm{d}u\\left[\\frac{\\partial L}{\\partial x(u)}\\delta(u-t)+\\frac{\\partial L}{\\partial \\dot{x}(u)}\\frac{\\mathrm{d}}{\\mathrm{d}t}\\delta(u-t)\\right]\\\\\\\\ \u0026=\\frac{\\partial L}{\\partial x(t)}+\\left[\\delta(u-t)\\frac{\\delta L}{\\delta \\dot{x}(u)}\\right]_{t_i}^{t_f}-\\int \\mathrm{d}u\\,\\delta(u-t)\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\delta L}{\\delta\\dot{x}(u)}\\\\\\\\ \u0026=\\frac{\\delta L}{\\delta x(t)}-\\frac{\\mathrm{d}}{\\mathrm{d}t}\\frac{\\delta L}{\\delta\\dot{x}(t)} \\end{aligned} $$ On introduit aussi la densit√© lagrangienne $\\mathcal{L}$¬†:\n$$ L=\\int \\mathscr{d}x\\,\\mathcal{L} $$ $$ S=\\int \\mathscr{d}t\\,\\mathscr{d}x\\,\\mathcal{L} $$\nPla√ßons-nous d√©sormais dans un cadre relativiste¬†: le lagrangien d√©pend maintenant d\u0026rsquo;une fonction $\\phi(x)$ o√π $x$ est un point de l\u0026rsquo;espace-temps dont la d√©riv√©e est le 4-vecteur $\\partial_\\mu\\phi$.\nL\u0026rsquo;action devient¬†:\n$$S=\\int \\mathrm{d}^4 x\\mathcal{L}(\\phi,\\partial_\\mu\\phi)$$\nLe principe de moindre action fournit maintenant une version quadrivectorielle des √©quations d\u0026rsquo;Euler-Lagrange¬†:\n$$ \\frac{\\delta S}{\\delta \\phi} = \\frac{\\delta L}{\\delta \\phi}-\\partial_\\mu\\left(\\frac{\\partial\\mathcal{L}}{\\partial(\\partial_\\mu\\phi)}\\right) = 0 $$ Le principe de moindre action tire sa justification de la m√©canique quantique o√π les particules ne sont plus des particules mais des ondes. La particule allant d\u0026rsquo;un point A √† un point B en empruntant toutes les trajectoires possibles (jusqu\u0026rsquo;aux plus saugrenues) devient donc une onde affubl√©e d\u0026rsquo;un facteur de phase $\\mathrm{e}^{\\mathrm{i}S/\\hbar}$ o√π $S$ est l\u0026rsquo;action. Une action stationnaire correspond alors √† une phase stationnaire.\nLorsqu\u0026rsquo;on se retrouve √† sommer sur toutes les trajectoires possibles, les diff√©rents termes vont interf√©rer. L\u0026rsquo;interf√©rence sera destructive dans l\u0026rsquo;immense majorit√© des cas, l√† o√π la phase fluctuera fortement d\u0026rsquo;une trajectoire √† l\u0026rsquo;autre. Au contraire, la trajectoire minimisant l\u0026rsquo;action va √©merger car toutes les trajectoires voisines auront des phases proches et interf√©reront constructivement.\nChapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc1/",
	"title": "TQC-1",
	"tags": [],
	"description": "",
	"content": " Th√©orie quantique des champs \u0026ndash; Partie 1 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell.\nLes oscillateurs harmoniques Une vid√©o vieille comme tout sur l\u0026rsquo;oscillateur harmonique quantique qui traite pas mal des points de ce chapitre¬†:\nPremi√®re quantification¬†: les particules se comportent comme des ondes.\nSeconde quantification¬†: les ondes se comportent comme des particules.\nPartons d\u0026rsquo;une masse $m$ accroch√©e √† un ressort de constante de raideur $K$. La quantit√© de mouvement de la masse est donn√©e par $p=m\\dot{x}$. L\u0026rsquo;√©nergie totale $E$ vaut la somme de l\u0026rsquo;√©nergie cin√©tique $p^2/2m$ et de l\u0026rsquo;√©nergie potentielle $\\frac{1}{2}Kx^2$.\nEn m√©canique quantique, on remplace $p$ par l\u0026rsquo;op√©rateur impulsion $-i\\hbar\\partial /\\partial x$ et on obtient alors l\u0026rsquo;√©quation de Schr√∂dinger d\u0026rsquo;un oscillateur harmonique¬†: $$\\left(-\\frac{\\hbar^2}{2m}\\frac{\\partial^2}{\\partial x^2}+\\frac{1}{2}K x^2\\right)\\psi=E\\psi$$\nLes solutions sont donn√©es par¬†:\n$$\\psi_n(\\xi)=\\frac{1}{\\sqrt{2^n n!}}\\left(\\frac{m\\omega}{\\pi\\hbar}\\right)^{1/4}H_n(\\xi)\\rm{e}^{-\\xi^2/2}$$\no√π les $H_n(\\xi)$ sont des polyn√¥mes de Hermite et o√π $\\xi=\\sqrt{m\\omega/\\hbar x}$\nSi ces fonctions propres ressemblent pas mal √† des ondes, les valeurs propres $E_n=\\left(n+\\frac{1}{2}\\right)\\hbar\\omega$ (avec $\\omega=\\sqrt{K/m}$) se rangent, elles, plut√¥t du c√¥t√© particules. On remarque que pour $n=0$, l\u0026rsquo;√©nergie n\u0026rsquo;est pas nulle mais vaut $\\hbar\\omega/2$. C\u0026rsquo;est l\u0026rsquo;√©nergie de point z√©ro.\nAjouter un quantum d\u0026rsquo;√©nergie $\\hbar\\omega$ permet de monter d\u0026rsquo;un barreau l\u0026rsquo;√©chelle des √©nergies, ce que l\u0026rsquo;on a bien envie de mod√©liser par l\u0026rsquo;absorption d\u0026rsquo;une particule. On peut formaliser √ßa √©l√©gamment (et sans se salir les mains avec les polyn√¥mes de Hermite).\nOn part de l\u0026rsquo;Hamiltonien de l\u0026rsquo;oscillateur harmonique¬†:\n$$\\hat{H}=\\frac{\\hat{p}^2}{2m}+\\frac{1}{2}m\\omega^2\\hat{x}^2$$\no√π on a r√©exprim√© la constante de raideur : $K=m\\omega^2$.\nL\u0026rsquo;Hamiltonien semble vouloir √™tre factoris√© en $\\frac{1}{2}m\\omega^2\\left(\\hat{x}-\\frac{\\mathrm{i}}{m\\omega}\\hat{p}\\right)\\left(\\hat{x}+\\frac{\\mathrm{i}}{m\\omega}\\hat{p}\\right)$, mais un probl√®me se dresse¬†: $\\hat{x}$ et $\\hat{p}$ ne commutent pas¬†! En effet, $\\left[\\hat{x},\\hat{p}\\right]\\equiv \\hat{x}\\hat{p}-\\hat{p}\\hat{x}=\\mathrm{i}\\hbar$.\nPar cons√©quent¬†:\n$$\\frac{1}{2}m\\omega^2\\left(\\hat{x}-\\frac{\\mathrm{i}}{m\\omega}\\hat{p}\\right)\\left(\\hat{x}+\\frac{\\mathrm{i}}{m\\omega}\\hat{p}\\right)=\\frac{1}{2}m\\omega^2\\hat{x}^2+\\frac{\\hat{p}^2}{2m}+\\frac{\\mathrm{i}\\omega}{2}[\\hat{x},\\hat{p}]$$ Plut√¥t que $\\hat{H}$, on obtient donc $\\hat{H} -\\frac{\\hbar\\omega}{2}$, l'Hamiltonien corrig√© de l'√©nergie de point z√©ro. Cela ne semble pas un probl√®me ind√©passable. Les deux op√©rateurs $\\hat{x}-\\frac{\\mathrm{i}}{m\\omega}\\hat{p}$ et $\\hat{x}+\\frac{\\mathrm{i}}{m\\omega}\\hat{p}$ semblent vou√©s √† jouer un r√¥le important dans cette histoire. Ils sont adjoints l\u0026rsquo;un de l\u0026rsquo;autre (puisque $\\hat{x}$ et $\\hat{p}$ sont hermitiens) ce qui leur interdit d\u0026rsquo;√™tre eux-m√™mes hermitiens et donc de correspondre √† une quelconque observable.\nApr√®s un petit toilettage, introduisons les op√©rateurs d\u0026rsquo;√©chelle¬†:\n$$\\hat{a}=\\sqrt{\\frac{m\\omega}{2\\hbar}}\\left(\\hat{x}+\\frac{\\mathrm{i}}{m\\omega}\\hat{p}\\right)$$\n$$\\hat{a}^\\dagger=\\sqrt{\\frac{m\\omega}{2\\hbar}}\\left(\\hat{x}-\\frac{\\mathrm{i}}{m\\omega}\\hat{p}\\right)$$\n$$[\\hat{a},\\hat{a}^\\dagger]=1$$\nOn peut inverser les d√©finitions de $\\hat{a}$ et $\\hat{a}^\\dagger$ pour obtenir¬†:\n$$\\hat{x}=\\sqrt{\\frac{\\hbar}{2m\\omega}}\\left(\\hat{a}+\\hat{a}^\\dagger\\right)$$\n$$\\hat{p}=-\\mathrm{i}\\sqrt{\\frac{\\hbar m \\omega}{2}}\\left(\\hat{a}-\\hat{a}^\\dagger\\right)$$\nEt l\u0026rsquo;Hamiltonien devient¬†:\n$$\\hat{H}=\\hbar\\omega\\left(\\hat{a}^\\dagger\\hat{a}+\\frac{1}{2}\\right)$$\nAppelons $|n\\rangle$ un √©tat propre de $\\hat{a}^\\dagger\\hat{a}$ pour une valeur propre $n$. Alors $|n\\rangle$ sera aussi vecteur propre de $\\hat{H}$ mais pour une valeur propre $\\hbar\\omega(n+\\frac{1}{2})$. Si $n$ vaut 0, 1, 2, \u0026hellip;., on aura bien retrouv√© les valeurs propres d\u0026rsquo;un oscillateur harmonique¬†!\nMontrons d\u0026rsquo;abord que $n‚â•0$¬†:\n$$n=\\langle n|\\hat{a}^\\dagger\\hat{a}|n\\rangle = |\\hat{a}|n\\rangle|^2‚â•0$$\nMontrons ensuite que $n$ ne prend que des valeurs enti√®res.\nOn commence par d√©finir l\u0026rsquo;op√©rateur nombre de quantas de vibration $\\hat{n} = \\hat{a}^\\dagger\\hat{a}$. On a alors $\\hat{n}|n\\rangle = n|n\\rangle$.\nNombre de quoi¬†? $n$ correspond au num√©ro du barreau d\u0026rsquo;√©chelle atteint et donc au nombre de quanta d\u0026rsquo;√©nergie $\\hbar\\omega$ qu\u0026rsquo;il a fallut ajout√©s au syst√®me dans son √©tat fondamental.\nOn peut r√©√©crire l\u0026rsquo;Hamiltonien $\\hat{H}=\\hbar\\omega\\left(\\hat{n}+\\frac{1}{2}\\right)$. Et donc $\\hat{H}|n\\rangle = \\left(\\hat{n}+\\frac{1}{2}\\right) \\hbar\\omega |n\\rangle$. $|n\\rangle$ est ainsi un raccourci simple pour les vilains $\\psi_n(\\xi)$.\nDe $[\\hat{a},\\hat{a}^\\dagger]=1$, on d√©duit $\\hat{a}\\hat{a}^\\dagger = 1 +\\hat{a}^\\dagger\\hat{a}=1+\\hat{n}$ et donc $\\hat{n}\\hat{a}^\\dagger|n\\rangle = \\hat{a}^\\dagger\\hat{a}\\hat{a}^\\dagger|n\\rangle=\\hat{a}^\\dagger(1+\\hat{n})|n\\rangle=(n+1)\\hat{a}^\\dagger|n\\rangle$. Par cons√©quent, $\\hat{a}^\\dagger|n\\rangle$ est un √©tat propre de $\\hat{H}$ pour une valeur propre un rang au-dessus de celle de $|n\\rangle$. $\\hat{a}^\\dagger$ a donc pour effet d\u0026rsquo;ajouter un quantum d\u0026rsquo;√©nergie¬†! C\u0026rsquo;est l\u0026rsquo;op√©rateur de cr√©ation.\nEn utilisant la relation de commutation $[\\hat{n},\\hat{a}]=\\hat{n}\\hat{a}-\\hat{a}\\hat{n}=\\hat{a}^\\dagger\\hat{a}\\hat{a}-{\\color{#970E53}\\hat{a}\\hat{a}^\\dagger}\\hat{a}=\\hat{a}^\\dagger\\hat{a}\\hat{a}-{\\color{#970E53}(1+\\hat{a}^\\dagger\\hat{a})}\\hat{a}=-\\hat{a}$, on obtient $\\hat{n}\\hat{a}|n\\rangle = (-\\hat{a}+\\hat{a}\\hat{n})|n\\rangle=(n-1)\\hat{a}|n\\rangle$.\n$\\hat{a}|n\\rangle$ est un √©tat propre de $\\hat{H}$ pour une valeur propre un rang en-dessous de celle de $|n\\rangle$. $\\hat{a}$ fait donc descendre d\u0026rsquo;un √©tage. C\u0026rsquo;est l\u0026rsquo;op√©rateur d\u0026rsquo;annihilation.\nEn appliquant de mani√®re r√©p√©t√©e l\u0026rsquo;op√©rateur $\\hat{a}$ √† $|n\\rangle$, on pourrait finir avec une √©nergie n√©gative, ce qui semble physiquement blasph√©matoire. Il faut donc qu\u0026rsquo;il existe un √©tat fondamental $|0\\rangle$ tel que $\\hat{n}|0\\rangle = 0$.\nEt en appliquant maintenant $\\hat{a}^\\dagger$ depuis $|0\\rangle$, on se retrouve bien avec des valeurs de $n$ enti√®res !\nApr√®s normalisation, on obtient¬†:\n$$\\hat{a}|n\\rangle = \\sqrt{n}|n-1\\rangle$$\n$$\\hat{a}^\\dagger|n\\rangle = \\sqrt{n+1}|n+1\\rangle$$\nPreuve : On a montr√© que $\\hat{a}|n\\rangle$ est proportionnel √† $|n-1\\rangle$¬†: $\\hat{a}|n\\rangle=k|n-1\\rangle$.\nPrenons la norme de cet √©tat¬†: $|a|n\\rangle|^2 = \\langle n|\\hat{a}^\\dagger\\hat{a}|n\\rangle=|k|^2\\langle n-1|n-1\\rangle=|k|^2$ (vu que les √©tats propres de l\u0026rsquo;oscillateur harmonique sont normalis√©s).\nMais on peut remarquer aussi que $\\langle n|\\hat{a}^\\dagger\\hat{a}|n\\rangle=\\langle n|\\hat{n}|n\\rangle =n$.\nOn obtient par cons√©quent $k=\\sqrt{n}$ (cela suppose $k$ r√©el mais comme les √©tats sont d√©finis √† une phase pr√®s, on peut toujours choisir la phase afin que $k$ soit bien r√©el).\nOn a aussi montr√© que $\\hat{a}^\\dagger=c|n+1\\rangle$ et $|a^\\dagger|n\\rangle|^2 = \\langle n|\\hat{a}\\hat{a}^\\dagger|n\\rangle=|c|^2\\langle n+1|n+1\\rangle=|c|^2$.\nEt comme $\\hat{a}\\hat{a}^\\dagger=1+\\hat{n}$, $\\langle n|\\hat{a}\\hat{a}^\\dagger|n\\rangle=\\langle n|1+\\hat{n}|n\\rangle=n+1$.\nEt donc $c=\\sqrt{n+1}$\nOn v√©rifie bien que $\\hat{a}|0\\rangle=0$. $|0\\rangle$ est effectivement l\u0026rsquo;√©tat fondamental de l\u0026rsquo;oscillateur harmonique, on ne peut pas aller plus bas.\nEt on retrouve aussi l\u0026rsquo;√©nergie de point z√©ro $E_0=\\frac{1}{2}\\hbar\\omega$¬†:\n$$\\displaystyle\\hat{H}|0\\rangle=\\hbar\\omega\\left(\\hat{n}+\\frac{1}{2}\\right)|0\\rangle=\\frac{1}{2}\\hbar\\omega|0\\rangle$$\nMontons maintenant les barreaux de l\u0026rsquo;√©chelle en cr√©ant √† chaque fois ce qui a tout l\u0026rsquo;air d\u0026rsquo;une particule d\u0026rsquo;√©nergie $\\hbar\\omega$¬†:\n$\\displaystyle\\hat{a}^\\dagger|0\\rangle=|1\\rangle$,\n$\\displaystyle\\hat{a}^\\dagger|1\\rangle=\\sqrt{2}|2\\rangle \\rightarrow |2\\rangle=\\frac{(\\hat{a}^\\dagger)^2}{\\sqrt{2}}|0\\rangle$,\n$\\displaystyle\\hat{a}^\\dagger|2\\rangle=\\sqrt{3}|3\\rangle \\rightarrow |3\\rangle=\\frac{(\\hat{a}^\\dagger)^3}{\\sqrt{3\\times 2}}|0\\rangle$\u0026hellip;\nEt en g√©n√©ralisant, $\\displaystyle |n\\rangle=\\frac{(\\hat{a}^\\dagger)^n}{\\sqrt{n!}}|0\\rangle$.\nLe probl√®me ondulatoire de d√©part a spontan√©ment produit des particules¬†!\nSupposons maintenant qu\u0026rsquo;on ait affaire √† un troupeau de $N$ oscillateurs harmoniques ind√©pendants, sans couplage. L\u0026rsquo;Hamiltonien devient $\\displaystyle \\hat{H}=\\sum_{k=1}^N\\hat{H}_k$ qu\u0026rsquo;on peut r√©√©crire¬†:\n$$\\displaystyle \\hat{H}=\\sum_{k=1}^N\\hbar\\omega_k\\left(\\hat{a}^\\dagger_k\\hat{a}^{\\phantom{\\dagger}}_k+\\frac{1}{2}\\right)$$\nUn √©tat g√©n√©ral du syst√®me est donn√© par $|n_1,n_2,\\cdots,n_N\\rangle$. C\u0026rsquo;est la repr√©sentation en nombre d\u0026rsquo;occupation. Et on a¬†:\n$$|n_1,n_2,\\cdots,n_N\\rangle=\\frac{1}{\\sqrt{n_1 !n_2 !\\cdots n_N !}}(\\hat{a}_1^\\dagger)^{n_1}(\\hat{a}_2^\\dagger)^{n_2}\\cdots (\\hat{a}_N^\\dagger)^{n_N}|0,0,\\cdots,0\\rangle$$ Ou de mani√®re plus succincte\u0026nbsp;: $$|{n_k}\\rangle=\\prod_k\\frac{1}{\\sqrt{n_k !}}(\\hat{a}^\\dagger_k)^{n_k}|0\\rangle$$\nCouplons maintenant tous ces petits ressorts.\nL\u0026rsquo;Hamiltonien qui tient compte de l\u0026rsquo;interaction entre les oscillateurs s\u0026rsquo;√©crit¬†:\n$$\\hat{H}=\\sum_j\\frac{\\hat{p}_j^2}{2m}+\\frac{1}{2}K(\\hat{x}_{j+1}-\\hat{x}_j)^2$$\nLes excitations de ce syst√®me se comportent exactement comme un jeu d\u0026rsquo;oscillateurs ind√©pendants. Par quel miracle¬†? La transform√©e de Fourier permet de diagonaliser l\u0026rsquo;Hamiltonien. En effet, si les masses sont bien coupl√©es dans l\u0026rsquo;espace r√©el, les excitations se d√©couplent dans l\u0026rsquo;espace r√©ciproque.\nEn supposant que les oscillateurs sont s√©par√©s d\u0026rsquo;une distance $a$, les transform√©es de Fourier de $x_j$ et $p_j$ s\u0026rsquo;√©crivent¬†:\n$$x_j = \\frac{1}{\\sqrt{N}}\\sum_k \\tilde{x}_k \\mathrm{e}^{\\mathrm{i}kja}\\qquad\\qquad p_j = \\frac{1}{\\sqrt{N}}\\sum_k \\tilde{p}_k \\mathrm{e}^{\\mathrm{i}kja}$$ o√π $\\tilde{x}_k$ et $\\tilde{p}_k$ sont les nouveaux op√©rateurs dans l'espace des modes de Fourier. En substituant dans l\u0026rsquo;Hamiltonien, on obtient¬†:\n$$\\hat{H}=\\sum_k\\left[\\frac{1}{2m}\\hat{p}_k\\hat{p}_{-k}+\\frac{1}{2}m\\omega_k^2\\hat{x}_k\\hat{x}_{-k}\\right]$$\no√π on a laiss√© tomber les tildes et o√π $\\omega_k^2=(4K/m)\\sin^2(ka/2)$.\nAstuce pour g√©rer des expressions du type¬†:\n$$ \\frac{1}{N} \\sum_j \\sum_{k q} \\tilde{p}_k \\tilde{p}_q \\mathrm{e}^{\\mathrm{i}(k+q) j a} $$\nL\u0026rsquo;id√©e est de commencer par la somme spatiale en utilisant l\u0026rsquo;identit√© $\\sum_j \\mathrm{e}^{\\mathrm{i}\\left(k-k^{\\prime}\\right) j a}=N \\delta_{k k^{\\prime}}$. Cela donne¬†:\n$$ \\sum_{k q} \\tilde{p}_k \\tilde{p}_q \\delta_{k,-q} $$\nPuis on utilise le Kronecker sur la somme des moments. Cela fixe $q=-k$, nous laissant avec une somme sur un seul indice¬†:\n$$ \\sum_k \\tilde{p}_k \\tilde{p}_{-k} $$\nChaque mode de Fourier $k$ se comporte donc comme un oscillateur harmonique ind√©pendant.\nOn peut √† nouveau introduire des op√©rateurs de cr√©ation et d\u0026rsquo;annihilation¬†:\n$$ \\hat{a}_k = \\sqrt{\\frac{m\\omega_k}{2\\hbar}}\\left(\\hat{x}_k+\\frac{\\mathrm{i}}{m\\omega_k}\\hat{p}_k\\right)\\qquad\\displaystyle \\hat{a}^\\dagger_k = \\sqrt{\\frac{m\\omega_k}{2\\hbar}}\\left(\\hat{x}_{-k}-\\frac{\\mathrm{i}}{m\\omega_k}\\hat{p}_{-k}\\right) $$\n√Ä partir d\u0026rsquo;eux, on peut isoler $\\hat{x}_k$ et $\\hat{p}_k$¬†:\n$$ \\hat{x}_k=\\sqrt{\\frac{\\hbar}{2 m \\omega_k}}\\left(\\hat{a}_k+\\hat{a}_{-k}^{\\dagger}\\right) \\qquad \\hat{p}_k= -\\mathrm{i} \\sqrt{\\frac{m \\hbar \\omega_k}{2}}\\left(\\hat{a}_k-\\hat{a}_{-k}^{\\dagger}\\right) $$\nEn r√©injectant dans l\u0026rsquo;Hamiltonien, on obtient finalement, apr√®s r√©indexation et utilisation de la relation de commutation $\\left[\\hat{a}_k, \\hat{a}_{k^{\\prime}}^{\\dagger}\\right]=\\delta_{k, k^{\\prime}}$¬†:\n$$\\hat{H}=\\sum_{k=1}^N\\hbar\\omega_k\\left(\\hat{a}_k^\\dagger\\hat{a}^{\\phantom{\\dagger}}_k+\\frac{1}{2}\\right)$$\nOn appelle ces modes √©tiquet√©s par le vecteur d\u0026rsquo;onde $k$ des phonons. Et chacun de ces phonons peut porter des multiples entiers du quantum d\u0026rsquo;√©nergie $\\hbar\\omega_k$.\nLes paquets d\u0026rsquo;√©nergie que peut accepter le phonon ressemblent √† des particules. Pourquoi alors ne pas consid√©rer les phonons eux-m√™mes comme des particules¬†?\nC\u0026rsquo;est le c≈ìur de la seconde quantification¬†: un probl√®me ondulatoire peut s\u0026rsquo;exprimer comme une collection d\u0026rsquo;oscillateurs et produit donc des particules.\nRepr√©sentation en nombre d‚Äôoccupation On va se d√©barrasser des fonctions d\u0026rsquo;ondes, en passant de la repr√©sentation $(x,p)$ √† un nouveau type de repr√©sentation.\nOn place une particule dans une boite de taille $L$ (on prend dans la suite $\\hbar=1$).\nL\u0026rsquo;op√©rateur impulsion $\\hat{p}$ pour un mouvement dans la direction $x$ est $\\hat{p}=-\\mathrm{i}\\frac{\\partial}{\\partial x}$. Les solutions de l\u0026rsquo;√©quation de Schr√∂dinger pour la particule dans la boite sont les √©tats propres de l\u0026rsquo;op√©rateur impulsion, les ondes planes $\\psi(x)=\\frac{1}{\\sqrt{L}}\\mathrm{e}^{\\mathrm{i}px}$. Utiliser des conditions aux limites p√©riodiques ($\\psi(x+L)=\\psi(x)$) entra√Æne que $\\mathrm{e}^{\\mathrm{i}p(x+L)}={\\mathrm{e}}^{\\mathrm{i}px}$ et implique donc que $\\mathrm{e}^{\\mathrm{i}pL}=1$. Satisfaire la condition n√©cessite que $pL=2\\pi m$ avec $m$ entier. Cela impose donc une quantification des √©tats d\u0026rsquo;impulsion que peut prendre la particule dans la boite¬†:\n$$p_m = \\frac{2\\pi m}{L}$$\nSi on a plusieurs particules sans interaction dans la boite, l\u0026rsquo;√©nergie totale est donn√©e par $\\sum_m n_{p_m}E_{p_m}$ o√π $n_{p_m}$ est le nombre de particules dans l\u0026rsquo;√©tat $|p_m\\rangle$.\nPlut√¥t que de noter un √©tat √† plusieurs particules identiques de cette fa√ßon $|p_1 p_2 p_1 p_3 p_5\\rangle$ (ici pour 5 particules), on va juste lister le nombre de particules dans chacun des √©tats, ce qui donne pour l\u0026rsquo;exemple pr√©c√©dent $|21101\\rangle$. On dit qu\u0026rsquo;on passe alors √† une repr√©sentation en nombre d\u0026rsquo;occupation qu\u0026rsquo;on a d√©j√† rencontr√©e dans le chapitre pr√©c√©dent.\nAgir avec l\u0026rsquo;Hamiltonien sur un √©tat dans la repr√©sentation en nombre d\u0026rsquo;occupation permet d\u0026rsquo;obtenir l\u0026rsquo;√©nergie vue un peu plus haut¬†:\n$$\\hat{H}|n_1n_2n_3\\ldots\\rangle=\\left[\\sum_m n_{p_m}E_{p_m}\\right]|n_1n_2n_3\\ldots\\rangle$$ On retrouve une structure en niveaux d'√©nergie similaire √† celle d'un syst√®me de $N$ oscillateurs harmoniques ind√©pendants diff√©rents (en laissant tomber l'√©nergie de point z√©ro, on trouverait effectivement une √©nergie totale valant $E=\\sum_{k=1}^N n_k \\hbar\\omega_k$). Dans les deux cas, on somme pour chaque mode (chaque niveau d'√©nergie), le nombre de quanta qu'il contient. Dans le cas d\u0026rsquo;une collection d\u0026rsquo;oscillateurs harmoniques, on a vu dans le chapitre pr√©c√©dent qu\u0026rsquo;on peut se d√©barrasser de quasiment tous les vecteurs d\u0026rsquo;√©tat (except√© le vide $|0\\rangle$) gr√¢ce √† l\u0026rsquo;op√©rateur de cr√©ation¬†:\n$$|n_1 n_2\\ldots\\rangle = \\prod_k\\frac{1}{(n_k !)^{\\frac{1}{2}}}(\\hat{a}_k^\\dagger)^{n_k}|0\\rangle$$\nOn cr√©e ainsi un √©tat g√©n√©ral de plusieurs oscillateurs harmoniques en agissant sur l\u0026rsquo;√©tat du vide. Mais on veut aller plus loin qu\u0026rsquo;une une cr√©ation de quanta dans des oscillateurs\u0026hellip; L\u0026rsquo;ambition est une cr√©ation de particules dans des √©tats d\u0026rsquo;impulsion donn√©s. On veut un op√©rateur de cr√©ation $\\hat{a}^\\dagger_{p_m}$ pour faire na√Ætre une particule dans l\u0026rsquo;√©tat d\u0026rsquo;impulsion $|p_m\\rangle$. Peut-on juste changer $k$ en $p_m$ pour passer de la cr√©ation d\u0026rsquo;un quantum dans l\u0026rsquo;oscillateur $k$ √† une particule dans l\u0026rsquo;√©tat d\u0026rsquo;impulsion $|p_m\\rangle$¬†? Presque\u0026hellip; Juste une petite question de sym√©trie √† r√©gler.\nConsid√©rons par exemple un syst√®me √† deux √©tats d\u0026rsquo;impulsion $p_1$ et $p_2$ d√©crits dans la repr√©sentation en nombre d\u0026rsquo;occupation $|n_1n_2\\rangle$. Chacun de ses √©tats est form√© en agissant sur l\u0026rsquo;√©tat du vide $|0\\rangle$. D√©finissons $\\hat{a}_{p_1}^\\dagger|0\\rangle=|10\\rangle$ et $\\hat{a}_{p_2}^\\dagger|0\\rangle=|01\\rangle$ et ajoutons une nouvelle particule dans l\u0026rsquo;√©tat inoccup√©¬†: $\\hat{a}_{p_2}^\\dagger\\hat{a}_{p_1}^\\dagger|0\\rangle\\propto|11\\rangle$, $\\hat{a}_{p_1}^\\dagger\\hat{a}_{p_2}^\\dagger|0\\rangle\\propto|11\\rangle$ o√π la constante de proportionnalit√© reste √† d√©terminer.\nSuivant qu\u0026rsquo;on ajoute une particule dans l\u0026rsquo;√©tat $p_1$ puis une autre dans l\u0026rsquo;√©tat $p_2$ ou dans l\u0026rsquo;ordre inverse, on doit finir avec le m√™me √©tat $|11\\rangle$, ce qui implique $\\hat{a}_{p_1}^\\dagger\\hat{a}_{p_2}^\\dagger = \\lambda\\hat{a}_{p_2}^\\dagger\\hat{a}_{p_1}^\\dagger$ o√π $\\lambda$ est une constante.\nDeux possibilit√©s √©videntes pour $\\lambda$¬†: $\\lambda=\\pm 1$. Elles correspondent √† des fonctions d\u0026rsquo;onde qui sont soit sym√©triques, soit antisym√©triques lors de l\u0026rsquo;√©change des deux particules. Les particules sont appel√©es bosons dans le cas sym√©trique et fermions dans le cas antisym√©trique. Ces deux cas correspondent √† deux relations de commutation possibles entre les op√©rateurs de cr√©ation et d\u0026rsquo;annihilation.\nCas 1¬†: $\\lambda=1$, les bosons\nOn a dans ce cas $\\hat{a}_{p_1}^\\dagger\\hat{a}_{p_2}^\\dagger = \\hat{a}_{p_2}^\\dagger\\hat{a}_{p_1}^\\dagger$ et donc (en √©tiquetant les op√©rateurs de mani√®re plus g√©n√©rale),\n$$\\left[\\hat{a}_i^\\dagger,\\hat{a}_j^\\dagger\\right]=0$$\nLes op√©rateurs de cr√©ation pour diff√©rents √©tats de particule commutent.\nOn a aussi $\\left[\\hat{a}_i,\\hat{a}_j\\right]=0$ et on d√©finit $\\left[\\hat{a}_i,\\hat{a}_j^\\dagger\\right]=\\delta_{ij}$.\nOn obtient donc un formalisme identique √† celui des oscillateurs harmoniques et on peut alors construire un √©tat g√©n√©ral √† plusieurs particules sur le m√™me mod√®le¬†:\n$$|n_1n_2\\cdots\\rangle=\\prod_m\\frac{1}{(n_{p_m}!)^{\\frac{1}{2}}}(\\hat{a}^\\dagger_{p_m})^{n_{p_m}}|0\\rangle$$\nLes particules ainsi d√©crites sont des bosons.\nTrois propri√©t√©s saillantes¬†:\nOn peut placer tout nombre de particules dans chaque √©tat quantique (on peut les empiler dans un m√™me √©tat d\u0026rsquo;impulsion). Ces √©tats sont sym√©triques lors de l\u0026rsquo;√©change de deux particules. Enfin, l\u0026rsquo;ordre d\u0026rsquo;ajout des particules ne modifie pas l\u0026rsquo;√©tat final obtenu¬†: $\\hat{a}^\\dagger_{p_1}\\hat{a}^\\dagger_{p_2}|0\\rangle=\\hat{a}^\\dagger_{p_2}\\hat{a}^\\dagger_{p_1}|0\\rangle=|1_{p_1}1_{p_2}\\rangle$. Action des op√©rateurs sur un √©tat g√©n√©ral¬†:\n$$ \\begin{aligned} \u0026amp;\\hat{a}_i^\\dagger |n_1\\cdots n_i\\cdots\\rangle = \\sqrt{n_i+1}|n_1\\cdots n_i+1\\cdots\\rangle\\\\ \u0026amp;\\hat{a}_i |n_1\\cdots n_i\\cdots\\rangle = \\sqrt{n_i}|n_1\\cdots n_i-1\\cdots\\rangle \\end{aligned} $$\nCas 2¬†: $\\lambda=-1$, les fermions\nOn va noter les op√©rateurs des fermions $\\hat{c}_i^\\dagger$ pour les diff√©rentier de ceux des bosons.\nOn obtient $\\left\\{\\hat{c}_i^\\dagger,\\hat{c}_j^\\dagger\\right\\} \\equiv \\hat{c}_i^\\dagger \\hat{c}_j^\\dagger + \\hat{c}_j^\\dagger\\hat{c}_i^\\dagger = 0$ o√π on a d√©fini l\u0026rsquo;anticommutateur de deux op√©rateurs.\nLes op√©rateurs des fermions anticommutent¬†: $\\hat{c}_i^\\dagger\\hat{c}_j^\\dagger+\\hat{c}_j^\\dagger\\hat{c}_i^\\dagger=0$.\nEn prenant $i=j$, on obtient $\\hat{c}_i^\\dagger\\hat{c}_i^\\dagger+\\hat{c}_i^\\dagger\\hat{c}_i^\\dagger=0\\Rightarrow \\hat{c}_i^\\dagger\\hat{c}_i^\\dagger=0$. Essayer de caser deux particules dans le m√™me √©tat d\u0026rsquo;impulsion aboutit √† leur annihilation compl√®te.\nC\u0026rsquo;est le principe de Pauli¬†; chaque √©tat quantique ne peut √™tre occup√© que par un et un seul fermion¬†!\nOn a aussi $\\left\\{\\hat{c}_i,\\hat{c}_j\\right\\}=0$ et on d√©finit $\\left\\{\\hat{c}_i,\\hat{c}_j^\\dagger\\right\\}=\\delta_{ij}$ pour pouvoir appliquer ici aussi l\u0026rsquo;analogie avec les oscillateurs harmoniques. Mais gare maintenant √† l\u0026rsquo;ordre des op√©rateurs qui n\u0026rsquo;est plus indiff√©rent¬†!\nAction des op√©rateurs sur un √©tat g√©n√©ral¬†:\n$$ \\begin{aligned} \u0026amp;\\hat{c}_i^\\dagger |n_1\\cdots n_i\\cdots\\rangle = (-1)^{\\sum_i}\\sqrt{1-n_i}|n_1\\cdots n_i+1\\cdots\\rangle\\\\ \u0026amp;\\hat{c}_i |n_1\\cdots n_i\\cdots\\rangle = (-1)^{\\sum_i}\\sqrt{n_i}|n_1\\cdots n_i-1\\cdots\\rangle \\end{aligned} $$\nO√π $(-1)^{\\sum_i}=(-1)^{n_1+n_2+\\cdots+n_{i-1}}$. Cela donne un facteur $(-1)$ pour chaque particule plac√©e √† gauche de l\u0026rsquo;√©tat √©tiquet√© par $n_i$ dans le vecteur d\u0026rsquo;√©tat.\nPrenons un exemple pour s'aguerrir et v√©rifier la formule. Pour √©changer de place de particules, on va leur faire suivre le processus suivant $|110\\rangle\\rightarrow|101\\rangle\\rightarrow|011\\rangle\\rightarrow|110\\rangle$.\nBouger une particule de place consiste √† d√©truire la particule √† un endroit et √† la cr√©er √† un autre.\nD√©placer une particule de l\u0026rsquo;√©tat 2 vers l\u0026rsquo;√©tat 3 s\u0026rsquo;√©crit donc $\\hat{a}^\\dagger_{3}\\hat{a}_2|110\\rangle$. L\u0026rsquo;encha√Ænement total propos√© peut √™tre d√©crit par l\u0026rsquo;encha√Ænement $\\hat{a}^\\dagger_{1}\\hat{a}_3\\hat{a}^\\dagger_{2}\\hat{a}_1\\hat{a}^\\dagger_{3}\\hat{a}_2|110\\rangle$. Et le r√©sultat est sens√© √™tre $\\pm|110\\rangle$ ($+$ pour des bosons et $-$ pour des fermions).\nV√©rifions que les relations de commutation donnent les bons r√©sultats. S\u0026rsquo;il s\u0026rsquo;agit de bosons, on peut √©changer de place deux op√©rateurs agissant sur des √©tats diff√©rents ($[\\hat{a}_i,\\hat{a}_j]=0$).\n$\\hat{a}^\\dagger_{1}\\hat{a}_3\\hat{a}^\\dagger_{2}\\hat{a}_1\\hat{a}^\\dagger_{3}\\hat{a}_2|110\\rangle=\\hat{a}_3\\hat{a}^\\dagger_{3}\\hat{a}^\\dagger_{1}\\hat{a}_1\\hat{a}^\\dagger_{2}\\hat{a}_2|110\\rangle$\nComme $\\hat{a}^\\dagger_i\\hat{a}_i=\\hat{n}_i$, l\u0026rsquo;op√©rateur nombre qui compte le nombre de particules dans l\u0026rsquo;√©tat $i$, on obtient¬†:\n$\\hat{a}_3\\hat{a}^\\dagger_{3}\\hat{a}^\\dagger_{1}\\hat{a}_1\\hat{a}^\\dagger_{2}\\hat{a}_2|110\\rangle=\\hat{a}_3\\hat{a}^\\dagger_{3}\\hat{n}_1\\hat{n}_2|110\\rangle =(1)\\times(1)\\times \\hat{a}_3\\hat{a}^\\dagger_{3}|110\\rangle$\nEn utilisant $[\\hat{a}_3,\\hat{a}^\\dagger_3]=1$, on obtient¬†:\n$\\hat{a}_3\\hat{a}^\\dagger_{3}|110\\rangle=(1+\\hat{n}_3)|110\\rangle = |110\\rangle + 0 =|110\\rangle$\nPassons aux fermions. L\u0026rsquo;√©change entre deux particules s\u0026rsquo;accompagne maintenant d\u0026rsquo;un changement de signe.\n$\\hat{c}^\\dagger_{1}\\hat{c}_3\\hat{c}^\\dagger_{2}\\hat{c}_1\\hat{c}^\\dagger_{3}\\hat{c}_2|110\\rangle = -\\hat{c}_3\\hat{c}^\\dagger_{3}\\hat{c}^\\dagger_{1}\\hat{c}_1\\hat{c}^\\dagger_{2}\\hat{c}_2|110\\rangle$ car on compte un nombre impair d\u0026rsquo;√©changes. Et $-\\hat{c}_3\\hat{c}^\\dagger_{3}\\hat{c}^\\dagger_{1}\\hat{c}_1\\hat{c}^\\dagger_{2}\\hat{c}_2|110\\rangle = -(1-\\hat{n}_3)|110\\rangle = -|110\\rangle$. Youpi¬†!\nOn √©tait jusque-l√† confin√© dans une boite, mais que se passe-t-il si on √©carte ses murs infiniment¬†? On passe alors √† la limite du continu. Le symbole de Kronecker $\\delta_{ij}$ des commutateurs devient une fonction delta de Dirac $\\delta^{(3)}(\\boldsymbol{p})$ et les sommes discr√®tes deviennent des int√©grales.\nLe commutateur se transforme donc en¬†:\n$$\\left[\\hat{a}_\\boldsymbol{p},\\hat{a}^\\dagger_\\boldsymbol{q}\\right]=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q})$$\net l\u0026rsquo;Hamiltonien¬†:\n$$\\hat{H}=\\int\\mathrm{d}^3 p\\, E_\\boldsymbol{p}\\hat{a}^\\dagger_\\boldsymbol{p}\\hat{a}_\\boldsymbol{p}$$\nPour montrer que cette nouvelle formulation est correcte, on va retrouver √† partir d\u0026rsquo;elle les bonnes fonctions d\u0026rsquo;onde dans l\u0026rsquo;espace des positions.\nPour des √©tats √† une seule particule, on a¬†:\n$$\\langle \\boldsymbol{p}|\\boldsymbol{p\u0026rsquo;}\\rangle=\\langle 0|\\hat{a}_\\boldsymbol{p}\\hat{a}^\\dagger_\\boldsymbol{p\u0026rsquo;}|0\\rangle$$\nEt avec les relations de commutation, on obtient¬†:\n$$\\langle \\boldsymbol{p}|\\boldsymbol{p\u0026rsquo;}\\rangle=\\langle 0|\\left[\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{p\u0026rsquo;})\\pm \\hat{a}^\\dagger_\\boldsymbol{p\u0026rsquo;}\\hat{a}_\\boldsymbol{p} \\right]|0\\rangle=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{p\u0026rsquo;})$$\nV√©rifions que c\u0026rsquo;est la relation attendue en passant en repr√©sentation position.\nPar un changement de base, $|\\boldsymbol{x}\\rangle = \\int\\mathrm{d}q\\,\\phi^*_\\boldsymbol{q}(\\boldsymbol{x})|\\boldsymbol{q}\\rangle$\nchangement de base L\u0026rsquo;√©tat $|\\boldsymbol{x}\\rangle$ peut s\u0026rsquo;√©crire dans une nouvelle base en utilisant la relation de fermeture $1=\\int\\mathrm{d}^3q\\,|\\boldsymbol{q}\\rangle\\langle\\boldsymbol{q}|$ de telle sorte que $|\\boldsymbol{x}\\rangle = \\int\\mathrm{d}^3q\\,|\\boldsymbol{q}\\rangle\\langle\\boldsymbol{q}|\\boldsymbol{x}\\rangle$. Et comme $\\langle\\boldsymbol{x}|\\boldsymbol{q}\\rangle=\\phi_\\boldsymbol{q}(\\boldsymbol{x})$ et $\\langle\\boldsymbol{q}|\\boldsymbol{x}\\rangle=\\phi^*_\\boldsymbol{q}(\\boldsymbol{x})$, on obtient bien $|\\boldsymbol{x}\\rangle = \\int\\mathrm{d}q\\,\\phi^*_\\boldsymbol{q}(\\boldsymbol{x})|\\boldsymbol{q}\\rangle$.\nCela donne $\\langle \\boldsymbol{x}|\\boldsymbol{p}\\rangle = \\int\\mathrm{d}^3q\\,\\phi_\\boldsymbol{q}(\\boldsymbol{x})\\langle\\boldsymbol{q}|\\boldsymbol{p}\\rangle=\\phi_\\boldsymbol{p}(\\boldsymbol{x})$.\nC\u0026rsquo;est bien ce qui √©tait attendu, mais rien de bien folichon.\nPassons maintenant au cas d\u0026rsquo;un √©tat √† deux particules¬†: $\\langle \\boldsymbol{p\u0026rsquo;}\\boldsymbol{q\u0026rsquo;}|\\boldsymbol{q}\\boldsymbol{p}\\rangle=\\langle 0|\\hat{a}_\\boldsymbol{p\u0026rsquo;}\\hat{a}_\\boldsymbol{q\u0026rsquo;}\\hat{a}^\\dagger_\\boldsymbol{q}\\hat{a}^\\dagger_\\boldsymbol{p}|0\\rangle$.\nPar utilisation des relations de commutation, on arrive √† $\\langle \\boldsymbol{p\u0026rsquo;}\\boldsymbol{q\u0026rsquo;}|\\boldsymbol{q}\\boldsymbol{p}\\rangle=\\delta^{(3)}(\\boldsymbol{p\u0026rsquo;}-\\boldsymbol{p})\\delta^{(3)}(\\boldsymbol{q\u0026rsquo;}-\\boldsymbol{q})\\pm\\delta^{(3)}(\\boldsymbol{p\u0026rsquo;}-\\boldsymbol{q})\\delta^{(3)}(\\boldsymbol{q\u0026rsquo;}-\\boldsymbol{p})$ avec le signe plus pour les bosons et moins pour les fermions.\nD√©terminons √† nouveau la fonction d\u0026rsquo;onde dans l\u0026rsquo;espace des positions gr√¢ce au changement de base¬†:\n$$|\\boldsymbol{x}\\boldsymbol{y}\\rangle = \\frac{1}{\\sqrt{2!}}\\int \\mathrm{d}^3p\u0026rsquo;\\mathrm{d}^3 q\u0026rsquo; \\phi^*_\\boldsymbol{p\u0026rsquo;}(\\boldsymbol{x})\\phi^*_\\boldsymbol{q\u0026rsquo;}(\\boldsymbol{y})|\\boldsymbol{p\u0026rsquo;}\\boldsymbol{q\u0026rsquo;}\\rangle$$\no√π le facteur $\\frac{1}{\\sqrt{2!}}$ compense le double comptage $q\u0026rsquo;p\u0026rsquo;$/$p\u0026rsquo;q\u0026rsquo;$ d√ª au fait que la somme n\u0026rsquo;est pas restreinte. Cela donne¬†:\n$$ \\frac{1}{\\sqrt{2!}}\\int \\mathrm{d}^3p\u0026rsquo;\\mathrm{d}^3 q\u0026rsquo; \\phi_\\boldsymbol{p\u0026rsquo;}(\\boldsymbol{x})\\phi_\\boldsymbol{q\u0026rsquo;}(\\boldsymbol{y}) \\langle \\boldsymbol{p\u0026rsquo;}\\boldsymbol{q\u0026rsquo;}|\\boldsymbol{p}\\boldsymbol{q}\\rangle = \\frac{1}{\\sqrt{2!}}[\\phi_\\boldsymbol{p}(\\boldsymbol{x})\\phi_\\boldsymbol{q}(\\boldsymbol{y})\\pm \\phi_\\boldsymbol{q}(\\boldsymbol{x})\\phi_\\boldsymbol{p}(\\boldsymbol{y})]$$\nOn retrouve l\u0026rsquo;expression d\u0026rsquo;un √©tat √† deux particules¬†!\nCette nouvelle formulation de la m√©canique quantique semble donc valide et dans la suite, elle va porter ses fruits.\nSeconde quantification On consid√®re √† nouveau ici des particules non relativistes dans une bo√Æte. Cela simplifie pas mal mais cela permet d√©j√† d\u0026rsquo;√©tudier le comportement d\u0026rsquo;√©lectrons dans un solide.\nParticule dans une bo√Æte de volume $\\mathcal{V}$ Un √©tat $|\\alpha\\rangle$ se d√©crit en repr√©sentation position $\\psi_\\alpha(\\boldsymbol{x})=\\langle\\boldsymbol{x}|\\alpha\\rangle$ et en repr√©sentation impulsion $\\tilde{\\psi}_\\alpha(\\boldsymbol{p})=\\langle\\boldsymbol{p}|\\alpha\\rangle$ et comme $\\langle\\boldsymbol{p}|\\alpha\\rangle:\\int\\mathrm{d}^3\\langle\\boldsymbol{p}|\\boldsymbol{x}\\rangle\\langle\\boldsymbol{x}|\\alpha\\rangle$, on d√©duit $\\langle\\boldsymbol{p}|\\boldsymbol{x}\\rangle=\\frac{1}{\\sqrt{\\mathcal{V}}}\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}$ (par identification avec la transform√©e de Fourier $\\tilde{\\psi}_\\alpha(\\boldsymbol{p})=\\frac{1}{\\sqrt{\\mathcal{V}}}\\int\\mathrm{d}^3 x\\,\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}\\psi_\\alpha(\\boldsymbol{x})$).\nLa transform√©e de Fourier inverse est discr√®te puisque les valeurs de $\\boldsymbol{p}$ le sont¬†: $\\psi_\\alpha(\\boldsymbol{x})=\\frac{1}{\\sqrt{\\mathcal{V}}}\\sum_\\boldsymbol{p}\\mathrm{e}^{\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}\\tilde{\\psi}_\\alpha(\\boldsymbol{p})$.\nEt on a aussi $\\int\\mathrm{d}^3 x\\,\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}} = \\mathcal{V}\\delta_{\\boldsymbol{p},0}$ et $\\frac{1}{\\mathcal{V}}\\sum_\\boldsymbol{p}\\mathrm{e}^{\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}=\\delta^{(3)}(\\boldsymbol{x})$.\nOn sait d\u0026rsquo;ores et d√©j√† cr√©er une particule d\u0026rsquo;impulsion $\\boldsymbol{p}$ √† partir de l\u0026rsquo;√©tat du vide en appliquant l\u0026rsquo;op√©rateur de cr√©ation¬†: $\\hat{a}^\\dagger_\\boldsymbol{p}|0\\rangle$. On obtient ainsi une particule compl√®tement localis√©e dans l\u0026rsquo;espace des impulsions et donc s\u0026rsquo;√©tendant sur tout l\u0026rsquo;espace des coordonn√©es. Supposons maintenant que l\u0026rsquo;on veuille cr√©er une particule localis√©e dans l\u0026rsquo;espace des coordonn√©es. Dans ce but, on construit des nouveaux op√©rateurs de cr√©ation et d\u0026rsquo;annihilation appel√©s op√©rateur de champ par transform√©e de Fourier discr√®te des op√©rateurs $\\hat{a}^\\dagger_\\boldsymbol{p}$ et $\\hat{a}_\\boldsymbol{p}$.\n$$\\hat{\\psi}^\\dagger(\\boldsymbol{x}) = \\frac{1}{\\sqrt{\\mathcal{V}}}\\sum_\\boldsymbol{p}\\hat{a}^\\dagger_\\boldsymbol{p}\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}$$\n$$\\hat{\\psi}(\\boldsymbol{x}) = \\frac{1}{\\sqrt{\\mathcal{V}}}\\sum_\\boldsymbol{p}\\hat{a}_\\boldsymbol{p}\\mathrm{e}^{\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}$$\nCes op√©rateurs de champ peuvent cr√©er et annihiler une particule en une position $\\boldsymbol{x}$.\nPour s\u0026rsquo;en convaincre, explorons l\u0026rsquo;√©tat $|\\Psi\\rangle = \\hat{\\psi}^\\dagger(\\boldsymbol{x})|0\\rangle$. On a d\u0026rsquo;une part $\\sum_\\boldsymbol{q}\\hat{n}_\\boldsymbol{q}|\\Psi\\rangle = |\\Psi\\rangle$, ce qui signifie que $|\\Psi\\rangle$ est un √©tat propre de l\u0026rsquo;op√©rateur nombre d\u0026rsquo;occupation pour la valeur propre 1 et donc qu\u0026rsquo;une seule particule a √©t√© cr√©√©e. D\u0026rsquo;autre part $\\langle\\boldsymbol{y}|\\Psi\\rangle=\\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y})$, ce qui prouve que la particule cr√©√©e est bien localis√©e en $\\boldsymbol{x}$.\n$$ \\begin{aligned} \\langle\\boldsymbol{y}|\\Psi\\rangle = \\langle\\boldsymbol{y}|\\psi^\\dagger(\\boldsymbol{x})|0\\rangle \u0026amp;= \\frac{1}{\\sqrt{\\mathcal{V}}}\\sum_\\boldsymbol{p}\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}\\langle\\boldsymbol{y}|\\boldsymbol{p}\\rangle\\\\ \u0026amp;=\\frac{1}{\\sqrt{\\mathcal{V}}}\\sum_\\boldsymbol{p}\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot(\\boldsymbol{x}-\\boldsymbol{y})}\\\\ \u0026amp;=\\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) \\end{aligned} $$\nLes op√©rateurs de champ satisfont les relations de commutation suivantes¬†:\npour des bosons¬†: $\\left[\\hat{\\psi}(\\boldsymbol{x}),\\hat{\\psi}^\\dagger(\\boldsymbol{y})\\right]=\\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y})$, $\\left[\\hat{\\psi}^\\dagger(\\boldsymbol{x}),\\hat{\\psi}^\\dagger(\\boldsymbol{y})\\right]=0$ et $\\left[\\hat{\\psi}(\\boldsymbol{x}),\\hat{\\psi}(\\boldsymbol{y})\\right]=0$\net pour des fermions¬†: $\\left\\{\\hat{\\psi}(\\boldsymbol{x}),\\hat{\\psi}^\\dagger(\\boldsymbol{y})\\right\\}=\\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y})$, $\\left\\{\\hat{\\psi}^\\dagger(\\boldsymbol{x}),\\hat{\\psi}^\\dagger(\\boldsymbol{y})\\right\\}=0$ et $\\left\\{\\hat{\\psi}(\\boldsymbol{x}),\\hat{\\psi}(\\boldsymbol{y})\\right\\}=0$\nC\u0026rsquo;est le moment¬†: on va maintenant apprendre √† upgrader les op√©rateurs issus de la premi√®re quantification en op√©rateurs de seconde quantification¬†!\nRappelons-nous d\u0026rsquo;abord qu\u0026rsquo;un op√©rateur $\\hat{\\mathcal{A}}$ associe √† l\u0026rsquo;√©tat quantique $|\\psi\\rangle$, √©l√©ment de l\u0026rsquo;espace de Hilbert, un nouvel √©tat quantique $\\hat{\\mathcal{A}}|\\psi\\rangle$.\nOn va supposer ici que $\\hat{\\mathcal{A}}$ n\u0026rsquo;agit que sur une seule particule √† la fois (contrairement √† un op√©rateur potentiel d\u0026rsquo;interaction par exemple). En utilisant doublement la relation de fermeture et en partant de l\u0026rsquo;√©galit√© triviale $\\hat{\\mathcal{A}}=\\hat{\\mathcal{A}}$, on peut obtenir la d√©composition de $\\hat{\\mathcal{A}}$ sur une base donn√©e de l\u0026rsquo;espace de Hilbert¬†:\n$$\\hat{\\mathcal{A}}=\\sum_{\\alpha,\\beta}|\\alpha\\rangle\\langle\\alpha|\\hat{\\mathcal{A}}|\\beta\\rangle\\langle\\beta|=\\sum_{\\alpha,\\beta}\\mathcal{A}_{\\alpha\\beta}|\\alpha\\rangle\\langle\\beta|$$\no√π les $\\mathcal{A}_{\\alpha\\beta}=\\langle\\alpha|\\hat{\\mathcal{A}}|\\beta\\rangle$ sont les √©l√©ments de matrice de la d√©composition de l\u0026rsquo;op√©rateur, ils donnent l\u0026rsquo;amplitude de la transition d\u0026rsquo;un √©tat $|\\beta\\rangle$ vers un √©tat $|\\alpha\\rangle$.\nL\u0026rsquo;id√©e va √™tre de passer de cette transition entre √©tats √† un transfert de particules (mort de l\u0026rsquo;une suivie de la naissance d\u0026rsquo;une autre).\nL\u0026rsquo;espace de Hilbert permet de d√©crire l\u0026rsquo;√©tat d\u0026rsquo;une particule. L\u0026rsquo;espace de Fock rend possible, lui, la description d\u0026rsquo;√©tats √† $N$ particules (l\u0026rsquo;espace de Fock $\\mathcal{F}$ inclut tous les √©tats √† $N$-particules pour toutes les valeurs de $N$¬†: $\\mathcal{F}=\\bigoplus_{N=0}^\\infty \\mathcal{F}_N=\\mathcal{F}_0\\oplus\\mathcal{F}_1\\oplus\\cdots$ o√π $\\mathcal{F}_0=\\{|0\\rangle\\}$ est un ensemble ne contenant que l\u0026rsquo;√©tat du vide et $\\mathcal{F}_1$ est l\u0026rsquo;espace de Hilbert. Les sous-espaces $\\mathcal{F}_{N‚â•2}$ doivent √™tre sym√©tris√©s pour des bosons et antisym√©tris√©s pour des fermions (donc $\\mathcal{F}_2$ n\u0026rsquo;est pas seulement $\\mathcal{F}_1\\otimes\\mathcal{F}_1$ mais seulement sa partie sym√©trique ou antisym√©trique). Les op√©rateurs de cr√©ation permettent de passer d\u0026rsquo;un √©l√©ment de $\\mathcal{F}_N$ √† un √©l√©ment de $\\mathcal{F}_{N+1}$ et les op√©rateurs d\u0026rsquo;annihilation font l\u0026rsquo;inverse.\nLa version seconde quantification $\\hat{A}$ de l\u0026rsquo;op√©rateur $\\hat{\\mathcal{A}}$ qui correspond √† l\u0026rsquo;upgrade d\u0026rsquo;un op√©rateur √† une particule agissant sur l\u0026rsquo;espace de Hilbert vers un op√©rateur multi-particules agissant sur l\u0026rsquo;espace de Fock est simplement donn√©e par¬†:\n$$\\hat{A} = \\sum_{\\alpha\\beta}\\mathcal{A}_{\\alpha\\beta}\\hat{a}^\\dagger_\\alpha\\hat{a}_\\beta$$\nLes √©l√©ments de matrice $\\mathcal{A}_{\\alpha\\beta}$ sont les m√™mes qu\u0026rsquo;au-dessus et ils continuent √† repr√©senter l\u0026rsquo;ensemble des transitions possibles d\u0026rsquo;une particule seule. Mais $\\hat{A}$ op√®re bien maintenant sur un √©tat √† plusieurs particules. On commence par utiliser $\\hat{a}_\\beta$ pour retirer une particule dans l\u0026rsquo;√©tat $|\\beta\\rangle$, on multiplie par l\u0026rsquo;√©l√©ment de matrice $\\mathcal{A}_{\\alpha\\beta}$ donnant l\u0026rsquo;amplitude de la transition vers le nouvel √©tat, et enfin on utilise $\\hat{a}^\\dagger_\\alpha$ pour placer la particules dans l\u0026rsquo;√©tat final $|\\alpha\\rangle$.\nDans les exemples suivants, on va construire pas √† pas un Hamiltonien simple d\u0026rsquo;une particule unique soumise √† un potentiel en version seconde quantification. L\u0026rsquo;id√©e est de se familiariser avec le formalisme et constater qu\u0026rsquo;il nous redonne bien les r√©sultats qu\u0026rsquo;on attend.\nLa relation de fermeture $\\hat{1}=\\sum_\\alpha|\\alpha\\rangle\\langle\\alpha|$ est l\u0026rsquo;exemple le plus simple de $\\hat{\\mathcal{A}} = \\sum_{\\alpha\\beta}\\mathcal{A}_{\\alpha\\beta}\\hat{a}^\\dagger_\\alpha\\hat{a}_\\beta$ avec $\\mathcal{A}_{\\alpha\\beta}=\\delta_{\\alpha\\beta}$.\nSa promotion √† l\u0026rsquo;√©tage de la seconde quantification est simplement $\\hat{n}=\\sum_\\alpha\\hat{a}^\\dagger_\\alpha\\hat{a}_\\alpha$, l\u0026rsquo;op√©rateur nombre qui compte un pour chaque particule dans l\u0026rsquo;√©tat sur lequel il agit.\nL\u0026rsquo;op√©rateur impulsion usuel peut s\u0026rsquo;√©crire $\\hat{\\mathcal{A}}=\\hat{\\boldsymbol{p}}=\\sum_\\boldsymbol{p}\\boldsymbol{p}|\\boldsymbol{p}\\rangle\\langle\\boldsymbol{p}|$ qui est promu automatiquement en $\\hat{\\boldsymbol{p}}=\\sum_\\boldsymbol{p}\\boldsymbol{p}\\,\\hat{a}^\\dagger_\\boldsymbol{p}\\hat{a}_\\boldsymbol{p}=\\sum_\\boldsymbol{p}\\boldsymbol{p}\\,\\hat{n}_\\boldsymbol{p}$.\nDe m√™me, une fonction de l\u0026rsquo;op√©rateur impulsion $\\hat{\\mathcal{A}}=f(\\boldsymbol{p})$ devient $\\hat{A}=\\sum_\\boldsymbol{p}f(\\boldsymbol{p})\\hat{a}^\\dagger_\\boldsymbol{p}\\hat{a}_\\boldsymbol{p}=\\sum_\\boldsymbol{p}f(\\boldsymbol{p})\\hat{n}_\\boldsymbol{p}$.\nUn exemple particulier d\u0026rsquo;une telle fonction¬†: l\u0026rsquo;Hamiltonien d\u0026rsquo;une particule libre $\\frac{\\boldsymbol{p}^2}{2m}$.\nL\u0026rsquo;Hamiltonien d\u0026rsquo;une particule libre devient ainsi en seconde quantification¬†:\n$\\displaystyle\\hat{H}=\\sum_\\boldsymbol{p}\\frac{\\boldsymbol{p}^2}{2m}\\hat{n} _\\boldsymbol{p}$\n$\\hat{H}$ est diagonal dans la base des √©tats nombre d\u0026rsquo;occupation puisqu\u0026rsquo;ils sont des √©tats propres de $\\hat{n}_\\boldsymbol{p}$. Et de fait, pour diagonaliser tout hamiltonien (ce qui revient √† trouver les √©nergies des √©tats propres), on n\u0026rsquo;a qu\u0026rsquo;√† l\u0026rsquo;exprimer en termes de nombres d\u0026rsquo;op√©rateurs. Donc ici, on dit finalement que l\u0026rsquo;√©nergie totale du syst√®me est donn√©e par la somme des √©nergies $\\frac{\\boldsymbol{p}^2}{2m}$ de toutes les particules. √áa para√Æt sens√©.\nEt si l\u0026rsquo;op√©rateur est une fonction de $\\hat{\\boldsymbol{x}}$ plut√¥t que $\\hat{\\boldsymbol{p}}$¬†?\nLa formule de promotion reste valable si les √©tats $|\\alpha\\rangle$ et $|\\beta\\rangle$ sont des √©tats de position. Il suffit de remplacer les op√©rateurs de cr√©ation et d\u0026rsquo;annihilation par les op√©rateurs de champ, et la somme sur les impulsions par une int√©grale sur l\u0026rsquo;espace.\nOn peut ainsi √©crire l\u0026rsquo;op√©rateur $\\hat{V}$ en version seconde-quantification¬†:\n$\\displaystyle \\hat{V}=\\int\\mathrm{d}^3 x\\,\\hat{\\phi}^\\dagger(\\boldsymbol{x})V(\\boldsymbol{x})\\hat{\\phi}(\\boldsymbol{x})$\nEt si on pr√©f√®re repasser dans l\u0026rsquo;espace des impulsions¬†:\n$\\displaystyle \\hat{V}=\\frac{1}{\\mathcal{V}}\\int\\mathrm{d}^3 x\\,\\sum_{\\boldsymbol{p}_1,\\boldsymbol{p}_2} \\hat{a}^\\dagger_{\\boldsymbol{p}_1}\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}_1\\cdot\\boldsymbol{x} }V(\\hat{\\boldsymbol{x}})\\hat{a}_{\\boldsymbol{p}_2}\\mathrm{e}^{\\mathrm{i}\\boldsymbol{p}_2\\cdot\\boldsymbol{x} }$\nEn introduisant la transform√©e de Fourier de $V(\\boldsymbol{x})$, $\\tilde{V}_{\\boldsymbol{p}}=\\frac{1}{\\mathcal{V}}\\int\\mathrm{d}^3 x\\,V(\\boldsymbol{x})\\mathrm{e}^{-\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x} }$, la formule s\u0026rsquo;√©claircit¬†:\n$\\displaystyle \\hat{V}=\\sum_{\\boldsymbol{p}_1,\\boldsymbol{p}_2}\\tilde{V}_{\\boldsymbol{p}_1-\\boldsymbol{p}_2}\\hat{a}^\\dagger_{\\boldsymbol{p}_1}\\hat{a}_{\\boldsymbol{p}_2}$\nSch√©matisation du processus¬†: une particule arrive avec l\u0026rsquo;impulsion $\\boldsymbol{p}_2$, interagit avec un champ de potentiel (repr√©sent√© par $\\tilde{V}_{\\boldsymbol{p}_1-\\boldsymbol{p}_2}$) puis repart avec l\u0026rsquo;impulsion $\\boldsymbol{p}_1$.\nRq¬†: l\u0026rsquo;op√©rateur $\\hat{V}$ n\u0026rsquo;est pas diagonal puisque les op√©rateurs $\\hat{a}$ cr√©ent et annihilent des particules avec des impulsions diff√©rentes.\n√âtudions l\u0026rsquo;influence du potentiel sur les √©tats et valeurs propres de l\u0026rsquo;Hamiltonien suivant¬†:\n$\\displaystyle\\hat{H} = E_0 \\sum_{\\boldsymbol{p}} \\hat{d}_{\\boldsymbol{p}}^\\dagger \\hat{d}_{\\boldsymbol{p}} - \\frac{V}{2} \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2} \\hat{d}_{\\boldsymbol{p}_1}^\\dagger \\hat{d}_{\\boldsymbol{p}_2}$\nOn limite le syst√®me √† 3 niveaux d\u0026rsquo;√©nergie, ce qui permet d\u0026rsquo;exprimer les √©tats dans une base $|n_{\\boldsymbol{p}_1}n_{\\boldsymbol{p}_2}n_{\\boldsymbol{p}_3}\\rangle$.\nCommen√ßons par d√©brancher le potentiel ($V=0$). Une particule plac√©e dans ce syst√®me ne peut √™tre que dans un des trois √©tats suivants¬†: $|100\\rangle$, $|010\\rangle$ ou $|001\\rangle$.\nRebranchons le potentiel et voyons comment il agit sur chacun des trois √©tats¬†:\n$\\displaystyle\\frac{V}{2} \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2} \\hat{d}_{\\boldsymbol{p}_1}^\\dagger \\hat{d}_{\\boldsymbol{p}_2} |100\\rangle = \\frac{V}{2} \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2} \\hat{d}_{\\boldsymbol{p}_1}^\\dagger \\hat{d}_{\\boldsymbol{p}_2} |010\\rangle = \\frac{V}{2} \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2} \\hat{d}_{\\boldsymbol{p}_1}^\\dagger \\hat{d}_{\\boldsymbol{p}_2} |001\\rangle = \\frac{V}{2} \\left( |100\\rangle + |010\\rangle + |001\\rangle \\right)$\nUne forme matricielle permet de synth√©tiser l\u0026rsquo;information¬†:\n$\\displaystyle H = \\left[E_0 \\begin{pmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{pmatrix} - \\frac{V}{2} \\begin{pmatrix} 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \\\\ 1 \u0026amp; 1 \u0026amp; 1 \\end{pmatrix}\\right]$\nLes valeurs propres de cette √©quation sont $\\varepsilon=E_0,E_0,E_0-\\frac{3V}{2}$. L\u0026rsquo;√©tat fondamental du syst√®me $|\\Omega\\rangle$ a donc l\u0026rsquo;√©nergie $E_0-\\frac{3V}{2}$ et l\u0026rsquo;√©tat propre correspondant s\u0026rsquo;√©crit $|\\Omega\\rangle = \\frac{1}{\\sqrt{3}} \\left( |100\\rangle + |010\\rangle + |001\\rangle \\right)$.\nL\u0026rsquo;√©tat fondamental est donc une superposition 1:1:1 des trois √©tats d\u0026rsquo;impulsion desquels on est parti.\nQuel est l\u0026rsquo;√©quivalent de la densit√© de probabilit√© d\u0026rsquo;une fonction d\u0026rsquo;onde dans le langage de la seconde quantification¬†? La densit√© de particules en un point $\\boldsymbol{x}$¬†!\nL\u0026rsquo;op√©rateur densit√© $\\hat{\\rho}(\\boldsymbol{x})$ est donn√© par¬†: $$ \\begin{aligned} \\hat{\\rho}(\\boldsymbol{x}) \u0026amp;= \\hat{\\psi}^\\dagger(\\boldsymbol{x})\\hat{\\psi}(\\boldsymbol{x})\\\\ \u0026amp;=\\frac{1}{\\mathcal{V}} \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2} \\mathrm{e}^{-\\mathrm{i} (\\boldsymbol{p}_1 - \\boldsymbol{p}_2) \\cdot \\boldsymbol{x}} \\hat{a}^\\dagger_{\\boldsymbol{p}_1} \\hat{a}_{\\boldsymbol{p}_2} \\end{aligned} $$\nOn v√©rifie que le terme de densit√© de particule est adapt√© puisqu\u0026rsquo;en int√©grant sur tout l\u0026rsquo;espace, on retrouve le nombre de particules¬†: $\\int \\mathrm{d}^3x\\hat{\\rho}(\\boldsymbol{x})= \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2} \\delta_{\\boldsymbol{p}_2,\\boldsymbol{p}_1} \\hat{a}^\\dagger_{\\boldsymbol{p}_1} \\hat{a}_{\\boldsymbol{p}_2} = \\sum_\\boldsymbol{p} \\hat{n}_\\boldsymbol{p}$\nCet op√©rateur permet de r√©√©crire l\u0026rsquo;op√©rateur √©nergie potentielle d\u0026rsquo;une particule unique soumise √† un potentiel ext√©rieur comme¬†:\n$\\displaystyle \\hat{V} = \\int \\mathrm{d}^3 x \\, V(\\boldsymbol{x}) \\hat{\\rho}(\\boldsymbol{x})$.\nLe cadre non-relativiste dans lequel on se place jusqu\u0026rsquo;ici est limit√© mais il permet d√©j√† de jouer avec des mod√®les de mati√®re condens√©e. Finissons donc le chapitre en appliquant notre nouveau formalisme √† des √©lectrons se d√©pla√ßant sur un r√©seau d\u0026rsquo;atomes.\nMod√®le des liaisons fortes pour l\u0026rsquo;√©nergie cin√©tique Travaillons dans une base o√π $\\hat{c}^\\dagger_i$ cr√©e un √©lectron sur un site du r√©seau √©tiquet√© $i$. Comme l\u0026rsquo;√©nergie cin√©tique d\u0026rsquo;une particule est d\u0026rsquo;autant plus grande que celle-ci est spatialement confin√©e (dans le cas d\u0026rsquo;un puits infini de largeur $L$, l\u0026rsquo;√©nergie cin√©tique vaut $E_n = \\frac{1}{2m}\\left( \\frac{n\\pi}{L}\\right)^2$), on va consid√©rer qu\u0026rsquo;un saut d\u0026rsquo;un site $j$ vers un site $i$ permet d\u0026rsquo;√©conomiser l\u0026rsquo;√©nergie cin√©tique $t_{ij}$ (ce terme d√©pend d\u0026rsquo;une fa√ßon ou d\u0026rsquo;une autre du recouvrement entre les orbitales atomiques du r√©seau). L\u0026rsquo;Hamiltonien somme tous les sauts possibles¬†:\n$$\\hat{H} = \\sum_{ij}(-t_{ij})\\hat{c}^\\dagger_i\\hat{c}_j$$\nChaque terme de la somme correspond √† un processus o√π une particule est annihil√©e au site $j$ puis recr√©√©e au site $i$, mod√©lisant un saut et sauvant ainsi l\u0026rsquo;√©nergie $t_{ij}$.\nOn va d\u0026rsquo;abord consid√©rer le cas le plus simple o√π $t_{ij}=t$ pour des voisins imm√©diats et $t_{ij}=0$ sinon. L\u0026rsquo;Hamiltonien devient¬†:\n$$\\hat{H} = -t\\sum_{i\\tau}\\hat{c}^\\dagger_i\\hat{c}_{i+\\tau}$$\no√π la somme sur $\\tau$ se fait sur les plus proches voisins.\nLa combinaison bilin√©aire $\\hat{c}^\\dagger_i\\hat{c}_j$ rend l\u0026rsquo;Hamiltonien non diagonal. Pour le diagonaliser, on passe √† nouveau en impulsion via les transform√©es de Fourier¬†:\n$$ \\hat{H} = \\sum_{\\boldsymbol{k}} E_{\\boldsymbol{k}}\\hat{c}^\\dagger_{\\boldsymbol{k}} \\hat{c}_{\\boldsymbol{k}} $$\no√π la relation de dispersion est donn√©e par $E_{\\boldsymbol{k}}=- \\sum_{\\tau} t \\mathrm{e}^{\\mathrm{i} \\boldsymbol{k} \\cdot \\boldsymbol{r}_{\\tau}}$.\nPour un r√©seau carr√© o√π $\\tau$ parcourt les vecteurs $(a,0)$, $(-a,0)$, $(0,a)$ et $(0,-a)$, on obtient $E_{\\boldsymbol{k}} = -2t \\left( \\cos(k_x a) + \\cos(k_y a) \\right)$\nOn a trac√© ci-dessus les contours du profil √©nerg√©tique dans l\u0026rsquo;espace r√©ciproque (l\u0026rsquo;√©nergie est constante sur une ligne). Pour $t\u0026gt;0$, l\u0026rsquo;√©nergie est minimale au centre $(k_x,k_y)=(0,0)$, ce qui correspond √† un √©lectron d√©localis√©, occupant tout le r√©seau et donc sans mouvement.\nPotentiel √† deux particules Ajoutons un deuxi√®me √©lectron capable d\u0026rsquo;interagir avec le premier.\nPour la promotion seconde quantification de $\\mathcal{A}_{\\alpha\\beta\\gamma\\delta}=\\langle\\alpha,\\beta|\\hat{A}|\\gamma,\\delta\\rangle$ on peut tenter intuitivement¬†:\n$$AÃÇ = \\sum_{\\alpha\\beta\\gamma\\delta} A_{\\alpha\\beta\\gamma\\delta} \\hat{a}^\\dagger_\\alpha \\hat{a}^\\dagger_\\beta \\hat{a}_\\gamma \\hat{a}_\\delta$$\nL\u0026rsquo;op√©rateurs √† deux particules typique est le potentiel $\\hat{V}$. Et il sera le plus souvent fonction des coordonn√©es spatiales. Son expression impliquant les op√©rateurs de champ est alors¬†:\n$$\\hat{V} = \\frac{1}{2} \\int \\mathrm{d}^3 x\\, \\mathrm{d}^3 y \\, \\hat{\\psi}^\\dagger(\\boldsymbol{x}) \\hat{\\psi}^\\dagger(\\boldsymbol{y}) V(\\boldsymbol{x}, \\boldsymbol{y}) \\hat{\\psi}(\\boldsymbol{y}) \\hat{\\psi}(\\boldsymbol{x})$$\nLe $\\frac{1}{2}$ compense le double comptage des interactions. L\u0026rsquo;ordre des op√©rateurs n\u0026rsquo;est pas anodin¬†! Celui utilis√© est appel√© ordre normal et sa vertu principale est d\u0026rsquo;assurer que l\u0026rsquo;op√©rateur $\\hat{V}$ ait une valeur moyenne pr√©dite (esp√©rance quantique) nulle pour l\u0026rsquo;√©tat de vide ($\\langle 0|\\hat{V}|0\\rangle=0$).\nOn restreint le potentiel √† la forme $V(\\boldsymbol{x}-\\boldsymbol{y})$ ne d√©pendant que de la s√©paration relative des particules (cela va garantir la conservation de l\u0026rsquo;impulsion dans l\u0026rsquo;interaction) et on d√©compose en modes d\u0026rsquo;impulsion les op√©rateurs champ¬†:\n$$ \\hat{V} = \\frac{1}{2\\mathcal{V}^2} \\int \\mathrm{d}^3x \\,\\mathrm{d}^3 y \\!\\!\\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2 \\boldsymbol{p}_3 \\boldsymbol{p}_4} \\mathrm{e}^{\\mathrm{i}(-\\boldsymbol{p}_1 \\cdot \\boldsymbol{x} - \\boldsymbol{p}_2 \\cdot \\boldsymbol{y} + \\boldsymbol{p}_3 \\cdot \\boldsymbol{y} + \\boldsymbol{p}_4 \\cdot \\boldsymbol{x})} \\hat{a}^\\dagger_{\\boldsymbol{p}_1} \\hat{a}^\\dagger_{\\boldsymbol{p}_2} V(\\boldsymbol{x} - \\boldsymbol{y}) \\hat{a}_{\\boldsymbol{p}_3} \\hat{a}_{\\boldsymbol{p}_4} $$\nChangement de variables et jeu sur les indices aboutissent √†¬†:\n$$\\hat{V} = \\frac{1}{2} \\sum_{\\boldsymbol{p}_1 \\boldsymbol{p}_2 \\boldsymbol{q}} \\tilde{V}_{\\boldsymbol{q}} \\,\\hat{a}^\\dagger_{\\boldsymbol{p}_1 + \\boldsymbol{q}} \\hat{a}^\\dagger_{\\boldsymbol{p}_2 - \\boldsymbol{q}} \\hat{a}_{\\boldsymbol{p}_2} \\hat{a}_{\\boldsymbol{p}_1}$$\no√π $\\tilde{V}_\\boldsymbol{q}$ est la transform√©e de Fourier du potentiel.\nLa formule peut s\u0026rsquo;interpr√©ter comme une diffusion dans l\u0026rsquo;espace des impulsions qui se repr√©sente conventionnellement par un dessin appel√© diagramme de Feynman.\nUne particule arrive avec une impulsion $\\boldsymbol{p}_2$, √©met une particule porteuse de force d\u0026rsquo;impulsion $\\boldsymbol{q}$, r√©duisant ainsi son impulsion finale √† $\\boldsymbol{p}_2-\\boldsymbol{q}$. La particule porteuse de force est absorb√©e par une autre particule dont l\u0026rsquo;impulsion passe alors de $\\boldsymbol{p}_1$ √† $\\boldsymbol{p}_1+\\boldsymbol{q}$. On constate que l\u0026rsquo;impulsion est conserv√©e aux points de croisement du diagramme.\nPour r√©ussir √† √©valuer l\u0026rsquo;√©nergie facilement, l\u0026rsquo;id√©e est de transformer toutes les combinaisons d\u0026rsquo;op√©rateurs en op√©rateurs nombre. Mais on se heurte √† la non diagonalit√© du potentiel (les probl√®mes impliquant une √©nergie potentielle ne peuvent g√©n√©ralement pas √™tre r√©solus de mani√®re exacte). Malgr√© cela, le potentiel √† deux particules peut nous mener √† des richesses physiques insoup√ßonn√©es (magn√©tisme, superfluidit√©, supraconductivit√©,\u0026hellip;).\nMod√®le de Hubbard Le mod√®le de Hubbard est central en mati√®re condens√©e. Il permet de capturer les implications physiques de la comp√©tition entre √©nergie cin√©tique (qui favorise la d√©localisation des √©lectrons) et √©nergie potentielle (qui favorise leur localisation).\nL\u0026rsquo;Hamiltonien du mod√®le de Hubbard est construit √† partir de nos deux ingr√©dients pr√©c√©dents¬†: l\u0026rsquo;Hamiltonien du mod√®le de liaisons fortes pour mod√©liser l\u0026rsquo;√©nergie cin√©tique et le potentiel √† deux particules pour mod√©liser les interactions electron-electron¬†:\n$$\\hat{H} = \\sum_{ij} \\left(-t_{ij} \\hat{c}^\\dagger_i \\hat{c}_j\\right) + \\frac{1}{2} \\sum_{ijkl} \\hat{c}^\\dagger_i \\hat{c}^\\dagger_j V_{ijkl} \\hat{c}_k \\hat{c}_l$$\nEt comme les √©lectrons poss√®dent un spin $\\sigma$ qui peut pointer soit vers le haut ($|\\uparrow\\rangle$), soit vers le bas ($|\\downarrow\\rangle$), l\u0026rsquo;Hamiltonien devient¬†:\n$$\\hat{H} = \\sum_{ij\\sigma} (-t_{ij}) \\hat{c}^\\dagger_{i\\sigma} \\hat{c}_{j\\sigma} + \\frac{1}{2} \\sum_{ijkl\\sigma\\sigma\u0026rsquo;} \\hat{c}^\\dagger_{i\\sigma} \\hat{c}^\\dagger_{j\\sigma\u0026rsquo;} V_{ijkl} \\hat{c}_{k\\sigma\u0026rsquo;} \\hat{c}_{l\\sigma}$$\nOn suppose ici que les spins ne peuvent pas basculer. Comme l\u0026rsquo;interaction entre √©lectrons vient de l\u0026rsquo;interaction de Coulomb, elle est ind√©pendante de l\u0026rsquo;orientation du spin. Mais pour simplifier, on supposera que l\u0026rsquo;interaction coulombienne n\u0026rsquo;est notable que lorsque les √©lectrons sont sur le m√™me site. Les √©lectrons interagissent donc via une √©nergie potentielle constante $U=V_{iiii}$. Mais comme le principe de Pauli impose que deux √©lectrons sur le m√™me site aient des spins oppos√©s, l\u0026rsquo;Hamiltonien devient¬†:\n$$\\hat{H} = \\sum_{ij\\sigma} (-t_{ij}) \\hat{c}^\\dagger_{i\\sigma} \\hat{c}_{j\\sigma} + U \\sum_i \\hat{n}_{i\\uparrow} \\hat{n}_{i\\downarrow}$$\nBien que simple d\u0026rsquo;allure, obtenir les √©tats propres est souvent une t√¢che complexe et ceux-ci sont g√©n√©ralement fortement corr√©l√©s.\nSimplifions √† l\u0026rsquo;extr√™me avec un r√©seau de seulement deux sites, et un seul √©lectron avec un spin haut $|\\uparrow\\rangle$. L\u0026rsquo;√©lectron peut √™tre soit sur le premier site, ce que l\u0026rsquo;on note $|\\uparrow,0\\rangle$ (spin haut sur le site 1, rien sur le site 2), soit sur le second $|0,\\uparrow\\rangle$ (rien sur le site 1, spin haut sur le site 2).\nUn √©tat g√©n√©ral est une combinaison de ces deux √©tats $|\\psi\\rangle = a|\\uparrow,0\\rangle+b|0,\\uparrow\\rangle$ et l\u0026rsquo;Hamiltonien de Hubbard peut s\u0026rsquo;√©crire dans cette base¬†:\n$ \\displaystyle \\hat{H} = \\begin{pmatrix} 0 \u0026amp; -t \\\\ -t \u0026amp; 0 \\end{pmatrix} $\nEn diagonalisant, on obtient un √©tat fondamental $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}(|\\uparrow,0\\rangle+|0,\\uparrow\\rangle )$ pour une √©nergie $E=-t$, et un √©tat excit√© $|\\psi\\rangle=\\frac{1}{\\sqrt{2}}(|\\uparrow,0\\rangle-|0,\\uparrow\\rangle )$ pour une √©nergie $E=t$.\nAjoutons un √©lectron dans le syst√®me. S\u0026rsquo;il a le m√™me spin, la solution est simplement $|\\uparrow,\\uparrow\\rangle$ et l\u0026rsquo;√©nergie associ√©e est $E=0$ (pas d\u0026rsquo;interaction possible puisque les deux √©lectrons ne peuvent pas occuper le m√™me site et ils ne peuvent pas sauter d\u0026rsquo;un site √† l\u0026rsquo;autre, ils sont bloqu√©s).\nMontrons-le par le calcul\u0026nbsp;: Pour la partie potentiel, on obtient bien 0 puisque $U (\\hat{n}_{1\\uparrow} \\hat{n}_{1\\downarrow}+\\hat{n}_{2\\uparrow} \\hat{n}_{2\\downarrow})|\\uparrow,\\uparrow\\rangle=U(1\\times 0+1\\times 0)|\\uparrow,\\uparrow\\rangle = 0$ (puisque l'op√©rateur qui compte les spins bas trouve toujours z√©ro).\nEt pour la partie cin√©tique, on ne garde que les termes de spin haut de l'Hamiltonien\u0026nbsp;: $-t (\\hat{c}^\\dagger_{1\\uparrow} \\hat{c}_{2\\uparrow}+\\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow})$. Et on r√©√©crit $|\\uparrow,\\uparrow\\rangle$ comme $\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}|0\\rangle$.\n$-t (\\hat{c}^\\dagger_{1\\uparrow} \\hat{c}_{2\\uparrow}+\\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow})\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}|0\\rangle=-t \\hat{c}^\\dagger_{1\\uparrow} \\hat{c}_{2\\uparrow}\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}|0\\rangle-t\\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow}\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}|0\\rangle=t \\hat{c}^\\dagger_{1\\uparrow} \\hat{c}_{2\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}\\hat{c}^\\dagger_{1\\uparrow}|0\\rangle-t\\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow}\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}|0\\rangle=t \\hat{c}^\\dagger_{1\\uparrow}(1-\\hat{n}_{2\\uparrow})\\hat{c}^\\dagger_{1\\uparrow}|0\\rangle-t\\hat{c}^\\dagger_{2\\uparrow}(1-\\hat{n}_{1\\uparrow})\\hat{c}^\\dagger_{2\\uparrow}|0\\rangle = t \\cancel{\\hat{c}^\\dagger_{1\\uparrow} \\hat{c}^\\dagger_{1\\uparrow} }|0\\rangle - t\\hat{c}^\\dagger_{1\\uparrow}\\cancel{\\hat{n}_{2\\uparrow}|\\uparrow,0\\rangle}- t\\cancel{\\hat{c}^\\dagger_{2\\uparrow}\\hat{c}^\\dagger_{2\\uparrow}}|0\\rangle+t\\hat{c}^\\dagger_{2\\uparrow}\\cancel{\\hat{n}_{1\\uparrow}|0,\\uparrow\\rangle}=0$ S\u0026rsquo;ils sont de spins oppos√©s, une base possible est $\\{|\\uparrow\\downarrow,0\\rangle,|\\uparrow,\\downarrow\\rangle,|\\downarrow,\\uparrow\\rangle,|0,\\uparrow\\downarrow\\rangle\\}$. Et dans cette base, l\u0026rsquo;Hamiltonien de Hubbard s\u0026rsquo;√©crit¬†:\n$ \\displaystyle \\hat{H} = \\begin{pmatrix} U \u0026amp; -t \u0026amp; t \u0026amp; 0 \\\\ -t \u0026amp; 0 \u0026amp; 0 \u0026amp; -t \\\\ t \u0026amp; 0 \u0026amp; 0 \u0026amp; t \\\\ 0 \u0026amp; -t \u0026amp; t \u0026amp; U \\end{pmatrix} $\nD√©taillons juste l'action de l'Hamiltonien sur $|\\uparrow\\downarrow,0\\rangle$ pour s'en convaincre\u0026nbsp;: Terme potentiel¬†: $U \\hat{n}_{1\\uparrow}\\hat{n}_{1\\downarrow}|\\uparrow\\downarrow,0\\rangle = U\\times 1\\times 1=U$ et $U \\hat{n}_{2\\uparrow}\\hat{n}_{2\\downarrow}|\\uparrow\\downarrow,0\\rangle = U\\times 0\\times 0=0$ (puisque l\u0026rsquo;op√©rateur $\\hat{n}$ ne fait que compter le nombres de particules dans un site donn√©).\nTerme cin√©tique¬†: comme $i$ et $j$ doivent √™tre diff√©rents, on a 4 termes dans la somme $(-t) \\hat{c}^\\dagger_{1\\uparrow} \\hat{c}_{2\\uparrow}+(-t) \\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow}+(-t) \\hat{c}^\\dagger_{1\\downarrow} \\hat{c}_{2\\downarrow}+(-t) \\hat{c}^\\dagger_{2\\downarrow} \\hat{c}_{1\\downarrow}$.\nSeuls les deux termes contenant un op√©rateur d\u0026rsquo;annihilation sur le site 1 ne donneront pas un r√©sultat nul. Il nous reste $-t(\\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow} +\\hat{c}^\\dagger_{2\\downarrow} \\hat{c}_{1\\downarrow})$.\nAppliquons cette somme d\u0026rsquo;op√©rateurs √† $|\\uparrow\\downarrow,0\\rangle$ qu\u0026rsquo;on peut aussi √©crire $\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{1\\downarrow}|0\\rangle$. Le permier terme nous donne $-t\\hat{c}^\\dagger_{2\\uparrow} \\hat{c}_{1\\uparrow} \\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{1\\downarrow}|0\\rangle=-t\\hat{c}^\\dagger_{2\\uparrow} (1-\\hat{n}_{1\\uparrow})\\hat{c}^\\dagger_{1\\downarrow}|0\\rangle=-t\\hat{c}^\\dagger_{2\\uparrow}\\hat{c}^\\dagger_{1\\downarrow}|0\\rangle+t\\hat{c}^\\dagger_{2\\uparrow} \\cancel{\\hat{n}_{1\\uparrow}|\\downarrow,0\\rangle}=-t\\hat{c}^\\dagger_{2\\uparrow}|\\downarrow,0\\rangle=t|\\downarrow,\\uparrow\\rangle$. Le dernier changement de signe venant du fait qu\u0026rsquo;il y ait d√©j√† un fermion √† gauche de celui cr√©√© dans l\u0026rsquo;√©tat 2 ($(-1)^{n_1+n_2+\\cdots+n_{i-1}}=(-1)^1=-1$).\nEt le deuxi√®me terme donne $-t\\hat{c}^\\dagger_{2\\downarrow} \\hat{c}_{1\\downarrow}\\hat{c}^\\dagger_{1\\uparrow}\\hat{c}^\\dagger_{1\\downarrow}|0\\rangle=t\\hat{c}^\\dagger_{2\\downarrow} \\hat{c}_{1\\downarrow}\\hat{c}^\\dagger_{1\\downarrow}\\hat{c}^\\dagger_{1\\uparrow}|0\\rangle=t\\hat{c}^\\dagger_{2\\downarrow} (1-\\hat{n}_{1\\downarrow})\\hat{c}^\\dagger_{1\\uparrow}|0\\rangle=t\\hat{c}^\\dagger_{2\\downarrow}\\hat{c}^\\dagger_{1\\uparrow}|0\\rangle-t\\hat{c}^\\dagger_{2\\downarrow}\\cancel{\\hat{n}_{1\\downarrow}|\\uparrow,0\\rangle}=-t|\\uparrow,\\downarrow\\rangle$.\nLa matrice peut se diagonaliser et l\u0026rsquo;√©tat fondamental s\u0026rsquo;√©crit $|\\psi\\rangle = N(|\\uparrow\\downarrow,0\\rangle+W|\\uparrow,\\downarrow\\rangle-W|\\downarrow,\\uparrow\\rangle+|0,\\uparrow\\downarrow\\rangle)$ o√π $N$ est une constante de normalisation et $W=\\frac{U}{4t}+\\frac{1}{4t}\\sqrt{U^2+16t^2}$ pour une √©nergie $E=\\frac{U}{2}-\\frac{1}{2}\\sqrt{U^2+16t^2}$. C\u0026rsquo;est d√©j√† un r√©sultat √©tonament complexe pour un syst√®me si simple (deux √©lectrons sur deux sites)\u0026hellip;\nOn a d√©couvert jusqu\u0026rsquo;ici comment des syst√®mes vari√©s peuvent se r√©duire √† un simple jeu d\u0026rsquo;oscillateurs harmoniques, chacun d√©crivant un certain mode normal du syst√®me. Et chacun de ces modes normaux peut-√™tre vu comme un √©tat d\u0026rsquo;impulsion pour des particules identiques sans interaction. Le nombre de particules correspond alors au nombre d\u0026rsquo;excitations quantifi√©es du mode. Et on a d√©fini des op√©rateurs qui peuvent cr√©er et annihiler ces particules.\nLes mod√®les consid√©r√©s se restreignaient √† des r√©seaux discrets adapt√©s √† la physique de la mati√®re condens√©e. Dans le prochain chapitre, on va g√©n√©raliser au continu.\nChapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc3/",
	"title": "TQC-3",
	"tags": [],
	"description": "",
	"content": " Th√©orie quantique des champs \u0026ndash; Partie 3 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Tr√®s souvent une simple traduction.\nRetour sommaire\n√âvolution temporelle Deux repr√©sentations de la m√©canique quantique s\u0026rsquo;opposent quant √† la description de l\u0026rsquo;√©volution temporelle d\u0026rsquo;un syst√®me¬†: la repr√©sentation de Schr√∂dinger et la repr√©sentation d\u0026rsquo;Heisenberg.\nRepr√©sentation de Schr√∂dinger Dans la repr√©sentation de Schr√∂dinger, ce sont les fonctions d\u0026rsquo;onde qui d√©pendent du temps et leur √©volution est d√©termin√©e par l\u0026rsquo;√©quation de Schr√∂dinger¬†:\n$$ \\mathrm{i} \\frac{\\partial \\psi(\\boldsymbol{x}, t)}{\\partial t}=\\hat{H} \\psi(\\boldsymbol{x}, t) $$\nOn acc√®de aux variables dynamiques comme la position et l\u0026rsquo;impulsion via des op√©rateurs ($\\hat{\\boldsymbol{x}}=\\boldsymbol{x}$, $\\hat{\\boldsymbol{p}}=-\\mathrm{i} \\nabla$) qui agissent sur la fonction d\u0026rsquo;onde.\nOn est tr√®s loin de la m√©canique classique o√π les variables dynamiques ($\\boldsymbol{p}(t)$, $\\boldsymbol{x}(t)$,\u0026hellip;) d√©pendent du temps et sont donc d√©crites par des √©quations du mouvement. Mais comme aucun \u0026ldquo;op√©rateur temps\u0026rdquo; n\u0026rsquo;existe en m√©canique quantique, il faut se contenter d\u0026rsquo;un param√®tre temporel $t$ que la repr√©sentation de Schr√∂dinger confine donc enti√®rement dans la fonction d\u0026rsquo;onde.\nPas d\u0026rsquo;op√©rateur donnant le temps mais quand m√™me un op√©rateur d\u0026rsquo;√©volution $\\hat{U}(t_2,t_1)$ permettant de faire √©voluer une particule de $t_1$ √† $t_2$.\n$$ \\psi\\left(t_2\\right)=\\hat{U}\\left(t_2, t_1\\right) \\psi\\left(t_1\\right) $$\nL\u0026rsquo;op√©rateur d\u0026rsquo;√©volution a les propri√©t√©s suivantes¬†:\n$\\hat{U}\\left(t_1, t_1\\right)=1$\nRien ne se passe si les instants sont les m√™mes. $\\hat{U}\\left(t_3, t_2\\right) \\hat{U}\\left(t_2, t_1\\right)=\\hat{U}\\left(t_3, t_1\\right)$\nCette relation de composition montre qu'on peut construire une translation temporelle quelconque en multipliant entre elles une multitude de translations temporelles minuscules. $\\displaystyle \\mathrm{i} \\frac{\\mathrm{~d}}{\\mathrm{~d} t_2} \\hat{U}\\left(t_2, t_1\\right)=\\hat{H} \\hat{U}\\left(t_2, t_1\\right)$\nL'op√©rateur d'√©volution lui-m√™me ob√©it √† l'√©quation de Schr√∂dinger. Pour le prouver, on part de $\\psi\\left(t_2\\right)=\\hat{U}\\left(t_2, t_1\\right) \\psi\\left(t_1\\right)$ et on d√©rive par rapport √† $t_2$¬†:\n$$ \\frac{\\mathrm{d} \\psi\\left(t_2\\right)}{\\mathrm{d} t_2}=\\frac{\\mathrm{d} \\hat{U}\\left(t_2, t_1\\right)}{\\mathrm{d} t_2} \\psi\\left(t_1\\right) $$\nEt en utilisant $\\mathrm{i} \\frac{\\partial \\psi(\\boldsymbol{x}, t)}{\\partial t}=\\hat{H} \\psi(\\boldsymbol{x}, t)$, on obtient¬†;\n$$ \\mathrm{i} \\frac{\\mathrm{~d} \\psi\\left(t_2\\right)}{\\mathrm{d} t_2}=\\hat{H} \\psi\\left(t_2\\right)=\\hat{H} \\hat{U}\\left(t_2, t_1\\right) \\psi\\left(t_1\\right) $$\n$\\hat{U}\\left(t_1, t_2\\right)=\\hat{U}^{-1}\\left(t_2, t_1\\right)$\nOn prenant l'inverse de l'op√©rateur d'√©volution, on remonte le temps. $\\hat{U}^{\\dagger}\\left(t_2, t_1\\right) \\hat{U}\\left(t_2, t_1\\right)=1,$\nL'op√©rateur d'√©volution est unitaire. Pour $t_2=t_1$, c\u0026rsquo;est trivial.\nPour $t_2\\neq t_1$, montrons que cette composition d\u0026rsquo;op√©rateurs est constante (et cette constante vaut 1 par normalisation)¬†:\n$$ \\begin{aligned} \\frac{\\mathrm{d}}{\\mathrm{~d} t_2}\\left[\\hat{U}^{\\dagger}\\left(t_2, t_1\\right) \\hat{U}\\left(t_2, t_1\\right)\\right] \u0026amp; =\\frac{\\mathrm{d} \\hat{U}^{\\dagger}}{\\mathrm{d} t_2} U+U^{\\dagger} \\frac{\\mathrm{d} \\hat{U}}{\\mathrm{~d} t_2} \\\\ \u0026amp; =-\\frac{\\hat{U}^{\\dagger} \\hat{H} \\hat{U}}{\\mathrm{i}}+\\frac{\\hat{U}^{\\dagger} \\hat{H} \\hat{U}}{\\mathrm{i}}\\\\ \u0026amp;=0 \\end{aligned} $$\no√π on a utilis√© $\\frac{\\mathrm{d} \\hat{U}}{\\mathrm{~d} t}=\\frac{\\hat{H} \\hat{U}}{\\mathrm{i}}$ et $\\frac{\\mathrm{d} \\hat{U}^{\\dagger}}{\\mathrm{d} t}=-\\frac{\\hat{U}^{\\dagger} \\hat{H}}{\\mathrm{i}}$.\nUne cons√©quence directe est que $\\hat{U}^\\dagger(t_2,t_1)=\\hat{U}^{-1}(t_2,t_1)$¬†; l\u0026rsquo;op√©rateur adjoint est l\u0026rsquo;op√©rateur inverse.\nLa propri√©t√© 3 permet d\u0026rsquo;exprimer explicitement $\\hat{U}(t_2,t_1)$¬†:\n$$ \\hat{U}\\left(t_2, t_1\\right)=\\mathrm{e}^{-\\mathrm{i} \\hat{H}\\left(t_2-t_1\\right)} $$\nCe type d\u0026rsquo;expression exponentiel cache le d√©veloppement¬†:\n$$ \\mathrm{e}^{\\hat{A}}=1+\\hat{A}+\\frac{1}{2!} \\hat{A} \\hat{A}+\\frac{1}{3!} \\hat{A} \\hat{A} \\hat{A}+\\ldots $$\nRepr√©sentation d\u0026rsquo;Heisenberg Plut√¥t que de placer la d√©pendance temporelle dans la fonction d\u0026rsquo;onde, dans la repr√©sentation d\u0026rsquo;Heisenberg, ce sont les op√©rateurs qui la prennent en charge.\nPour passer d\u0026rsquo;une repr√©sentation √† l\u0026rsquo;autre, commen√ßons par √©crire la valeur moyenne de l\u0026rsquo;op√©rateur $O$ dans l\u0026rsquo;√©tat $\\psi(t)$¬†:\n$$ \\langle\\hat{O}(t)\\rangle=\\langle\\psi(t)| \\hat{O}|\\psi(t)\\rangle $$\nToutes les repr√©sentations doivent s\u0026rsquo;accorder sur ce r√©sultat.\nUtilisons maintenant l\u0026rsquo;op√©rateur d\u0026rsquo;√©volution pour ne plus avoir √† s\u0026rsquo;occuper que de l\u0026rsquo;√©tat √† l\u0026rsquo;instant initial $\\psi(0)$¬†:\n$$ \\psi(t)=\\hat{U}(t, 0) \\psi(0)=\\mathrm{e}^{-\\mathrm{i} \\hat{H} t} \\psi(0) $$\nEt en repla√ßant $\\psi(t)$ dans l\u0026rsquo;expression de la valeur moyenne¬†:\n$$ \\langle\\psi(t)| \\hat{O}|\\psi(t)\\rangle=\\langle\\psi(0)| \\hat{U}^{\\dagger}(t, 0) \\hat{O} \\hat{U}(t, 0)|\\psi(0)\\rangle $$\nDans la repr√©sentation de Schr√∂dinger, on consid√®re les op√©rateurs comme ind√©pendants du temps ($\\hat{O}_{\\mathrm{S}} \\equiv \\hat{O}$), contrairement aux √©tats ($\\left|\\psi_{\\mathrm{S}}(t)\\right\\rangle \\equiv \\hat{U}(t, 0)|\\psi(0)\\rangle$) de telle sorte que¬†:\n$$ \\langle\\psi(0)| \\hat{U}^{\\dagger}(t, 0)[\\hat{O}] \\hat{U}(t, 0)|\\psi(0)\\rangle=\\left\\langle\\psi_{\\mathrm{S}}(t)\\right| \\hat{O}_{\\mathrm{S}}\\left|\\psi_{\\mathrm{S}}(t)\\right\\rangle $$\nMais en √©tendant un peu les crochets, on obtient une nouvelle fa√ßon de voir les choses o√π les op√©rateurs deviennent des objets dynamiques¬†:\n$$ \\langle\\psi(0)|\\left[\\hat{U}^{\\dagger}(t, 0) \\hat{O} \\hat{U}(t, 0)\\right]|\\psi(0)\\rangle=\\left\\langle\\psi_{\\mathrm{H}}\\right| \\hat{O}_{\\mathrm{H}}(t)\\left|\\psi_{\\mathrm{H}}\\right\\rangle $$\nOn obtient ainsi la repr√©sentation d\u0026rsquo;Heisenberg o√π les √©tats sont ind√©pendants du temps ($\\psi_{\\mathrm{H}} \\equiv \\psi(0)$), contrairement aux op√©rateurs¬†:\n$$ \\hat{O}_{\\mathrm{H}}(t) \\equiv \\hat{U}^{\\dagger}(t, 0) \\hat{O}_{\\mathrm{S}} \\hat{U}(t, 0) $$\nOn retrouve une situation similaire √† la m√©canique classique avec ses variables dynamiques.\nEt pour expliciter la d√©pendance temporelle de $\\hat{O}_H(t)$, on diff√©rentie l\u0026rsquo;√©quation pr√©c√©dente et on utilise la propri√©t√© 3 de l\u0026rsquo;op√©rateur d\u0026rsquo;√©volution¬†:\n$$ \\frac{\\mathrm{d} \\hat{O}_{\\mathrm{H}}(t)}{\\mathrm{d} t}=\\frac{\\mathrm{d} \\hat{U}^{\\dagger}}{\\mathrm{d} t} \\hat{O}_{\\mathrm{S}} \\hat{U}+\\hat{U}^{\\dagger} \\hat{O}_{\\mathrm{S}} \\frac{\\mathrm{~d} \\hat{U}}{\\mathrm{~d} t}=\\frac{1}{\\mathrm{i}}\\left(-\\hat{U}^{\\dagger} \\hat{H} \\hat{O}_{\\mathrm{S}} \\hat{U}+\\hat{U}^{\\dagger} \\hat{O}_{\\mathrm{S}} \\hat{H} \\hat{U}\\right) $$\nEn utilisant la d√©finition de $\\hat{O}_H(t)$ et le fait que $\\hat{U}$ et $\\hat{H}$ commutent (puisque $\\hat{U}$ s\u0026rsquo;exprime √† partir de $\\hat{H}$), on obtient l\u0026rsquo;√©quation du mouvement d\u0026rsquo;Heisenberg¬†:\n$$ \\frac{\\mathrm{d} \\hat{O}_{\\mathrm{H}}(t)}{\\mathrm{d} t}=\\frac{1}{\\mathrm{i} \\hbar}\\left[\\hat{O}_{\\mathrm{H}}(t), \\hat{H}\\right] $$\nLes limites de la description du monde par la m√©canique quantique La m√©canique quantique \u0026ldquo;classique\u0026rdquo; est une description formidable du monde √† petite √©chelle. Mais c\u0026rsquo;est fondamentalement une description √† une particule. Tant qu\u0026rsquo;on n\u0026rsquo;est pas emb√™t√© par la relativit√©, √ßa suffit. Mais les deux r√©sultats suivants t√©moignent de l\u0026rsquo;incapacit√© de la m√©canique quantique √† une particule √† s\u0026rsquo;accommoder d\u0026rsquo;un cadre relativiste.\nPremier coup dur¬†: la probabilit√© de trouver la particule en dehors de son c√¥ne de lumi√®re n\u0026rsquo;est pas nulle. En effet, $\\langle\\boldsymbol{x}| \\mathrm{e}^{-\\mathrm{i} \\hat{H} t}|\\boldsymbol{x}=0\\rangle$ donne un r√©sultat proportionnel √† $\\mathrm{e}^{-m|x|}$ pour un intervalle de type espace (tel que $|x|\u0026gt;t$). C\u0026rsquo;est certes infime pour des grands $|\\boldsymbol{x}|$, mais cela reste difficilement acceptable\u0026hellip;\nDeuxi√®me coup port√©¬†: imaginons que l\u0026rsquo;on cloisonne la particule entre deux murs (des barri√®res de potentiel) s√©par√©s d\u0026rsquo;une distance bien inf√©rieure √† sa longueur d\u0026rsquo;onde de Compton $\\lambda=h m / c$ ($1/m$ en unit√©s naturelles). La minuscule incertitude sur la position ($\\Delta x \\ll \\lambda$) entra√Æne une incertitude sur l\u0026rsquo;impulsion telle ($\\Delta p \\gg 1/\\lambda=m$) que l\u0026rsquo;√©nergie de la particule devient suffisante pour faire jaillir des paires de particules-antiparticules. La boite devrait alors contenir en moyenne plus d\u0026rsquo;une particule, situation que la m√©canique quantique \u0026ldquo;classique\u0026rdquo; n\u0026rsquo;est pas c√¢bl√©e pour d√©crire.\nCes objets d√©finis localement (en un point $x$ de l\u0026rsquo;espace-temps) que sont les champs et plus pr√©cis√©ment les champs d\u0026rsquo;op√©rateurs $\\hat{\\phi}(x)$ vont tirer la quantique de ce mauvais pas. Et puisqu\u0026rsquo;il s\u0026rsquo;agit de placer au premier plan des op√©rateurs dynamiques, la repr√©sentation d\u0026rsquo;Heisenberg va se trouver √™tre la repr√©sentation idoine.\nPour rendre un champ d\u0026rsquo;op√©rateurs compatible avec la relativit√©, il suffit de s\u0026rsquo;assurer que des op√©rateurs √©loign√©s d\u0026rsquo;un intervalle de type espace commutent ($[\\hat{\\phi}(x), \\hat{\\phi}(y)]=0$ si $(x-y)^2\u0026lt;0$). Cela emp√™che que le r√©sultat de l\u0026rsquo;une des mesures puisse influer causalement le r√©sultat de l\u0026rsquo;autre.\nTransformations continues Translations dans l\u0026rsquo;espace-temps Pour translater une particule dans l\u0026rsquo;espace, il nous faut un op√©rateur $\\hat{U}$ qui transforme un √©tat localis√© en $\\boldsymbol{x}$ en un √©tat localis√© en $\\boldsymbol{x}+\\boldsymbol{a}$¬†:\n$$ \\hat{U}(\\boldsymbol{a})|\\boldsymbol{x}\\rangle=|\\boldsymbol{x}+\\boldsymbol{a}\\rangle $$\nOn suppose ici qu\u0026rsquo;on a transport√© la particule √† sa nouvelle place¬†; on parle alors de point de vue actif.\nFaisons maintenant agir l\u0026rsquo;op√©rateur sur une fonction d\u0026rsquo;onde $\\psi(x)$¬†:\n$$ \\hat{U}(\\boldsymbol{a})\\psi(\\boldsymbol{x})=\\psi(\\boldsymbol{x}-\\boldsymbol{a}) $$\nSoit $|\\psi\\rangle$ l\u0026rsquo;√©tat d√©crit par la fonction d\u0026rsquo;onde $\\psi(\\boldsymbol{x})=\\langle\\psi|\\boldsymbol{x}\\rangle$ et soit $|\\phi\\rangle=\\hat{U}(\\boldsymbol{a})|\\psi\\rangle$, l\u0026rsquo;√©tat translat√©.\n$$ \\begin{aligned} \\phi(\\boldsymbol{x}) \u0026amp; =\\langle \\boldsymbol{x} | \\phi\\rangle\\\\ \u0026amp;=\\langle \\boldsymbol{x} | \\hat{U}(\\boldsymbol{a})|\\psi\\rangle\\\\ \u0026amp;=\\int d \\boldsymbol{x}^{\\prime} \\langle \\boldsymbol{x} | \\hat{U}(\\boldsymbol{a}) | \\boldsymbol{x}^{\\prime}\\rangle \\langle \\boldsymbol{x}^{\\prime} | \\psi \\rangle\\\\ \u0026amp;=\\int d \\boldsymbol{x}^{\\prime} \\langle \\boldsymbol{x} | \\boldsymbol{x}^{\\prime}+\\boldsymbol{a} \\rangle \\langle \\boldsymbol{x}^{\\prime} | \\psi\\rangle \\\\ \u0026amp; =\\int d \\boldsymbol{x}^{\\prime} \\delta (\\boldsymbol{x}-\\boldsymbol{x}^{\\prime}-\\boldsymbol{a}) \\psi (\\boldsymbol{x}^{\\prime})\\\\ \u0026amp;=\\psi(\\boldsymbol{x}-\\boldsymbol{a}) \\end{aligned} $$\nEn r√©√©crivant √ßa $\\phi(\\boldsymbol{x}+\\boldsymbol{a})=\\psi(\\boldsymbol{x})$, on obtient un moyen simple de se souvenir du signe¬†:\nla valeur de la nouvelle fonction d\u0026rsquo;onde au nouveau point est √©gale √† la valeur de l\u0026rsquo;ancienne fonction d\u0026rsquo;onde √† l\u0026rsquo;ancien point.\nPropri√©t√©s de l\u0026rsquo;op√©rateur de translation¬†:\n$\\hat{U}(\\boldsymbol{0})=1$\nPr√©sence d'un √©l√©ment neutre. $\\hat{U}(\\boldsymbol{a})\\hat{U}(\\boldsymbol{b})=\\hat{U}(\\boldsymbol{a}+\\boldsymbol{b})$\nLoi de composition. $\\hat{U}(\\boldsymbol{a})^{-1}=\\hat{U}(-\\boldsymbol{a})$\nL'op√©rateur de translation admet un inverse. $\\hat{U}(\\boldsymbol{a})^{-1}=\\hat{U}(\\boldsymbol{a})^\\dagger$\nL'op√©rateur de translation est unitaire. Ces propri√©t√©s nous montrent que ces transformations forment un groupe et comme chaque √©l√©ment d√©pend d\u0026rsquo;un param√®tre continu ($\\boldsymbol{a}$) entra√Ænant que le groupe poss√®de un nombre infini d\u0026rsquo;√©l√©ments, il s\u0026rsquo;agit d\u0026rsquo;un groupe de Lie.\nUn groupe est un ensemble $G$ muni d\u0026rsquo;une loi de composition interne $\\bullet$ associative admettant un √©l√©ment neutre et, pour chaque √©l√©ment de l\u0026rsquo;ensemble, un √©l√©ment sym√©trique¬†:\n$\\forall a, b \\in G, a \\bullet b \\in G$ (loi de composition interne assurant la fermeture du groupe) $\\forall a, b, c \\in G, a \\bullet(b \\bullet c)=(a \\bullet b) \\bullet c$ (associativit√©) $\\exists e \\in G$ tel que $\\forall a \\in G, a \\bullet e=e \\bullet a=a$ (√©l√©ment neutre) $\\forall a \\in G, \\exists a^{-1} \\in G$ tel que $a \\bullet a^{-1}=a^{-1} \\bullet a=e$ (inverse) note\nDes vieilles notes de lecture d\u0026rsquo;un autre chouette livre pour en savoir un peu plus sur les groupes discrets.\nL\u0026rsquo;op√©rateur de translation peut aussi √™tre vu comme la transformation d\u0026rsquo;un op√©rateur¬†:\n$$ \\hat{U}^{\\dagger}(\\boldsymbol{a}) \\hat{\\boldsymbol{x}} \\hat{U}(\\boldsymbol{a})=(\\hat{\\boldsymbol{x}}+\\boldsymbol{a}) . $$\nOn a en effet\u0026nbsp;: $$ \\begin{aligned} \\hat{U}(\\boldsymbol{a})|\\boldsymbol{x}\\rangle \u0026amp; =|\\boldsymbol{x}+\\boldsymbol{a}\\rangle \\\\ \\hat{\\boldsymbol{x}} U(\\boldsymbol{a})|\\boldsymbol{x}\\rangle \u0026amp; =\\hat{\\boldsymbol{x}}|\\boldsymbol{x}+\\boldsymbol{a}\\rangle=(\\boldsymbol{x}+\\boldsymbol{a})|\\boldsymbol{x}+\\boldsymbol{a}\\rangle \\\\ U^{\\dagger}(\\boldsymbol{a}) \\hat{\\boldsymbol{x}} U(\\boldsymbol{a})|\\boldsymbol{x}\\rangle \u0026amp; =(\\boldsymbol{x}+\\boldsymbol{a}) U^{\\dagger}(\\boldsymbol{a})|\\boldsymbol{x}+\\boldsymbol{a}\\rangle=(\\boldsymbol{x}+\\boldsymbol{a})|\\boldsymbol{x}\\rangle \\end{aligned} $$\nImaginons une observable repr√©sent√©e par l\u0026rsquo;op√©rateur $\\hat{O}$. Si une translation n\u0026rsquo;a pas d\u0026rsquo;effet sur la propri√©t√© mesur√©e par cet op√©rateur, on peut √©crire¬†:\n$$ \\langle\\psi(\\boldsymbol{x})| \\hat{O}|\\psi(\\boldsymbol{x})\\rangle=\\langle\\psi(\\boldsymbol{x})| \\hat{U}^{-1}(\\boldsymbol{a}) \\hat{O} \\hat{U}(\\boldsymbol{a})|\\psi(\\boldsymbol{x})\\rangle $$\nOn dit alors que l\u0026rsquo;op√©rateur est un invariant et la condition pour qu\u0026rsquo;un op√©rateur $\\hat{O}$ soit un invariant est donc¬†:\n$$ \\hat{U}^{-1}(\\boldsymbol{a}) \\hat{O} \\hat{U}(\\boldsymbol{a})=\\hat{O} $$\nCe qui devient en faisant agir $\\hat{U}$ √† gauche de chaque membre de l\u0026rsquo;√©galit√©¬†:\n$$ [\\hat{O},\\hat{U}]=0 $$\nL\u0026rsquo;invariance par rapport √† une transformation implique la commutation des op√©rateurs.\nEssayons maintenant d\u0026rsquo;obtenir une formule explicite pour l\u0026rsquo;op√©rateur de translation.\n$$ \\psi(x-\\delta a)=\\psi(x)-\\frac{\\mathrm{d} \\psi(x)}{\\mathrm{d} x} \\delta a+\\ldots $$\nCe qu\u0026rsquo;on peut r√©√©crire au premier ordre, en se rappelant que $\\hat{p}=-\\mathrm{i} \\frac{\\mathrm{~d}}{\\mathrm{~d} x}$¬†:\n$$ \\psi(x-\\delta a)=(1-\\mathrm{i} \\hat{p} \\delta a) \\psi(x) $$\nOn dit que l\u0026rsquo;op√©rateur $\\hat{p}$ est le g√©n√©rateur des translations spatiales. Pour op√©rer une translation d\u0026rsquo;une distance $a$, on peut translater de $\\delta a$ un grand nombre $N$ de fois¬†:\n$$ \\begin{aligned} \\psi(x-a) \u0026amp; =\\lim _{N \\rightarrow \\infty}(1- \\mathrm{i} \\hat{p} \\delta a)^N \\psi(x) \\\\ \u0026amp; =\\mathrm{e}^{-\\mathrm{i} \\hat{p} a} \\psi(x) \\end{aligned} $$\nPar identification¬†:\n$$ \\hat{U}(\\boldsymbol{a})=\\mathrm{e}^{-\\mathrm{i} \\hat{\\boldsymbol{p}} \\cdot \\boldsymbol{a}} $$\nAction de l\u0026rsquo;op√©rateur de translation sur un √©tat d\u0026rsquo;impulsion $|\\boldsymbol{q}\\rangle¬†:$\n$$ \\begin{aligned} \\hat{U}(\\boldsymbol{a})|\\boldsymbol{q}\\rangle \u0026amp; =\\mathrm{e}^{-\\mathrm{i} \\hat{p} \\cdot \\boldsymbol{a}}|\\boldsymbol{q}\\rangle \\\\ \u0026amp; =\\mathrm{e}^{-\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{a}}|\\boldsymbol{q}\\rangle \\end{aligned} $$\nProjet√© sur l\u0026rsquo;axe des coordonn√©es, on obtient bien une fonction d\u0026rsquo;onde translat√©e¬†:\n$$ \\langle\\boldsymbol{x}| \\hat{U}(\\boldsymbol{a})|\\boldsymbol{q}\\rangle=\\langle\\boldsymbol{x} | \\boldsymbol{q}\\rangle \\mathrm{e}^{-\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{a}}=\\frac{1}{\\sqrt{\\mathcal{V}}} \\mathrm{e}^{\\mathrm{i} \\boldsymbol{q} \\cdot(\\boldsymbol{x}-\\boldsymbol{a})} $$\nL\u0026rsquo;op√©rateur d\u0026rsquo;√©volution de la section pr√©c√©dente peut aussi √™tre vu comme un op√©rateur de translation temporelle.\nOn peut le reconstruire sur le mod√®le des translations spatiales en partant d\u0026rsquo;une petite variation $\\delta t_a$ dans la fonction d\u0026rsquo;onde¬†:\n$$ \\psi\\left(t-\\delta t_a\\right)=\\psi(t)-\\frac{\\mathrm{d} \\psi(t)}{\\mathrm{d} t} \\delta t_a+\\ldots $$\nEt comme $\\hat{H}=\\mathrm{i} \\frac{\\mathrm{~d}}{\\mathrm{~d} t}$, on obtient¬†:\n$$ \\psi\\left(t-\\delta t_a\\right)=\\left(1+\\mathrm{i} \\hat{H} \\delta t_a\\right) \\psi(t) $$\nCe qui donne finalement¬†:\n$$ \\hat{U}(t_a)=\\mathrm{e}^{\\mathrm{i}\\hat{H}t_a} $$\nOn peut d√®s lors combiner les deux translations en un seul op√©rateur de translation spatio-temporelle en d√©finissant l\u0026rsquo;op√©rateur quadri-impulsion $\\hat{p}=(\\hat{H}, \\hat{\\boldsymbol{p}})$¬†:\n$$ \\hat{U}(a)=\\mathrm{e}^{\\mathrm{i} \\hat{p} \\cdot a}=\\mathrm{e}^{\\mathrm{i} \\hat{H} t_a-\\mathrm{i} \\hat{\\boldsymbol{p}} \\cdot \\boldsymbol{a}} $$\nRotations Une matrice de rotation $\\mathbf{R}(\\boldsymbol{\\theta})$ (o√π $\\boldsymbol{\\theta}$ est le vecteur dont l\u0026rsquo;axe est celui de la rotation et la norme est donn√©e par l\u0026rsquo;angle) agit sur une grandeur vectorielle comme l\u0026rsquo;impulsion¬†: $\\boldsymbol{p}^{\\prime}=\\mathbf{R}(\\boldsymbol{\\theta}) \\boldsymbol{p}$. L\u0026rsquo;op√©rateur vectoriel associ√© peut se d√©finir comme¬†:\n$$ \\left|\\boldsymbol{p}^{\\prime}\\right\\rangle=\\hat{U}(\\boldsymbol{\\theta})|\\boldsymbol{p}\\rangle=|\\mathbf{R}(\\boldsymbol{\\theta}) \\boldsymbol{p}\\rangle $$\nL\u0026rsquo;op√©rateur poss√®de √† nouveau les propri√©t√©s cl√©s attendues pour une telle transformation¬†:\nunitarit√©\u0026nbsp;: $\\hat{U}^{\\dagger}(\\boldsymbol{\\theta}) \\hat{U}(\\boldsymbol{\\theta})=1$ pr√©sence d'un √©l√©ment neutre\u0026nbsp;: $\\hat{U}(0)=1$ loi de composition interne\u0026nbsp;: $\\hat{U}(\\boldsymbol{\\theta}\\_1)\\hat{U}(\\boldsymbol{\\theta}\\_2)=\\hat{U}(\\boldsymbol{\\theta}\\_{12})$ o√π $\\mathbf{R}(\\boldsymbol{\\theta}\\_{12})=\\mathbf{R}(\\boldsymbol{\\theta}\\_1) \\mathbf{R}(\\boldsymbol{\\theta}\\_2)$ Preuve de l'unitarit√©\u0026nbsp;: $$ \\begin{aligned} \\hat{U}(\\boldsymbol{\\theta}) \\hat{U}^{\\dagger}(\\boldsymbol{\\theta}) \u0026amp; =\\hat{U}(\\boldsymbol{\\theta})\\left(\\int \\mathrm{d}^3 p|\\boldsymbol{p}\\rangle\\langle\\boldsymbol{p}|\\right) \\hat{U}^{\\dagger}(\\boldsymbol{\\theta}) \\\\ \u0026amp; =\\int \\mathrm{d}^3 p|\\mathbf{R}(\\boldsymbol{\\theta}) \\boldsymbol{p}\\rangle\\langle\\mathbf{R}(\\boldsymbol{\\theta}) \\boldsymbol{p}|\\\\ \u0026amp;=\\int \\mathrm{d}^3 \\boldsymbol{p}^{\\prime}|\\boldsymbol{p}^{\\prime}\\rangle\\langle\\boldsymbol{p}^{\\prime}|\\\\ \u0026amp;=1 \\end{aligned} $$\nPuisque $\\boldsymbol{p}^{\\prime}=\\mathbf{R}(\\boldsymbol{\\theta}) \\boldsymbol{p}$ et $\\mathrm{d}^3 p^{\\prime}=\\mathrm{d}^3 p$ car $\\operatorname{det} \\mathbf{R}(\\boldsymbol{\\theta})=1$ par conservation de l\u0026rsquo;orientation (et donc le jacobien vaut 1).\nCes op√©rateurs forment un nouveau groupe de Lie appel√© le groupe des rotations.\nTranslations dans l\u0026rsquo;espace-temps, rotations et boosts de Lorentz et leurs combinaisons sont tous des √©l√©ments du groupe de Poincar√© et peuvent tous √™tre repr√©sent√©s par des op√©rateurs unitaires.\nLa rotation, non plus d\u0026rsquo;un √©tat, mais d\u0026rsquo;un op√©rateur, est donn√©e par¬†:\n$$ \\hat{U}^{\\dagger}(\\boldsymbol{\\theta}) \\,\\hat{\\boldsymbol{p}} \\,\\hat{U}(\\boldsymbol{\\theta})=\\mathbf{R}(\\boldsymbol{\\theta}) \\hat{\\boldsymbol{p}} $$\nCherchons l√† encore √† exprimer explicitement l\u0026rsquo;op√©rateur en partant d\u0026rsquo;une petite rotation selon l\u0026rsquo;axe des $z$ d\u0026rsquo;une fonction d\u0026rsquo;onde¬†:\n$$ \\psi\\left(\\theta^z-\\delta \\theta^z\\right)=\\psi\\left(\\theta^z\\right)-\\frac{\\mathrm{d} \\psi\\left(\\theta^z\\right)}{\\mathrm{d} \\theta^z} \\delta \\theta^z+\\ldots $$\nC\u0026rsquo;est maintenant l\u0026rsquo;op√©rateur moment angulaire qui va jouer le r√¥le de g√©n√©rateur de la transformation. Et dans notre cas, on utilise le fait que $\\hat{J}^z=-\\mathrm{i} \\frac{\\mathrm{~d}}{\\mathrm{~d} \\theta^z}$¬†:\n$$ \\psi\\left(\\theta-\\delta \\theta^z\\right)=\\left(1-\\mathrm{i} \\hat{J}^z \\delta \\theta^z\\right) \\psi\\left(\\theta^z\\right) $$\nEt finalement, en r√©p√©tant $N\\rightarrow\\infty$ fois l\u0026rsquo;op√©ration, l\u0026rsquo;op√©rateur s\u0026rsquo;√©crit $\\hat{U}(\\theta^z)=\\mathrm{e}^{-\\mathrm{i} \\hat{J}^z \\theta^z}$. Et en g√©n√©ralisant √† une rotation quelconque¬†:\n$$ \\hat{U}(\\boldsymbol{\\theta})=\\mathrm{e}^{-\\mathrm{i} \\hat{\\boldsymbol{J}} \\cdot \\boldsymbol{\\theta}} $$\nRepr√©sentation des transformations Transformer un champ est un poil plus compliqu√© puisqu\u0026rsquo;il faut √† la fois bouger le champ au nouvel endroit et transformer aussi l\u0026rsquo;objet que le champ produit. Or cet objet peut √™tre de nature diff√©rente¬†: scalaire, vectorielle ou spinorielle. Il faut donc pouvoir adapter une m√™me transformation √† ces diff√©rents objets et c\u0026rsquo;est la th√©orie des repr√©sentations qui permet cela.\nUne repr√©sentation d\u0026rsquo;un groupe, not√©e $D$, est obtenue en associant chaque √©l√©ment $g_i$ du groupe $G$ √† un op√©rateur lin√©aire continu qui agit sur un espace vectoriel. Cette association doit pr√©server la r√®gle de composition¬†: si $g_1\\bullet g_2=g_3$, alors $D(g_1)D(g_2)=D(g_3)$.\nEn pratique, il s\u0026rsquo;agit de repr√©senter les √©l√©ments du groupe par des matrices.\nExemple des rotations\u0026nbsp;: Toute rotation $\\boldsymbol{R}(\\boldsymbol{\\theta})$ peut √™tre repr√©sent√©e par une matrice $D(\\boldsymbol{\\theta})$ qui prend une forme tr√®s similaire √† l\u0026rsquo;op√©rateur¬†:\n$$ D(\\boldsymbol{\\theta})=\\mathrm{e}^{-\\mathrm{i} \\boldsymbol{J} \\cdot \\boldsymbol{\\theta}} $$\n$\\boldsymbol{J}$ est une matrice carr√©e et une repr√©sentation de l\u0026rsquo;op√©rateur $\\hat{\\boldsymbol{J}}$.\nOn peut isoler $J^i$ dans l\u0026rsquo;√©quation pr√©c√©dente¬†:\n$$ J^i=-\\left.\\frac{1}{\\mathrm{i}} \\frac{\\partial D\\left(\\theta^i\\right)}{\\partial \\theta^i}\\right|_{\\theta^i=0} $$\nOn note souvent les repr√©sentations des rotations d\u0026rsquo;un champ caract√©ris√© par un nombre quantique de moment cin√©tique $j$, $D^{(j)}(\\boldsymbol{\\theta})$.\nConsid√©rons des rotations autour de l\u0026rsquo;axe des $z$.\nUne repr√©sentation triviale est $D(\\theta^z)=1$, et donc $J^z=-\\left.\\frac{1}{\\mathrm{i}} \\frac{\\partial D\\left(\\theta^z\\right)}{\\partial \\theta^z}\\right|\\_{\\theta^z=0}=0$. Et par extension, $J^x=J^y=0$. C'est une repr√©sentation appropri√©e pour un champ scalaire puisqu'un scalaire ne peut avoir de moment cin√©tique. On a donc obtenu la repr√©sentation des rotations d'un champ scalaire\u0026nbsp;: $D^{(0}(\\boldsymbol{\\theta})=1$. Pour un champ de spineurs (d√©crivant des particules de spin $\\frac{1}{2}$), la repr√©sentation d'une rotation selon l'axe des $z$ peut s'√©crire\u0026nbsp;: $$ D^{1/2}\\left(\\theta^z\\right)=\\left(\\begin{array}{cc} \\mathrm{e}^{-\\mathrm{i} \\theta^z / 2} \u0026amp; 0 \\\\ 0 \u0026amp; \\mathrm{e}^{\\mathrm{i} \\theta^z / 2} \\end{array}\\right) $$\nEt donc\n$$ J^z=-\\left.\\frac{1}{\\mathrm{i}} \\frac{\\partial D^{1/2}\\left(\\theta^z\\right)}{\\partial \\theta^z}\\right|_{\\theta^z=0}=\\frac{1}{2}\\left(\\begin{array}{cc} 1 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \\end{array}\\right) $$\nPour un champ vectoriel, la repr√©sentation matricielle des rotations est plus famili√®re. Pour une rotation selon l'axe $z$, on a\u0026nbsp;: $$ D^{(1)}(\\theta^z)=\\mathbf{R}\\left(\\theta^z\\right)=\\left(\\begin{array}{cccc} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; \\cos \\theta^z \u0026amp; -\\sin \\theta^z \u0026amp; 0 \\\\ 0 \u0026amp; \\sin \\theta^z \u0026amp; \\cos \\theta^z \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right) $$\nCe qui donne¬†:\n$$ J^z=-\\left.\\frac{1}{\\mathrm{i}} \\frac{\\partial \\mathbf{R}\\left(\\theta^z\\right)}{\\partial \\theta^z}\\right|_{\\theta^z=0}=\\mathrm{i}\\left(\\begin{array}{cccc} 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\end{array}\\right) $$\nLa formule g√©n√©rale pour les √©l√©ments de matrice de $D^{(j)}(\\boldsymbol{\\theta})$ est donn√©e par¬†:\n$$ \\begin{aligned} {\\left[D^{(j)}(\\boldsymbol{\\theta})\\right]_{m, m^{\\prime}} } \u0026amp; =\\left\\langle j m^{\\prime}\\right| \\hat{U}(\\boldsymbol{\\theta})|j m\\rangle \\\\ \u0026amp; =\\left\\langle j m^{\\prime}\\right| \\mathrm{e}^{-\\mathrm{i} \\hat{\\boldsymbol{J}} \\cdot \\boldsymbol{\\theta}}|j m\\rangle \\end{aligned} $$\nOn peut ensuite les relations standards de l\u0026rsquo;op√©rateur moment cin√©tique¬†:\n$$ \\begin{gathered} \\hat{J}^z|j \\, m\\rangle=m|j \\, m\\rangle, \\\\ \\hat{J}^{ \\pm}|j \\, m\\rangle=\\sqrt{(j \\mp m)(j+1 \\pm m)}|j \\,m \\pm 1\\rangle, \\\\ \\hat{J}^{ \\pm}=\\hat{J}^x \\pm \\mathrm{i} \\hat{J}^y . \\end{gathered} $$\nLe point remarquable de toutes ces repr√©sentations est qu\u0026rsquo;elles partagent la m√™me structure alg√©brique sous-jacente de l\u0026rsquo;op√©rateur rotation. Cette alg√®bre est appel√© alg√®bre de Lie et peut appara√Ætre d√®s qu\u0026rsquo;on a un groupe continu.\nLa continuit√© entra√Æne en effet qu\u0026rsquo;il existe des √©l√©ments du groupe arbitrairement proches de l\u0026rsquo;identit√© pour lesquels on peut √©crire $g(\\boldsymbol{\\alpha})=1+\\mathrm{i} \\alpha^i T^i+O(\\alpha^2)$ o√π les $T^i$ sont les g√©n√©rateurs du groupe. L\u0026rsquo;alg√®bre de Lie s\u0026rsquo;exprime alors par le commutateur $\\left[T^i, T^j\\right]=\\mathrm{i} f^{i j k} T^k$ o√π les $f^{ijk}$ sont appel√©es constantes de structure.\nPour les rotations $T^i = J^i$ et $f^{ijk}=\\varepsilon^{ijk}$.\nTransformations d\u0026rsquo;un champ quantique Commen√ßons par translater un champ scalaire.\nSi on translate √† la fois un √©tat et un op√©rateur du m√™me vecteur $\\boldsymbol{a}$, alors rien ne devrait changer¬†:\n$$ \\langle\\boldsymbol{y}| \\hat{\\phi}(\\boldsymbol{x})|\\boldsymbol{y}\\rangle=\\langle\\boldsymbol{y}+\\boldsymbol{a}| \\hat{\\phi}(\\boldsymbol{x}+\\boldsymbol{a})|\\boldsymbol{y}+\\boldsymbol{a}\\rangle $$\nOr $|\\boldsymbol{y}+\\boldsymbol{a}\\rangle=\\hat{U}(\\boldsymbol{a})|\\boldsymbol{y}\\rangle$, donc $\\hat{\\phi}(\\boldsymbol{x}) = \\hat{U}^\\dagger (\\boldsymbol{a}) \\hat{\\phi}(\\boldsymbol{x}+\\boldsymbol{a}) \\hat{U}(\\boldsymbol{a})$ et par cons√©quent¬†:\n$$ \\hat{U}^{\\dagger}(\\boldsymbol{a}) \\hat{\\phi}(\\boldsymbol{x}) \\hat{U}(\\boldsymbol{a})=\\hat{\\phi}(\\boldsymbol{x}-\\boldsymbol{a}) $$\nEn passant aux rotations, on doit se rappeler que transformer le champ agit √† la fois sur le point sur lequel le champ agit et aussi sur la polarisation du champ. Et cette modification de la polarisation du champ se fait via la repr√©sentation appropri√©e de l\u0026rsquo;op√©rateur rotation (pour un champ scalaire, par exemple, $D(\\boldsymbol{\\theta})=1$).\n$$ \\hat{U}^{\\dagger}(\\boldsymbol{\\theta}) \\hat{\\boldsymbol{\\Phi}}(x) \\hat{U}(\\boldsymbol{\\theta})=D(\\boldsymbol{\\theta}) \\hat{\\boldsymbol{\\Phi}}\\left(\\mathbf{R}^{-1}(\\boldsymbol{\\theta}) x\\right) $$\nRemarque¬†: dans le dessin, $D(\\boldsymbol{\\theta}) $ s\u0026rsquo;occupe de tourner la fl√®che rouge √† sa nouvelle place.\nTransformations de Lorentz Apr√®s les translations et les rotations cherchons √† exprimer les boosts de Lorentz.\nUne transformation de Lorentz selon l\u0026rsquo;axe $x$ est donn√©e par $x^{\\prime \\mu}=\\boldsymbol{\\Lambda}\\left(\\beta^1\\right)_{\\;\\nu}^\\mu x^\\nu$ o√π¬†:\n$$ \\boldsymbol{\\Lambda}\\left(\\beta^1\\right)=\\left(\\begin{array}{cccc} \\gamma^1 \u0026amp; \\beta^1 \\gamma^1 \u0026amp; 0 \u0026amp; 0 \\\\ \\beta^1 \\gamma^1 \u0026amp; \\gamma^1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right) $$\nCette transformation connecte deux r√©f√©rentiels inertiels en mouvement relatif avec une vitesse $v=c\\beta^1$ selon $x$. En introduisant la rapidit√© $\\phi^i$ d√©finie par $\\tanh \\phi^i=\\beta^i$, la matrice devient¬†:\n$$ \\boldsymbol{\\Lambda}\\left(\\phi^1\\right)=\\left(\\begin{array}{cccc} \\cosh \\phi^1 \u0026amp; \\sinh \\phi^1 \u0026amp; 0 \u0026amp; 0 \\\\ \\sinh \\phi^1 \u0026amp; \\cosh \\phi^1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right) $$\nIntroduisons un op√©rateur pour cette transformation dans l\u0026rsquo;espace de Hilbert¬†:\n$$ \\hat{U}(\\boldsymbol{\\phi})|\\boldsymbol{p}\\rangle=|\\boldsymbol{\\Lambda}(\\boldsymbol{\\phi}) \\boldsymbol{p}\\rangle $$\nEt sur le mod√®le des rotations, donnons une forme g√©n√©rale matricielle pour les repr√©sentations de la transformation de Lorentz¬†:\n$$ D(\\phi)=\\mathrm{e}^{\\mathrm{i} \\boldsymbol{K} \\cdot \\boldsymbol{\\phi}} $$\nLes g√©n√©rateurs de la transformation sont donn√©s par¬†:\n$$ K^i=\\left.\\frac{1}{\\mathrm{i}} \\frac{\\partial D\\left(\\phi^i\\right)}{\\partial \\phi^i}\\right|_{\\phi^i=0} $$\nUn champ quantique se transforme sous l\u0026rsquo;action d\u0026rsquo;un boost comme¬†:\n$$ \\hat{U}^{\\dagger}(\\boldsymbol{\\phi}) \\hat{\\boldsymbol{\\Phi}}(x) \\hat{U}(\\boldsymbol{\\phi})=D(\\boldsymbol{\\phi}) \\hat{\\boldsymbol{\\Phi}}\\left(\\boldsymbol{\\Lambda}^{-1}(\\boldsymbol{\\phi}) x\\right) $$\nEnfin, en jouant avec un jeu de g√©n√©rateurs, on obtient la surprenante relation de commutation suivante¬†:\n$$ \\left[K^1, K^2\\right]=K^1 K^2-K^2 K^1=-\\mathrm{i} J^3 $$\nLa diff√©rence entre un boost selon les $x$ suivi d\u0026rsquo;un boost selon les $y$ et un boost selon les $y$ suivi d\u0026rsquo;un boost selon les $x$ est une rotation autour des $z$¬†!\nMath√©matiquement, cela implique que l\u0026rsquo;alg√®bre de Lie des transformations de Lorentz n\u0026rsquo;est pas ferm√©e et que ses g√©n√©rateurs ne forment pas un groupe.\nEn g√©n√©ralisant, les relations de commutations s\u0026rsquo;√©crivent¬†:\n$$ \\left[J^i, K^j\\right]=\\mathrm{i} \\varepsilon^{i j k} K^k $$\nPris ensemble, les boosts et les rotations forment une alg√®bre de Lie ferm√©e et c\u0026rsquo;est ce groupe √©largi qu\u0026rsquo;on appelle le groupe de Lorentz. Et de fait, une transformation g√©n√©rale de Lorentz s\u0026rsquo;√©crit¬†:\n$$ D(\\boldsymbol{\\theta}, \\boldsymbol{\\phi})=\\mathrm{e}^{-\\mathrm{i}(\\boldsymbol{J} \\cdot \\boldsymbol{\\theta}-\\boldsymbol{K} \\cdot \\boldsymbol{\\phi})} $$\nnote\nLes transformations $L$ du groupe de Lorentz sont l\u0026rsquo;ensemble des transformations lin√©aires de $\\mathbb{R}^{1,3}$ qui pr√©servent la forme bilin√©aire de Lorentz (ou pseudo-norme de Minkowski)¬†: $(L(x),L(y))=(x,y)$\nEt en ajoutant les translations √† la f√™te, on obtient le groupe de Poincar√©.\nSym√©trie Invariance et conservation Une quantit√© est invariante lorsqu\u0026rsquo;elle garde la m√™me valeur apr√®s une transformation. On dit alors qu\u0026rsquo;elle pr√©sente une certaine sym√©trie. Un cylindre par exemple est invariant par rotation autour de son axe et on dit alors qu\u0026rsquo;il poss√®de une sym√©trie de rotation autour de cet axe.\nUne quantit√© est conserv√©e lorsqu\u0026rsquo;elle garde la m√™me valeur avant et apr√®s un √©v√®nement. Dans une collision entre particules par exemple, la quadri-impulsion est conserv√©e dans un r√©f√©rentiel donn√©.\nCe sont ces deux notions diff√©rentes que le th√©or√®me de Noether lie entre elles en stipulant qu\u0026rsquo;une invariance conduit √† une loi de conservation.\nParam√©trons par une quantit√© $\\lambda$ les variations du champ $\\phi(x^\\mu)$ soumis √† une transformation continue.\nPour une transformation infinit√©simale, on va noter¬†:\n$$ D \\phi=\\left.\\frac{\\partial \\phi}{\\partial \\lambda}\\right|_{\\lambda=0} $$\nde telle fa√ßon que le changement infinit√©simal $\\delta\\phi$ du champ induit par $\\delta\\lambda$ puisse se noter¬†:\n$$ \\delta \\phi=D \\phi \\delta \\lambda $$\nPour une translation d\u0026rsquo;un quadrivecteur $a^\\mu$, par exemple, on peut √©crire¬†:\n$$ \\phi\\left(x^\\mu\\right) \\rightarrow \\phi\\left(x^\\mu+\\lambda a^\\mu\\right) $$\nEt en posant $y^\\mu=x^\\mu+\\lambda a^\\mu$, on obtient¬†:\n$$ \\frac{\\partial \\phi}{\\partial \\lambda}=\\frac{\\partial \\phi}{\\partial y^\\mu} \\frac{\\partial y^\\mu}{\\partial \\lambda}=\\frac{\\partial \\phi}{\\partial y^\\mu} a^\\mu $$\nEt en prenant la limite $\\lambda\\rightarrow 0$, on a finalement¬†:\n$$ D \\phi=a^\\mu \\partial_\\mu \\phi $$\nTh√©or√®me de Noether Consid√©rons une petite variation du champ $\\phi(x)\\rightarrow\\phi(x)+\\delta\\phi(x)$ o√π $\\delta\\phi(x) = 0$ sur les bords de la r√©gion d\u0026rsquo;espace-temps consid√©r√©e (ce sont les conditions aux limites de Dirichlet imposant un champ fixe sur les bords permettant d\u0026rsquo;avoir un principe variationnel bien d√©fini).\nLa petite variation de la densit√© lagrangienne $\\mathcal{L}$ s\u0026rsquo;√©crit¬†:\n$$ \\delta \\mathcal{L}=\\frac{\\partial \\mathcal{L}}{\\partial \\phi} \\delta \\phi+\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_\\mu \\phi\\right)} \\delta\\left(\\partial_\\mu \\phi\\right) $$\nPosons¬†:\n$$ \\Pi^\\mu(x)=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_\\mu \\phi\\right)} $$\n$\\Pi^\\mu(x)$ est la densit√© d\u0026rsquo;impulsion.\nC\u0026rsquo;est une g√©n√©ralisation quadrivectorielle du moment conjugu√© $\\pi(x)=\\delta \\mathcal{L} / \\delta \\dot{\\phi}$. Et d\u0026rsquo;ailleurs, le moment conjugu√© est la composante temporelle de la densit√© d\u0026rsquo;impulsion¬†: $\\Pi^0(x)=\\pi(x)$.\nLa variation de la densit√© lagrangienne peut maintenant s\u0026rsquo;√©crire¬†:\n$$ \\delta \\mathcal{L}=\\frac{\\partial \\mathcal{L}}{\\partial \\phi} \\delta \\phi+\\Pi^\\mu \\delta\\left(\\partial_\\mu \\phi\\right) $$\nEn utilisant $\\delta\\left(\\partial_\\mu \\phi\\right)=\\partial_\\mu(\\delta \\phi)$ et $\\partial_\\mu\\left(\\Pi^\\mu \\delta \\phi\\right)=\\Pi^\\mu \\partial_\\mu(\\delta \\phi)+\\left(\\partial_\\mu \\Pi^\\mu\\right) \\delta \\phi$, on obtient¬†:\n$$ \\delta \\mathcal{L}=\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\phi}-\\partial_\\mu \\Pi^\\mu\\right) \\delta \\phi+\\partial_\\mu\\left(\\Pi^\\mu \\delta \\phi\\right) $$\nSupposons que l\u0026rsquo;action n\u0026rsquo;est pas modifi√©e par la transformation¬†:\n$$ \\begin{aligned} \\delta S \u0026amp;= \\int \\mathrm{d}^4 x\\, \\delta \\mathcal{L}\\\\ \u0026amp;=\\int \\mathrm{d}^4 x\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\phi}-\\partial_\\mu \\Pi^\\mu\\right) \\delta \\phi+\\cancel{\\int \\mathrm{d}^4 x\\, \\partial_\\mu\\left(\\Pi^\\mu \\delta \\phi\\right)}\\\\ \u0026amp;=0 \\end{aligned} $$\nPourquoi $\\int \\mathrm{d}^4 x\\, \\partial_\\mu\\left(\\Pi^\\mu \\delta \\phi\\right)=0$¬†?\nLe th√©or√®me de la divergence (Gauss-Ostrogradski) nous dit que la variation d\u0026rsquo;une quantit√© dans un volume vaut le flux de cette quantit√© √† travers les parois du volume¬†:\n$$ \\int_\\mathcal{V} \\mathrm{d}^4 x\\, \\partial_\\mu\\left(\\Pi^\\mu \\delta \\phi\\right) = \\int_\\mathcal{\\partial V} \\mathrm{d}\\mathcal{A} \\ n_\\mu \\left(\\Pi^\\mu \\delta \\phi\\right) $$\no√π $n_\\mu$ est le vecteur normal √† la surface $\\mathcal{A}$ qui d√©limite le volume d\u0026rsquo;int√©gration.\nOr, comme stipul√©e plus haut, les variations du champ $\\delta\\phi$ s\u0026rsquo;√©vanouissent sur la fronti√®re.\nPar cons√©quent, on obtient¬†:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\phi}=\\partial_\\mu \\Pi^\\mu $$\nC\u0026rsquo;est l\u0026rsquo;√©quation d\u0026rsquo;Euler-Lagrange.\nPartons maintenant du fait que le champ respecte l\u0026rsquo;√©quation du mouvement¬†:\n$$ \\delta\\mathcal{L}=\\cancel{\\left(\\frac{\\partial \\mathcal{L}}{\\partial \\phi}-\\partial_\\mu \\Pi^\\mu\\right) \\delta \\phi}+\\partial_\\mu\\left(\\Pi^\\mu \\delta \\phi\\right) $$\nEn utilisant $\\delta \\phi=D \\phi \\delta \\lambda$, on obtient¬†:\n$$ \\delta \\mathcal{L}=\\partial_\\mu\\left(\\Pi^\\mu D \\phi\\right) \\delta \\lambda $$\nComme la transformation est sens√©e √™tre une sym√©trie du syst√®me, son action doit rest√©e inchang√©e. Cela implique une densit√© lagrangienne inchang√©e √† la quadri-divergence d\u0026rsquo;une fonction $W^\\mu(x)$ pr√®s¬†:\n$$ \\delta \\mathcal{L}=\\left(\\partial_\\mu W^\\mu\\right) \\delta \\lambda $$\nEn effet, l\u0026rsquo;int√©gration sur l\u0026rsquo;espace d\u0026rsquo;une divergence totale va donner une constante. L\u0026rsquo;action sera donc la m√™me √† une constante pr√®s, ce qui sera sans effet sur ses variations (de m√™me qu\u0026rsquo;en m√©canique des particules, le lagrangien est d√©fini √† une d√©riv√©e totale du temps pr√®s).\nFinalement, une action stationnaire implique¬†:\n$$ \\partial_\\mu\\left(\\Pi^\\mu D \\phi-W^\\mu\\right)=0 $$\nOu encore $\\partial_\\mu J_{\\mathrm{N}}^\\mu=0$ o√π\n$$ J_{\\mathrm{N}}^\\mu(x)=\\Pi^\\mu(x) D \\phi(x)-W^\\mu(x) $$\nest le courant de Noether.\nLe courant de Noether est donc conserv√© localement.\nTh√©or√®me de Noether¬†:\nSi une transformation de sym√©trie continue $\\phi\\rightarrow\\phi+D\\phi$ ne change $\\mathcal{L}$ que par l\u0026rsquo;addition d\u0026rsquo;une quadridivergence ($D\\mathcal{L}=\\partial_\\mu W^\\mu$) pour un $\\phi$ arbitraire, alors cela implique l\u0026rsquo;existence d\u0026rsquo;un courant $J_{\\mathrm{N}}^\\mu(x)=\\Pi^\\mu(x) D \\phi(x)-W^\\mu(x)$.\nSi $\\phi$ ob√©it aux √©quations du mouvement, alors le courant est conserv√©¬†: $\\partial_\\mu J_{\\mathrm{N}}^\\mu=0$.\nLes champs conserv√©s donnent naissance √† des charges conserv√©es $Q_{\\mathrm{N}}=\\int J_{\\mathrm{N}}^\\mu \\,\\mathrm{d} \\mathcal{A}_\\mu$ aussi appel√©es charges de Noether.\nEn effet, si on fixe le temps, la \u0026ldquo;surface\u0026rdquo; d\u0026rsquo;int√©gration devient le volume tridimensionnel et donc¬†: $Q_{\\mathrm{N}}=\\int \\mathrm{d}^3 x J_{\\mathrm{N}}^0$ o√π $J^0_N$ est la composante temporelle (normale √† la surface).\nEt comme $\\partial_\\mu J^\\mu_N=0$,\n$$ \\int \\mathrm{d}^3 x\\left(\\partial_\\mu J_{\\mathrm{N}}^\\mu\\right)=\\int \\mathrm{d}^3 x\\left(\\partial_0 J_{\\mathrm{N}}^0+\\partial_k J_{\\mathrm{N}}^k\\right)=0 $$\nEn utilisant le th√©or√®me de la divergence, le second terme devient¬†:\n$$ \\int \\mathrm{d}^3 x \\,\\partial_k J_{\\mathrm{N}}^k=\\int \\mathrm{d} \\mathcal{A}_k J_{\\mathrm{N}}^k $$\nEt il dispara√Æt si le volume est assez grand.\nOn a donc finalement¬†:\n$$ \\int \\mathrm{d}^3 x\\, \\partial_0 J_{\\mathrm{N}}^0=\\frac{\\mathrm{d} Q_{\\mathrm{N}} }{ \\mathrm{d} t} = 0 $$\n√áa correspond bien √† une charge conserv√©e.\nRecette pour trouver des charges conserv√©es √† partir du th√©or√®me de Noether¬†:\nd√©terminer $D \\phi=\\left.\\frac{\\partial \\phi}{\\partial \\lambda}\\right|_{\\lambda \\rightarrow 0}$ d√©terminer $\\Pi^\\mu(x)=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_\\mu \\phi\\right)}$ d√©terminer $\\partial_\\mu W^\\mu=D \\mathcal{L}$ √©crire $J_{\\mathrm{N}}^\\mu=D \\phi \\Pi^\\mu-W^\\mu$ d√©terminer $Q_{\\mathrm{N}}=\\int \\mathrm{d}^3 x J_{\\mathrm{N}}^0$ Application : translations de l\u0026rsquo;espace-temps Supposons une translation de l\u0026rsquo;espace-temps $x^{\\prime \\mu}=x^\\mu+a^\\mu$ qui nous donne $D \\phi=a^\\mu \\partial_\\mu \\phi$. On a aussi $D \\mathcal{L}=a^\\mu \\partial_\\mu \\mathcal{L}=\\partial_\\mu\\left(a^\\mu \\mathcal{L}\\right)$.\nOn reconnait alors que $D \\mathcal{L}=\\partial_\\mu W^\\mu$ avec $W^\\mu=a^\\mu \\mathcal{L}$. Le courant conserv√© s\u0026rsquo;en d√©duit¬†:\n$$ \\begin{aligned} J_{\\mathrm{N}}^\\mu \u0026amp; =\\Pi^\\mu D \\phi-W^\\mu \\\\ \u0026amp; =\\Pi^\\mu a^\\nu \\partial_\\nu \\phi-a^\\mu \\mathcal{L} \\\\ \u0026amp; =a^\\nu\\left[\\Pi^\\mu \\partial_\\nu \\phi-\\delta_\\nu^\\mu \\mathcal{L}\\right] \\\\ \u0026amp; =a_\\nu T^{\\mu \\nu} \\end{aligned} $$\no√π $T^{\\mu \\nu}=\\Pi^\\mu \\partial^\\nu \\phi-g^{\\mu \\nu} \\mathcal{L}$ est le tenseur √©nergie-impulsion.\nLa charge conserv√©e correspondante peut s\u0026rsquo;√©crire¬†:\n$$ P^\\alpha=\\int \\mathrm{d}^3 x \\,T^{0 \\alpha} $$\nLa composante temporelle de cette charge est¬†:\n$$ P^0=\\int \\mathrm{d}^3 x \\,T^{00}=\\int \\mathrm{d}^3 x[\\pi(x) \\dot{\\phi}(x)-\\mathcal{L}(x)]=\\int \\mathrm{d}^3 x\\, \\mathcal{H} $$\nOn reconna√Æt l\u0026rsquo;√©nergie du champ (on a utilis√© $g^{00} = 1$).\nEt les composantes spatiales nous donnent¬†:\n$$ P^k=\\int \\mathrm{d}^3 x\\, T^{0 k}=\\int \\mathrm{d}^3 x\\, \\pi(x) \\partial^k \\phi(x) $$\nOn reconna√Æt l√† l\u0026rsquo;impulsion du champ (on a utilis√© $g^{0k}=0$).\nUn champ sym√©trique par rapport aux translations voit son √©nergie et son impulsion conserv√©es.\nnote\nPetit article de Quantamagazine sur le th√©or√®me de Noether.\nSym√©tries internes Pour des champs plus complexes que des champs scalaires r√©els, en plus des sym√©tries de l\u0026rsquo;espace-temps, il faudra se pr√©occuper de la fa√ßon dont les sym√©tries affecte les champ eux-m√™mes. On verra par exemple un peu plus loin que la sym√©trie $U(1)$ d\u0026rsquo;un champ scalaire complexe entra√Æne la conservation du nombre de particules¬†!\nChapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc4/",
	"title": "TQC-4",
	"tags": [],
	"description": "",
	"content": " Th√©orie quantique des champs \u0026ndash; Partie 4 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Tr√®s souvent une simple traduction.\nRetour sommaire\nLa recette de la quantification canonique des champs La th√©orie quantique des champs nous permet de d√©crire un monde o√π des particules indistinguables peuvent √™tre cr√©√©es ou d√©truites lors d\u0026rsquo;interactions (entre elles ou avec des entit√©s ext√©rieures), un monde fonci√®rement non lin√©aire donc.\nL\u0026rsquo;id√©e est de consid√©rer les particules comme de simples excitations de champs quantiques obtenus en quantifiant leurs alter ego classiques.\nPour obtenir une th√©orie quantique des champs √† partir d\u0026rsquo;une th√©orie des champs classique, on suit la m√©thode suivante¬†:\n√âtape 1\u0026nbsp;: √©crire la densit√© lagrangienne classique en termes de champs. C'est la partie \"cr√©ative\", le reste est algorithmis√©. √âtape 2\u0026nbsp;: calculer la densit√© d'impulsion et d√©terminer la densit√© hamiltonienne en termes de champs. √âtape 3\u0026nbsp;: consid√©rer les champs et la densit√© d'impulsion comme des op√©rateurs et leur appliquer les relations de commutation pour les rendre quantique. √âtape 4\u0026nbsp;: d√©composer les champs en termes d'op√©rateurs de cr√©ation et d'annihilation. √âtape 5\u0026nbsp;: respecter l'ordre normal pour se d√©barrasser des infinis. D√©roulons la recette sur des exemples pour voir la quantification en action.\nQuantification canonique d\u0026rsquo;un champ scalaire r√©el Application de la recette √âtape¬†1¬†: Le Lagrangien¬†:\n$$ \\mathcal{L}=\\frac{1}{2}\\left[\\partial_\\mu \\phi(x)\\right]^2-\\frac{1}{2} m^2[\\phi(x)]^2 $$\n√âtape¬†2¬†: La densit√© d\u0026rsquo;impulsion¬†:\n$$ \\Pi^\\mu(x)=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_\\mu \\phi(x)\\right)} $$\nIci, cela donne $\\Pi^\\mu(x)=\\partial^\\mu \\phi(x)$ avec pour composante temporelle $\\Pi^0(x)=\\pi(x)=\\partial^0 \\phi(x)$ (on utilise la m√©trique $(+- - -)$).\nOn peut maintenant √©crire l\u0026rsquo;Hamiltonien¬†:\n$$ \\begin{aligned} \\mathcal{H}\u0026amp;=\\Pi^0(x) \\partial_0 \\phi(x)-\\mathcal{L}\\\\ \u0026amp;=\\partial^0 \\phi(x) \\partial_0 \\phi(x)-\\mathcal{L} \\end{aligned} $$\nOn obtient¬†:\n$$ \\mathcal{H}=\\frac{1}{2}\\left[\\partial_0 \\phi(x)\\right]^2+\\frac{1}{2}[\\nabla \\phi(x)]^2+\\frac{1}{2} m^2[\\phi(x)]^2 $$\nCet Hamiltonien est tr√®s mignon. On y retrouve la somme d\u0026rsquo;une √©nergie cin√©tique (co√ªt d\u0026rsquo;une variation temporelle du champ) et d\u0026rsquo;une √©nergie potentielle, elle-m√™me d√©compos√©e en deux termes¬†: un terme de gradient (co√ªt d\u0026rsquo;une variation spatiale du champ) et un terme de masse (co√ªt d\u0026rsquo;avoir un champ plut√¥t que rien).\n√âtape¬†3¬†: On promeut les champs en op√©rateurs¬†: $\\phi(x) \\rightarrow \\hat{\\phi}(x)$ et $\\Pi^0(x) \\rightarrow \\hat{\\Pi}^0(x)$.\nEt pour les rendre quantiques, on leur impose des relations de commutation. En m√©canique quantique √† une particule, on a $[\\hat{x}, \\hat{p}]=\\mathrm{i} \\hbar$. Par analogie, on d√©finit le commutateur √† temps √©gaux pour les op√©rateurs champ¬†:\n$$ \\left[\\hat{\\phi}(t, \\boldsymbol{x}), \\hat{\\Pi}^0(t, \\boldsymbol{y})\\right]=\\mathrm{i} \\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) $$\nSi les instants sont diff√©rents, les champs commutent, et on a aussi $[\\hat{\\phi}(x), \\hat{\\phi}(y)]=\\left[\\hat{\\Pi}^0(x), \\hat{\\Pi}^0(y)\\right]=0$.\nExprim√©e √† partir de ces champs, la densit√© hamiltonienne $\\mathcal{H}$ se mue en op√©rateur $\\hat{\\mathcal{H}}$ agissant sur les vecteurs d\u0026rsquo;√©tat.\nMais on ne sait pas encore comment un op√©rateur comme $\\hat{\\phi}(x)$ agit sur un √©tat nombre d\u0026rsquo;occupation $|n_1n_2n_3\\ldots\\rangle$. Par contre, on sait comment les op√©rateurs de cr√©ation et d\u0026rsquo;annihilation, eux, agissent sur ces vecteurs\u0026hellip;\n√âtape¬†4¬†: D√©composons les op√©rateurs de champ en termes d\u0026rsquo;op√©rateurs de cr√©ation et d\u0026rsquo;annihilation.\nRetour au premier chapitre, au moment d\u0026rsquo;√©voquer les oscillateurs coupl√©s¬†: en combinant la d√©composition en modes de Fourier de $x_j$, $x_j=\\frac{1}{\\sqrt{N}} \\sum_k \\tilde{x}_k \\mathrm{e}^{\\mathrm{i}kj a}$ et l\u0026rsquo;√©criture de l\u0026rsquo;op√©rateur correspondant √† un de ces modes en fonction des op√©rateurs de cr√©ation et d\u0026rsquo;annihilation, $\\hat{\\tilde{x}}_k=\\sqrt{\\frac{\\hbar}{2 m \\omega_k}}\\left(\\hat{a}_k+\\hat{a}_{-k}^{\\dagger}\\right)$, on obtient¬†:\n$$ \\hat{x}_j=\\left(\\frac{\\hbar}{m}\\right)^{\\frac{1}{2}} \\sum_k \\frac{1}{\\left(2 \\omega_k N\\right)^{\\frac{1}{2}}}\\left[\\hat{a}_k \\mathrm{e}^{\\mathrm{i} k j a}+\\hat{a}_k^{\\dagger} \\mathrm{e}^{-\\mathrm{i} k j a}\\right] $$\nPar analogie, on peut √©crire la version continue d\u0026rsquo;un op√©rateur champ¬†:\n$$ \\hat{\\phi}(\\boldsymbol{x})=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}\\left(\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{\\mathrm{i} \\boldsymbol{p} \\cdot \\boldsymbol{x}}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{-\\mathrm{i} \\boldsymbol{p} \\cdot \\boldsymbol{x}}\\right) $$\no√π on est pass√© de $\\boldsymbol{k}$ √† $\\boldsymbol{p}$ pour les moments, et de $\\omega_{\\boldsymbol{k}}$ √† $E_\\boldsymbol{p}=\\left(\\boldsymbol{p}^2+m^2\\right)^{\\frac{1}{2}}$ pour l\u0026rsquo;√©nergie.\nEt comme avant, la relation de commutation entre les op√©rateurs de cr√©ation et d\u0026rsquo;annihilation est¬†: $\\left[\\hat{a}_{\\boldsymbol{p}}, \\hat{a}_{\\boldsymbol{q}}^{\\dagger}\\right]=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q})$.\nnote\nL\u0026rsquo;ensemble des quadri-impulsions $p$ satisfaisant la relation de dispersion relativiste $p^2=m^2$ forme ce qu\u0026rsquo;on appelle la \u0026ldquo;coquille de masse\u0026rdquo; (masse shell). C\u0026rsquo;est l\u0026rsquo;√©quivalent dans l\u0026rsquo;espace de Minkowkski de la sph√®re dans l\u0026rsquo;espace euclidien et elle forme un hyperbolo√Øde de r√©volution.\nOn obtient la mesure invariante de Lorentz $ \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}$ en restreignant la mesure naturelle de Lebesgue $ \\frac{\\mathrm{d}^4 p}{(2 \\pi)^{4}}$ √† la coquille de masse.\nIl faudrait maintenant ajouter une d√©pendance temporelle √† notre champ, i.e. le rendre dynamique. Appliquons la m√©thode d\u0026rsquo;Heisenberg¬†:\n$$ \\hat{\\phi}(x)=\\hat{\\phi}(t, \\boldsymbol{x})=\\hat{U}^{\\dagger}(t, 0) \\hat{\\phi}(\\boldsymbol{x}) \\hat{U}(t, 0)=\\mathrm{e}^{\\mathrm{i} \\hat{H} t} \\hat{\\phi}(\\boldsymbol{x}) \\mathrm{e}^{-\\mathrm{i} \\hat{H} t} $$\nSeuls les op√©rateurs de cr√©ation et annihilation sont affect√©s par l\u0026rsquo;op√©rateur d\u0026rsquo;√©volution $\\hat{U}(t, 0)=\\mathrm{e}^{-\\mathrm{i} \\hat{H} t}$ :\n$$ \\begin{aligned} \\hat{U}^{\\dagger}(t, 0) \\hat{a}_{\\boldsymbol{p}} \\hat{U}(t, 0)\u0026amp;=\\mathrm{e}^{-\\mathrm{i} E_p t} \\hat{a}_{\\boldsymbol{p}}\\\\ \\hat{U}^{\\dagger}(t, 0) \\hat{a}^\\dagger_{\\boldsymbol{p}} \\hat{U}(t, 0)\u0026amp;=\\mathrm{e}^{\\mathrm{i} E_p t} \\hat{a}^\\dagger_{\\boldsymbol{p}} \\end{aligned} $$\nPreuve\u0026nbsp;: Convainquons-nous sur un cas simplifi√©¬†:\n$$ \\begin{aligned} \u0026amp; \\mathrm{e}^{\\mathrm{i} \\hat{H} t} \\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{x}} \\mathrm{e}^{-\\mathrm{i} \\hat{H} t}\\left|n_{\\boldsymbol{p}} n_{\\boldsymbol{q}} n_{\\boldsymbol{r}}\\right\\rangle \\\\ = \u0026amp; \\mathrm{e}^{\\mathrm{i} \\hat{H} t} \\hat{a}_{\\boldsymbol{q}}\\left|n_{\\boldsymbol{p}} n_{\\boldsymbol{q}} n_{\\boldsymbol{r}}\\right\\rangle \\mathrm{e}^{\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{x}} \\mathrm{e}^{-\\mathrm{i}\\left(n_{\\boldsymbol{p}} E_{\\boldsymbol{p}}+n_{\\boldsymbol{q}} E_{\\boldsymbol{q}}+n_{\\boldsymbol{r}} E_{\\boldsymbol{r}}\\right) t} \\\\ = \u0026amp; \\sqrt{n_{\\boldsymbol{q}}} \\mathrm{e}^{\\mathrm{i} \\hat{H} t}\\left|n_{\\boldsymbol{p}}\\left(n_{\\boldsymbol{q}}-1\\right) n_{\\boldsymbol{r}}\\right\\rangle \\mathrm{e}^{\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{x}} \\mathrm{e}^{-\\mathrm{i}\\left(n_{\\boldsymbol{p}} E_{\\boldsymbol{p}}+n_{\\boldsymbol{q}} E_{\\boldsymbol{q}}+n_{\\boldsymbol{r}} E_{\\boldsymbol{r}}\\right) t} \\\\ = \u0026amp; \\sqrt{n_{\\boldsymbol{q}}}\\left|n_{\\boldsymbol{p}}\\left(n_{\\boldsymbol{q}}-1\\right) n_{\\boldsymbol{r}}\\right\\rangle \\mathrm{e}^{\\mathrm{i}\\left(n_{\\boldsymbol{p}} E_{\\boldsymbol{p}}+\\left(n_{\\boldsymbol{q}}-1\\right) E_{\\boldsymbol{q}}+n_{\\boldsymbol{r}} E_{\\boldsymbol{r}}\\right) t} \\mathrm{e}^{\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{x}} \\mathrm{e}^{-\\mathrm{i}\\left(n_{\\boldsymbol{p}} E_{\\boldsymbol{p}}+n_{\\boldsymbol{q}} E_{\\boldsymbol{q}}+n_{\\boldsymbol{r}} E_{\\boldsymbol{r}}\\right) t} \\\\ = \u0026amp; \\sqrt{n_{\\boldsymbol{q}}}\\left|n_{\\boldsymbol{p}}\\left(n_{\\boldsymbol{q}}-1\\right) n_{\\boldsymbol{r}}\\right\\rangle \\mathrm{e}^{-\\mathrm{i} E_{\\boldsymbol{q}} t} \\mathrm{e}^{\\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{x}} . \\end{aligned} $$\nAvec $\\hat{a}_\\boldsymbol{q}$ seul, on aurait obtenu $\\sqrt{n_{\\boldsymbol{q}}}\\left|n_{\\boldsymbol{p}}\\left(n_{\\boldsymbol{q}}-1\\right) n_{\\boldsymbol{r}}\\right\\rangle$. Rendre l\u0026rsquo;op√©rateur dynamique a pour effet de multiplier le r√©sultat par un facteur $\\mathrm{e}^{\\mathrm{i}E_\\boldsymbol{q}t}$.\nOn obtient au bout du compte¬†:\n$$ \\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{-\\mathrm{i}\\left(E_{\\boldsymbol{q}} t-\\boldsymbol{q} \\cdot \\boldsymbol{x}\\right)}=\\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{-\\mathrm{i} q \\cdot x} $$\nAu final, la d√©composition en modes du champ scalaire est donn√©e par¬†:\n$$ \\hat{\\phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}\\left(\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\navec $E_{\\boldsymbol{p}}=+\\left(\\boldsymbol{p}^2+m^2\\right)^{\\frac{1}{2}}$\nLa d√©composition du champ position nous offre en prime celle du champ impulsion puisque $\\Pi^\\mu(x)=\\partial^\\mu \\phi(x)$.\nCela va permettre de valider √† posteriori les facteurs de normalisation\u0026hellip;\nL\u0026rsquo;intensit√© d\u0026rsquo;impulsion est donn√©e par¬†:\n$$ \\hat{\\Pi}^\\mu(x)=\\partial^\\mu \\hat{\\phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}\\left(2 E_\\boldsymbol{p}\\right)^{\\frac{1}{2}}}\\left(-\\mathrm{i} p^\\mu\\right)\\left(\\hat{a}_\\boldsymbol{p} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}-\\hat{a}_\\boldsymbol{p}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nEt sa composante temporelle vaut¬†:\n$$ \\hat{\\Pi}^0(x) =\\partial^0 \\hat{\\phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_\\boldsymbol{p}\\right)^{\\frac{1}{2}}}\\left(-\\mathrm{i}E_{\\boldsymbol{p}}\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\mathrm{i}E_{\\boldsymbol{p}}\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nLe commutateur donne alors¬†:\n$$ \\begin{aligned} \\left[\\hat{\\phi}(x),\\hat{\\Pi}^0(x)\\right] \u0026amp;= \\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{\\mathrm{d}^3 q}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(4 E_{\\boldsymbol{p}}E_{\\boldsymbol{q}}\\right)^\\frac{1}{2}}\\left[\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}, -\\mathrm{i}E_{\\boldsymbol{q}}\\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{-\\mathrm{i} q \\cdot y}+\\mathrm{i}E_{\\boldsymbol{q}}\\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} q \\cdot y}\\right]\\\\ \u0026amp;=\\int\\frac{\\mathrm{d}^3 p\\; \\mathrm{d}^3 q}{2(2 \\pi)^{3}\\sqrt{ E_{\\boldsymbol{p}}E_{\\boldsymbol{q}}}}\\left(\\mathrm{i}E_{\\boldsymbol{q}}[\\hat{a}_{\\boldsymbol{p}},\\hat{a}^\\dagger_{\\boldsymbol{q}}]\\mathrm{e}^{-\\mathrm{i} p \\cdot x + \\mathrm{i} q \\cdot y} - \\mathrm{i}E_{\\boldsymbol{q}}[\\hat{a}^\\dagger_{\\boldsymbol{p}},\\hat{a}_{\\boldsymbol{q}}]\\mathrm{e}^{\\mathrm{i} p \\cdot x - \\mathrm{i} q \\cdot y}\\right) \\end{aligned} $$\nS√©parons composantes spatiales et temporelles (en prenant des temps √©gaux pour les deux champs) et utilisons$\\left[\\hat{a}_{\\boldsymbol{p}}, \\hat{a}_{\\boldsymbol{q}}^{\\dagger}\\right]=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q})$¬†:\n$$ \\begin{aligned} \\left[\\hat{\\phi}(\\boldsymbol{x},t),\\hat{\\Pi}^0(\\boldsymbol{y},t)\\right] \u0026amp;=i\\int\\frac{\\mathrm{d}^3 p\\; \\mathrm{d}^3 q}{2(2 \\pi)^{3}\\sqrt{ E_{\\boldsymbol{p}}E_{\\boldsymbol{q}}}}E_{\\boldsymbol{q}}\\,\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q}) \\mathrm{e}^{\\mathrm{i}t(E_\\boldsymbol{q}-E_\\boldsymbol{q})+\\mathrm{i} \\boldsymbol{p} \\cdot \\boldsymbol{x} - \\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{y}} - i\\int\\frac{\\mathrm{d}^3 p\\; \\mathrm{d}^3 q}{2(2 \\pi)^{3}\\sqrt{ E_{\\boldsymbol{p}}E_{\\boldsymbol{q}}}}E_{\\boldsymbol{q}}(-\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q})) \\mathrm{e}^{\\mathrm{i}t(E_\\boldsymbol{p}-E_\\boldsymbol{q})-\\mathrm{i} \\boldsymbol{p} \\cdot \\boldsymbol{x} + \\mathrm{i} \\boldsymbol{q} \\cdot \\boldsymbol{y}}\\\\ \u0026amp;=\\frac{i}{2}\\int\\frac{\\mathrm{d}^3 p } {(2 \\pi)^{3}}\\mathrm{e}^{\\mathrm{i} \\boldsymbol{p} \\cdot (\\boldsymbol{x}-\\boldsymbol{y})} +\\frac{i}{2}\\int\\frac{\\mathrm{d}^3 p } {(2 \\pi)^{3}}\\mathrm{e}^{\\mathrm{i} \\boldsymbol{p} \\cdot (\\boldsymbol{y}-\\boldsymbol{x})}\\\\ \u0026amp;=i\\delta(\\boldsymbol{x}-\\boldsymbol{y}) \\end{aligned} $$\nLes facteurs de normalisation dans la formule de d√©composition du champ $\\hat{\\phi}(x)$ permettent donc de retrouver la relation de commutation √† temps √©gaux attendue¬†!\n√ânergie infinie¬†? Au tour de l\u0026rsquo;Hamiltonien de subir la quantification. Il va suffire d\u0026rsquo;y substituer la d√©composition en modes de $\\hat{\\phi}(x)$.\nL\u0026rsquo;Hamiltonien est donn√© par l\u0026rsquo;int√©grale sur le volume de la densit√© hamiltonienne¬†:\n$$ \\hat{H}=\\int \\mathrm{d}^3 x \\frac{1}{2}\\left\\{\\left[\\partial_0 \\hat{\\phi}(x)\\right]^2+[\\boldsymbol{\\nabla} \\hat{\\phi}(x)]^2+m^2[\\hat{\\phi}(x)]^2\\right\\} $$\nRenotons la densit√© d\u0026rsquo;impulsion¬†:\n$$ \\hat{\\Pi}_\\mu(x)=\\partial_\\mu \\hat{\\phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}\\left(2 E_\\boldsymbol{p}\\right)^{\\frac{1}{2}}}\\left(-\\mathrm{i} p_\\mu\\right)\\left(\\hat{a}_\\boldsymbol{p} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}-\\hat{a}_\\boldsymbol{p}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nEt sa composante temporelle vaut¬†:\n$$ \\partial_0 \\hat{\\phi}(x)=\\int \\frac{d^3 p}{(2 \\pi)^{\\frac{3}{2}}\\left(2 E_\\boldsymbol{p}\\right)^{\\frac{1}{2}}}\\left(-\\mathrm{i} E_\\boldsymbol{p}\\right)\\left(\\hat{a}_\\boldsymbol{p} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}-\\hat{a}_\\boldsymbol{p}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\n√âcrivons maintenant sa composante spatiale¬†:\n$$ \\boldsymbol{\\nabla} \\hat{\\phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}(\\mathrm{i} \\boldsymbol{p})\\left(\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}-\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nOn a ainsi tous les ingr√©dients pour calculer l\u0026rsquo;Hamiltonien¬†:\n$$ \\hat{H}= \\frac{1}{2} \\int^{} \\frac{\\mathrm{~d}^3 x \\mathrm{~d}^3 p \\mathrm{~d}^3 q}{(2 \\pi)^3\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}\\left(2 E_{\\boldsymbol{q}}\\right)^{\\frac{1}{2}}} \\left[ (-E_{\\boldsymbol{p}} E_{\\boldsymbol{q}}-\\boldsymbol{p} \\cdot \\boldsymbol{q})[\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}-\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}] \\times [\\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{-\\mathrm{i} q \\cdot x}-\\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} q \\cdot x}]+m^2 [\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}] [\\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{-\\mathrm{i} q \\cdot x}+\\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} q \\cdot x}] \\right] $$\nOn commence par l\u0026rsquo;int√©grations sur les $x$ en utilisant $\\int \\mathrm{d}^3 x \\mathrm{e}^{\\mathrm{i} \\boldsymbol{p} \\cdot \\boldsymbol{x}}=(2 \\pi)^3 \\delta^{(3)}(\\boldsymbol{p})$¬†:\n$$ \\hat{H}= \\frac{1}{2} \\int \\frac{\\mathrm{~d}^3 p \\mathrm{~d}^3 q}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}\\left(2 E_{\\boldsymbol{q}}\\right)^{\\frac{1}{2}}} \\times\\left[\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q}) (E_{\\boldsymbol{p}} E_{\\boldsymbol{q}}+\\boldsymbol{p} \\cdot \\boldsymbol{q}+m^2) [\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{\\mathrm{i}(E_{\\boldsymbol{p}}-E_{\\boldsymbol{q}}) t}+\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\mathrm{e}^{-\\mathrm{i}(E_{\\boldsymbol{p}}-E_{\\boldsymbol{q}}) t}] +\\delta^{(3)}(\\boldsymbol{p}+\\boldsymbol{q})(-E_{\\boldsymbol{p}} E_{\\boldsymbol{q}}-\\boldsymbol{p} \\cdot \\boldsymbol{q}+m^2)[\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\mathrm{e}^{\\mathrm{i}(E_{\\boldsymbol{p}}+E_{\\boldsymbol{q}}) t}+\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{q}} \\mathrm{e}^{-\\mathrm{i}(E_{\\boldsymbol{p}}+E_{\\boldsymbol{q}}) t}]\\right] $$\nEnsuite, l\u0026rsquo;int√©grale sur les $q$ permet de se d√©barrasser des distributions de Dirac¬†:\n$$ \\hat{H}= \\frac{1}{2} \\int \\mathrm{~d}^3 p \\frac{1}{2 E_{\\boldsymbol{p}}}\\left[(E_{\\boldsymbol{p}}^2+\\boldsymbol{p}^2+m^2) (\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}+\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{p}}^{\\dagger}) + (-E_{\\boldsymbol{p}}^2+\\boldsymbol{p}^2+m^2) (\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{-\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{2 \\mathrm{i} E_{\\boldsymbol{p}} t}+\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{-\\boldsymbol{p}} \\mathrm{e}^{-2 \\mathrm{i} E_{\\boldsymbol{p}} t})\\right] $$\nEt puisque $E_{\\boldsymbol{p}}^2=\\boldsymbol{p}^2+m^2$, cela se simplifie en¬†:\n$$ \\hat{H}=\\frac{1}{2} \\int \\mathrm{~d}^3 p\\, E_{\\boldsymbol{p}}\\left(\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{p}}^{\\dagger}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}\\right) $$\nOn termine en utilisant $\\left[\\hat{a}_{\\boldsymbol{p}}, \\hat{a}_{\\boldsymbol{q}}^{\\dagger}\\right]=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q})$.\nOn obtient¬†:\n$$ \\hat{H}=\\int \\mathrm{d}^3 p \\,E_{\\boldsymbol{p}}\\left(\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}+\\frac{1}{2} \\delta^{(3)}(0)\\right) $$\nLe terme $\\langle0|\\hat{H}|0\\rangle = \\frac{1}{2} \\int\\!\\! \\mathrm{~d}^3 p \\, \\delta^3(0)$ donne une √©nergie infinie pour le vide üò± Mais ce n\u0026rsquo;est pas si alarmant si on se convainc qu\u0026rsquo;en pratique, seul le mesurable nous int√©resse. Or on ne mesure que des diff√©rences d\u0026rsquo;√©nergie, et ces diff√©rences auront √©videmment le bon go√ªt de faire dispara√Ætre les infinis (en annulant les vilains $\\frac{1}{2} \\delta^{(3)}(0)$). L\u0026rsquo;infini obtenu ne correspondrait finalement qu\u0026rsquo;√† une mauvaise d√©finition de l\u0026rsquo;√©nergie du niveau z√©ro.\nMalgr√© tout, ces infinis qui tra√Ænent partout, √ßa fait d√©sordre. En ordonnant savamment les op√©rateurs, on va pouvoir les glisser discr√®tement sous le tapis.\nnote\nLe terme constant devient par contre un gros (!) probl√®me lorsqu\u0026rsquo;on essaye de r√©concilier th√©orie quantique des champs et relativit√© g√©n√©rale o√π ce ne sont plus les diff√©rences d\u0026rsquo;√©nergie qui importent mais directement la densit√© d\u0026rsquo;√©nergie-impulsion.\nC\u0026rsquo;est le \u0026ldquo;probl√®me de la constante cosmologique\u0026quot;¬†: la densit√© d\u0026rsquo;√©nergie du vide pr√©vue par la TQC est $10^{120}$ üòµ‚Äçüí´ ordres de grandeur trop grands par rapport √† la valeur mesur√©e (facile la pire pr√©diction jamais faite en physique)\u0026hellip;\nOrdre normal L\u0026rsquo;ordre normal consiste simplement √† placer tous les op√©rateurs de cr√©ation √† gauche.\nC\u0026rsquo;est sans douleur pour les champs de Bose, mais pour ceux de Fermi, on doit multiplier par un terme $(-1)^P$ o√π $P$ est le nombre de permutations n√©cessaires pour obtenir l\u0026rsquo;ordre normal.\nExemples¬†:\n$\\color{#D41876 }N\\left[\\color{#000 }\\hat{a} \\hat{a}^{\\dagger}\\color{#D41876 }\\right]\\color{#000 }=\\hat{a}^{\\dagger} \\hat{a}$, $\\color{#D41876 }N\\left[\\color{#000 }\\hat{a}^{\\dagger} \\hat{a}\\color{#D41876 }\\right]\\color{#000 }=\\hat{a}^{\\dagger} \\hat{a}$, $\\color{#D41876 }N\\left[\\color{#000 }\\hat{a}^{\\dagger} \\hat{a} \\hat{a} \\hat{a}^{\\dagger} \\hat{a}^{\\dagger}\\color{#D41876 }\\right]\\color{#000 }=\\hat{a}^{\\dagger} \\hat{a}^{\\dagger} \\hat{a}^{\\dagger} \\hat{a} \\hat{a}$, $\\color{#D41876 }N[\\color{#000 }\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\hat{a}_{\\boldsymbol{r}}\\color{#D41876 }]\\color{#000 }=\\hat{a}_{\\boldsymbol{q}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{r}}$, $\\color{#D41876 }N[\\color{#000 }\\hat{c}_{\\boldsymbol{p}} \\hat{c}_{\\boldsymbol{q}}^{\\dagger} \\hat{c}_r\\color{#D41876 }]\\color{#000 }=-\\hat{c}_{\\boldsymbol{q}}^{\\dagger} \\hat{c}_{\\boldsymbol{p}} \\hat{c}_r$.\n√âtape 5¬†: On arrive finalement au bout du programme en mettant dans l\u0026rsquo;ordre normal les op√©rateurs dans l\u0026rsquo;Hamiltonien¬†:\n$$ \\begin{aligned} \\color{#D41876 }N[\\color{#000 }\\hat{H}\\color{#D41876 }] \\color{#000 }\u0026amp; =\\frac{1}{2} \\int \\mathrm{d}^3 p E_{\\boldsymbol{p}} \\,\\color{#D41876 }N\\left[\\color{#000 }\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{p}}^{\\dagger}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}\\color{#D41876 }\\right] \\\\ \u0026amp; =\\frac{1}{2} \\int \\mathrm{d}^3 p E_{\\boldsymbol{p}} 2 \\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}} \\end{aligned} $$\nD\u0026rsquo;o√π¬†:\n$$ \\color{#D41876 }N[\\color{#000 }\\hat{H}\\color{#D41876 }] \\color{#000 }=\\int \\mathrm{d}^3 p E_{\\boldsymbol{p}} \\hat{n}_{\\boldsymbol{p}} $$\n$\\hat{n}_{\\boldsymbol{p}}=\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}$ est l\u0026rsquo;op√©rateur nombre. $\\hat{n}_{\\boldsymbol{p}}|\\boldsymbol{p}\\rangle$ nous dit combien il y a d\u0026rsquo;excitations dans l\u0026rsquo;√©tat √† impulsion $\\boldsymbol{p}$.\nLe niveau z√©ro (le vide) a maintenant une √©nergie bien mieux d√©finie¬†:\n$$ \\langle0|N[\\hat{H}]|0\\rangle = 0 $$\nOn retrouve le m√™me Hamiltonien que pour des particules ind√©pendantes¬†! Les √©tats d\u0026rsquo;excitation de l\u0026rsquo;√©quation d\u0026rsquo;onde peuvent √™tre vues comme des particules poss√©dant une impulsion quantifi√©e. Ce sont des bosons avec un spin $S=0$.\nSignification de la d√©composition en modes G√©n√©ralisons un poil la d√©composition en modes de l\u0026rsquo;op√©rateur champ en changeant $\\hat{a}_{\\boldsymbol{p}}^{\\dagger}$ en $\\hat{b}_{\\boldsymbol{p}}^{\\dagger}$¬†:\n$$ \\hat{\\phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}\\left(\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nPour coller √† l\u0026rsquo;interpr√©tation de Feynman des √©nergies n√©gatives\n$$ \\phi(x) = \\sum_\\boldsymbol{p}\\left[ \\begin{array}{c} \\text{annihilation d\u0026rsquo;une particule} \\\\ \\text{incidente d\u0026rsquo;√©nergie positive }E_\\boldsymbol{p} \\\\ \\end{array} \\right] + \\sum_\\boldsymbol{p}\\left[ \\begin{array}{c} \\text{cr√©ation d\u0026rsquo;une antiparticule} \\\\ \\text{sortante d\u0026rsquo;√©nergie positive }E_\\boldsymbol{p}\\\\ \\end{array} \\right]$$\nil faut que $\\hat{a}_{\\boldsymbol{p}}$ annihile les particules et $\\hat{a}_{\\boldsymbol{p}}^{\\dagger}$ les cr√©e, alors que $\\hat{b}_{\\boldsymbol{p}}$ doit annihiler les antiparticules et $\\hat{b}_{\\boldsymbol{p}}^{\\dagger}$ les cr√©er. Et l\u0026rsquo;√©nergie des particules et antiparticules vaut $E_{\\boldsymbol{p}}=+\\left(\\boldsymbol{p}^2+m^2\\right)^{\\frac{1}{2}}$.\nDans le cas du champ scalaire, chaque particule est sa propre antiparticule. Et donc $\\hat{b}_{\\boldsymbol{p}}^{\\dagger}=\\hat{a}_{\\boldsymbol{p}}^{\\dagger}$.\nRegardons ce qu\u0026rsquo;il advient lorsqu\u0026rsquo;on fait agir l\u0026rsquo;op√©rateur champ sur le vide.\nComme $\\hat{a}_{\\boldsymbol{p}}^{\\dagger}|0\\rangle=|\\boldsymbol{p}\\rangle$, on a¬†:\n$$ \\hat{\\phi}(x)|0\\rangle=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}} \\mathrm{e}^{\\mathrm{i} p \\cdot x}|\\boldsymbol{p}\\rangle $$\nOn obtient une superposition de modes sortants. Cherchons l\u0026rsquo;amplitude correspondant √† un de ces √©tats $\\langle q|=(2 \\pi)^{\\frac{3}{2}}\\left(2 E_{\\boldsymbol{q}}\\right)^{\\frac{1}{2}}\\langle\\boldsymbol{q}|$ correctement normalis√©¬†:\n$$ (2 \\pi)^{\\frac{3}{2}}\\left(2 E_{\\boldsymbol{q}}\\right)^{\\frac{1}{2}}\\langle\\boldsymbol{q}| \\hat{\\phi}(x)|0\\rangle=\\int \\mathrm{d}^3 p\\, \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\langle\\boldsymbol{q} \\mid \\boldsymbol{p}\\rangle=\\int \\mathrm{d}^3 p\\, \\mathrm{e}^{\\mathrm{i} p \\cdot x} \\delta^{(3)}(\\boldsymbol{q}-\\boldsymbol{p})=\\mathrm{e}^{\\mathrm{i}\\left(E_{\\boldsymbol{q}} t-\\boldsymbol{q} \\cdot \\boldsymbol{x}\\right)}=\\mathrm{e}^{\\mathrm{i} q \\cdot x} $$\n$\\mathrm{e}^{\\mathrm{i} q \\cdot x}$ est ainsi l\u0026rsquo;amplitude dans le $q$e mode pour une particule scalaire cr√©√©e au point $x$ de l\u0026rsquo;espace-temps.\nLa recette de quantification canonique ne fonctionne que sur des Lagrangiens de th√©ories sans interaction (des champs libres) car la possibilit√© de leur diagonalisation (la d√©composition en modes d\u0026rsquo;impulsion est bien, de fait, une diagonalisation) repose sur l\u0026rsquo;√©criture du Lagrangien en termes quadratiques des champs et de leurs d√©riv√©es et les couplages viennent mettre le bazar dans ces jolies √©critures.\nQuantification canonique d\u0026rsquo;un champ scalaire complexe Application de la recette √âtape 1¬†: Le Lagrangien d\u0026rsquo;un champ scalaire complexe poss√®de deux composantes¬†:\n$$ \\mathcal{L}= \\frac{1}{2}\\left[\\partial_\\mu \\phi_1(x)\\right]^2-\\frac{1}{2} m^2\\left[\\phi_1(x)\\right]^2 +\\frac{1}{2}\\left[\\partial_\\mu \\phi_2(x)\\right]^2-\\frac{1}{2} m^2\\left[\\phi_2(x)\\right]^2 $$\nEn posant\n$$ \\psi=\\frac{1}{\\sqrt{2}}\\left[\\phi_1(x)+\\mathrm{i} \\phi_2(x)\\right] \\qquad \\psi^{\\dagger}=\\frac{1}{\\sqrt{2}}\\left[\\phi_1(x)-\\mathrm{i} \\phi_2(x)\\right] $$\non obtient quasiment le Lagrangien du champ scalaire r√©el, mais sans le facteur $\\frac{1}{2}$¬†:\n$$ \\mathcal{L}=\\partial^\\mu \\psi^{\\dagger}(x) \\partial_\\mu \\psi(x)-m^2 \\psi^{\\dagger}(x) \\psi(x) $$\n√âtape 2¬†: Chaque composante $\\sigma$ du champ ($\\sigma=\\psi$ ou $\\psi^\\dagger$) a une densit√© d\u0026rsquo;impulsion diff√©rente¬†:\n$$ \\Pi_{\\sigma=\\psi}^0=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_0 \\psi\\right)}=\\partial^0 \\psi^{\\dagger} \\qquad \\Pi_{\\sigma=\\psi^{\\dagger}}^0=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_0 \\psi^{\\dagger}\\right)}=\\partial^0 \\psi $$\nD√©terminons l\u0026rsquo;Hamiltonien¬†:\n$$ \\mathcal{H} =\\sum_\\sigma \\Pi_\\sigma^0(x) \\partial_0 \\psi^\\sigma(x)-\\mathcal{L} $$\nOn obtient apr√®s simplification¬†:\n$$ \\mathcal{H}=\\partial_0 \\psi^{\\dagger}(x) \\partial_0 \\psi(x)+\\nabla \\psi^{\\dagger}(x) \\cdot \\nabla \\psi(x)+m^2 \\psi^{\\dagger}(x) \\psi(x) $$\n√âtape 3¬†: On promeut les champs au rang d\u0026rsquo;op√©rateurs quantiques et on impose les relations de commutation √† temps √©gaux¬†:\n$$ \\left[\\hat{\\psi}(t, \\boldsymbol{x}), \\hat{\\Pi}_\\psi^0(t, \\boldsymbol{y})\\right]=\\left[\\hat{\\psi}^{\\dagger}(t, \\boldsymbol{x}), \\hat{\\Pi}_{\\psi^{\\dagger}}^0(t, \\boldsymbol{y})\\right]=\\mathrm{i} \\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) $$\nTous les autres commutateurs sont nuls.\n√âtape 4¬†: On d√©compose les champs en modes¬†:\n$$ \\begin{aligned} \\hat{\\psi}(x) \u0026amp; =\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}\\left(\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) \\\\ \\hat{\\psi}^{\\dagger}(x) \u0026amp; =\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}\\left(\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}+\\hat{b}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}\\right) \\end{aligned} $$\navec $E_\\boldsymbol{p}=+\\left(\\boldsymbol{p}^2+m^2\\right)^{\\frac{1}{2}}$. Les op√©rateurs $\\hat{a}_{\\boldsymbol{p}}$ et $\\hat{b}_{\\boldsymbol{p}}$ annihilent deux types diff√©rents de particules. Ils satisfont les relations de commutation $\\left[\\hat{a}_{\\boldsymbol{p}}, \\hat{a}_{\\boldsymbol{q}}^{\\dagger}\\right]=\\left[\\hat{b}_{\\boldsymbol{p}}, \\hat{b}_{\\boldsymbol{q}}^{\\dagger}\\right]=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q})$ et toute autre combinaison est nulle.\n√âtape 5¬†: On substitue les relations de commutation dans l\u0026rsquo;Hamiltonien et on ordonne les op√©rateurs¬†:\n$$ \\begin{aligned} N[\\hat{H}] \u0026amp; =\\int \\mathrm{d}^3 p E_{\\boldsymbol{p}}\\left(\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}+\\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\hat{b}_{\\boldsymbol{p}}\\right) \\\\ \u0026amp; =\\int \\mathrm{d}^3 p E_{\\boldsymbol{p}}\\left(\\hat{n}_{\\boldsymbol{p}}^{(a)}+\\hat{n}_{\\boldsymbol{p}}^{(b)}\\right), \\end{aligned} $$\no√π $\\hat{n}_{\\boldsymbol{p}}^{(a)}$ compte les particules $a$ avec une impulsion $\\boldsymbol{p}$ et $\\hat{n}_{\\boldsymbol{p}}^{(b)}$ compte les particules $b$ avec une impulsion $\\boldsymbol{p}$.\nLes particules $a$ et $b$ ayant la m√™me √©nergie $E_\\boldsymbol{p}$, on les interpr√®te respectivement comme des particules et antiparticules.\n$\\hat{\\psi}(x)$ s\u0026rsquo;interpr√®te alors comme une somme sur les impulsions d\u0026rsquo;op√©rateurs qui annihilent des particules ($\\hat{a}_{\\boldsymbol{p}}$) et d\u0026rsquo;op√©rateurs qui cr√©ent des antiparticules ($\\hat{b}^\\dagger_{\\boldsymbol{p}}$).\nCourant de Noether associ√© Le champ scalaire complexe poss√®de une sym√©trie interne $U(1)$ puisque les transformations globales suivantes n\u0026rsquo;ont pas d\u0026rsquo;effet sur le Lagrangien¬†:\n$$ \\psi \\rightarrow \\mathrm{e}^{\\mathrm{i} \\alpha} \\psi, \\quad \\psi^{\\dagger} \\rightarrow \\mathrm{e}^{-\\mathrm{i} \\alpha} \\psi^{\\dagger} $$\nPour d√©terminer le courant associ√© gr√¢ce au th√©or√®me de Noether, on passe √† des transformations infinit√©simales¬†:\n$$ \\begin{array}{cc} \\psi \\rightarrow \\psi+\\mathrm{i} \\psi \\delta \\alpha, \u0026amp; D \\psi=+\\mathrm{i} \\psi, \\\\ \\psi^{\\dagger} \\rightarrow \\psi^{\\dagger}-\\mathrm{i} \\psi^{\\dagger} \\delta \\alpha, \u0026amp; D \\psi^{\\dagger}=-\\mathrm{i} \\psi^{\\dagger} \\end{array} $$\n$$ \\mathcal{L} \\rightarrow \\partial^\\mu( \\psi^{\\dagger}-\\mathrm{i} \\psi^{\\dagger} \\delta \\alpha)\\partial_\\mu(\\psi+\\mathrm{i} \\psi \\delta \\alpha) - m^2( \\psi^{\\dagger}-\\mathrm{i} \\psi^{\\dagger} \\delta \\alpha)(\\psi+\\mathrm{i} \\psi \\delta \\alpha) $$\ndonne $\\mathcal{L} \\rightarrow \\mathcal{L} + \\delta a^2\\mathcal{L}$ et on a donc bien $D\\mathcal{L}=0$.\nEt comme $D\\mathcal L=\\partial_\\mu W^\\mu$, cela implique que $W^\\mu = 0$ (c\u0026rsquo;est le cas pour toutes les sym√©tries internes !).\nLe courant de Noether est alors donn√© par $J_{\\mathrm{N}}^\\mu=\\sum_\\sigma \\Pi_\\sigma^\\mu D \\sigma$ (avec $\\sigma=\\psi$ ou $\\psi^\\dagger$)¬†:\n$$ \\begin{aligned} J_{\\mathrm{N}}^\\mu \u0026amp; =\\sum_\\sigma \\Pi_\\sigma^\\mu D \\sigma=\\Pi_\\psi^\\mu D \\psi+\\Pi_{\\psi^{\\dagger}}^\\mu D \\psi^{\\dagger} \\\\ \u0026amp; =\\mathrm{i}\\left[\\left(\\partial^\\mu \\psi^{\\dagger}\\right) \\psi-\\left(\\partial^\\mu \\psi\\right) \\psi^{\\dagger}\\right] \\end{aligned} $$\nLe courant devient op√©rateur en utilisant les op√©rateurs champ $\\hat{\\psi}$ et $\\hat{\\psi}^\\dagger$.\nL\u0026rsquo;op√©rateur charge conserv√©e est donn√©e par¬†:\n$$ \\hat{Q}_{\\mathrm{N}}=\\int \\mathrm{d}^3 x\\, \\hat{J}_{\\mathrm{N}}^0=\\int \\mathrm{d}^3 x\\,\\mathrm{i}\\left[\\left(\\partial^0 \\hat{\\psi}^{\\dagger}\\right) \\hat{\\psi}-\\left(\\partial^0 \\hat{\\psi}\\right) \\hat{\\psi}^{\\dagger}\\right] $$\nApr√®s injection des d√©compositions en modes des champs, on obtient¬†:\n$$ \\hat{Q}_{\\mathrm{N}}=\\frac{1}{2} \\int \\mathrm{~d}^3 p\\left(-\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}+\\hat{b}_{\\boldsymbol{p}} \\hat{b}_{\\boldsymbol{p}}^{\\dagger}-\\hat{a}_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{p}}^{\\dagger}+\\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\hat{b}_{\\boldsymbol{p}}\\right) $$\nOn ordonne ce petit monde¬†:\n$$ \\begin{aligned} N\\left[\\hat{Q}_{\\mathrm{N}}\\right]\u0026amp;=\\int \\mathrm{d}^3 p\\left(\\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\hat{b}_{\\boldsymbol{p}}-\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}\\right)\\\\ \u0026amp;=\\int \\mathrm{d}^3 p\\left(\\hat{n}_{\\boldsymbol{p}}^{(b)}-\\hat{n}_{\\boldsymbol{p}}^{(a)}\\right) \\end{aligned} $$\nLa charge conserv√©e est donc la diff√©rence entre le nombre d\u0026rsquo;antiparticules et le nombre de particules¬†! Chacun des deux types de particules (alias excitations du champ) porte une charge de Noether de signe oppos√©. Pour avoir conservation de la charge globale, il faut bien que la diff√©rence entre chaque type reste constante. L\u0026rsquo;existence m√™me des antiparticules est donc impliqu√©e par cette n√©cessit√© de conserver la charge.\nSi $J^\\mu_N$ est conserv√©, il en est de m√™me de $-J^\\mu_N$ et donc le choix du type de particules portant une charge de Noether positive est arbitraire. Par convention le courant nombre de particules $\\hat{J}^\\mu_{Nc}$ (\u0026ldquo;c\u0026rdquo; pour conventionnel) est d√©fini positivement pour les particules et n√©gativement pour les antiparticules. Cela am√®ne √† poser $\\hat{J}_{\\mathrm{Nc}}^\\mu=-N\\left[\\hat{J}_{\\mathrm{N}}^\\mu\\right]$, $\\hat{Q}_{\\mathrm{Nc}}=-N\\left[\\hat{Q}_{\\mathrm{N}}\\right]$, pour aboutir √†¬†:\n$$ \\hat{Q}_{\\mathrm{Nc}}=\\int \\mathrm{d}^3 p\\left(\\hat{n}_{\\boldsymbol{p}}^{(a)}-\\hat{n}_{\\boldsymbol{p}}^{(b)}\\right) $$\nLimite non-relativiste Dans le domaine non-relativiste, les √©nergies d\u0026rsquo;excitation des particules sont infimes compar√©es √† l\u0026rsquo;√©nergie de masse¬†: $E=mc^2+\\varepsilon$ o√π $\\varepsilon\\ll mc^2$.\nUne strat√©gie possible pour trouver la limite non-relativiste d\u0026rsquo;une th√©orie est de factoriser le \u0026ldquo;gros\u0026rdquo; terme¬†:\n$$ \\phi(\\boldsymbol{x}, t) \\rightarrow \\Psi(\\boldsymbol{x}, t) \\mathrm{e}^{-\\mathrm{i} m c^2 t / \\hbar} $$\nRegardons ce que √ßa donne avec l\u0026rsquo;√©quation de Klein-Gordon¬†:\n$$ \\left(\\hbar^2 \\frac{\\partial^2}{\\partial t^2}-\\hbar^2 c^2 \\nabla^2+m^2 c^4\\right) \\Psi(\\boldsymbol{x}, t) \\mathrm{e}^{-\\mathrm{i} m c^2 t / \\hbar}=0 $$\nLe premier terme donne¬†:\n$$ \\hbar^2 \\frac{\\partial^2}{\\partial t^2} \\Psi(\\boldsymbol{x}, t) \\mathrm{e}^{-\\mathrm{i} m c^2 t / \\hbar}=\\hbar^2\\left(\\frac{\\partial^2 \\Psi}{\\partial t^2}-\\frac{2 \\mathrm{i} m c^2}{\\hbar} \\frac{\\partial \\Psi}{\\partial t}-\\frac{m^2 c^4}{\\hbar^2} \\Psi\\right) \\mathrm{e}^{-\\mathrm{i} m c^2 t / \\hbar} $$\nEn rempla√ßant dans l\u0026rsquo;√©quation, le dernier terme de la d√©riv√©e seconde se t√©lescope avec le terme de masse¬†:\n$$ \\hbar^2 \\frac{\\partial^2 \\Psi}{\\partial t^2}-2 \\mathrm{i} m c^2 \\hbar \\frac{\\partial \\Psi}{\\partial t}-\\hbar^2 c^2 \\nabla^2 \\Psi=0 $$\nOn peut n√©gliger le premier terme puisqu\u0026rsquo;il ne contient pas de facteur $c^2$ et on obtient¬†:\n$$ \\mathrm{i} \\hbar \\frac{\\partial}{\\partial t} \\Psi(\\boldsymbol{x}, t)=-\\frac{\\hbar^2}{2 m} \\nabla^2 \\Psi(\\boldsymbol{x}, t) $$\nOn a retrouv√© l\u0026rsquo;√©quation de Schr√∂dinger d\u0026rsquo;une particule libre¬†!\nPour arriver √† la limite non-relativiste du champ scalaire complexe, on prend (en unit√©s naturelles) $\\psi=\\frac{1}{\\sqrt{2 m}} \\mathrm{e}^{-\\mathrm{i} m t} \\Psi$ (o√π $1/\\sqrt{2m}$ est un facteur de normalisation).\nOn utilise maintenant le Lagrangien avec interaction\n$$ \\mathcal{L}=\\partial^\\mu \\psi^{\\dagger}(x) \\partial_\\mu \\psi(x)-m^2 \\psi^{\\dagger}(x) \\psi(x)-\\lambda\\left[\\psi^{\\dagger}(x) \\psi(x)\\right]^2 $$\nOn obtient¬†:\n$$ \\partial_0 \\psi^{\\dagger} \\partial_0 \\psi=\\frac{1}{2 m}\\left[\\partial_0 \\Psi^{\\dagger} \\partial_0 \\Psi+\\mathrm{i} m\\left(\\Psi^{\\dagger} \\partial_0 \\Psi-\\left(\\partial_0 \\Psi^{\\dagger}\\right) \\Psi\\right)+m^2 \\Psi^{\\dagger} \\Psi\\right] $$\nLe premier terme, en $1/m$, est n√©gligeable par rapport aux autres et le troisi√®me se t√©lescope avec le terme de masse dans le Lagrangien. La partie dynamique de la th√©orie est donc confin√©e dans le deuxi√®me terme.\nEn d√©composant en ondes planes $\\mathrm{e}^{-\\mathrm{i}p\\cdot x}$, la d√©riv√©e temporelle de $\\Psi$ apporte un facteur $-\\mathrm{i}E_\\boldsymbol{p}$ et celle de $\\Psi^\\dagger$ un facteur $\\mathrm{i}E_\\boldsymbol{p}$ et donc $(\\Psi^{\\dagger} \\partial_0 \\Psi-\\Psi \\partial_0 \\Psi^{\\dagger})$ peut √™tre remplac√© par $2 \\Psi^{\\dagger} \\partial_0 \\Psi$. On aurait aussi pu remplacer par $-2 \\Psi^{\\dagger} \\partial_0 \\Psi$ si on avait choisi une d√©composition en modes $\\mathrm{e}^{\\mathrm{i}p\\cdot x}$, mais on pr√©f√®re favoriser la mati√®re par rapport √† l\u0026rsquo;antimati√®re.\nOn obtient au final¬†:\n$$ \\mathcal{L}=\\mathrm{i} \\Psi^{\\dagger}(x) \\partial_0 \\Psi(x)-\\frac{1}{2 m} \\boldsymbol{\\nabla} \\Psi^{\\dagger}(x) \\cdot \\nabla \\Psi(x)-\\frac{g}{2}\\left[\\Psi^{\\dagger}(x) \\Psi(x)\\right]^2 $$\navec $g=\\lambda/2m^2$ L\u0026rsquo;asym√©trie (entre mati√®re et antimati√®re) qu\u0026rsquo;on a inject√© dans le Lagrangien lui a fait perdre sa belle covariance relativiste.\nPassons √† la quantification canonique¬†:\n√âtape 1¬†: la densit√© lagrangienne avec un potentiel ext√©rieur est donn√© par¬†:\n$$ \\mathcal{L}=\\mathrm{i} \\Psi^{\\dagger}(x) \\partial_0 \\Psi(x)-\\frac{1}{2 m} \\nabla \\Psi^{\\dagger}(x) \\cdot \\nabla \\Psi(x)-V(x) \\Psi^{\\dagger}(x) \\Psi(x) $$\nLes √©quations d\u0026rsquo;Euler-Lagrange redonne logiquement l\u0026rsquo;√©quation de Schr√∂dinger et, pour $V(x)=0$, la relation de dispersion $E_\\boldsymbol{p}=\\frac{\\boldsymbol{p}^2}{2 m}$. Comme il n\u0026rsquo;y a plus que des √©nergies positives, on n\u0026rsquo;aura pas besoin des fr√©quences n√©gatives dans la d√©composition en modes.\n√âtape 2¬†: on calcule les densit√©s d\u0026rsquo;impulsion¬†:\n$$ \\Pi_{\\Psi}^0=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_0 \\Psi\\right)}=\\mathrm{i} \\Psi^{\\dagger} \\qquad \\Pi_{\\Psi^{\\dagger}}^0=\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_0 \\Psi^{\\dagger}\\right)}=0 $$\nL\u0026rsquo;absence de moment conjugu√© au champ $\\Psi^\\dagger$ d√©coule de notre choix de favoriser la mati√®re. On peut maintenant calculer la densit√© hamiltonienne¬†:\n$$ \\begin{aligned} \\mathcal{H} \u0026amp; =\\Pi_{\\Psi}^0 \\partial_0 \\Psi-\\mathcal{L} \\\\ \u0026amp; =\\frac{1}{2 m} \\nabla \\Psi^{\\dagger}(x) \\cdot \\nabla \\Psi(x)+V(x) \\Psi^{\\dagger}(x) \\Psi(x) \\end{aligned} $$\nUne densit√© √† la Schr√∂dinger\u0026hellip;\n√âtape 3¬†: les commutateurs √† temps √©gaux entre les positions et les impulsions s\u0026rsquo;√©crivent¬†:\n$$ \\begin{aligned} {\\left[\\hat{\\Psi}(t, \\boldsymbol{x}), \\hat{\\Pi}_{\\Psi}^0(t, \\boldsymbol{y})\\right] } \u0026amp; =\\mathrm{i} \\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) \\\\ {\\left[\\hat{\\Psi}(t, \\boldsymbol{x}), \\hat{\\Psi}^{\\dagger}(t, \\boldsymbol{y})\\right] } \u0026amp; =\\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) \\end{aligned} $$\n√âtape 4¬†: une d√©composition en modes avec des fr√©quences positives et n√©gatives ne respecterait pas la relation de commutation ci-dessus. La d√©composition idoine est¬†:\n$$ \\hat{\\Psi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x} $$\navec $E_{\\boldsymbol{p}}=\\frac{\\boldsymbol{p}^2}{2 m}$.\n√âtape 5¬†: on substitue la d√©composition en modes dans l\u0026rsquo;Hamiltonien¬†:\n$$ \\hat{H}=\\int \\mathrm{d}^3 p\\left(\\frac{\\boldsymbol{p}^2}{2 m} \\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}\\right)+\\int \\frac{\\mathrm{d}^3 x \\mathrm{~d}^3 p \\mathrm{~d}^3 q}{(2 \\pi)^3}\\left(V(t, \\boldsymbol{x}) \\mathrm{e}^{\\mathrm{i}\\left(E_{\\boldsymbol{p}}-E_{\\boldsymbol{q}}\\right) t} \\mathrm{e}^{-\\mathrm{i}(\\boldsymbol{p}-\\boldsymbol{q}) \\cdot \\boldsymbol{x}} \\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{q}}\\right) $$\nLa partie d√©pendante du temps doit garantir la conservation de l\u0026rsquo;√©nergie, ce qui force le potentiel √† s\u0026rsquo;√©crire $V(t, \\boldsymbol{x})=\\mathrm{e}^{-\\mathrm{i}\\left(E_{\\boldsymbol{p}}-E_{\\boldsymbol{q}}\\right) t} V(\\boldsymbol{x})$. L\u0026rsquo;Hamiltonien devient alors¬†:\n$$ \\hat{H}=\\int \\mathrm{d}^3 p\\left(\\frac{\\boldsymbol{p}^2}{2 m} \\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{p}}\\right)+\\int \\mathrm{d}^3 p \\mathrm{~d}^3 q\\left(\\tilde{V}(\\boldsymbol{p}-\\boldsymbol{q}) \\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\hat{a}_{\\boldsymbol{q}}\\right) $$\navec $\\tilde{V}(\\boldsymbol{p}-\\boldsymbol{q})=\\int \\mathrm{d}^3 x \\frac{1}{(2 \\pi)^3} V(\\boldsymbol{x}) \\mathrm{e}^{-\\mathrm{i}(\\boldsymbol{p}-\\boldsymbol{q}) \\cdot \\boldsymbol{x}}$.\nOn retrouve un Hamiltonien tr√®s ressemblant √† celui pr√©dit pour des syst√®mes discrets. Et on constate que la partie potentielle de l\u0026rsquo;Hamiltonien n\u0026rsquo;est pas diagonale (comme pr√©dit, le couplage emp√®che la diagonalisation).\nComme le champ scalaire complexe non-relativiste garde sa sym√©trie $U(1)$, on peut l\u0026rsquo;envisager d\u0026rsquo;une autre fa√ßon en l\u0026rsquo;√©crivant en termes d\u0026rsquo;amplitude et de phase¬†:\n$$ \\Psi(x)=\\sqrt{\\rho(x)} \\mathrm{e}^{\\mathrm{i} \\theta(x)} $$\nUne transformation de $U(1)$ correspond maintenant √† $\\theta\\rightarrow\\theta+\\alpha$.\nOn est ainsi pass√© des deux champs $\\phi_1(x)$ et $\\phi_2(x)$ aux deux nouveaux champs $\\rho(x)$ et $\\theta(x)$.\n√âtape 1¬†: on substitue ces nouveaux champs dans le Lagrangien¬†:\n$$ \\mathcal{L}=\\frac{\\mathrm{i}}{2} \\partial_0 \\rho-\\rho \\partial_0 \\theta-\\frac{1}{2 m}\\left[\\frac{1}{4 \\rho}(\\boldsymbol{\\nabla} \\rho)^2+\\rho(\\boldsymbol{\\nabla} \\theta)^2\\right]-\\frac{g}{2} \\rho^2 $$\nOn va maintenant √©teindre les interactions ($g=0$).\n√âtape 2¬†: on d√©termine les densit√©s d\u0026rsquo;impulsion¬†:\n$$ \\begin{aligned} \\Pi_\\rho^0(x) \u0026amp; =\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_0 \\rho(x)\\right)}=\\frac{\\mathrm{i}}{2} \\\\ \\Pi_\\theta^0(x) \u0026amp; =\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_0 \\theta(x)\\right)}=-\\rho(x) \\end{aligned} $$\n√âtape 3¬†: on impose les relations de commutation¬†:\n$$ \\left[\\hat{\\theta}(\\boldsymbol{x}, t), \\hat{\\Pi}_\\theta^0(\\boldsymbol{y}, t)\\right]=-[\\hat{\\theta}(\\boldsymbol{x}, t), \\hat{\\rho}(\\boldsymbol{y}, t)]=\\mathrm{i} \\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) $$\nQue nous dit le th√©or√®me de Noether¬†?\n$D\\theta = \\left.\\frac{\\partial \\theta}{\\partial \\alpha}\\right|_{\\alpha\\rightarrow 0}=1$.\n$D\\mathcal{L}=0$ puisque $\\alpha$ est une constante (on regarde une transformation globale et non locale $\\alpha(\\cancel{t,\\boldsymbol{x}})$). Et donc $W^\\mu =0$.\n$\\Pi^0_\\theta=\\frac{\\partial\\mathcal{L}}{\\partial(\\partial_0\\theta)}=-\\rho$ et $\\Pi^i_\\theta=\\frac{\\partial\\mathcal{L}}{\\partial(\\partial_i\\theta)}=\\frac{\\rho}{m}\\partial^i\\theta$.\nD√©tail $\\Pi^i_\\theta=\\frac{\\partial\\mathcal{L}}{\\partial(\\partial_i\\theta)}=\\frac{-\\rho}{2m}\\frac{\\partial(\\boldsymbol{\\nabla}\\theta)^2}{\\partial(\\partial_i\\theta)}=\\frac{-\\rho}{2m}\\frac{\\partial(\\eta^{kl}\\partial_k\\theta\\partial_l\\theta)}{\\partial(\\partial_i\\theta)}=\\frac{-\\rho}{2m}(\\eta^{kl}\\delta^i_k\\partial_l\\theta+\\eta^{kl}\\delta^i_l\\partial_k\\theta)=\\frac{-\\rho}{2m}(-\\delta^{kl}\\delta^i_k\\partial_l\\theta-\\delta^{kl}\\delta^i_l\\partial_k\\theta)=\\frac{\\rho}{m}\\partial^i\\theta$ avec la signature $(+,-,-,-)$ On en d√©duit $J^0_\\mathrm{N}=\\Pi^0_\\theta D\\theta = -\\rho(x)$ et $\\boldsymbol{J}_\\mathrm{N}=\\Pi^i_\\theta D\\theta = -\\frac{\\rho}{m}\\boldsymbol{\\nabla}\\theta$\nEt enfin $Q_{\\mathrm{Nc}}=\\int \\mathrm{d}^3 x \\rho(x)$ et $\\boldsymbol{J}_{\\mathrm{Nc}}=\\frac{\\rho}{m} \\boldsymbol{\\nabla} \\theta$.\nLa composante temporelle du courant conserv√© est $\\rho(x)$. On d√©finit alors le nombre total de particules comme $\\hat{N}(t)=\\int\\mathrm{d}^3x\\,\\hat{\\rho}(\\boldsymbol{x},t)$ et en int√©grant la relation de commutation, on obtient¬†:\n$$ [\\hat{N}(t), \\hat{\\theta}(\\boldsymbol{x}, t)]=\\mathrm{i} $$\nC\u0026rsquo;est la relation d\u0026rsquo;incertitude nombre-phase. Elle nous dit que pour les syst√®mes coh√©rents en mati√®re condens√©e, l\u0026rsquo;op√©rateur nombre d\u0026rsquo;excitations du champ est conjugu√© √† sa phase.\nQuantification canonique d\u0026rsquo;un champ √† plusieurs composantes Sym√©tries internes Il est tentant de consid√©rer que des particules se ressemblant, comme un neutron et un proton, sont en fait une seule et m√™me particule poss√©dant un curseur interne permettant de passer d\u0026rsquo;une forme √† l\u0026rsquo;autre. L\u0026rsquo;invariance du Lagrangien par rapport √† ce curseur est alors d√©crite par une sym√©trie interne (sans rapport √©vident avec les sym√©tries de l\u0026rsquo;espace-temps).\nOn a d√©j√† crois√© une sym√©trie interne avec la sym√©trie $U(1)$ du champ scalaire complexe. L\u0026rsquo;isospin est un autre exemple. C\u0026rsquo;est lui, le curseur permettant de passer d\u0026rsquo;un proton √† un neutron.\nLe proton et le neutron ont tous les deux un isospin $I=\\frac{1}{2}$ avec, comme pour le spin conventionnel, deux valeurs propres possibles de l\u0026rsquo;op√©rateur $\\hat{I}_z$¬†: $I_z=1/2$ pour le proton et $I_z=-1/2$ pour le neutron. Par analogie avec le moment cin√©tique, on peut assembler proton et neutron dans un objet √† deux composantes $\\binom{p}{n}$, le doublet d\u0026rsquo;isospin. Et on op√®re une rotation de ce doublet avec les m√™mes matrices permettant de tourner les spins 1/2. Remarquons enfin que cette sym√©trie n\u0026rsquo;est qu\u0026rsquo;approximative car on n\u0026rsquo;a pas strictement $m_p=m_n$.\nPour explorer l\u0026rsquo;id√©e de ces sym√©tries internes, prenons trois particules scalaires $t$, $d$ et $h$ arrang√©es au sein d\u0026rsquo;un vecteur $(t,d,h)^t$ poss√©dant une sym√©trie $SO(3)$. Tourner le curseur interne de 90¬∞ selon l\u0026rsquo;axe $h$ transforme ainsi une particule $t$, $(1,0,0)^t$, en une particule $d$, $(0,1,0)^t$. Toute superposition de particules obtenue en tournant le curseur est aussi valide que l\u0026rsquo;une des 3 particules originelles.\nComme les particules sont des excitations du champ, on doit pouvoir √©tudier l\u0026rsquo;isospin en th√©orie des champs. Pour cela, on arrange les champs responsables de ces particules en un vecteur $(\\phi_1,\\phi_2,\\phi_3)^t$. Soumettre ce vecteur √† une rotation interne en tournant le curseur revient √† pouvoir transformer $\\phi_1$ en $\\phi_2$ en tournant autour de l\u0026rsquo;axe correspondant √† $\\phi_3$. Et si le Lagrangien d√©crivant la th√©orie est invariant par rapport √† ces rotations internes, le th√©or√®me de Noether va nous offrir une loi de conservation concernant les charges de ces champs.\nD√©roulons la m√©canique de la quantification canonique¬†:\n√âtape 1 : posons $ \\boldsymbol{\\Phi}(x)=\\left(\\begin{array}{l} \\phi_1(x) \\\\ \\phi_2(x) \\\\ \\phi_3(x) \\end{array}\\right) $\nLe Lagrangien libre pour cette th√©orie peut s\u0026rsquo;√©crire¬†:\n$$ \\mathcal{L}=\\frac{1}{2}\\left(\\partial^\\mu \\boldsymbol{\\Phi}\\right) \\cdot\\left(\\partial_\\mu \\boldsymbol{\\Phi}\\right)-\\frac{m^2}{2} \\boldsymbol{\\Phi} \\cdot \\boldsymbol{\\Phi} $$\nqui n\u0026rsquo;est que la contraction de la somme des Lagrangiens de chaque champ¬†:\n$$ \\mathcal{L}=\\frac{1}{2}\\left[\\left(\\partial_\\mu \\phi_1\\right)^2-m^2 \\phi_1^2+\\left(\\partial_\\mu \\phi_2\\right)^2-m^2 \\phi_2^2+\\left(\\partial_\\mu \\phi_3\\right)^2-m^2 \\phi_3^2\\right] $$\nNotons bien que $\\boldsymbol{\\Phi}(x)$ n\u0026rsquo;est pas un champ vectoriel √©voluant dans l\u0026rsquo;espace de Minkowski comme $x^\\mu$ ou $p^\\mu$.\nDe fait, le produit scalaire n\u0026rsquo;est pas d√©fini en utilisant la m√©trique ($g_{\\mu\\nu}A^\\mu A^\\nu$), mais par $\\boldsymbol{\\Phi} \\cdot \\boldsymbol{\\Phi}=\\phi_1 \\phi_1+\\phi_2 \\phi_2+\\phi_3 \\phi_3$. Et de m√™me $\\partial^\\mu \\boldsymbol{\\Phi} \\cdot \\partial_\\mu \\boldsymbol{\\Phi}=\\partial^\\mu \\phi_1 \\partial_\\mu \\phi_1+\\partial^\\mu \\phi_2 \\partial_\\mu \\phi_2+\\partial^\\mu \\phi_3 \\partial_\\mu \\phi_3$.\nL\u0026rsquo;indice $\\alpha$ dans $\\Phi_\\alpha$ n\u0026rsquo;est donc pas un indice tensoriel, il n\u0026rsquo;y a pas de diff√©rence entre $\\Phi^\\alpha$ et $\\Phi_\\alpha$.\nNotre exemple poss√®dant une sym√©trie $SO(3)$, on peut transformer le vecteur $\\boldsymbol{\\Phi}$ en $\\boldsymbol{\\Phi\u0026rsquo;}$ en utilisant une matrice de rotation 3d sans modifier le Lagrangien¬†:\n$$ \\left(\\begin{array}{c} \\phi_1^{\\prime} \\\\ \\phi_2^{\\prime} \\\\ \\phi_3^{\\prime} \\end{array}\\right)=\\left(\\begin{array}{ccc} \\cos \\theta \u0026amp; -\\sin \\theta \u0026amp; 0 \\\\ \\sin \\theta \u0026amp; \\cos \\theta \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\end{array}\\right)\\left(\\begin{array}{l} \\phi_1 \\\\ \\phi_2 \\\\ \\phi_3 \\end{array}\\right) $$\n√âtape 2 : l\u0026rsquo;Hamiltonien s\u0026rsquo;√©crit¬†:\n$$ \\hat{\\mathcal{H}}=\\sum_\\alpha\\left[\\frac{1}{2}\\left(\\partial_0 \\hat{\\phi}_\\alpha\\right)^2+\\frac{1}{2}\\left(\\nabla \\hat{\\phi}_\\alpha\\right)^2+\\frac{1}{2} m^2 \\hat{\\phi}_\\alpha^2\\right] $$\n√âtape 3 : les relations de commutation √† temps √©gaux sont donn√©es par¬†:\n$$ \\left[\\hat{\\Phi}_\\alpha(t, \\boldsymbol{x}), \\hat{\\Pi}_\\beta^0(t, \\boldsymbol{y})\\right]=\\mathrm{i} \\delta^{(3)}(\\boldsymbol{x}-\\boldsymbol{y}) \\delta_{\\alpha \\beta} $$\nLe Kronecker fait en sorte que les valeurs non nulles correspondent bien au commutateur entre une composante de $\\boldsymbol{\\Phi}$ et la m√™me composante de sa densit√© d\u0026rsquo;impulsion.\n√âtape 4 : la d√©composition en modes donne¬†:\n$$ \\boldsymbol{\\Phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}}\\left(\\begin{array}{l} \\hat{a}_{\\boldsymbol{p} 1} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p} 1}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x} \\\\ \\hat{a}_{\\boldsymbol{p} 2} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x} \\\\ \\hat{a}_{\\boldsymbol{p} 3} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p} 3}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x} \\end{array}\\right) $$\no√π les $\\hat{a}_{\\boldsymbol{p} \\alpha}$ sont les op√©rateurs d\u0026rsquo;annihilation pour le champ $\\alpha$ avec les commutateurs $\\left[\\hat{a}_{\\boldsymbol{p} \\alpha}, \\hat{a}_{\\boldsymbol{q} \\beta}^{\\dagger}\\right]=\\delta^{(3)}(\\boldsymbol{p}-\\boldsymbol{q}) \\delta_{\\alpha \\beta}$.\nCompactons l\u0026rsquo;expression¬†:\n$$ \\boldsymbol{\\Phi}(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}} \\sum_{\\alpha=1}^3 \\boldsymbol{h}_\\alpha\\left(\\hat{a}_{\\boldsymbol{p} \\alpha} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{a}_{\\boldsymbol{p} \\alpha}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\no√π $\\boldsymbol{h}_1=\\left(\\begin{array}{l}1 \\\\0 \\\\0\\end{array}\\right)$, $\\boldsymbol{h}_2=\\left(\\begin{array}{l}0 \\\\1 \\\\0\\end{array}\\right)$ et $\\boldsymbol{h}_3=\\left(\\begin{array}{l}0 \\\\0 \\\\1\\end{array}\\right)$ nous renseignent sur la polarisation du champ dans l\u0026rsquo;espace interne.\n√âtapes 5 : en substituant dans l\u0026rsquo;Hamiltonien et apr√®s r√©ordonancement, on obtient¬†:\n$$ \\hat{H}=\\int \\mathrm{d}^3 p\\, E_{\\boldsymbol{p}} \\sum_{\\alpha=1}^3 \\hat{a}_{\\boldsymbol{p} \\alpha}^{\\dagger} \\hat{a}_{\\boldsymbol{p} \\alpha} $$\nMoralit√©, on somme maintenant √† la fois sur toutes les impulsions et sur toutes les polarisations.\nPenchons-nous enfin sur les charges de Noether conserv√©es en partant de la sym√©trie du Lagrangien par rapport aux transformations $\\boldsymbol{\\Phi} \\rightarrow \\boldsymbol{\\Phi} - \\boldsymbol{\\theta} \\times \\boldsymbol{\\Phi}$.\nPrenons l\u0026rsquo;exemple d\u0026rsquo;une rotation autour de l\u0026rsquo;axe $\\phi_3$¬†: on a $\\Phi^a \\rightarrow \\Phi^a-\\varepsilon^{a 3 c} \\theta^3 \\Phi^c$ et $D^3\\phi^1 = \\phi^2$, $D^3\\phi^2 = -\\phi^1$, $D^3\\phi^3 = 0$, o√π on appelle la sym√©trie de rotation autour de l\u0026rsquo;axe $b$, $D^b\\phi^a$.\nPour toute sym√©trie interne, $D\\mathcal{L}=0$ (par d√©finition, elles ne modifient pas le Lagrangien) et donc $W^\\mu = 0$. Pour les rotations autour de l\u0026rsquo;axe 3, le courant de Noether $J_{\\mathrm{N}}^{3 \\mu}$ est¬†:\n$$ J_{\\mathrm{N}}^{3 \\mu}=\\Pi^{a \\mu} D^3 \\Phi^a=\\left(\\partial^\\mu \\phi^1\\right) \\phi^2-\\left(\\partial^\\mu \\phi^2\\right) \\phi^1 $$\nEn injectant les d√©compositions en modes et apr√®s inversion de signes et ordre normal, on obtient¬†:\n$$ \\hat{Q}_{\\mathrm{Nc}}^3=-\\mathrm{i} \\int \\mathrm{~d}^3 p\\left(\\hat{a}_{1 \\boldsymbol{p}}^{\\dagger} \\hat{a}_{2 p}-\\hat{a}_{2 \\boldsymbol{p}}^{\\dagger} \\hat{a}_{1 \\boldsymbol{p}}\\right) $$\nOn obtient des formules similaires √† partir des autres axes et on peut g√©n√©raliser avec¬†:\n$$ \\boldsymbol{Q}_{\\mathrm{Nc}}=\\int \\mathrm{d}^3 x\\left(\\boldsymbol{\\Phi} \\times \\partial_0 \\boldsymbol{\\Phi}\\right) $$\n$$ \\hat{Q}_{\\mathrm{N} c}^a=-\\mathrm{i} \\int \\mathrm{~d}^3 p\\, \\varepsilon^{a b c} \\hat{a}_{b \\boldsymbol{p}}^{\\dagger} \\hat{a}_{c \\boldsymbol{p}} $$\nCette charge conserv√©e est l\u0026rsquo;isospin¬†!\nQuantification canonique d\u0026rsquo;un champ massif de spin 1 Prenons le Lagrangien de l\u0026rsquo;√©lectromagn√©tisme $-\\frac{1}{4} F_{\\mu \\nu} F^{\\mu \\nu}$ (o√π $F_{\\mu \\nu}=\\partial_\\mu A_\\nu-\\partial_\\nu A_\\mu$) et ajoutons-lui un terme de masse √† la Klein-Gordon¬†:\n$$ \\mathcal{L}=-\\frac{1}{4} F_{\\mu \\nu} F^{\\mu \\nu}+\\frac{1}{2} m^2 A_\\mu A^\\mu $$\nCe Lagrangien d√©crit un champ dont les excitations sont des bosons vectoriels de spin 1.\nLes √©quations du mouvement de ce champ sont obtenues gr√¢ce aux √©quations d\u0026rsquo;Euler-Lagrange¬†:\n$$ \\partial_\\mu F^{\\mu \\nu}+m^2 A^\\nu=0 $$\nC\u0026rsquo;est l\u0026rsquo;√©quation de Proca.\nPar sym√©trie, $\\partial_\\mu \\partial_\\nu F^{\\mu \\nu}=0$. On obtient donc $m^2 \\partial_\\nu A^\\nu=0$ et comme $m‚â†0$, le champ d√©crit est √† divergence nulle¬†: $\\partial_\\mu A^\\mu=0$. Et le champ pouvant √† nouveau se d√©composer en ondes planes, on peut aussi √©crire $p_\\mu A^\\mu = 0$. Cela fournit une contrainte sur les composantes du champ en en liant l\u0026rsquo;une aux trois autres. Au final, le champ aura donc 3 degr√©s de libert√© de polarisation au lieu de 4.\nComme dans le cas du champ $\\boldsymbol{\\Phi}$ avec la sym√©trie $SO(3)$, on va avoir besoin, pour la d√©composition en modes, d\u0026rsquo;op√©rateurs de cr√©ation et d\u0026rsquo;annihilation s√©par√©s pour chaque polarisation. Mais en plus, chacun de ces op√©rateurs devra √™tre multipli√© par un vecteur polarisation $\\epsilon^\\mu_\\lambda(p)$ qui vit dans l\u0026rsquo;espace de Minkowski et dont les composantes d√©pendent de l\u0026rsquo;impulsion de la particule consid√©r√©e.\n$$ \\begin{aligned} \\hat{A}^\\mu(x)= \\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_\\boldsymbol{p}\\right)^{\\frac{1}{2}}}\\left[\\left(\\begin{array}{c} \\epsilon_1^0(p) \\\\ \\epsilon_1^1(p) \\\\ \\epsilon_1^2(p) \\\\ \\epsilon_1^3(p) \\end{array}\\right) \\hat{a}_{\\boldsymbol{p} 1} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\left(\\begin{array}{c} \\epsilon_1^{0 *}(p) \\\\ \\epsilon_1^{1 *}(p) \\\\ \\epsilon_1^{2 *}(p) \\\\ \\epsilon_1^{3 *}(p) \\end{array}\\right) \\hat{a}_{\\boldsymbol{p} 1}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x} +\\left(\\begin{array}{c} \\epsilon_2^0(p) \\\\ \\epsilon_2^1(p) \\\\ \\epsilon_2^2(p) \\\\ \\epsilon_2^3(p) \\end{array}\\right) \\hat{a}_{\\boldsymbol{p} 2} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\left(\\begin{array}{c} \\epsilon_2^{0 *}(p) \\\\ \\epsilon_2^{1 *}(p) \\\\ \\epsilon_2^{2 *}(p) \\\\ \\epsilon_2^{3 *}(p) \\end{array}\\right) \\hat{a}_{\\boldsymbol{p} 2}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x} +\\left(\\begin{array}{c} \\epsilon_3^0(p) \\\\ \\epsilon_3^1(p) \\\\ \\epsilon_3^2(p) \\\\ \\epsilon_3^3(p) \\end{array}\\right) \\hat{a}_{\\boldsymbol{p} 3} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\left(\\begin{array}{c} \\epsilon_3^{0 *}(p) \\\\ \\epsilon_3^{1 *}(p) \\\\ \\epsilon_3^{2 *}(p) \\\\ \\epsilon_3^{4 *}(p) \\end{array}\\right) \\hat{a}_{\\boldsymbol{p} 3}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right] \\end{aligned} $$\nEn plus compact, cela donne¬†:\n$$ \\hat{A}^\\mu(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}} \\sum_{\\lambda=1}^3\\left(\\epsilon_\\lambda^\\mu(p) \\hat{a}_{\\boldsymbol{p} \\lambda} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\epsilon_\\lambda^{\\mu *}(p) \\hat{a}_{\\boldsymbol{p} \\lambda}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\nLa condition $p_\\mu A^\\mu=0$ devient $p_\\mu \\epsilon_\\lambda^\\mu(p)=0$ montrant bien que les vecteurs polarisation d√©pendent de l\u0026rsquo;impulsion. C\u0026rsquo;est leur r√¥le de rendre le champ $A^\\mu$ perpendiculaire √† $p^\\mu$.\n√Ä quoi doivent ressembler les vecteurs polarisation¬†? Comme tout quadrivecteur de l\u0026rsquo;espace de Minkowski, ils se transforment selon les transformations de Lorentz. On peut donc partir d\u0026rsquo;une particule dans son r√©f√©rentiel propre, puis g√©n√©raliser en lui faisant subir un boost.\nConsid√©rons donc une particule dans son r√©f√©rentiel propre avec une impulsion $p^\\mu=(m,0,0,0)^t$. On veut $p^\\mu \\epsilon_{\\lambda \\mu}(p)=0$ pour tout $\\lambda$, c\u0026rsquo;est √† dire que l\u0026rsquo;on veut des vecteurs polarisation normaux √† $p^\\mu$ dans ce r√©f√©rentiel. Un choix simple de vecteurs lin√©airement ind√©pendants consiste √† prendre¬†:\n$$ \\epsilon_1(m, 0)=\\left(\\begin{array}{l} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array}\\right), \\epsilon_2(m, 0)=\\left(\\begin{array}{l} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right), \\epsilon_3(m, 0)=\\left(\\begin{array}{l} 0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{array}\\right) $$\nPour obtenir $\\epsilon_\\lambda(p)$ pour un r√©f√©rentiel arbitraire, on applique la transformation de Lorentz $\\boldsymbol{\\Lambda}(p)$. Par exemple, pour une particule avec une impulsion $p_z=|\\boldsymbol{p}|$ dans la direction $z$ (avec donc $p^\\mu=\\left(E_{\\boldsymbol{p}}, 0,0,|\\boldsymbol{p}|\\right)^t$), on applique le boost donn√© par la matrice¬†:\n$$ \\Lambda^\\mu{ }_\\nu(p)=\\frac{1}{m}\\left(\\begin{array}{cccc} E_{\\boldsymbol{p}} \u0026amp; 0 \u0026amp; 0 \u0026amp; |\\boldsymbol{p}| \\\\ 0 \u0026amp; m \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; m \u0026amp; 0 \\\\ |\\boldsymbol{p}| \u0026amp; 0 \u0026amp; 0 \u0026amp; E_{\\boldsymbol{p}} \\end{array}\\right) $$\nEt on obtient les vecteurs polarisations¬†:\n$$ \\epsilon_1\\left(E_{\\boldsymbol{p}}, 0,0,|\\boldsymbol{p}|\\right)=\\left(\\begin{array}{c} 0 \\\\ 1 \\\\ 0 \\\\ 0 \\end{array}\\right), \\epsilon_2\\left(E_{\\boldsymbol{p}}, 0,0,|\\boldsymbol{p}|\\right)=\\left(\\begin{array}{c} 0 \\\\ 0 \\\\ 1 \\\\ 0 \\end{array}\\right), \\epsilon_3\\left(E_{\\boldsymbol{p}}, 0,0,|\\boldsymbol{p}|\\right)=\\left(\\begin{array}{c} |\\boldsymbol{p}| / m \\\\ 0 \\\\ 0 \\\\ E_{\\boldsymbol{p}} / m \\end{array}\\right) $$\nLa forme diagonalis√©e de l\u0026rsquo;Hamiltonien est pour sa part¬†:\n$$ \\hat{H}=\\int \\mathrm{d}^3 p E_{\\boldsymbol{p}} \\sum_{\\lambda=1}^3 \\hat{a}_{\\boldsymbol{p} \\lambda}^{\\dagger} \\hat{a}_{\\boldsymbol{p} \\lambda} $$\n√ânergie de toutes les particules dans toutes les polarisations.\nChapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/tqcd/",
	"title": "TQC-4",
	"tags": [],
	"description": "",
	"content": " Th√©orie quantique des champs note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Tr√®s souvent une simple traduction.\nRetour sommaire\nPartie 4 : √âvolution temporelle Chapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc5/",
	"title": "TQC-5",
	"tags": [],
	"description": "",
	"content": " Th√©orie quantique des champs \u0026ndash; Partie 5 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Tr√®s souvent une simple traduction.\nRetour sommaire\nChamps de jauge et th√©orie de jauge Une invariance de jauge trahit moins une sym√©trie du syst√®me qu\u0026rsquo;une redondance dans sa description (diff√©rentes configurations du champ aboutissent √† des observables identiques). Cette redondance nous laisse une certaine latitude quant au choix de la meilleure formulation, c\u0026rsquo;est le choix de jauge. Une transformation d\u0026rsquo;une description √† une autre est appel√©e transformation de jauge et l\u0026rsquo;invariance sous-jacente est l\u0026rsquo;invariance de jauge.\nL\u0026rsquo;invariance de jauge n\u0026rsquo;est pas une sym√©trie dans le sens o√π il n\u0026rsquo;est pas question ici de curseurs internes permettant de passer d\u0026rsquo;une particule √† une autre. C\u0026rsquo;est plut√¥t l\u0026rsquo;affirmation de notre incapacit√© √† trouver une description unique du syst√®me. On peut citer comme exemple l\u0026rsquo;ind√©termination de l\u0026rsquo;origine des potentiels √©lectriques (le choix d\u0026rsquo;associer 0¬†V √† la terre est arbitraire), celle de l\u0026rsquo;origine des phases en m√©canique quantique (on peut passer de $\\psi(x)$ √† $\\psi(x)\\mathrm{e}^{\\mathrm{i}\\alpha}$ sans changer la physique), et, en √©lectromagn√©tisme, la libert√© sur le choix de $A$ laiss√© par la transformation $A \\rightarrow A+\\nabla \\chi$ (o√π $\\chi(\\boldsymbol{x})$ est une fonction de la position) qui est sans effet sur $\\boldsymbol{B}=\\boldsymbol{\\nabla} \\times \\boldsymbol{A}$. On verra que ces trois ind√©terminations sont en fait li√©es entre elles. On constate aussi que dans les trois cas, seules des variations de la grandeur ind√©termin√©e peuvent faire l\u0026rsquo;objet d\u0026rsquo;observations.\nPour avoir des d√©finitions propres, on doit lever toute ambigu√Øt√© en fixant des jauges.\nIl peut s\u0026rsquo;agir de choix de jauges globaux, identiques en tout point, ou de choix locaux, susceptibles de varier d\u0026rsquo;un point √† l\u0026rsquo;autre.\nPartons du Lagrangien du champ scalaire complexe¬†:\n$$ \\mathcal{L}=\\left(\\partial^\\mu \\psi\\right)^{\\dagger}\\left(\\partial_\\mu \\psi\\right)-m^2 \\psi^{\\dagger} \\psi $$\nComme on l\u0026rsquo;a vu, la sym√©trie $U(1)$ se traduit par la non variation du Lagrangien (et par extension des √©quations du mouvement) lors de la transformation $\\psi(x) \\rightarrow \\psi(x) \\mathrm{e}^{\\mathrm{i} \\alpha}$. Il s\u0026rsquo;agit l√† d\u0026rsquo;une transformation globale puisqu\u0026rsquo;elle change le champ d\u0026rsquo;une m√™me valeur en tout point de l\u0026rsquo;espace-temps. La th√©orie est donc dite invariante par transformation $U(1)$ globale.\nQue se passerait-il si on imposait une invariance locale par rapport √† la phase¬†? Il faudrait que la transformation $\\psi(x) \\rightarrow \\psi(x) \\mathrm{e}^{\\mathrm{i} \\alpha(x)}$ (o√π $\\alpha(x)$ peut maintenant diff√©rer d\u0026rsquo;un point √† l\u0026rsquo;autre) soit sans effet sur les √©quations du mouvement. Cela semble une demande un peu extr√™me, mais elles se r√©v√®le surprenemment f√©conde¬†: l\u0026rsquo;√©lectromagn√©tisme en d√©coule¬†!\nOn n\u0026rsquo;est pas emb√™t√© par le terme de masse¬†: $m^2\\psi^\\dagger\\psi\\rightarrow m^2\\psi^\\dagger \\mathrm{e}^{-\\mathrm{i} \\alpha(x)} \\mathrm{e}^{\\mathrm{i} \\alpha(x)} \\psi = m^2\\psi^\\dagger\\psi $. Par contre, le terme contenant les d√©riv√©es pose probl√®me puisque la d√©riv√©e agit maintenant sur $\\alpha(x)$¬†:\n$$ \\begin{aligned} \\partial_\\mu \\psi(x) \u0026amp; \\rightarrow \\partial_\\mu \\psi(x) \\mathrm{e}^{\\mathrm{i} \\alpha(x)} \\\\ \u0026amp; =\\mathrm{e}^{\\mathrm{i} \\alpha(x)} \\partial_\\mu \\psi(x)+\\psi(x) \\mathrm{e}^{\\mathrm{i} \\alpha(x)} \\mathrm{i} \\partial_\\mu \\alpha(x) \\\\ \u0026amp; =\\mathrm{e}^{\\mathrm{i} \\alpha(x)}\\left[\\partial_\\mu+\\mathrm{i} \\partial_\\mu \\alpha(x)\\right] \\psi(x) \\end{aligned} $$\nEt de m√™me, on a $\\partial^\\mu \\psi^{\\dagger}(x) \\rightarrow \\mathrm{e}^{-\\mathrm{i} \\alpha(x)}\\left[\\partial^\\mu-\\mathrm{i} \\partial^\\mu \\alpha(x)\\right] \\psi^{\\dagger}(x)$.\nLe premier terme du Lagrangien est donc tout chamboul√©¬†:\n$$ \\left(\\partial^\\mu \\psi^{\\dagger}\\right)\\left(\\partial_\\mu \\psi\\right)-\\mathrm{i}\\left(\\partial^\\mu \\alpha\\right) \\psi^{\\dagger}\\left(\\partial_\\mu \\psi\\right)+\\mathrm{i}\\left(\\partial^\\mu \\psi^{\\dagger}\\right)\\left(\\partial_\\mu \\alpha\\right) \\psi+\\left(\\partial^\\mu \\alpha\\right)\\left(\\partial_\\mu \\alpha\\right) \\psi^{\\dagger} \\psi $$\nFaire d√©pendre $\\alpha$ de la position a logiquement retir√© sa sym√©trie $U(1)$ √† la th√©orie qui n\u0026rsquo;est donc pas invariante sous une transformation $U(1)$ locale. Mais peut-on restaurer cette sym√©trie¬†?\nOui, en ajoutant un nouveau champ $A^\\mu(x)$ dont la mission sera d\u0026rsquo;annuler les variations de la phase d\u0026rsquo;un point √† l\u0026rsquo;autre. On greffe ce champ √† la d√©riv√©e pour cr√©er une sorte de \u0026ldquo;super d√©riv√©e\u0026rdquo;¬†: la d√©riv√©e covariante $D_\\mu$.\n$$ D_\\mu=\\partial_\\mu+\\mathrm{i} q A_\\mu(x) $$\nLa d√©riv√©e covariante peut r√©parer la sym√©trie $U(1)$ si le nouveau champ $A_\\mu$ se transforme comme¬†:\n$$ A_\\mu \\rightarrow A_\\mu-\\frac{1}{q} \\partial_\\mu \\alpha(x) $$\n$q$ est le param√®tre de couplage, il nous informe sur la force de l\u0026rsquo;interaction entre $A_\\mu$ et les autres champs.\nSi $\\psi(x) \\rightarrow \\psi(x) \\mathrm{e}^{\\mathrm{i} \\alpha(x)}$, alors $\\partial_\\mu \\psi \\rightarrow\\left(\\partial_\\mu \\psi\\right) \\mathrm{e}^{\\mathrm{i} \\alpha}+\\mathrm{i}\\left(\\partial_\\mu \\alpha\\right) \\psi$ et donc\n$$ \\begin{aligned} D_\\mu \\psi=\\left(\\partial_\\mu+\\mathrm{i} q A_\\mu\\right) \\psi \u0026amp; \\rightarrow\\left(\\partial_\\mu \\psi\\right) \\mathrm{e}^{\\mathrm{i} \\alpha}+\\mathrm{i}\\left(\\partial_\\mu \\alpha\\right) \\psi+\\mathrm{i} q A_\\mu \\psi \\mathrm{e}^{\\mathrm{i} \\alpha}-\\mathrm{i}\\left(\\partial_\\mu \\alpha\\right) \\psi \\\\ \u0026amp; =D_\\mu\\left(\\psi \\mathrm{e}^{\\mathrm{i} \\alpha}\\right) \\end{aligned} $$\nLe Lagrangien entier devient invariant si on remplace les d√©riv√©es ordinaires par des d√©riv√©es covariantes¬†:\n$$ \\mathcal{L}=\\left(D^\\mu \\psi\\right)^{\\dagger}\\left(D_\\mu \\psi\\right)-m^2 \\psi^{\\dagger} \\psi $$\nPour imposer une sym√©trie $U(1)$ locale, la th√©orie se doit alors d\u0026rsquo;√™tre invariante par rapport √† deux jeux de transformations en parall√®le¬†:\n$$ \\begin{aligned} \\psi(x) \u0026amp; \\rightarrow \\psi(x) \\mathrm{e}^{\\mathrm{i} \\alpha(x)} \\\\ A_\\mu(x) \u0026amp; \\rightarrow A_\\mu(x)-\\frac{1}{q} \\partial_\\mu \\alpha(x) \\end{aligned} $$\nUne th√©orie o√π un champ $A^\\mu(x)$ est introduit pour permettre une invariance par rapport √† une transformation locale est appel√©e th√©orie de jauge. Le champ $A^\\mu(x)$ est appel√© champ de jauge.\nLe champ de jauge, introduit pour satisfaire notre envie soudaine d\u0026rsquo;invariance locale, peut-il s\u0026rsquo;av√©rer suffisamment r√©el jusqu\u0026rsquo;√† avoir sa propre dynamique¬†?\nTh√©orie de jauge la plus simple¬†: l\u0026rsquo;√©lectromagn√©tisme Une th√©orie dont le Lagrangien contient des termes d√©crivant $A^\\mu(x)$ se doit d\u0026rsquo;√™tre invariante sous des transformations du type $A_\\mu(x) \\rightarrow A_\\mu(x)-\\frac{1}{q} \\partial_\\mu \\alpha(x)$. L\u0026rsquo;√©lectromagn√©tisme est justement un exemple d\u0026rsquo;une telle th√©orie avec son champ vectoriel $A^\\mu(x)=(V(x), \\boldsymbol{A}(x))$ formant le Lagrangien¬†:\n$$ \\mathcal{L}=-\\frac{1}{4}\\left(\\partial_\\mu A_\\nu-\\partial_\\nu A_\\mu\\right)\\left(\\partial^\\mu A^\\nu-\\partial^\\nu A^\\mu\\right)-J_{\\mathrm{em}}^\\mu A^\\mu $$\nLes √©quations du mouvement qu\u0026rsquo;on en d√©duit ne sont autres que les deux √©quations de Maxwell inhomog√®nes¬†:\n$$ \\partial^2 A^\\nu-\\partial^\\nu\\left(\\partial_\\mu A^\\mu\\right)=J_{\\mathrm{em}}^\\nu $$\nNi le Lagrangien, ni les √©quations du mouvement ne sont modifi√©s par la transformation $A_\\mu(x) \\rightarrow A_\\mu(x)-\\partial_\\mu \\chi(x)$ qui se d√©compose en¬†:\n$$ \\begin{aligned} V \u0026amp; \\rightarrow V-\\partial_0 \\chi \\\\ \\boldsymbol{A} \u0026amp; \\rightarrow \\boldsymbol{A}+\\boldsymbol{\\nabla} \\chi \\end{aligned} $$\nC\u0026rsquo;est bien ce qu\u0026rsquo;on nomme en √©lectromagn√©tisme l\u0026rsquo;invariance de jauge (si $A_\\mu$ d√©crit correctement le champ √©lectromagn√©tique dans une certaine situation, alors $A_\\mu-\\partial_\\mu \\chi$ aussi). Et on en d√©duit que l\u0026rsquo;√©lectromagn√©tisme est une th√©orie de jauge puisqu\u0026rsquo;en choisissant de red√©finir $\\chi(x)$ comme $\\alpha(x)/q$, on retrouve bien la d√©finition vue plus haut.\nComment choisir $\\chi(x)$¬†? Il est commun d\u0026rsquo;en passer par la jauge de Lorenz (sans \u0026ldquo;t\u0026rdquo;)¬†:\n$$ \\partial_\\mu A^\\mu(x)=0 $$\n$A_\\mu$ se transforme en $A_\\mu^{\\prime}=A_\\mu-\\partial_\\mu \\chi$ et la jauge de Lorenz impose $\\partial^\\mu A_\\mu^{\\prime}=\\partial^\\mu A_\\mu-\\partial^\\mu \\partial_\\mu \\chi=0$. Pour la respecter, il faut donc poser $\\partial^2 \\chi=\\partial^\\mu A_\\mu$.\nGr√¢ce √† la jauge de Lorenz et en l\u0026rsquo;absence de courant $J_{\\mathrm{em}}^\\mu$, on obtient l\u0026rsquo;√©quation d\u0026rsquo;un champ libre sans masse. En effet l\u0026rsquo;√©quation du mouvement $\\partial^2 A^\\nu-\\partial^\\nu\\left(\\partial_\\mu A^\\mu\\right)=J_{\\mathrm{em}}^\\nu$ devient $\\partial^2 A^{\\prime \\nu}-\\partial^\\nu\\left(\\partial_\\mu A^{\\prime \\mu}\\right)=\\partial^2 A^{\\prime \\nu}=0$ dont les solutions sont des ondes planes de la forme $A^\\mu=\\epsilon^\\mu(p) \\mathrm{e}^{-\\mathrm{i} p \\cdot x}$ avec $E_{\\boldsymbol{p}}=|\\boldsymbol{p}|$. La jauge de Lorenz fait donc ressembler l\u0026rsquo;√©lectromagn√©tisme √† une th√©orie de champ vectoriel.\nOn avait d√©j√† rencontr√© la condition de Lorenz dans le cas du champ massif de spin 1 mais elle n\u0026rsquo;avait alors rien d\u0026rsquo;un choix¬†; on l\u0026rsquo;obtenait en prenant la divergence de l\u0026rsquo;√©quation de Proca\u0026hellip; Mais dans tous les cas, la condition r√©duit le nombre de composantes ind√©pendantes de $A\u0026rsquo;^\\mu$ de quatre √† trois.\nCela ne rend toujours pas $A^{\\prime \\mu}$ unique ici puisqu\u0026rsquo;on peut continuer √† transformer le champ $A_\\mu^\\prime \\rightarrow A_\\mu^{\\prime \\prime}=A_\\mu^{\\prime}-\\partial_\\mu \\xi$ tant que $\\partial^2 \\xi = 0$ ($A^{\\prime \\mu}$ et $A^{\\prime \\prime \\mu}$ respectent tous deux la condition de Lorenz). Pour rendre $A^{\\prime \\prime \\mu}$ unique, on choisit en plus de fixer $\\partial_0 \\xi=A_0^{\\prime}$, ce qui implique $A_0^{\\prime \\prime}=0$.\nAvec ce choix, la condition de Lorenz implique finalement la jauge de Coulomb¬†:\n$$ \\boldsymbol{\\nabla} \\cdot \\boldsymbol{A}^{\\prime \\prime}=0 $$\nLe nombre de degr√©s de libert√© du champ est encore r√©duit d\u0026rsquo;un cran.\nLa physique impose finalement au champ $A^\\mu$ de n\u0026rsquo;avoir que deux composantes ind√©pendantes¬†!\nLes √©quations du mouvement sous la jauge de Lorenz donnent $\\partial^2A^\\mu = 0$. Avec la condition $A^0 = 0$, cela implique des ondes planes de la forme $\\boldsymbol{A}=\\boldsymbol{\\epsilon} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}$.\nLa jauge de Coulomb $\\boldsymbol{\\nabla} \\cdot \\boldsymbol{A}=0$ impose alors $\\boldsymbol{p} \\cdot \\boldsymbol{A}=\\boldsymbol{p} \\cdot \\boldsymbol{\\epsilon}=0$ qui nous dit que la direction de propagation de l\u0026rsquo;onde est perpendiculaire √† la polarisation¬†; l\u0026rsquo;onde est transverse¬†!\nEn supposant une propagation selon l\u0026rsquo;axe $z$ avec une impulsion $q^\\mu=(|\\boldsymbol{q}|, 0,0,|\\boldsymbol{q}|)$, on peut par exemple se donner une polarisation lin√©aire¬†:\n$$ \\boldsymbol{\\epsilon}_1(q)=\\left(\\begin{array}{l} 1 \\\\ 0 \\\\ 0 \\end{array}\\right), \\quad \\boldsymbol{\\epsilon}_2(q)=\\left(\\begin{array}{l} 0 \\\\ 1 \\\\ 0 \\end{array}\\right) $$\nou encore une polarisation circulaire avec¬†:\n$$ \\epsilon_{\\mathrm{R}}^*(q)=-\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{l} 1 \\\\ \\mathrm{i} \\\\ 0 \\end{array}\\right), \\quad \\epsilon_{\\mathrm{L}}^*(q)=\\frac{1}{\\sqrt{2}}\\left(\\begin{array}{c} 1 \\\\ -\\mathrm{i} \\\\ 0 \\end{array}\\right) $$\nPour observer les effets du champ √©lectromagn√©tique, il faut le coupler √† un champ de mati√®re. La recette la plus simple consiste √† remplacer les d√©riv√©es ordinaires par les d√©riv√©es covariantes dans le Lagrangien. On nomme ce proc√©d√© couplage minimal.\nConsid√©rons un champ scalaire complexe en pr√©sence d\u0026rsquo;un champ √©lectromagn√©tique. Si les champs sont ind√©pendants, le Lagrangien total s\u0026rsquo;√©crit comme la somme des Lagrangiens de chacune des th√©ories¬†:\n$$ \\mathcal{L}=\\left(\\partial^\\mu \\psi\\right)^{\\dagger}\\left(\\partial_\\mu \\psi\\right)-m^2 \\psi^{\\dagger} \\psi-\\frac{1}{4} F_{\\mu \\nu} F^{\\mu \\nu} $$\nOn obtient un couplage entre les champs en passant de $\\partial$ √† $D$¬†:\n$$ \\begin{aligned} \\mathcal{L}\u0026amp;= \\left(D^\\mu \\psi\\right)^{\\dagger}\\left(D_\\mu \\psi\\right)-m^2 \\psi^{\\dagger} \\psi-\\frac{1}{4} F_{\\mu \\nu} F^{\\mu \\nu} \\\\ \u0026amp;= \\left(\\partial^\\mu \\psi^{\\dagger}-\\mathrm{i} q A^\\mu \\psi^{\\dagger}\\right)\\left(\\partial_\\mu \\psi+\\mathrm{i} q A_\\mu \\psi\\right)-m^2 \\psi^{\\dagger} \\psi-\\frac{1}{4} F_{\\mu \\nu} F^{\\mu \\nu} \\\\ \u0026amp;= \\partial^\\mu \\psi^{\\dagger} \\partial_\\mu \\psi-m^2 \\psi^{\\dagger} \\psi-\\frac{1}{4} F_{\\mu \\nu} F^{\\mu \\nu} + {\\color{#D41876}\\left(-\\mathrm{i} q A^\\mu \\psi^{\\dagger}\\left(\\partial_\\mu \\psi\\right)+\\mathrm{i} q\\left(\\partial^\\mu \\psi^{\\dagger}\\right) A_\\mu \\psi+q^2 \\psi^{\\dagger} \\psi A^\\mu A_\\mu\\right) } \\end{aligned} $$\nLe couplage entre le champ $A^\\mu$ et les champs $\\psi$ et $\\psi^\\dagger$ est contenu dans le dernier terme et l\u0026rsquo;importance du couplage est fix√©e par $q$, la charge √©lectromagn√©tique.\nOn appelle principe de jauge la notion selon laquelle un champ de jauge introduit pour assurer une sym√©trie locale dicte la forme du couplage, c\u0026rsquo;est-√†-dire des interactions, dans la th√©orie.\nQuantification canonique du champ √©lectromagn√©tique Le terme de masse du champ massif de spin 1 vu pr√©c√©demment lui √¥tait toute possibilit√© d\u0026rsquo;invariance de jauge alors que la nature non massive du champ de jauge lui conf√®re cette invariance et lui retire une composante.\nOn obtient in fine¬†:\n$$ \\hat{A}^\\mu(x)=\\int \\frac{\\mathrm{d}^3 p}{(2 \\pi)^{\\frac{3}{2}}} \\frac{1}{\\left(2 E_{\\boldsymbol{p}}\\right)^{\\frac{1}{2}}} \\sum_{\\lambda=1}^2\\left(\\epsilon_\\lambda^\\mu(p) \\hat{a}_{\\boldsymbol{p} \\lambda} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\epsilon_\\lambda^{\\mu *}(p) \\hat{a}_{\\boldsymbol{p} \\lambda}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right) $$\navec $E_p=|\\boldsymbol{p}|$.\nEt l\u0026rsquo;Hamiltonien est donn√© par¬†:\n$$ \\hat{H}=\\int \\mathrm{d}^3 p \\sum_{\\lambda=1}^2 E_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{p} \\lambda}^{\\dagger} \\hat{a}_{\\boldsymbol{p} \\lambda} $$\nLes excitations du champ √©lectromagn√©tique sont des photons qu\u0026rsquo;on peut observer dans deux √©tats de polarisation transverses.\nCes particules ont un spin $S=1$ et on en trouve deux types¬†: $\\hat{a}_{\\boldsymbol{p} 1}^{\\dagger}|0\\rangle$ et $\\hat{a}_{\\boldsymbol{p} 2}^{\\dagger}|0\\rangle.$\nConsid√©rons un photon se propageant selon la direction $z$ avec l\u0026rsquo;impulsion $q^\\mu=(|\\boldsymbol{q}|, 0,0,|\\boldsymbol{q}|)$. Dans une base de polarisation circulaire, on peut √©crire $\\epsilon_{\\lambda=\\mathrm{R}}^*(q)=-\\frac{1}{\\sqrt{2}}(0,1, \\mathrm{i}, 0)$ (correspondant √† $S_z = 1$) et $\\epsilon_{\\lambda=\\mathrm{L}}^*(q)=\\frac{1}{\\sqrt{2}}(0,1,-\\mathrm{i}, 0)$ (correspondant √† $S_z = -1$). Il n\u0026rsquo;y a pas de photon avec $S^z = 0$ puisque cela correspondrait √† une polarisation longitudinale interdite $\\epsilon_{\\lambda=3}^*(p)=(0,0,0,1).$\nSym√©tries discr√®tes On a rencontr√© jusque-l√† des sym√©tries port√©es par des transformations continues (translations, rotations) repr√©sent√©es par des groupes continus (groupes de Lie). Mais on peut aussi rencontrer des sym√©tries correspondant √† des transformations discr√®tes repr√©sent√©es cette fois-ci par des groupes finis.\nConjugaison de charge On appelle conjugaison de charge la transformation qui change une particule en son antiparticule. C\u0026rsquo;est l\u0026rsquo;op√©rateur $\\text { C }$ qui se charge de cette prouesse. Il ne permute pas seulement la charge d\u0026rsquo;une particule, mais aussi son nombre leptonique, son hypercharge et tout autre \u0026ldquo;nombre de charge\u0026rdquo;. On a alors¬†:\n$$ \\mathrm{C}|p\\rangle=|\\bar{p}\\rangle $$\nLa charge d\u0026rsquo;une particule $p$ de charge $q$ est mesur√©e par un op√©rateur $\\hat{Q}$¬†: $\\hat{Q}|p\\rangle=q|p\\rangle$. Par contre¬†: $\\hat{Q}|\\bar{p}\\rangle=-q|\\bar{p}\\rangle$. On peut donc √©crire $\\mathrm{C} \\hat{Q}|p\\rangle=q \\mathrm{C}|p\\rangle=q|\\bar{p}\\rangle$, mais $\\hat{Q} \\mathrm{C}|p\\rangle=\\hat{Q}|\\bar{p}\\rangle=-q|\\bar{p}\\rangle$, ce qui implique que $\\hat{Q} \\mathrm{C}=-\\mathrm{C} \\hat{Q}$, ou de mani√®re √©quivalente¬†:\n$$ \\mathrm{C}^{-1} \\hat{Q} \\mathrm{C}=-\\hat{Q} $$\nL\u0026rsquo;√©change entre particule et antiparticule impose¬†:\n$$ \\mathrm{C}^{-1} \\hat{a}_{\\boldsymbol{p}} \\mathrm{C}=\\hat{b}_{\\boldsymbol{p}} \\quad\\quad \\mathrm{C}^{-1} \\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{C}=\\hat{a}_{\\boldsymbol{p}}^{\\dagger} $$\nEt comme un champ scalaire $\\hat{\\psi}(x)$ peut s\u0026rsquo;√©crire $\\hat{\\psi}(x)=\\int_{\\boldsymbol{p}}\\left(\\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}+\\hat{b}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}\\right)$, on doit avoir $\\mathrm{C}^{-1} \\hat{\\psi} \\mathrm{C}=\\hat{\\psi}^{\\dagger}$ ($\\hat{\\psi}^{\\dagger}=\\int_{\\boldsymbol{p}}\\left(\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i} p \\cdot x}+\\hat{b}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i} p \\cdot x}\\right)$).\nComme $\\mathrm{C}^2=I$, les valeurs propre de $\\mathrm{C}$ ne peuvent √™tre que $\\pm 1$. La plupart des particules ne sont pas des √©tats propres de $\\mathrm{C}$ puisque si elles l\u0026rsquo;√©taient, on aurait $\\mathrm{C}|p\\rangle=|\\bar{p}\\rangle= \\pm|p\\rangle$, ce qui impliquerait que $|\\bar{p}\\rangle$ est le m√™me √©tat que $|p\\rangle$ et donc que la particule est sa propre antiparticule. C\u0026rsquo;est vrai pour les particules sans charge quantique.\nLe photon $\\gamma$, lui, est un √©tat propre de $\\mathrm{C}$ avec la valeur propre $-1$, puisqu\u0026rsquo;en changeant totues les particules en leurs antiparticules, le champ √©lectromagn√©tique est renvers√© ($A^\\mu \\rightarrow-A^\\mu$). C\u0026rsquo;est aussi le cas pour le pion neutre $\\pi^0$, mais avec la valeur propre $+1$. Cela explique pourquoi la r√©action $\\pi^0 \\rightarrow \\gamma+\\gamma$ est autoris√©e alors que $\\pi^0 \\rightarrow \\gamma+\\gamma+\\gamma$ est impossible.\nParit√© Une sym√©trie miroir inverse la direction de l\u0026rsquo;axe perpendiculaire au miroir et conserve les autres. Si on fait suivre cette transformation d\u0026rsquo;une rotation √† 180¬∞ autour de l\u0026rsquo;axe perpendiculaire au miroir, on aura invers√© toutes les directions spatiales, op√©rant ainsi une inversion spatiale ($\\boldsymbol{x}\\rightarrow-\\boldsymbol{x}$). Cette transformation, appel√©e parit√©, est prise en charge par l\u0026rsquo;op√©rateur $\\mathrm{P}$. L\u0026rsquo;op√©rateur position va donc anticommuter avec l\u0026rsquo;op√©rateur parit√©¬†:\n$$ \\hat{\\boldsymbol{x}} \\mathrm{P}=-\\mathrm{P} \\hat{\\boldsymbol{x}} $$\nOu de mani√®re √©quivalente¬†:\n$$ \\mathrm{P}^{-1} \\hat{\\boldsymbol{x}} \\mathrm{P}=-\\hat{\\boldsymbol{x}} $$\nL\u0026rsquo;effet sur les coordonn√©es de l\u0026rsquo;impulsion est le m√™me ($p \\rightarrow-\\boldsymbol{p}$) et donc¬†:\n$$ \\mathrm{P}^{-1} \\hat{\\boldsymbol{p}} \\mathrm{P}=-\\hat{\\boldsymbol{p}} $$\nL\u0026rsquo;op√©rateur $\\mathrm{P}$ est hermitien et son propre inverse ($\\mathrm{P}^2=I$), donc $\\mathrm{P}$ est aussi un op√©rateur unitaire.\nL\u0026rsquo;op√©rateur de parit√© est sans effet sur les scalaires mais renverse les vecteurs. Mais il existe une classe sp√©ciale de scalaire et de vecteurs (m√™me si pas r√©ellement des scalaires et des vecteurs mais plut√¥t des ojets composites) pour lesquels ce n\u0026rsquo;est pas vrai¬†: les pseudoscalaires form√©s par un produit mixte et les pseudovecteurs (aussi appel√©s vecteurs axiaux) form√©s par un produit vectoriel entre vecteurs ordinaires (aussi appel√©s vecteurs polaires).\nLe champ √©lectrique $\\boldsymbol{E}$ agit comme un vecteur ordianaire (polaire) alors que le champ magnitique et le moment cin√©tique $\\boldsymbol{L}$ sont des pesudovecteurs (vecteurs axiaux).\n$$ \\begin{aligned} \\mathrm{P}(\\text { scalaire })\u0026amp;=\\text { scalaire }\\\\ \\mathrm{P}(\\text { pseudoscalaire })\u0026amp;=-\\text { pseudoscalaire }\\\\ \\mathrm{P}(\\textbf{ vecteur })\u0026amp;=- \\text {\\bf vecteur }\\\\ \\mathrm{P}(\\textbf{ pseudovecteur })\u0026amp;= \\text {\\bf pseudovecteur } \\end{aligned} $$\nComme $\\mathrm{P}^2 = I$, sous la multiplication, le groupe ${I, P}$ est isomorphe √† $\\mathbb{Z}_2$, le groupe cyclique d\u0026rsquo;ordre 2.\n$$ \\begin{array}{c|cc} \u0026amp; I \u0026amp; \\mathrm{P} \\\\ \\hline I \u0026amp; I \u0026amp; \\mathrm{P} \\\\ \\mathrm{P} \u0026amp; \\mathrm{P} \u0026amp; I \\end{array} $$\nComme pour $\\mathrm{C}$, cela signifie que les valeurs propres de $\\mathrm{P}$ sont $\\pm1$. Les scalaires et pseudovecteurs ont une valeur de parit√© de $+1$ alors que pseudoscalaires et vecteurs ont une parit√© de $-1$.\nLe photon √©tant une excitation d\u0026rsquo;un champ vectoriel non massique, il poss√®de une parit√© intrins√®que de $-1$. Le pion est, lui, d√©crit par un champ pseudoscalaire et a donc aussi une parit√© de $-1$. On verra plus tard que la parit√© d\u0026rsquo;un fermion est oppos√©e √† celle de son antiparticule.\nLa complication avec l\u0026rsquo;op√©rateur de parit√© est qu\u0026rsquo;il agit √† la fois sur les coordonn√©es du champ et sur la nature m√™me du champ.\nPrenons le cas d\u0026rsquo;un champ scalaire. La parit√© va faire en sorte que $\\phi(t, \\boldsymbol{x}) \\rightarrow \\phi(t,-\\boldsymbol{x})$. √âtudions maintenant son action sur les op√©rateurs de cr√©ation et d\u0026rsquo;annihilation¬†:\n$$ \\mathrm{P}^{-1} \\hat{\\phi}(t, \\boldsymbol{x}) \\mathrm{P}=\\hat{\\phi}(t,-\\boldsymbol{x})=\\int_{\\boldsymbol{p}} \\hat{a}_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i}(E t+\\boldsymbol{p} \\cdot \\boldsymbol{x})}+\\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{e}^{\\mathrm{i}(E t+\\boldsymbol{p} \\cdot \\boldsymbol{x})} $$\nC\u0026rsquo;est possible si $\\mathrm{P}^{-1} \\hat{a}_{\\boldsymbol{p}} \\mathrm{P}=\\hat{a}_{-{\\boldsymbol{p}}}$ et $\\mathrm{P}^{-1} \\hat{a}_{\\boldsymbol{p}}^{\\dagger} \\mathrm{P}=\\hat{a}_{-\\boldsymbol{p}}^{\\dagger}$. L\u0026rsquo;op√©rateur de parit√© renverse simplement les impulsions pour les op√©rateurs de cr√©ation et d\u0026rsquo;annihilation.\nRenversement du temps L\u0026rsquo;op√©rateur renversement du temps $\\mathrm{T}$ transforme un champ scalaire $\\phi(t, \\boldsymbol{x})$ en $\\phi(-t, \\boldsymbol{x})$. Elle laisse donc le vecteur position tranquille¬†:\n$$ \\mathrm{T}^{-1} \\hat{\\boldsymbol{x}} \\mathrm{~T}=\\hat{\\boldsymbol{x}} $$\nMais elle renverse l\u0026rsquo;impulsion¬†:\n$$ \\mathrm{T}^{-1} \\hat{\\boldsymbol{p}} \\mathrm{~T}=-\\hat{\\boldsymbol{p}} $$\nLa seule possibilit√© pour pr√©server la relation de commutation $\\left[\\hat{x}, \\hat{p}_x\\right]=\\mathrm{i}$ est que $\\mathrm{T}$ soit antiunitaire puisqu\u0026rsquo;il faut que $\\mathrm{T}^{-1} \\mathrm{i} \\mathrm{~T}=-\\mathrm{i}$.\nPour un op√©rateur antiunitaire, $\\mathrm{T}^2=-I$. On peut noter aussi que $\\mathrm{T}^{-1} \\mathrm{i} \\mathrm{~T}=-\\mathrm{i}$ √©quivaut √† $\\mathrm{i} T=-\\mathrm{Ti}$ ce qui signifie que $\\mathrm{i}$ anticommute avec $\\mathrm{T}$.\nL\u0026rsquo;op√©rateur antiunitaire arch√©typal est $\\mathrm{K}$, l\u0026rsquo;op√©rateur de conjugaison complexe. Et on peut construire un op√©rateur antiunitaire g√©n√©ral par le produit d\u0026rsquo;un op√©rateur unitaire $\\mathrm{U}$ et de $\\mathrm{K}$. √âcrivons ainsi $\\mathrm{T}=\\mathrm{UK}$, qui √©quivaut (en multipliant les deux membres √† droite par $\\mathrm{K}$) √† $\\mathrm{U}=\\mathrm{T} \\mathrm{K}$.\nPour des particules sans spin, on peut choisir $\\mathrm{U}=I$ et $\\mathrm{U}=\\mathrm{K}$. Ce n\u0026rsquo;est pas surprenant si on regarde l\u0026rsquo;effet de la conjugaison complexe sur l\u0026rsquo;√©quation de Schr√∂dinger¬†:\n$$ \\begin{gathered} \\hat{H} \\psi=\\mathrm{i} \\frac{\\partial \\psi}{\\partial t} \\\\ \\hat{H} \\psi^*=-\\mathrm{i} \\frac{\\partial \\psi^*}{\\partial t}=\\mathrm{i} \\frac{\\partial \\psi^*}{\\partial(-t)} \\end{gathered} $$\nLa combinaison d\u0026rsquo;un renversement du temps et d\u0026rsquo;une conjugaison complexe laisse invariante l\u0026rsquo;√©quation de Schr√∂dinger. √áa semble bien montrer que dans ce cas, $\\mathrm{K}$ et $\\mathrm{T}$ sont une seule et m√™me transformation.\nPour des particules avec spin, les choses se compliquent puisque le moment cin√©tique est renvers√© lorsqu\u0026rsquo;on change le sens d\u0026rsquo;√©coulement du temps. Et donc l\u0026rsquo;action de $\\mathrm{T}$ sur l\u0026rsquo;op√©rateur de spin $\\hat{\\boldsymbol{S}}$ s\u0026rsquo;√©crit¬†:\n$$ \\mathrm{T}^{-1} \\hat{\\boldsymbol{S}} \\mathrm{~T}=-\\hat{\\boldsymbol{S}} $$\nEn se rappelant que seul la matrice de Pauli $\\sigma_y$ a des composantes complexes, l\u0026rsquo;action de l\u0026rsquo;op√©rateur de conjugaison complexe sur les op√©rateurs de spin est plus tordue¬†:\n$$ \\mathrm{K}^{-1} \\hat{S}_x \\mathrm{~K}=\\hat{S}_x, \\quad \\mathrm{~K}^{-1} \\hat{S}_y \\mathrm{~K}=-\\hat{S}_y, \\quad \\mathrm{~K}^{-1} \\hat{S}_z \\mathrm{~K}=\\hat{S}_z $$\nUne forme appropri√©e pour $\\mathrm{U}$ serait donc $\\mathrm{U}=\\exp \\left(-\\mathrm{i} \\pi \\hat{S}_y\\right)$ correspondant √† une rotation de œÄ autour de la direction $y$ de telle sorte qu\u0026rsquo;en combinant $\\mathrm{U}$ et $\\mathrm{K}$, on renverse bien les trois composantes du spin. On a ainsi¬†:\n$$ \\mathrm{T}=\\exp \\left(-\\mathrm{i} \\pi \\hat{S}_y\\right) \\mathrm{K} $$\nOn obtient alors¬†:\n$$ \\mathrm{T}^2=\\mathrm{U}\\mathrm{K}\\mathrm{U}\\mathrm{K}=\\exp \\left(- \\mathrm{i} \\pi \\hat{S}_y\\right)\\exp \\left(+\\mathrm{i} \\pi (-\\hat{S}_y)\\right)=\\exp \\left(- 2\\mathrm{i} \\pi \\hat{S}_y\\right)=(-1)^{2S} $$\nPour un √©lectron unique, on a $S=\\frac{1}{2}$ et donc $\\mathrm{T}^2=-1$. Cela reste le cas si on a un nombre impair d\u0026rsquo;√©lectrons, mais si le nombre est pair, alors $\\mathrm{T}^2=1$.\nPla√ßons-nous dans le cas o√π le nombre d\u0026rsquo;√©lectrons est impair et supposons que l\u0026rsquo;Hamiltonien $\\mathcal{H}$ du syst√®me est invariant par rapport √† une inversion temporelle ($\\mathcal{H}$ commute avec $\\mathrm{T}$). Les √©tats $|\\psi\\rangle$ et $\\mathrm{T}|\\psi\\rangle$ ont alors la m√™me √©nergie. Mais correspondent-ils au m√™me √©tat¬†? S\u0026rsquo;ils l\u0026rsquo;√©taient, on aurait $\\mathrm{T}|\\psi\\rangle=\\alpha|\\psi\\rangle$ o√π $\\alpha$ est un nombre complexe. Mais alors, $\\mathrm{T}^2|\\psi\\rangle=\\mathrm{T} \\alpha|\\psi\\rangle=\\alpha^* \\mathrm{~T}|\\psi\\rangle=|\\alpha|^2|\\psi\\rangle$ et comme $\\mathrm{T}^2=-1$, on aboutit √† une contradiction $|\\alpha|^2=-1$. Conclusion, $|\\psi\\rangle$ et $\\mathrm{T}|\\psi\\rangle$ sont lin√©airement ind√©pendants et sont appel√©s doublets de Kramers. On vient ainsi de d√©duire que les niveaux d\u0026rsquo;√©nergie d\u0026rsquo;un syst√®me temporellement sym√©trique avec un nombre impair d\u0026rsquo;√©lectrons sont $n$-fois d√©g√©n√©r√©s avec un $n$ pair. C\u0026rsquo;est le th√©or√®me de Kramers. Pour s√©parer ces paires, il faut introduire une perturbation qui brise la sym√©trie temporelle, comme un champ magn√©tique.\nCombinaisons de transformations discr√®tes En renversant √† la fois le temps $t$ avec $\\mathrm{T}$ et les coordonn√©es spatiales $\\boldsymbol{x}$ avec $\\mathrm{P}$, on obtient un renversement complet de l\u0026rsquo;espace-temps $x$. Sur un champ scalaire, on obtient¬†:\n$$ (\\mathrm{PT})^{-1} \\hat{\\phi}(x)(\\mathrm{PT})=\\hat{\\phi}(-x) $$\nCette op√©ration laisse les op√©rateurs de cr√©ation et d\u0026rsquo;annihilation inchang√©s puisque l\u0026rsquo;impulsion est retourn√©e une fois par l\u0026rsquo;op√©ration de parit√© et une nouvelle fois par le renversement du temps.\n$$ (\\mathrm{PT})^{-1} \\hat{a}_{\\boldsymbol{p}}(\\mathrm{PT})=\\hat{a}_{\\boldsymbol{p}} \\quad(\\mathrm{PT})^{-1} \\hat{a}_{\\boldsymbol{p}}^{\\dagger}(\\mathrm{PT})=\\hat{a}_{\\boldsymbol{p}}^{\\dagger} $$\nLe seul effet sur la d√©composition en modes est alors de changer le signe de $\\mathrm{i}$ dans l\u0026rsquo;exponentielle. $\\mathrm{P}\\mathrm{T}$ agit donc comme un op√©rateur de conjugaison complexe.\nLes sym√©tries li√©es √† $\\mathrm{C}$, $\\mathrm{P}$ et $\\mathrm{T}$ sont chacune conserv√©es dans la plupart des processus √† plusieurs particules, mais pas tous. $\\mathrm{P}$ est par exemple \u0026ldquo;viol√©e\u0026rdquo; en interaction faible.\nTh√©or√®me $\\mathrm{CPT}$¬†:\nSi le Lagrangien d\u0026rsquo;une th√©orie est invariant de Lorentz, local, hermitien et normalement ordonn√©, alors la th√©orie poss√®de la sym√©trie $\\textrm{CPT}$¬†; renverser √† la fois l\u0026rsquo;espace-temps et les particules en antiparticules doit laisser la th√©orie invariante.\nLa preuve consiste √† montrer que $(\\mathrm{CPT})^{-1} \\mathcal{L}(x)(\\mathrm{CPT})=\\mathcal{L}(-x)$ et ainsi d\u0026rsquo;en d√©duire que $\\mathrm{CPT}$ commute avec l\u0026rsquo;Hamiltonien et est donc une sym√©trie. Jusqu\u0026rsquo;ici, la sym√©trie $\\mathrm{CPT}$ a r√©sist√© √† tous les tests.\nCombinaisons de transformations discr√®tes et continues $SO(3)$, le groupe orthogonal sp√©cial, est le groupe des rotations √† 3 dimensions repr√©sent√©es par des matrices $3\\times 3$ orthogonales et de d√©terminant $+1$ (sp√©ciales). Ces rotations qui respectent l\u0026rsquo;orientation (c\u0026rsquo;est ce qu\u0026rsquo;assure le d√©terminant de $+1$) sont dites propres.\nLa transformation de parit√© peut √™tre repr√©sent√©e par la matrice $\\text{diag}(-1,-1,-1)$¬†:\n$$ \\left(\\begin{array}{ccc} -1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; -1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; -1 \\end{array}\\right) $$\nL√†, le d√©terminant est clairement $-1$. En combinant avec $SO(3)$, c\u0026rsquo;est-√†-dire en s\u0026rsquo;autorisant les rotations impropres (ne conservant pas l\u0026rsquo;orientation), on obtient le groupe $O(3)$ de toutes les matrices orthogonales $3\\times 3$.\nL\u0026rsquo;othogonalit√© implique $R^TR = I$ et en prenant le d√©terminant $\\operatorname{det} \\mathbf{R} \\times \\operatorname{det} \\mathbf{R}^{\\mathrm{T}}=1$. Et comme $\\operatorname{det} \\mathbf{R}=\\operatorname{det} \\mathbf{R}^{\\mathrm{T}}$, on obtient $(\\operatorname{det} \\mathbf{R})^2=1$. D\u0026rsquo;o√π les deux possibilit√©s $\\operatorname{det} \\mathbf{R}= \\pm 1$.\nLe groupe $O(3)$ est compos√© de deux ensembles disjoints li√©s l\u0026rsquo;un √† l\u0026rsquo;autre par une parit√©. Seul l\u0026rsquo;ensemble sp√©cial correspond √† un groupe ind√©pendant car lui seul poss√®de l\u0026rsquo;identit√©.\nPour obtenir $\\text{diag}(-1,-1,-1)$, l\u0026rsquo;op√©ration de parit√©, on peut faire le produit d\u0026rsquo;une r√©flexion par un miroir dans le plan $x-y$, repr√©sent√©e par $\\text{diag}(1,1,-1)$ par une rotation de $\\pi$ autour de l\u0026rsquo;axe $z$, repr√©sent√©e par $\\text{diag}(-1,-1,1)$. En tant que produit entre une rotation impropre et une rotation propre, l\u0026rsquo;op√©ration de parit√© est une rotation impropre.\n$SO(3)$ est un groupe connexe dans le sens o√π on peut se promener continument d\u0026rsquo;un √©l√©ment √† l\u0026rsquo;autre. Au contraire, $O(3)$ consiste en l\u0026rsquo;union de deux ensembles disjoints¬†; celui des √©l√©ments de d√©terminant $+1$ et celui des d√©terminants $-1$.\nOn obtient quelque chose de similaire avec le groupe de Lorentz (souvent appel√© $O(3,1)$, pour distinguer les 3 directions spatiales de la direction temporelle) contenant toutes les rotations, r√©flexions et boosts de Lorentz. Ce groupe consiste en 4 composants s√©par√©s topologiquement car en plus de $\\mathrm{P}$, on doit consid√©rer $\\mathrm{T}$.\nDans une repr√©sentation √† 4 dimensions, $\\mathrm{P}=\\operatorname{diag}(1,-1,-1,-1)$ et $\\mathrm{T}=\\operatorname{diag}(-1,1,1,1)$. Le sous-groupe du groupe de Lorentz qui ne renverse ni les coordonn√©es spatiales ni temporelles est appel√© sous-groupe propre (conserve l\u0026rsquo;orientation spatiale) orthochrone (conserve l\u0026rsquo;orientation du temps) de Lorentz $SO^+(1,3)$. Ce sous-groupe connexe est une des quatre composantes du groupe de Lorentz. On acc√®de aux autres composantes √† partir de $SO^+(1,3)$¬†:\npar action de $\\mathrm{P}$, par action de $\\mathrm{T}$, par action de $\\mathrm{PT}$. Revenons enfin sur $SO(3)$ et sa topologie. Une rotation est caract√©ris√©e par un axe et un angle. Par cons√©quent, tous les points dans une boule de rayon $\\pi$ peuvent repr√©senter une rotation (l\u0026rsquo;axe est donn√© par le vecteur entre le centre de la sph√®re et le point choisi et l\u0026rsquo;angle est donn√© par la norme de ce vecteur). Dans cette repr√©sentation, deux points antipodaux correspondent √† le m√™me rotation (une rotation de $\\pi$ autour d\u0026rsquo;un axe est √©quivalente √† une rotation de $-\\pi$ autour de l\u0026rsquo;axe inverse). La topologie de $SO(3)$ est donc celle d\u0026rsquo;une boule dont les points antipodaux de la surface sont identifi√©s entre eux (on peut se t√©l√©porter d\u0026rsquo;un point √† l\u0026rsquo;autre).\nCela signifie que l\u0026rsquo;espace topologique de $SO(3)$ est connexe mais pas simplement connexe. En effet, dans un espace simplement connexe, tout lacet (chemin continu ferm√©) doit pouvoir se r√©duire continument √† un point. Or ici, le lacet allant d\u0026rsquo;un p√¥le √† l\u0026rsquo;autre (ce chemin est bien un lacet puisque ses extr√©mit√©s correspondent √† un seul et m√™me point) n\u0026rsquo;est pas d√©formable en un point puisque tout mouvement d\u0026rsquo;une extr√©mit√© s\u0026rsquo;accompagne d\u0026rsquo;un mouvement oppos√© de l\u0026rsquo;autre extr√©mit√© pour rester antipodal. Par contre, en faisant un deuxi√®me tour d\u0026rsquo;un p√¥le √† l\u0026rsquo;autre, on peut maintenant faire dispara√Ætre le lacet comme le montre le dessin ci-dessus. Cela montre que les rotations de $4\\pi$ sont continument d√©formables en un point alors que les rotations de $2\\pi$ ne le sont pas. La \u0026ldquo;ceinture de Dirac\u0026rdquo; ou les \u0026ldquo;assiettes de Feynman\u0026rdquo; tentent d\u0026rsquo;illustrer exp√©rimentalement ce ph√©nom√®ne.\nOn peut faire correspondre les rotations 3D aux √©l√©ments d\u0026rsquo;un autre groupe¬†: $SU(2)$, le groupe sp√©cial unitaire repr√©sent√© par des matrices $2\\times 2$ de d√©terminant 1. Les √©l√©ments de $SU(2)$ permettent de faire tourner les spineurs.\nUne matrice de rotation peut en effet s\u0026rsquo;√©crire $\\mathbf{R}(\\hat{\\boldsymbol{n}}, \\theta)$ avec¬†:\n$$ \\mathbf{R}(\\hat{\\boldsymbol{n}}, \\theta)=\\exp \\left(-\\mathrm{i} \\frac{\\theta}{2} \\boldsymbol{\\sigma} \\cdot \\boldsymbol{n}\\right)=I \\cos \\frac{\\theta}{2}-\\mathrm{i} \\sin \\frac{\\theta}{2} \\boldsymbol{\\sigma} \\cdot \\boldsymbol{n} $$\no√π $\\boldsymbol{\\sigma}=\\left(\\sigma_x, \\sigma_y, \\sigma_z\\right)$ sont les matrices de Pauli et $I$ la matrice identit√©. On remarque alors que¬†:\n$$ \\mathbf{R}(\\hat{\\boldsymbol{n}}, 0)=I\\quad,\\quad \\mathbf{R}(\\hat{\\boldsymbol{n}}, 2 \\pi)=-I \\quad,\\quad \\mathbf{R}(\\hat{\\boldsymbol{n}}, 4 \\pi)=I $$\nOn dit que $SU(2)$ est un double recouvrement de $SO(3)$. Prenons l\u0026rsquo;identit√© par exemple¬†: dans $SO(3)$, l\u0026rsquo;absence de rotation est repr√©sent√©e par $\\operatorname{diag}(1,1,1)$ et dans $SU(2)$, √† la fois par $\\operatorname{diag}(1,1)$ et $\\operatorname{diag}(-1,-1)$.\nUn spineur peut s\u0026rsquo;√©crire comme une entit√© √† deux composantes $\\binom{a}{b}$ o√π $a$ et $b$ sont des nombres complexes tels que $|a|^2+|b|^2=1$. En √©crivant $a=x_0+\\mathrm{i} x_1$ et $b=x_2+\\mathrm{i} x_3$ o√π les $x_i$ sont des nombres r√©els, la condition $|a|^2+|b|^2=1$ devient $x_0^2+x_1^2+x_2^2+x_3^2=1$ et donc $SU(2)$ est isomorphe √† $S^3$, la 3-sph√®re, ce qui montre que $SU(2)$ est simplement connexe, contrairement √† $SO(3)$. $SO(3)$ est finalement un groupe quotient¬†: $S O(3) \\cong S U(2) / \\mathbb{Z}_2$.\nOn peut g√©n√©raliser ces arguments √† la composante connexe du groupe de Lorentz¬†: $S O(1,3) \\cong S L(2, \\mathbb{C}) / \\mathbb{Z}_2$ o√π $S L(2, \\mathbb{C})$ est le groupe des matrices $2\\times 2$ complexes de d√©terminant unit√©.\nChapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc6/",
	"title": "TQC-5",
	"tags": [],
	"description": "",
	"content": " Th√©orie quantique des champs \u0026ndash; Partie 6 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Tr√®s souvent une simple traduction.\nRetour sommaire\nPropagateurs et fonctions de Green Chapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/electromag/arcs/",
	"title": "Arcs √©lectriques",
	"tags": [],
	"description": "",
	"content": " Arcs √©lectriques Le champ disruptif d\u0026rsquo;un isolant, ou plut√¥t sa rigidit√© di√©lectrique, d√©signe la valeur maximum du champ √©lectrique que le milieu peut supporter avant le d√©clenchement d‚Äôun arc √©lectrique, ou claquage.\nPour l\u0026rsquo;air, la valeur fr√©quemment admise est de $\\pu{36 kV/cm}$ dans l\u0026rsquo;air sec et tombe √† $\\pu{10 kV/cm}$ pour un air satur√© en humidit√©. Cela signifie qu\u0026rsquo;il faut, dans l\u0026rsquo;air sec, une tension d\u0026rsquo;au moins $\\pu{36 kV}$ entre deux √©lectrodes s√©par√©es d\u0026rsquo;un centim√®tre pour qu\u0026rsquo;une √©tincelle se cr√©e entre elles.\nComment retrouver cet ordre de grandeur √† partir de principes physiques simples¬†?\nUn arc √©lectrique est un conduit d\u0026rsquo;air ionis√©. Son origine¬†? Imaginons qu\u0026rsquo;un √©lectron soit arrach√© √† une mol√©cule d\u0026rsquo;air. Il est alors acc√©l√©r√© par le champ √©lectrique ambiant et gagne ainsi de l\u0026rsquo;√©nergie cin√©tique. Si au moment de rencontrer une nouvelle mol√©cule, l\u0026rsquo;√©lectron a atteint une √©nergie suffisante pour la ioniser, on peut se retrouver avec une cascade d\u0026rsquo;ionisations successives.\nLe secret est donc d\u0026rsquo;avoir un champ suffisant pour donner √† un √©lectron une √©nergie de quelques dizaines d\u0026rsquo;√©lectronvolts sur une distance correspondant √† son libre parcours moyen dans l\u0026rsquo;air.\nFaisons le point sur le libre parcours moyen dans un gaz de particules identiques de rayon $d$ et de densit√© $n$¬†:\nLa section efficace de collision $\\sigma$ vaut $\\pi d^2$.\nLa valeur de la vitesse relative entre deux particules vaut\u0026nbsp;: $$ \\begin{aligned} v_{rel}\u0026amp;=\\sqrt{\\vec{v}_{rel}\\cdot \\vec{v}_{rel}}\\\\ \u0026amp;=\\sqrt{\\left(\\vec{v}_2-\\vec{v}_1\\right)\\cdot\\left(\\vec{v}_2-\\vec{v}_1\\right)}\\\\ \u0026amp;=\\sqrt{\\vec{v}_2\\cdot\\vec{v}_2+\\vec{v}_1\\cdot\\vec{v}_1-2\\vec{v}_2\\cdot\\vec{v}_1} \\end{aligned} $$\nPour obtenir la vitesse relative moyenne, on va supposer que les vitesses des particules se r√©partissent al√©atoirement selon une certaine distribution de probabilit√©.\n$$ \\begin{aligned} \\overline{v_{rel}}\u0026amp;=\\sqrt{\\overline{\\vec{v}_2\\cdot\\vec{v}_2}+\\overline{\\vec{v}_1\\cdot\\vec{v}_1}-2\\,{\\cancel{\\overline{\\vec{v}_2\\cdot\\vec{v}_1}}}}\\\\ \u0026amp;= \\sqrt{\\overline{\\vec{v}_2^2}+\\overline{\\vec{v}_1^2}}\\\\ \u0026amp;= \\sqrt{2}\\overline{v} \\end{aligned} $$\nOn suppose en effet que les vitesses des particules 1 et 2 ne sont pas corr√©l√©es et que leurs valeurs moyennes sont les m√™mes.\nLe nombre de collisions d\u0026rsquo;une particule pendant un laps de temps $\\Delta t$ peut √™tre estim√© comme le nombre moyen de fois que le centre de masse d\u0026rsquo;une particule se trouve dans le volume balay√© par la section efficace pendant $\\Delta t$, c\u0026rsquo;est-√†-dire sur une distance $\\overline{v_{rel}}\\Delta t$.\nLe volume en question vaut $V=\\sigma \\overline{v_{rel}}\\Delta t$. Et le nombre de particules rencontr√©es vaut donc $nV=n\\,\\sigma\\, \\overline{v_{rel}}\\,\\Delta t= \\sqrt{2}\\,n\\,\\sigma\\, \\overline{v}\\,\\Delta t= \\sqrt{2} \\,n\\,\\pi \\,d^2 \\,\\overline{v}\\,\\Delta t$.\nLe libre parcours moyen $\\lambda$ va alors correspondre √† la distance parcourue pendant $\\Delta t$ divis√©e par le nombre de collisions ayant eu lieu pendant ce laps de temps. D\u0026rsquo;o√π¬†:\n$$ \\begin{aligned} \\lambda \u0026amp;= \\frac{\\cancel{\\overline{v}\\Delta t}}{\\sqrt{2}n\\sigma {\\cancel{\\overline{v}\\Delta t}}}\\\\ \u0026amp;=\\frac{1}{\\sqrt{2}n\\sigma}\\\\ \u0026amp;=\\frac{1}{\\sqrt{2}n\\pi d^2} \\end{aligned} $$\nLe libre parcours moyen ne d√©pend donc que de la taille des particules et de leur densit√©, pas de leur vitesse relative.\nPour un gaz √† pression $P$ et temp√©rature $T$, on peut utiliser en premi√®re approximation la relation des gaz parfaits pour obtenir la densit√© de particules¬†: $n=P/k_B T$. En rempla√ßant dans $\\lambda$, on obtient¬†:\n$$ \\lambda = \\frac{k_B T}{\\sqrt{2}P\\pi d^2} $$\nUne mol√©cule de diazote a un diam√®tre $d=\\pu{0,37 nm}$ et pour une pression $P=\\pu{1 bar}$ et une temp√©rature $T = \\pu{293 K}$, on obtient un libre parcours moyen $\\lambda\\approx \\pu{66 nm}$. On peut remarquer que c\u0026rsquo;est pr√®s de 200 fois plus grand que la distance moyenne entre particules donn√©e par $1/n^{\\frac{1}{3}}=\\left(\\frac{k_B T}{P}\\right)^{\\frac{1}{3}}\\approx \\pu{3,4 nm}$¬†!\nPour un √©lectron, on peut reprendre la formule du libre parcours moyen en modifiant seulement un peu la section efficace puisque $\\sigma=\\pi\\left(r_\\text{mol√©cule}+r_\\text{√©lectron}\\right)^2\\approx \\pi \\left(r_\\text{mol√©cule}\\right)^2 = \\pi d^2 /4$.\nEt donc $\\lambda= \\frac{2\\sqrt{2}k_B T}{P\\pi d^2}\\approx 0,27 \\text{ Œºm}$.\nNotre √©lectron devant r√©cup√©rer une √©nergie d\u0026rsquo;environ $\\pu{20 eV}$ sur la distance $\\lambda$ pour r√©ussir √† ioniser la prochaine mol√©cule de diazote rencontr√©e, le champ doit donc √™tre de $\\pu{20 V}/\\lambda$, soit √† peu pr√®s $\\pu{20 V}$ pour $0,27 \\text{ Œºm}$ $\\rightarrow$ $0,74\\text{ MV/cm.}$ C\u0026rsquo;est plus d\u0026rsquo;un ordre de grandeur au-dessus de ce qu\u0026rsquo;on aurait aim√© obtenir üò¢\nUne des raisons possibles de cet √©cart est notre trop grand optimisme quant √† l\u0026rsquo;efficacit√© des collisions¬†; il est en effet fort peu probable qu\u0026rsquo;un √©lectron parvienne √† ioniser syst√©matiquement chaque mol√©cule qu\u0026rsquo;il rencontre, en particulier avec une √©nergie √† peine suffisante. Cela revient au final √† surestimer la section efficace de collision\u0026hellip; Penchons-nous alors sur la litt√©rature pour trouver une valeur plus r√©aliste.\nLe graphique suivant rapporte un tas de sections efficaces diff√©rentes pour des collisions √©lectron-N2 donnant lieu √† des rotations, des vibrations, ou, pour ce qui nous int√©resse ici, des ionisations¬†:\nY. Itikawa et al. Cross Sections for Collisions of Electrons and Photons with Nitrogen Molecules, Journal of Physical and Chemical Reference Data 15, 985 (1986) Pour des √©lectrons de $\\pu{20 eV}$, la section efficace semble donc √™tre d\u0026rsquo;approximativement $\\pu{4e-17 cm2}$. Cela nous donne un libre parcours moyen de $7\\text{ Œºm}$. Et hop, environ $\\pu{30 kV/cm}$, youpi¬†!\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/arithmetique/",
	"title": "Arithm√©tique",
	"tags": [],
	"description": "",
	"content": " Arithm√©tique et th√©orie des nombres¬†: Infinis d√©nombrables et ind√©nombrables Des entiers aux transcendants en passant par l\u0026rsquo;hypoth√®se du continu ($\\aleph_0 = 2^{\\aleph_1}$)¬†:\nNum√©ration \u0016De nombreux syst√®mes de num√©ration √† bases enti√®res ont √©t√© utilis√©s par diff√©rents peuples et √† diff√©rentes √©poques.\nPar exemple :\nsyst√®me binaire (base 2) utilis√© dans des langues d\u0026rsquo;Am√©rique du Sud et d\u0026rsquo;Oc√©anie, et utilis√© de nos jours en informatique.\nsyst√®me quinaire (base 5) dont il reste des traces jusqu\u0026rsquo;au xxe si√®cle dans des langues africaines, mais aussi, partiellement, dans les notations tchouvache, suzhou, romaine et maya. Le nom des chiffres 6, 7, 8 et 9 dans de nombreuses langues t√©moignent de ce syst√®me quinaire: ils se disent 5+1, 5+2, 5+3 et 5+4 en wolof (langue de la famille nig√©ro-congolaise), en khmer (langue austro-asiatique), en nahuatl (langue uto-azt√®que), et, dans de nombreuses langues austron√©siennes telles qu\u0026rsquo;en lote ou en ngadha (sous forme partielle). La base quinaire apparait parfois comme base auxiliaire ou sous-base de la base d√©cimale, comme dans le syst√®me romain, ou de la base vig√©simale.\nsyst√®me duod√©cimal (base 12), d√©j√† utilis√© par les Sum√©riens et Assyro-babyloniens pour des mesures de longueur et de temps. On le retrouve dans un certain nombre de monnaies et d\u0026rsquo;unit√©s de compte courantes en Europe au Moyen √Çge, notamment dans le syst√®me imp√©rial d\u0026rsquo;unit√©s (il faut 12 pouces pour faire un pied), et dans le commerce. Il sert encore, par exemple, pour compter les mois, les heures, les fleurs, les hu√Ætres et les ≈ìufs.\nsyst√®me hexad√©cimal (base 16), tr√®s couramment utilis√© en √©lectronique ainsi qu\u0026rsquo;en informatique. Son int√©r√™t r√©side dans les conversions triviales avec la base 2, tout en permettant une √©criture plus compacte des nombres.\nsyst√®me vig√©simal (ou vic√©simal, base 20) existe au Bhoutan en langue dzongkha, et √©tait en usage chez les Azt√®ques vers 1200 et, quoiqu\u0026rsquo;irr√©gulier, pour la num√©ration maya. Il √©tait aussi pr√©sent en vieux fran√ßais, ce qui explique l\u0026rsquo;usage du mot quatre-vingts pour le nombre 80, ou encore le nom de l\u0026rsquo;h√¥pital des Quinze-Vingts, qui pouvait accueillir 300 patients.\nnote\nChiffres de Kaktovik :\nToutes les langues eskimo-al√©outes d\u0026rsquo;Alaska et du Canada utilisent un syst√®me vig√©simal pour compter. Les chiffres arabes, qui ont √©t√© con√ßus pour un syst√®me d√©cimal, sont inad√©quats pour l\u0026rsquo;i√±upiaq et les autres langues inuites. Pour rem√©dier √† ce probl√®me, des √©l√®ves d\u0026rsquo;une √©cole de Kaktovik, en Alaska, ont invent√© un syst√®me √† base 20 en 1994, qui s\u0026rsquo;est r√©pandu parmi les I√±upiat en Alaska et a √©t√© envisag√© au Canada.\nsyst√®me sexag√©simal (base 60) √©tait utilis√© pour la num√©ration babylonienne et en M√©sopotamie vers 3300 av. J.-C., ainsi que par les Indiens et les Arabes en trigonom√©trie. Il sert encore actuellement dans la mesure du temps et certaines mesures des angles. Les chiffres et nombres m√©sopotamiens de 1 √† 59 : Source : Wikipedia\nNombres alg√©briques Un nombre alg√©brique est un nombre r√©el ou complexe solution d\u0026rsquo;une √©quation polynomiale √† coefficients dans le corps $\\mathbb{Q}$ des rationnels.\nTra√ßons l\u0026rsquo;ensemble des racines des $2^{21}$ (‚âà 2 millions) polytn√¥mes de degr√© 20 possible si chacun des coefficients vaut soit 1, soit -1.\nExemple d\u0026rsquo;un de ces polyn√¥mes¬†: $-x^{20}+x^{19}+x^{18}+x^{17}-x^{16}-x^{15}-x^{14}-x^{13}+x^{12}-x^{11}+x^{10}-x^{9}+x^{8}+x^{7}+x^{6}-x^{5}-x^{4}-x^{3}-x^{2}+x+1$\nUn polyn√¥me de degr√© 20 a 20 racines complexes. On se retrouve donc avec 40 millions de points (dont beaucoups sont aux m√™mes endroits)\u0026hellip; Et cela donne √ßa¬†:\nSont trac√©s ci-dessous les racines pour des degr√©s croissants de ces polyn√¥mes √† coefficients unitaires.\nnote\nCette page pour en savoir plus. √áa parle m√™me de dragons¬†!\nEn repr√©sentant dans le plan complexe les racines d\u0026rsquo;un polyn√¥me de degr√© 2 dont on fait varier chacun des trois coefficients entre -100 et 100, on obtient la jolie figure suivante (ici dans une zone centr√©e sur z√©ro et de rayon 1,7)¬†:\nFractions continues Algorithme d\u0026rsquo;Euclide Une √©criture r√©cursive de l\u0026rsquo;algo en Python :\ndef pgcd(a,b): if b == 0: return a else: return pgcd(b,a%b) Un des plus anciens algorithmes connus, tr√®s simple dans sa formulation mais qui rec√®le des surpises comme l\u0026rsquo;apparition du nombre d\u0026rsquo;or dans la d√©termination de sa complexit√© temporelle (nombre d\u0026rsquo;√©tapes de calcul en fonction des entiers a et b ).\nTriplets pythagoriciens Divisibilit√© et nombres premiers Complexes et quaternions note\nJolie s√©rie de vid√©os sur les complexes.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/electromag/",
	"title": "√âlectromagn√©tisme",
	"tags": [],
	"description": "",
	"content": " √âlectromagn√©tisme Rayonnement Source¬†: Magnetism, Radiation, and Relativity Supplementary notes for a calculus-based introductory physics course Daniel V. Schroeder üåê\nFormule de Larmor et Diffusion Rayleigh sans douleur jusqu\u0026rsquo;au bleu du ciel et au rougeoiement du soleil couchant.\nTrois animations montrant le champ rayonn√© (densit√© et lignes) pour diverses acc√©l√©rations (rebond, arr√™t brutal, osccillation).\n‚ñ∂Ô∏é\u0026nbsp;Code Python pour cette derni√®re vid√©o import numpy as np import matplotlib.pyplot as plt from scipy.optimize import fsolve import os x0 = 0.2 def beta_function(t): return np.array([0.,-x0*c*np.sin(np.pi*t),0]) def beta_dot_function(t): return np.array([0,-x0*c*np.pi*np.cos(np.pi*t),0]) c = 1 def r_0(t): return np.array([float(0.),float(x0*c/np.pi*np.cos(np.pi*t)),0]) def determinetprime(r,r_0,t): \"\"\" r_0(t_prime) est la fonction donnant la position au cours du temps r est le point o√π on cherche la valeur du champ \"\"\" def func(t_prime): return t - t_prime - np.linalg.norm(r - r_0(t_prime)) / c t_prime_initial_guess = t - 1 t_prime_solution = fsolve(func, t_prime_initial_guess) return t_prime_solution[0] def champ_vectorized(x_grid, y_grid, r_0_func, beta_function, beta_dot_function, t): q = 1 E_field = np.zeros(x_grid.shape + (3,)) # Initialisation d'un tableau 3D pour le champ √©lectrique for i in range(x_grid.shape[0]): for j in range(x_grid.shape[1]): r = np.array([x_grid[i, j], y_grid[i, j], 0]) tprime = determinetprime(r, r_0_func, t) beta = beta_function(tprime) beta_dot = beta_dot_function(tprime) r0 = r_0_func(tprime) R = r - r0 R_norm = np.linalg.norm(R) R_hat = R / R_norm gamma = 1 / np.sqrt(1 - np.dot(beta, beta)) term1_numerator = R_hat - beta term1_denominator = gamma**2 * R_norm**2 * (1 - np.dot(R_hat, beta))**3 term1 = term1_numerator / term1_denominator term2_numerator = np.cross(R_hat, np.cross(R_hat - beta, beta_dot)) term2_denominator = c * R_norm * (1 - np.dot(R_hat, beta))**3 term2 = term2_numerator / term2_denominator E = q * (term1 + term2) E_field[i, j, :] = E return E_field def calculate_field_line_point(r0, beta, n_hat, t, t_prime): gamma = 1 / np.sqrt(1 - np.dot(beta, beta)) # Facteur de Lorentz if np.linalg.norm(beta) != 0: beta_hat = beta/np.linalg.norm(beta) else: beta_hat = np.array([0.0,0.0,0.0]) term_inside_brackets = beta + ((1/gamma - 1) * np.dot(n_hat, beta_hat) * beta_hat + n_hat) / (gamma * (1 + np.dot(n_hat, beta))) return r0 + c * (t - t_prime) * term_inside_brackets # Fonction pour tracer les lignes de champ √† partir d'une position donn√©e √† l'instant t def plot_field_lines(r0_func, beta_func, t, num_lines=24, num_points=500, step=0.05): for i in range(num_lines): n_hat = np.array([np.cos(i/num_lines*2*np.pi),np.sin(i/num_lines*2*np.pi),0]) line_points = [] t_prime = t # Initialisation de t' √† la valeur de t pour le d√©but de la ligne de champ for _ in range(num_points): r0 = r0_func(t_prime) beta = beta_func(t_prime) point = calculate_field_line_point(r0, beta, n_hat, t, t_prime) line_points.append(point) t_prime -= step # D√©cr√©mentation de t' pour suivre la ligne de champ vers le pass√© line_points = np.array(line_points) plt.plot(line_points[:, 0], line_points[:, 1], 'r-') # Trace la ligne en rouge # Set up grid for field calculation x_range = np.linspace(-12, 12, 1200) y_range = np.linspace(-6, 6, 600) x_grid, y_grid = np.meshgrid(x_range, y_range) # Cr√©er le r√©pertoire pour les images si n√©cessaire output_dir = \"animation\" if not os.path.exists(output_dir): os.makedirs(output_dir) # D√©finir la plage de temps et le nombre d'images time_values = np.linspace(0, 2, num=51) # 401 images pour t de 0 √† 4 for i, t in enumerate(time_values): plt.figure(figsize=(15, 10), dpi=150) plot_field_lines(r_0, beta_function, t) E_field_grid = champ_vectorized(x_grid, y_grid, r_0, beta_function, beta_dot_function, t) E_field_magnitude = np.linalg.norm(E_field_grid, axis=2) plt.imshow(np.log10(E_field_magnitude), extent=(x_range.min(), x_range.max(), y_range.min(), y_range.max()), cmap='Spectral_r', aspect='equal') plt.axis('off') # Construire le nom de fichier avec un remplissage de z√©ros filename = f\"image{str(i).zfill(4)}.png\" filepath = os.path.join(output_dir, filename) # Sauvegarder l'image plt.savefig(filepath, bbox_inches='tight', pad_inches=0, transparent=True) # Fermer la figure pour lib√©rer de la m√©moire plt.close() Ce code a √©t√© obtenu √† partir de cet article et de cette page de forum. "
},
{
	"uri": "https://sciencesilencieuse.github.io/info/",
	"title": "Informatique",
	"tags": [],
	"description": "",
	"content": " Un peu d\u0026rsquo;informatique Cours d\u0026rsquo;informatique pour pr√©pas TSI1 et TSI2 avec TP corrig√©s\nBase de Python\nProjets et d√©fis\nAlgorithmique G√©n√©ralit√©s et quelques d√©tails\nOrdinateur Fonctionnement simplifi√© d\u0026rsquo;un ordinateur\nIntelligence Artificielle Histoire, exemples et dangers\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/quantique/spekkens/",
	"title": "Mod√®le jouet de Spekkens",
	"tags": [],
	"description": "",
	"content": " Mod√®le jouet de Spekkens note\nSpekkens, Robert W. (March 19, 2007). \u0026ldquo;Evidence for the epistemic view of quantum states: A toy theory\u0026rdquo;. Physical Review A. 75 (3): 032110 üåê\nLeifer, M. S. ¬´ Is the Quantum State Real? An Extended Review of œà-ontology Theorems ¬ª, Quanta, vol. 3, no 1, p. 67‚Äì155 (2014) üåê\nSpekkens propose en‚ÄØ2007 un mod√®le jouet‚ÄØqui reproduit qualitativement la physique d‚Äôune particule de spin‚ÄØ1/2 (ou, plus g√©n√©ralement, d‚Äôun qubit) pr√©par√©e et mesur√©e dans les bases‚ÄØx,‚ÄØy‚ÄØet‚ÄØz. La version compl√®te inclut la dynamique et les syst√®mes compos√©s ‚Äî elle recr√©e m√™me certains ph√©nom√®nes d‚Äôintrication ‚Äî mais, √† titre illustratif, on se limite ici au cas √©l√©mentaire d‚Äôun seul bit-jouet.\nLe but de ce mod√®le est de faire la d√©monstration du pouvoir explicatif de l\u0026rsquo;interpr√©tations œà-√©pist√©mique en rendant quasi naturel des ph√©nom√®nes quantiques qu\u0026rsquo;une interpr√©tation œà-ontique rend bien plus myst√©rieux.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/optique/",
	"title": "Optique",
	"tags": [],
	"description": "",
	"content": " Optique Arcs-en-ciel La physique de l\u0026rsquo;arc-en-ciel avec des simulations en VPython.\nLentilles Physique et maths des lentilles avec simulations VPython.\nPolarisation "
},
{
	"uri": "https://sciencesilencieuse.github.io/info/ordinateur/",
	"title": "Ordinateur",
	"tags": [],
	"description": "",
	"content": " Dans la b√™te Ordinateur Petite s√©rie de vid√©o sur le fonctionnement simplifi√© d\u0026rsquo;un ordinateur.\nnote\nSource : \u0026ldquo;But how do it know?\u0026rdquo; de J. Clark Scott\nPortes et logique Article de Fredkin et Toffoli de 1981 sur la possibilit√© d\u0026rsquo;un ordinateur r√©versible d√©montr√©e par l\u0026rsquo;impl√©mentation de portes √† base de boules de billard¬†!\nPlus r√©cemment, une √©quipe japonaise a remplac√© les boules de billard par des crabes\u0026hellip;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/meca/paraboles/",
	"title": "Paraboles",
	"tags": [],
	"description": "",
	"content": " Chutes libres et paraboles "
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique2/",
	"title": "Propositions - S√©mantique",
	"tags": [],
	"description": "",
	"content": " info\nNotes de lecture du livre La logique pas √† pas de Jacques Duparc que je paraphrase all√©grement.\nCalcul des propositions Syntaxe S√©mantique Preuve S√©mantique Mod√®le La s√©mantique concerne l\u0026rsquo;interpr√©tation des formules. Le calcul propositionnel est tr√®s frustre √† cet √©gard. Une variable propositionnelle vaut soit 1, soit 0, c\u0026rsquo;est-√†-dire qu\u0026rsquo;elle est soit vraie, soit fausse. Et il en est de m√™me pour une formule dont l\u0026rsquo;interpr√©tation se d√©duit de celles des variables et des r√®gles qu\u0026rsquo;impliquent les connecteurs.\n√Ä partir de l\u0026rsquo;ensemble des variables $VAR$, on peut construire une fonction $\\delta$ qui associe une valeur de v√©rit√© √† certains √©l√©ments de $VAR$¬†:\n$\\delta :\\;VAR\\rightarrow\\set{0,1}$\n$\\delta$ est une distribution de valeurs de v√©rit√©.\n$\\delta$ permet de d√©finir un mod√®le $\\mathcal{M}$ du calcul propositionnel.\nImaginons qu\u0026rsquo;un ensemble de formules ne poss√®dent que deux variables $P$ et $Q$. On a alors 4 mod√®les diff√©rents possibles¬†:\n$\\mathcal{M}_1$ pour lequel la distribution de valeur de v√©rit√© $\\delta_1$ est d√©finie par¬†: $\\delta_1(P) = 0$ et $\\delta_1(Q)=0$ $\\mathcal{M}_2$ pour lequel la distribution de valeur de v√©rit√© $\\delta_2$ est d√©finie par¬†: $\\delta_2(P) = 0$ et $\\delta_2(Q)=1$ $\\mathcal{M}_3$ pour lequel la distribution de valeur de v√©rit√© $\\delta_3$ est d√©finie par¬†: $\\delta_3(P) = 1$ et $\\delta_3(Q)=0$ $\\mathcal{M}_4$ pour lequel la distribution de valeur de v√©rit√© $\\delta_4$ est d√©finie par¬†: $\\delta_4(P) = 1$ et $\\delta_4(Q)=1$ Pour un mod√®le $\\mathcal{M}$, si $\\delta(P)=1$, on note $\\mathcal{M}\\models P$. Cela signifie que $P$ est vraie dans le mod√®le $\\mathcal{M}$.\n√Ä linverse, si $\\delta(P)=0$, on √©crira $\\mathcal{M}\\not\\models P$.\nSi $\\mathcal{M}\\not\\models P$ alors $\\mathcal{M}\\models \\neg P$.\nExemple :\nsi $\\mathcal{M}\\models P,\\neg Q,R$, cela signifie que dans le mod√®le $\\mathcal{M}$, $P$ et $R$ sont vraies, mais $Q$ est fausse.\n√âvaluation des formules Pour √©valuer des formules, il faut pouvoir √©tendre la fonction de distribution de valeurs de v√©rit√© des seules variables propositionnelles √† toutes les formules de $\\mathcal{F}$. $\\delta$ devient alors $\\delta_\\mathcal{F}$ et on construit $\\delta_\\mathcal{F}$ √† partir de $\\delta$ en utilisant les r√®gles suivantes¬†:\n$\\delta_\\mathcal{F}(\\phi) = \\delta(\\phi)$ si $\\phi$ est une variable propositionnelle. $\\delta_\\mathcal{F}(\\neg\\phi) = 1- \\delta_\\mathcal{F}(\\phi)$ (non $\\phi$ est vraie seulement si $\\phi$ est fausse). $\\delta_\\mathcal{F}(\\phi \\lor\\psi) = \\max(\\delta_\\mathcal{F}(\\phi) ,\\delta_\\mathcal{F}(\\psi) ) $ ($\\phi \\lor\\psi$ est toujours vraie sauf lorsque les deux sous-formules $\\phi$ et $\\psi$ sont toutes deux fausses). $\\delta_\\mathcal{F}(\\phi \\land \\psi) = \\delta_\\mathcal{F}(\\phi) \\cdot \\delta_\\mathcal{F}(\\psi)$ ($\\phi \\land\\psi$ est toujours fausse sauf lorsque les deux sous-formules $\\phi$ et $\\psi$ sont toutes deux vraies). $\\delta_\\mathcal{F}(\\phi \\rightarrow \\psi) = \\max(1- \\delta_\\mathcal{F}(\\phi) ,\\delta_\\mathcal{F}(\\psi) )$ ($\\phi \\rightarrow \\psi$ est toujours vraie sauf quand √† la fois $\\phi$ est vraie et $\\psi$ est fausse). $\\delta_\\mathcal{F}(\\phi \\leftrightarrow \\psi) = \\max(\\delta_\\mathcal{F}(\\phi) \\cdot\\delta_\\mathcal{F}(\\psi), (1-\\delta_\\mathcal{F}(\\phi) )\\cdot(1-\\delta_\\mathcal{F}(\\psi) ) )$ ($\\phi \\leftrightarrow \\psi$ est vraie lorsque $\\phi$ et $\\psi$ ont la m√™me valeur de v√©rit√©). Avec nos petits coloriages, on peut maintenant s\u0026rsquo;atteler √† la valeur de v√©rit√© d\u0026rsquo;une formule complexe dans un mod√®le donn√©.\nSoit le mod√®le $\\mathcal{M}$ d√©fini par $\\mathcal{M}\\models P,\\neg Q, R$ et la formule $\\phi$ qu\u0026rsquo;on s\u0026rsquo;est amus√© √† lin√©ariser √† la fin du chapitre sur la syntaxe. On va partir des feuilles et remonter jusqu\u0026rsquo;√† la racine de l\u0026rsquo;arbre. Comme la racine est fausse, la formule $\\phi$ est fausse dans ce mod√®le¬†: $\\mathcal{M}\\not\\models\\phi$.\nLa formule n\u0026rsquo;est d\u0026rsquo;ailleurs vraie que dans un seul des 8 mod√®les possibles. Si la hauteur $n$ de la formule est grande, le coloriage devient vite impraticable car trop long. En imaginant que tous les op√©rateurs sont binaires et que chaque branche est de longueur $n$, cela nous demanderait le coloriage de $0+2+2^2+\\ldots+2^{n-1}=2^n-1$ nouveaux n≈ìuds. Avec une formule de hauteur 31, ce qui ne semble pas d√©lirant, on pourrait se retrouver √† devoir colorier plus d\u0026rsquo;un milliard de n≈ìuds\u0026hellip;\nFormules logiquement √©quivalentes Une formule $\\phi$ est satisfaite dans un mod√®le $\\mathcal{M}$ si elle est vraie dans ce mod√®le. La distribution de valeur de v√©rit√© $\\delta$ qui caract√©rise $\\mathcal{M}$ vaut alors 1.\n$\\mathcal{M}\\models\\phi$ signifie que $\\delta_\\mathcal{F}(\\phi) = 1$ et $\\mathcal{M}\\not\\models\\phi$ signifie que $\\delta_\\mathcal{F}(\\phi) = 0$.\nDeux formules $\\phi$ et $\\psi$ sont deux formules √©quivalentes ($\\phi\\equiv\\psi$) ssi elles sont satisfaites dans les m√™mes mod√®les (m√™mes valeurs de v√©rit√© dans les m√™mes mod√®les).\ninfo\n$\\equiv$ est une √©quivalence s√©mantique alors que $\\leftrightarrow$ est une √©quivalence syntaxique, parfois appel√©e √©quivalence mat√©rielle.\n$\\phi\\equiv\\psi$ ssi (pour tout mod√®le $\\mathcal{M}$, $\\mathcal{M}\\models \\phi\\leftrightarrow \\psi$).\nPreuve :\nSi les deux formules sont √©quivalentes dans un mod√®le, alors $\\delta_\\mathcal{F}(\\phi)=\\delta_\\mathcal{F}(\\psi)$ dans ce mod√®le. Et par cons√©quent $\\delta_\\mathcal{F}(\\phi\\leftrightarrow\\psi)=1$. Inversement, si la formule $\\phi\\leftrightarrow\\psi$ est vraie dans tout mod√®le, les valeurs que prennent $\\phi$ et $\\psi$ dans ces mod√®les sont les m√™mes. La relation est\u0026nbsp;:\nr√©flexive : $\\phi\\equiv\\phi$ sym√©trique : $\\phi\\equiv\\psi$ ssi $\\psi\\equiv\\phi$ transitive : Si ($\\phi\\equiv\\psi$ et $\\psi \\equiv\\theta$) alors $\\psi\\equiv\\theta$ Donc il s'agit bien d'une relation d'√©quivalence.\nTh√©orie et cons√©quence s√©mantique Une th√©orie $\\mathcal{T}$ du calcul des propositions est un ensemble de formules¬†: $\\mathcal{T} \\sube \\mathcal{F}$.\n$\\mathcal{T}$ est satisfaite dans le mod√®le $\\mathcal{M}$ ($\\mathcal{M}$ est un mod√®le de $\\mathcal{T}$), not√© $\\mathcal{M}\\models\\mathcal{T}$, si pour toute formule $\\phi$ de $\\mathcal{T}$, $\\mathcal{M}\\models\\phi$. $\\mathcal{T}$ est satisfaisable ou consistante s'il existe au moins un mod√®le $\\mathcal{M}$ tel que $\\mathcal{M}\\models\\mathcal{T}$. $\\mathcal{T}$ est inconsistante si $\\mathcal{T}$ n'est pas satisfaisable. Une th√©orie est inconsistante lorsqu\u0026rsquo;elle se contredit. Elle dit alors quelque chose et son contraire, ce qui ne peut √™tre v√©rifi√© dans aucun mod√®le.\nComparons maintenant deux th√©ories $\\mathcal{T}$ et $\\mathcal{T\u0026rsquo;}$¬†:\n$\\mathcal{T'}$ est une cons√©quence s√©mantique de $\\mathcal{T}$, not√© $\\mathcal{T} \\models \\mathcal{T'}$ si tout mod√®le satisfaisant $\\mathcal{T}$ satisfait aussi $\\mathcal{T'}$. $\\mathcal{T}$ et $\\mathcal{T'}$ sont deux th√©ories √©quivalentes, not√© $\\mathcal{T} \\equiv \\mathcal{T'}$, si elles ont exactement les m√™mes mod√®les. Plus une th√©orie raconte de choses, moins elle poss√®de de mod√®les. Et si elle raconte trop de choses, elle finit par devenir inconsistante car plus aucun mod√®le ne peut la satisfaire.\nOn √©crit $\\mathcal{T}\\models\\phi$ si la th√©orie $\\mathcal{T\u0026rsquo;}$ se r√©duit au singleton $\\set{\\phi}$.\nEt $\\models\\phi$ est un raccourci pour $\\empty \\models \\phi$ ce qui signifie que $\\phi$ est une cons√©quence logique de la th√©orie vide. Or la th√©orie vide ne dit rien et est donc satisfaite dans tous les mod√®les. Cela revient donc √† dire que $\\phi$ est vraie dans tous les mod√®les. $\\phi$ est appel√©e une tautologie.\n$\\mathcal{T}\\models\\mathcal{T\u0026rsquo;}$ ssi l\u0026rsquo;ensemble des mod√®les de $\\mathcal{T}$ est contenu dans l\u0026rsquo;ensemble des mod√®les de $\\mathcal{T\u0026rsquo;}$.\nEn effet, $\\mathcal{T\u0026rsquo;}$ est une cons√©quence s√©mantique de $\\mathcal{T}$ si elle est vraie partout o√π $\\mathcal{T}$ est vraie.\nDeux th√©ories √©quivalentes ont exactement les m√™mes mod√®les ($\\mathcal{T}\\equiv \\mathcal{T\u0026rsquo;}$ correspond √† avoir √† la fois $\\mathcal{T}\\models \\mathcal{T\u0026rsquo;}$ et $\\mathcal{T\u0026rsquo;}\\models \\mathcal{T}$), ce qui revient √† dire qu\u0026rsquo;il n\u0026rsquo;existe pas de mod√®le pouvant les discriminer. Elles ne sont pas n√©cessairement √©gales (pas n√©cessairement le m√™me ensemble de formules), mais elles sont √©gales sur le plan s√©mantique puisqu\u0026rsquo;elles signifient la m√™me chose.\ninfo\nLa notion de cons√©quence s√©mantique est la version s√©mantique de la notion de d√©duction. Dire qu\u0026rsquo;une formule est une cons√©quence s√©mantique d\u0026rsquo;une th√©orie, c\u0026rsquo;est affirmer que partout o√π la th√©orie est satisfaite (c.-√†-d. quand les hypoth√®ses sont vraies), la formule l\u0026rsquo;est √©galement. La formule d√©coule donc de la th√©orie.\nSi $\\mathcal{T}$ est satisfaisable et $\\mathcal{T'}\\sube\\mathcal{T}$, alors $\\mathcal{T'}$ est √©galement satisfaisable.\nEn restreignant une th√©orie, on conserve la non contradiction.\nLe contraire n'est √©videmment pas vrai. Si on √©tend une th√©orie (en ajoutant de nouvelles formules), on diminue le nombre de mod√®les qui la satisfait. Et on accro√Æt ce nombre de mod√®les en enlevant des formules √† la th√©orie, pour atteindre √† la limite tous les mod√®les possibles lorsque la th√©orie devient vide. Si $\\mathcal{T'}$ est inconsistante et $\\mathcal{T'}\\sube\\mathcal{T}$, alors $\\mathcal{T}$ est √©galement inconsistante.\n√âtendre une th√©orie inconsistante ne peut pas la rendre satisfaisable. Si $\\mathcal{T'}\\models\\phi$ et $\\mathcal{T'}\\sube\\mathcal{T}$, alors $\\mathcal{T}\\models\\phi$. Si $\\phi$ est la cons√©quence d'une th√©orie, alors elle est la cons√©quence de toute th√©orie qui √©tend celle-ci (puisque le nombre de mod√®les satisfaisant cette th√©orie √©tendue s'est r√©duit et qu'on ne peut donc pas y trouver un nouveau mod√®le o√π $\\phi$ deviendrait fausse). $\\mathcal{T}$ est inconsistante ssi $\\mathcal{T}\\models\\phi\\land\\neg\\phi$. $\\mathcal{T}$ est inconsistante ssi pour toute formule $\\phi$, $\\mathcal{T}\\models\\phi$. En effet, la plus grande th√©orie possible ($\\mathcal{F}$) contient toute formule $\\phi$ et sa n√©gation. $\\mathcal{T}\\models\\phi$ ssi $\\mathcal{T}\\cup\\set{\\neg\\phi}$ est inconsistante.\nEn effet, dire que $\\phi$ est cons√©quence s√©mantique de $\\mathcal{T}$, c'est dire que $\\phi$ est vraie dans tous les mod√®les de $\\mathcal{T}$ et donc que $\\neg\\phi$ est fausse dans tous les mod√®les de $\\mathcal{T}$. Impossible donc pour un mod√®le de satisfaire √† la fois $\\mathcal{T}$ et $\\neg\\phi$.\n√Ä l'inverse, si $\\mathcal{T}\\cup\\set{\\neg\\phi}$ est inconsistante, alors $\\neg\\phi$ est fausse dans tous les mod√®les de $\\mathcal{T}$ et donc $\\phi$ est vraie dans tous les mod√®les de $\\mathcal{T}$. D'o√π $\\mathcal{T}\\models\\phi$. $\\mathcal{T}\\cup\\set{\\phi}\\models\\psi$ ssi $\\mathcal{T}\\models\\phi\\rightarrow\\psi$.\nLa seule possibilit√© pour que $\\phi\\rightarrow\\psi$ soit fausse est si √† la fois $\\phi$ est vraie et $\\psi$ est fausse. Or si $\\mathcal{T}\\cup\\set{\\phi}\\models\\psi$, cela signifie que si $\\phi$ est vraie dans un mod√®le, alors $\\psi$ est n√©cessairement vraie dans ce m√™me mod√®le.\n√Ä l'inverse, si $\\phi\\rightarrow\\psi$ est cons√©quence de $\\mathcal{T}$, alors $\\psi$ est vraie dans tous les mod√®les de $\\mathcal{T}\\cup\\set{\\phi}$ puisque $\\phi\\rightarrow\\psi$ est vraie dans tous les mod√®les de $\\mathcal{T}$.\nEn d'autres termes, d√©duire $\\phi\\rightarrow\\psi$ de nos hypoth√®ses $\\mathcal{T}$, c'est la m√™me chose que d√©duire $\\psi$ en faisant l'hypoth√®se suppl√©mentaire qu'on a $\\phi$. Substitution de sous formules √©quivalentes Dans une formule, substituer une sous-formule par une autre revient √† retirer toutes la partie de l\u0026rsquo;arbre qui descend d\u0026rsquo;un n≈ìud et la remplacer par un autre sous-arbre.\nSi la sous-formule \u0026ldquo;greff√©e\u0026rdquo; est √©quivalente √† la sous-formule retir√©e, alors la nouvelle formule est √©quivalente √† l\u0026rsquo;originale.\nL\u0026rsquo;int√©r√™t des substitutions est de simplifier la formule pour rendre son interpr√©tation plus facile.\nUne simplification forte (bien qu\u0026rsquo;elle agrandisse l\u0026rsquo;arbre) consiste √† restreindre le nombre de symboles utilis√©s.\nOn peut ainsi toujours transformer une formule $\\phi$ par une formule $\\phi_{\\neg,\\lor,\\land}\\equiv\\phi$ avec tous ses connecteurs logiques dans $\\set{\\neg,\\lor,\\land}$.\nIl suffit en effet de substituer les n≈ìuds $\\rightarrow$ et $\\leftrightarrow$¬†:\nOn peut aller encore plus loin en transformant une formule $\\phi$ par une formule $\\phi_{\\neg,\\lor}\\equiv\\phi$ avec tous ses connecteurs logiques dans $\\set{\\neg,\\lor}$.\nOu bien en transformant une formule $\\phi$ par une formule $\\phi_{\\neg,\\land}\\equiv\\phi$ avec tous ses connecteurs logiques dans $\\set{\\neg,\\land}$.\nOn utilise pour cela les deux substitutions suivantes¬†:\nDans l\u0026rsquo;exemple suivant, on d√©termine une formule √©quivalente avec $\\neg$ et $\\lor$ comme seuls connecteurs logiques (et on √©limine les doubles n√©gations dans la derni√®re √©tape.\nJeux d\u0026rsquo;√©valuation Ernst Zermelo a d√©montr√© que les jeux tour √† tour √† deux joueurs finis, √† information parfaite (pas comme la bataille navale), sans hasard, et sans match nul, sont tous d√©termin√©s.\nCela signifie qu\u0026rsquo;un des deux joueurs a une strat√©gie gagnante.\nPour pouver ce r√©sultat, on va repr√©senter toutes les configurations possibles dans le jeu par un arbre dont les n≈ìuds sont √©tiquet√©s par un des deux joueurs (0 ou 1) et les ar√™tes sont ses coups possibles depuis cette position. Les feuilles de l\u0026rsquo;arbre correspondent √† des positions gagnantes, soit pour 0, soit pour 1. Puis on va d√©crire un algorithme permettant de d√©terminer lequel des deux joueurs a une strat√©gie gagnante sur une position donn√©e.\nOn commence par colorier en rouge les feuilles gagnantes pour 0 et en vert les feuilles gagnantes pour 1.\nEnsuite on regarde les pr√©d√©cesseurs de ces feuilles¬†:\ns\u0026rsquo;il s\u0026rsquo;agit d\u0026rsquo;une position de 0, alors on le colorie en rouge si au moins un des successeurs est rouge car alors 0 peut y aller et gagner. on le colorie en vert si tous ses successeurs sont verts (car 0 n\u0026rsquo;a alors pas d\u0026rsquo;autre choix que d\u0026rsquo;aller sur une position gagnante de 1 et lui donner la victoire). s\u0026rsquo;il s\u0026rsquo;agit d\u0026rsquo;une position de 1, alors on inverse le raisonnement¬†: on le colorie en vert si au moins un des successeurs est vert car alors 1 peut y aller et gagner. on le colorie en rouge si tous ses successeurs sont rouges (car 1 n\u0026rsquo;a alors pas d\u0026rsquo;autre choix que d\u0026rsquo;aller sur une position gagnante de 0 et lui donner la victoire). On r√©p√®te ensuite la proc√©dure avec les pr√©d√©cesseurs des n≈ìuds que l\u0026rsquo;on vient de colorer jusqu\u0026rsquo;√† remonter √† la racine.\nComme on finira fatalement par colorier ainsi chaque n≈ìud de l\u0026rsquo;arbre, on prouve qu\u0026rsquo;il existe au moins une strat√©gie gagnante pour chacun des sous-arbres et pour le jeu complet.\nUn petit exemple¬†:\n√âtant donn√© un mod√®le $\\mathcal{M}$ et une formule $\\phi \\in \\mathcal{F}$, la valeur que prend la formule dans le mod√®le peut √™tre obtenue gr√¢ce √† un jeu invent√© par Jaakko Hintikka, not√© $\\mathbb{E}v(\\mathcal{M},\\phi)$ et appel√© jeu d\u0026rsquo;√©valuation.\n$\\phi$ doit √™tre √©crite avec seulement les connecteurs logiques $\\neg$, $\\land$ et $\\lor$ (ce qui, comme on l\u0026rsquo;a vu, est toujours possible).\nLes deux joueurs s\u0026rsquo;appellent le V√©rificateur (V) dont le but est de prouver que la formule est satisfaite dans le mod√®le ($\\mathcal{M}\\models\\phi$) et le Falsificateur (F) qui doit montrer que la formule est fausse ($\\mathcal{M}\\not\\models\\phi$).\nL\u0026rsquo;arbre du jeu correspond √† l\u0026rsquo;arbre de la formule. Si un n≈ìud est une disjonction $\\lor$, c\u0026rsquo;est au tour de V et si c\u0026rsquo;est une conjonction $\\land$, c\u0026rsquo;est au tour de F. Si le n≈ìud est une n√©gation $\\neg$, les deux joueurs √©changent leur r√¥le. De plus, les propositions vraies correspondent √† des positions gagnantes pour V et les propositions fausses sont gagnantes pour F.\nGr√¢ce √† ces r√®gles, on obtient le th√©or√®me suivant¬†:\n$\\mathcal{M}\\models\\phi$ ssi V a une strat√©gie gagnante dans $\\mathbb{E}v(\\mathcal{M},\\phi)$. $\\mathcal{M}\\models\\neg\\phi$ ssi F a une strat√©gie gagnante dans $\\mathbb{E}v(\\mathcal{M},\\phi)$. Prouvons-le par induction sur la hauteur de la formule $\\phi$¬†:\nhauteur $0$\u0026nbsp;: correspond √† une formule $\\phi$ r√©duite √† une variable propositionnelle. On v√©rifie alors tout de suite 1. et 2.. hauteur $n+1$\u0026nbsp;: si $\\phi=\\neg\\psi$, alors l'hypoth√®se d'induction op√®re sur $\\psi$.\nLe jeu d'√©valuation impliquant $\\psi$ est le m√™me que celui impliquant $\\phi$ √† un changement de r√¥le pr√®s. Si $\\psi$ est vraie, alors V a une strat√©gie gagnante dans $\\mathbb{E}v(\\mathcal{M},\\psi)$ (par hypoth√®se d'induction) et donc F a une strat√©gie gagnante dans $\\mathbb{E}v(\\mathcal{M},\\phi)$ (par la r√®gle de changement de r√¥le). Une $\\phi$ fausse donne bien une strat√©gie gagnante pour F. Et inversement, une $\\phi$ vraie donnera bien une strat√©gie gagnante pour V. si $\\phi=\\psi_1\\land\\psi_2$, alors l'hypoth√®se d'induction op√®re sur $\\psi_1$ et $\\psi_2$.\nPuisqu'on est sur un $\\land$, les r√®gles du jeu stipulent que c'est au tour de F de jouer.\n$\\phi$ vraie implique $\\psi_1$ et $\\psi_2$ vraie, ce qui donne une strat√©gie gagnante pour V puisque les deux branches sont gagnantes. Et √† l'inverse, si $\\phi$ est fausse, cela implique qu'au moins une des sous-formules soit fausse et donc que F dispose d'au moins une branche avec une strat√©gie gagnante ce qui assure que le n≈ìud $\\land$ est gagnant pour F. si $\\phi=\\psi_1\\lor\\psi_2$, alors l'hypoth√®se d'induction op√®re sur $\\psi_1$ et $\\psi_2$.\nC'est au tour de V de jouer d'apr√®s les r√®gles. Il n'a besoin que d'une branche gagnante pour rendre le n≈ìud victorieux. Or si $\\phi$ est vraie, c'est bien le cas\u0026nbsp;: au moins une des sous-formules est vraie et donc au moins une branche victorieuse s'ouvre √† V.\nEt une $\\phi$ fausse implique que les deux sous-formules sont fausses aussi et donc victorieuses pour F, ce qui donne le n≈ìud $\\lor$ √† F. Exemple¬†: consid√©rons la formule $\\bar{\\phi}=\\neg (( \\neg( \\neg P \\land (( Q\\land ( R \\land \\neg P )) \\lor ( Q \\land \\neg P ))) \\land ( P \\land ( R \\lor \\neg R )) \\lor ( ( R \\land Q ) ))$ dont l\u0026rsquo;arbre est repr√©sent√© ci-dessous. Et on se donne le mod√®le $\\mathcal{M}\\models P,Q,\\neg R$\nOn commence par colorier les feuilles conform√©ment au mod√®le, puis on remplace chaque occurrence de $\\lor$, $\\land$, $\\neg$ respectivement par les symboles V, F et ‚áî.\nEnsuite on s\u0026rsquo;occupe des changements de r√¥le¬†: si la branche connectant un n≈ìud √† la racine contient un nombre impair d\u0026rsquo;inversions, on intervertit l\u0026rsquo;√©tiquette du n≈ìud, et sinon on la laisse telle quelle.\nOn peut maintenant retirer les symboles d\u0026rsquo;inversion et chercher lequel de V ou F a une strat√©gie gagnante en remontant depuis les feuilles.\nVictoire au Falsificateur F¬†! D\u0026rsquo;apr√®s le th√©or√®me, on a donc $\\mathcal{M}\\not\\models\\phi$.\nLes trois strat√©gies gagnantes possibles pour le Falsificateur sont les suivantes¬†:\nTable de v√©rit√© Une table de v√©rit√© est un tableau regroupant de mani√®re synth√©tique tous les mod√®les d\u0026rsquo;une fomrule donn√©e.\nSi la formule poss√®de $n$ variables propositionnelles, le tableau contient $2^n$ lignes, une ligne par mod√®le.\nExemple¬†:\nConstruisons la table de v√©rit√© de $\\phi := \\left(\\left(P\\land\\neg Q\\right)\\rightarrow\\left(\\neg\\left( P\\lor R\\right) \\leftrightarrow\\neg Q\\right)\\right)$\nTautologies et contradictions Comme on l'a vu plus haut, $\\phi$ est une tautologie si elle est vraie dans tous les mod√®les, ce qu'on note $\\models \\phi$. On √©crit alors aussi¬†: $\\phi \\equiv \\top$\nEt rappelons le lien √©troit entre la notion de tautologie et celle d\u0026rsquo;√©quivalence $\\leftrightarrow$¬†:\n$\\phi\\equiv \\psi$ ssi $\\phi\\leftrightarrow\\psi\\equiv\\top$ $\\phi$ est une contradiction ssi $\\phi$ est une tautologie. On note alors $\\phi\\equiv\\bot$. Une contradiction est fausse dans tout mod√®le. Elle n\u0026rsquo;a donc pas de mod√®le¬†; une contradiction ne peut jamais avoir lieu.\nTautologies importantes¬†:\nidempotence de la conjonction et de la disjonction $(P\\land P)\\leftrightarrow P$$(P\\lor P)\\leftrightarrow P$ commutativit√© de la conjonction, disjonction et de la double implication $(P\\lor Q) \\leftrightarrow (Q\\lor P)$$(P\\land Q) \\leftrightarrow (Q\\land P)$$(P\\leftrightarrow Q) \\leftrightarrow (Q\\leftrightarrow P)$ associativit√© de la disjonction, de la conjonction et de la double implication $((P\\lor Q) \\lor R)\\leftrightarrow (P\\lor(Q\\lor R))$$((P\\land Q) \\land R)\\leftrightarrow (P\\land(Q\\land R))$$((P\\leftrightarrow Q) \\leftrightarrow R)\\leftrightarrow (P\\leftrightarrow(Q\\leftrightarrow R))$ distributivit√© de la disjonction par rapport √† la conjonction et r√©ciproquement $(P\\lor (Q\\land R))\\leftrightarrow ((P\\lor Q)\\land (P\\lor R))$$(P\\land (Q\\lor R))\\leftrightarrow ((P\\land Q)\\lor (P\\land R))$ lois d'absorption $(P\\land(P\\lor Q))\\leftrightarrow P$$(P\\lor(P\\land Q))\\leftrightarrow P$ lois de De Morgan $\\neg (P\\lor Q) \\leftrightarrow (\\neg P \\land \\neg Q)$$\\neg (P\\land Q) \\leftrightarrow (\\neg P \\lor \\neg Q)$ contrapos√©e $(P\\rightarrow Q) \\leftrightarrow (\\neg Q \\rightarrow \\neg P)$ Un raisonnement consiste g√©n√©ralement √† partir d\u0026rsquo;une tautologie, la pr√©misse, et √† aboutir √† de nouvelles tautologies √† partir d\u0026rsquo;implications (une tautologie √©tant vraie dans tout mod√®le, elle implique n√©cessairement une autre tautologie).\nDans un raisonnement par l\u0026rsquo;absurde (preuve par contradiction), pour montrer que $\\phi$ est une tautologie, on suppose que $\\neg \\phi$ est vraie dans au moins un mod√®le pour aboutir √† une contradiction $\\psi$ ($\\psi\\equiv\\bot$). Dans les mod√®les o√π $\\neg \\phi$ est vraie, $\\neg \\phi \\rightarrow \\psi$ est √©galement vraie. Or $\\neg\\phi\\rightarrow\\psi\\equiv \\neg\\neg\\phi\\lor\\psi \\equiv \\phi\\lor\\bot \\equiv \\phi$. Donc dans les mod√®les o√π $\\neg\\phi$ est vraie, $\\phi$ est vraie. Par cons√©quent, il n\u0026rsquo;y a pas de mod√®le o√π $\\neg \\phi$ est vraie. D\u0026rsquo;o√π $\\phi\\equiv\\top$.\nFormes normales Supposons que l\u0026rsquo;on ait un nombre fini de variables propositionnelles $\\set{P_1,P_2,\\ldots,P_n}$.\nOn va construire une formule $\\phi$ qui n\u0026rsquo;est vraie que dans un mod√®le $\\mathcal{M}$ associ√© √† une distribution de v√©rit√© $\\delta$¬†:\n$\\displaystyle \\phi = (\\epsilon_1 P_1\\land \\epsilon_2 P_2\\land\\cdots\\land\\epsilon_n P_n) = \\bigwedge_{i=1}^n \\epsilon_i P_i$ o√π $\\epsilon_i$ d√©signe $\\neg$ si $\\delta(P_i) = 0$ et $\\not \\! \\neg$ si $\\delta(P_i) = 1$ (avec $\\not \\! \\neg P= P$).\nLa table de v√©rit√© aura bien des z√©ros partout sauf dans la ligne correspondant au mod√®le $\\mathcal{M}$.\nPassons maintenant d\u0026rsquo;une formule qui v√©rifie un seul mod√®le √† une formule qui en v√©rifie plusieurs. Choisissons $\\set{\\mathcal{M}_i:i\\in I}$ parmi les $2^n$ mod√®les possibles. En appelant $\\phi$ la formule vraie dans $\\mathcal{M}_i$, il suffit de former la formule suivante¬†:\n$\\displaystyle \\phi = \\bigvee_{i\\in I}\\phi_i$\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/geometrie/geo1/",
	"title": "Pythagore et Thal√®s",
	"tags": [],
	"description": "",
	"content": " Le triangle Pythagore Pythagore a v√©cu √† Samos (une √Æle grecque) au 6e si√®cle avant notre √®re. Malgr√© le nom qu\u0026rsquo;on lui donne, le th√©or√®me de Pythagore √©tait connu et utilis√© par les Babyloniens et les Indiens des si√®cles avant Pythagore. Mais peut-√™tre que Pythagore a √©t√© le premier √† l\u0026rsquo;introduire en Gr√®ce et cela a suffi √† lui octroyer une reconnaissance √©ternelle.\nTh√©or√®me de Pythagore¬†:\nSi un triangle est rectangle, alors le carreÃÅ de la longueur de l‚ÄôhypoteÃÅnuse est eÃÅgal aÃÄ la somme des carreÃÅs des longueurs des deux autres coÃÇteÃÅs. R√©ciproque du th√©or√®me de Pythagore¬†:\nSi dans un triangle le carreÃÅ de la longueur du plus grand coÃÇteÃÅ est eÃÅgal aÃÄ la somme des carreÃÅs des longueurs des deux autres coÃÇteÃÅs, alors ce triangle est rectangle. Contrapos√©e du th√©or√®me de Pythagore¬†:\nSi dans un triangle le carreÃÅ de la longueur du plus grand coÃÇteÃÅ n\u0026rsquo;est pas eÃÅgal aÃÄ la somme des carreÃÅs des longueurs des deux autres coÃÇteÃÅs, alors ce triangle n\u0026rsquo;est pas rectangle. Contrapos√©e de la r√©ciproque du th√©or√®me de Pythagore¬†:\nSi un triangle n\u0026rsquo;est pas rectangle, alors le carreÃÅ de la longueur du plus grand coÃÇteÃÅ n\u0026rsquo;est pas eÃÅgal aÃÄ la somme des carreÃÅs des longueurs des deux autres coÃÇteÃÅs. La d√©monstration historique par Euclide du th√©or√®me de Pythagore¬†:\nDeux d√©monstrations par d√©coupage et r√©arrangement¬†:\nCes deux d√©monstrations par d√©coupage (et toutes leurs cousines) reposent sur la possibilit√© de paver le plan avec deux carr√©s de c√¥t√©s diff√©rents.\nUne d√©monstration plus \u0026ldquo;physique\u0026rdquo; (reposant sur ce qu\u0026rsquo;implique une relation de proportionnalit√© sur une surface)¬†:\nHistoire du th√©or√®me¬†: M√©sopotamie\nLes historiens des math√©matiques et assyriologues ont d√©couvert √† la fin des ann√©es 1920 que s\u0026rsquo;√©tait forg√©e en M√©sopotamie (l\u0026rsquo;ancien Irak), √† l\u0026rsquo;√©poque pal√©o-babylonienne une culture math√©matique dont l\u0026rsquo;objet n\u0026rsquo;√©tait pas purement utilitariste.\nPlusieurs des tablettes d\u0026rsquo;argile qui ont √©t√© retrouv√©es et analys√©es montrent que la relation entre les longueurs des c√¥t√©s du rectangle et celle de sa diagonale (soit entre les longueurs des c√¥t√©s d‚Äôun triangle rectangle) √©tait connue et utilis√©e pour r√©soudre des probl√®mes calculatoires.\nDes \u0026ldquo;tablettes cadastrales\u0026rdquo; (dont la plus ancienne date de -2340 √† -2200 ) √©tablies pour le commerce ou l\u0026rsquo;administration de parcelles exposent ainsi cette connaissance. La tablette Si427 datant de -1900 √† -1600, d√©couverte √† Sippar (Irak), montre ainsi un terrain avec tour, aire de battage et mar√©cage dont une des parcelles a √©t√© mise en vente par son propri√©taire. Elle pr√©sente des d√©coupes de trap√®zes et de triangles rectangles ainsi que quelques triplets pythagoriciens.\nLa tablette Plimpton 322 datant de vers -1800 donne une liste ordonn√©e de nombres associ√©s √† des triplets pythagoriciens, soit des entiers (a, b, c) satisfaisant la relation a2 + b2 = c2. La tablette ne donne que deux nombres du triplet, mais les associe explicitement au plus petit c√¥t√© et √† la diagonale d\u0026rsquo;un rectangle. La premi√®re colonne ne contient pas un de 3 √©l√©ments du triplet mais une combinaison des 3 dont l\u0026rsquo;interpr√©tation varie selon les hypoth√®ses et les reconstitutions des parties manquantes. Ce pourrait √™tre une tablette d\u0026rsquo;exercice pour √©tudiant mais la r√©gularit√© des √©l√©ments laissent penser √† une table. Les inscriptions sont en caract√®re cun√©iforme, en base 60 avec num√©rotation de position.\nIl n\u0026rsquo;y a pas trace de l\u0026rsquo;√©nonc√© d\u0026rsquo;un th√©or√®me, et les historiens pr√©f√®rent souvent utiliser un autre mot, certains parlent par exemple de ¬´ r√®gle de Pythagore ¬ª. Ni celle-ci, ni le principe qui la sous-tend ne sont explicitement √©nonc√©s non plus, mais les exemples montrent bien qu\u0026rsquo;une r√®gle g√©n√©rale est connue.\nLa datation et l\u0026rsquo;origine exacte des tablettes d\u0026rsquo;argile n\u0026rsquo;est pas toujours √©vidente, beaucoup de celles-ci ont √©t√© achet√©es sur le march√© des antiquit√©s comme la tablette Plimpton 322, mais les historiens peuvent s\u0026rsquo;appuyer sur des √©l√©ments linguistiques, et les similarit√©s avec celles dont l\u0026rsquo;origine est connue, ayant √©t√© obtenues par des fouilles arch√©ologiques r√©guli√®res. Les traces que l\u0026rsquo;on a des cultures ant√©rieures rendent peu vraisemblable la d√©couverte de la ¬´ r√®gle de Pythagore ¬ª, avant -2300, celle-ci pourrait appara√Ætre entre -2025 et -1825.\nInde\nEn Inde, un √©nonc√© du th√©or√®me, sous sa forme la plus g√©n√©rale, appara√Æt dans l\u0026rsquo;Apastamba, l\u0026rsquo;un des ≈öulba-S≈´tras, ces trait√©s du cordeau qui codifient les r√®gles des constructions destin√©es aux rituels v√©diques. Ceux-ci ont √©t√© r√©dig√©s entre le viiie et le ive si√®cle avant notre √®re (par ailleurs certains triplets pythagoriciens sont mentionn√©s dans des textes bien ant√©rieurs). Les Sulbasutras parlent du rectangle et de sa diagonale, plut√¥t que de triangle.\nChine\nLe th√©or√®me appara√Æt √©galement en Chine dans le Zhoubi suanjing (¬´ Le Classique math√©matique du Gnomon des Zhou ¬ª), un des plus anciens ouvrages math√©matiques chinois. Ce dernier, √©crit probablement durant la dynastie Han (206 av. J.-C. √† 220), regroupe des techniques de calcul datant de la dynastie Zhou (xe si√®cle av. J.-C. √† -256). Le th√©or√®me ou proc√©dure s‚Äô√©nonce de la mani√®re suivante¬†:\n¬´¬†En r√©unissant l‚Äôaire (mi) de la base (gou) et l‚Äôaire de la hauteur (gu), on engendre l‚Äôaire de l‚Äôhypot√©nuse.¬†¬ª\nMais la question se pose de savoir si ce th√©or√®me ‚Äî ou cette proc√©dure ‚Äî √©tait muni ou non d‚Äôune d√©monstration. Sur ce point les avis sont partag√©s. Le th√©or√®me, sous le nom de Gougu (√† partir des mots ¬´ base ¬ª et ¬´ altitude ¬ª), est repris dans le Jiuzhang suanshu (Les neuf chapitres sur l\u0026rsquo;art math√©matique, 100 av. J.-C. √† 50), avec une d√©monstration, utilisant un d√©coupage et une reconstitution, qui ne ressemble pas √† celle d‚ÄôEuclide et qui illustre l\u0026rsquo;originalit√© du syst√®me d√©monstratif chinois.\nSource : Wikip√©dia Deux vieux exos Un probl√®me babylonien Ce probl√®me a √©t√© d√©couvert sur des tablettes d‚Äôargile babylonienne entre -2000 et -1600.\nune perche est pos√©e verticalement contre un mur. Si son extr√©mit√© haute glisse de 6 unit√©s vers le bas contre le mur, de combien d‚Äôunit√© glisse horizontalement l‚Äôextr√©mit√© basse ?\nR√©ponse (cliquer pour afficher) 18 Un probl√®me chinois Un probl√®me plus difficile datant d‚Äôentre -250 et +50 (en Chine, le th√©or√®me de Pythagore se nomme Goo Gu ÂãæËÇ°).\nDans une mare, un lotus d√©passe de 10 cm √† la verticale. Un coup de vent le pousse de 60 cm et la fleur touche alors la surface. Quelle est la profondeur de la mare ?\nR√©ponse (cliquer pour afficher) 175 cm Le th√©or√®me de Pythagore peut aussi nous aider √† trouver la distance de l\u0026rsquo;horizon comme dans cette activit√©.\nEscargot de Pythagore L\u0026rsquo;escargot de Pythagore, spirale de Th√©odore ou encore spirale d\u0026rsquo;Anderhub est une figure qui permet de construire g√©om√©triquement les racines carr√©es des entiers cons√©cutifs.\nCette construction est utilis√©e dans la jolie preuve de la d√©pendance en $v^2$ de l\u0026rsquo;√©nergie cin√©tique par Johann Bernoulli.\nThal√®s Deux triangles semblables (ayant les m√™mes angles) ont leurs c√¥t√©s correspondants proportionnels. On parle de th√©or√®me de Thal√®s lorsque les deux triangles partagent un sommet.\nL\u0026rsquo;histoire de Thal√®s et de la pyramide serait en fait une l√©gende et Thal√®s de Millet n\u0026rsquo;aurait pas grand chose √† voir avec le th√©or√®me qui porte son nom.\nCertains textes de l\u0026rsquo;Antiquit√© grecque font r√©f√©rence aux travaux de Thal√®s de Milet au vie si√®cle av. J.-C., dont aucun √©crit ne nous est parvenu. Cependant, aucun texte ancien n\u0026rsquo;attribue la d√©couverte du th√©or√®me de Thal√®s √† celui-ci. L\u0026rsquo;attribution en France du th√©or√®me √† Thal√®s semble associ√©e √† la mesure de la hauteur d\u0026rsquo;une pyramide √©gyptienne que celui-ci aurait effectu√©e.\nDans son commentaire sur les √âl√©ments d\u0026rsquo;Euclide, Proclus affirme que la g√©om√©trie avait √©t√© d√©couverte en √âgypte, et transport√©e en Gr√®ce par Thal√®s apr√®s son voyage dans cette contr√©e. Selon une anecdote rapport√©e par Pline l\u0026rsquo;Ancien, Plutarque et Diog√®ne La√´rce, lors de ce voyage Thal√®s aurait obtenu la hauteur d\u0026rsquo;une des pyramides en mesurant l\u0026rsquo;ombre de celle-ci. Pour Pline de m√™me que pour Diog√®ne La√´rce (qui se r√©f√®re √† Hieronymus de Rhodes, un auteur actif au iiie si√®cle av. J.-C., ce qui est d√©j√† autour de trois si√®cles apr√®s Thal√®s), Thal√®s attend que son ombre soit √©gale √† sa taille pour mesurer l\u0026rsquo;ombre de la pyramide dont il d√©duit alors la hauteur.\n¬´ Hi√©ronyme dit que Thal√®s mesura les pyramides d'apr√®s leur ombre, ayant observ√© le temps o√π notre propre ombre √©gale notre hauteur.\u0026nbsp;¬ª La version que donne Plutarque dans Le Banquet des Sept Sages est clairement romanc√©e¬†:\n¬´ Ainsi, vous, Thal√®s, le roi d'√âgypte vous admire beaucoup, et, entre autres choses, il a √©t√©, au-del√† de ce qu'on peut dire, ravi de la mani√®re dont vous avez mesur√© la pyramide sans le moindre embarras et sans avoir eu besoin d'aucun instrument. Apr√®s avoir dress√© votre b√¢ton √† l'extr√©mit√© de l'ombre que projetait la pyramide, vous construis√Ætes deux triangles par la tangence d'un rayon, et vous d√©montr√¢tes qu'il y avait la m√™me proportion entre la hauteur du b√¢ton et la hauteur de la pyramide qu'entre la longueur des deux ombres. ¬ª La version de Plutarque fait intervenir des rapports de proportionnalit√©, et donc peut renvoyer au th√©or√®me de Thal√®s. Ce n\u0026rsquo;est pas vraiment le cas de la version plus √©l√©mentaire rapport√©e par Pline et Diog√®ne La√´rce, qui correspond tr√®s probablement √† la version originale de Hieronymus. De toute fa√ßon, comme le remarque Maurice Caveing, ¬´ il est peu vraisemblable que le souverain d\u0026rsquo;un pays qui, plus de 1 000 ans avant Thal√®s, connaissait le calcul du seqed, ait ignor√© comment mesurer la hauteur des pyramides ¬ª.\nSource : Wikip√©dia La vid√©o suivante pr√©sente une d√©monstration du th√©or√®me de Thal√®s et de sa r√©ciproque (permettant de s\u0026rsquo;assurer du parall√©lisme entre deux droites) √† partir de calculs d\u0026rsquo;aires de triangles.\nGr√¢ce au th√©or√®me de Thal√®s, on peut partager de mani√®re √©gale un segment sans faire la moindre mesure.\nPr√©sentation d\u0026rsquo;un petit probl√®me pos√© √† l\u0026rsquo;examen d\u0026rsquo;entr√©e au MIT (grande universit√© scientifique am√©ricaine) en 1869¬†:\nGr√¢ce √† Thal√®s, on peut aussi savoir si deux bateaux vont se rencontrer (on dit faire route de collision).\nSupposons qu\u0026rsquo;un bateau A avance selon un cap fixe (direction constante) √† vitesse constante et continue de voir dans la m√™me direction un bateau B, lui aussi en mouvement rectiligne uniforme (suffit de remarquer que le bateau B reste align√© avec un rep√®re fixe pris sur le bateau A). Alors les droites joignant les deux bateaux √† un instant donn√© sont parall√®les.\nAppelons C le point de croisement entre les deux trajectoires des bateaux et appelons $t_C$ et ${t\u0026rsquo;}_C$ les instants respectifs o√π les bateaux A et B arrivent en C.\nAppliquons le th√©or√®me de Thal√®s¬†:\n$$\\frac{A(t_C)A(t_i)}{A(t_C)A(t_0)}=\\frac{B({t\u0026rsquo;}_C)B(t_i)}{B({t\u0026rsquo;}_C)B(t_0)} \\Rightarrow\\frac{v_A(t_C-t_i)}{v_A(t_C-t_0)}=\\frac{v_B({t\u0026rsquo;}_C-t_i)}{v_B({t\u0026rsquo;}_C- t_0)}$$\nOn en d√©duit $t_C = {t\u0026rsquo;}_C$, ce qui implique que les deux bateaux arrivent ensemble en C. Il va y avoir collision¬†! Donc si on voit un bateau toujours dans la m√™me direction, la plus grande m√©fiance s\u0026rsquo;impose.\nDroite d\u0026rsquo;Euler "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/tqc2/",
	"title": "TQC-2",
	"tags": [],
	"description": "",
	"content": " Th√©orie quantique des champs \u0026ndash; Partie 2 note\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Tr√®s souvent une simple traduction.\nLe premier encadr√© gris est issu de No-Nonsense Classical Mechanics de Jakob Schwichtenberg.\nRetour sommaire\nRevenons dans un premier temps √† la m√©canique du 19e si√®cle, en particulier la m√©canique hamiltonienne pour constater sa similarit√© avec la m√©canique quantique avant de pr√©senter la th√©orie classique des champs.\nTh√©orie classique des champs Du Lagrangien √† l\u0026rsquo;Hamiltonien Le taux de variation du Lagrangien est donn√© par¬†:\n$$\\frac{dL}{dt} = \\frac{\\partial L}{\\partial q_i} \\dot{q}_i + \\frac{\\partial L}{\\partial \\dot{q}_i} \\ddot{q}_i$$\nEt en utilisant les √©quations d\u0026rsquo;Euler-Lagrange, on obtient¬†:\n$$\\frac{dL}{dt} = \\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{q}_i} \\right) \\dot{q}_i + \\frac{\\partial L}{\\partial \\dot{q}_i} \\ddot{q}_i = \\frac{d}{dt} \\left( \\frac{\\partial L}{\\partial \\dot{q}_i} \\dot{q}_i \\right)$$\nOn d√©finit le moment canonique conjugu√© $p_i$¬†:\n$$p_i = \\frac{\\partial L}{\\partial \\dot{q}_i}$$\nCela permet de r√©√©crire l\u0026rsquo;√©quation pr√©c√©dente en lui donnant la forme d\u0026rsquo;une √©quation de conservation¬†:\n$$ \\frac{d}{dt} (p_i \\dot{q}_i - L) = 0 $$\nOn appelle Hamiltonien $H$ la quantit√© conserv√©e et on montrera plus loin qu\u0026rsquo;elle correspond √† l\u0026rsquo;√©nergie du syst√®me¬†:\n$$H = p_i \\dot{q}_i - L$$\nLe passage de $L$ √† $H$ correspond math√©matiquement √† une transformation de Legendre.\nLa transformation de Legendre permet d\u0026rsquo;encoder diff√©remment l\u0026rsquo;information d\u0026rsquo;une fonction. En particulier, elle permet de changer la d√©pendance en une coordonn√©e en sa coordonn√©e conjugu√©e. Ici, elle va nous permettre de passer de $\\dot{q}$ √† $p$.\nImaginons une fonction $L(v)$ convexe et calculons la pente $p(v)=\\frac{\\partial L(v)}{\\partial v}$ (un fonction convexe voit sa pente cro√Ætre de mani√®re monotone ce qui implique que $p$ et $v$ sont en bijection (si la fonction est concave, il suffit de consid√©rer son oppos√©e)).\nOn peut aussi √©crire que la fonction de d√©part est la primitve de sa pente¬†: $L(v)=\\int_0^vp(v\u0026rsquo;)\\mathrm{d}v\u0026rsquo;$. $L(v)$ devient ainsi l\u0026rsquo;aire sous la courbe d√©finie par la pente $p(v)$.\nMais comme on s\u0026rsquo;est assur√© gr√¢ce √† la convexit√© de $L$ qu\u0026rsquo;√† chaque $v$ corresponde un et un unique $p$, on peut aussi consid√©rer la fonction $v(p)$.\nAppelons $H(p)$ l\u0026rsquo;aire sous la courbe de $v(p)$¬†: $H(p)\\equiv\\int_0^p v(p\u0026rsquo;)\\mathrm{d}p\u0026rsquo;$.\nOn peut voir sur le sch√©ma suivant que ces deux aires ont un lien tr√®s simple¬†: leur somme vaut $p\\times v$¬†!\nOn a ainsi $H(p) = pv - L(v)$.\nPar cons√©quent, la transform√©e de Legendre du Lagrangien $L(q,\\dot{q})$ par rapport √† $\\dot{q}$ est donn√©e par $H(q,p)=p\\dot{q}(p)-L(q,\\dot{q})$ en appelant $p$ la pente $\\frac{\\partial L}{\\partial\\dot{q}}$.\nFaisons varier $H$¬†:\n$$ \\begin{aligned} \\delta H \u0026amp;= p_i \\delta \\dot{q}_i + \\delta p_i \\dot{q}_i - \\frac{\\partial L}{\\partial q_i} \\delta q_i - \\frac{\\partial L}{\\partial \\dot{q}_i} \\delta \\dot{q}_i\\\\ \u0026amp;= \\delta\\dot{q}_i\\left(\\cancel{p_i - \\frac{\\partial L}{\\partial \\dot{q}_i}}\\right)+ \\delta p_i \\dot{q}_i - \\frac{\\partial L}{\\partial q_i} \\delta q_i \\\\ \u0026amp;=\\delta p_i \\dot{q}_i -\\frac{\\partial L}{\\partial q_i}\\delta q_i \\\\ \u0026amp;=\\delta p_i {\\color{#D41876}\\dot{q}_i} - {\\color{#0076BA}\\dot{p}_i}\\delta q_i \\end{aligned} $$\nEt comme $H$ ne d√©pend que de $q_i$ et $p_i$, on a¬†:\n$$ \\delta H = {\\color{#0076BA}\\frac{\\partial H}{\\partial q_i}} \\delta q_i + {\\color{#D41876}\\frac{\\partial H}{\\partial p_i}} \\delta p_i $$\nPar identification, on obtient les √©quations d\u0026rsquo;Hamilton qui permettent de d√©terminer les √©quations du mouvement du syst√®me d\u0026rsquo;une nouvelle fa√ßon¬†:\n$$ {\\color{#D41876} \\frac{\\partial H}{\\partial p_i} = \\dot{q}_i} \\qquad {\\color{#0076BA}\\frac{\\partial H}{\\partial q_i} = -\\dot{p}_i} $$\nD√©finissons maintenant le crochet de Poisson $\\{A,B\\}$¬†:\n$$ \\{A, B\\} = \\frac{\\partial A}{\\partial q_i} \\frac{\\partial B}{\\partial p_i} - \\frac{\\partial A}{\\partial p_i} \\frac{\\partial B}{\\partial q_i} $$\nPuis consid√©rons une fonction $F$ des coordonn√©es g√©n√©ralis√©es $q_i$ et $p_i$. Le taux de variation de $F$ est donn√© par¬†:\n$$ \\begin{aligned} \\frac{\\mathrm{d}F}{\\mathrm{d}t} \u0026amp;= \\frac{\\partial F}{\\partial t} + \\frac{\\partial F}{\\partial q_i}{\\color{#0076BA}\\dot{q}_i} + \\frac{\\partial F}{\\partial p_i}{\\color{#D41876}\\dot{p}_i}\\\\ \u0026amp;= \\frac{\\partial F}{\\partial t} + \\frac{\\partial F}{\\partial q_i}\\left({\\color{#0076BA}-\\frac{\\partial H}{\\partial p_i}}\\right) + \\frac{\\partial F}{\\partial p_i}{\\color{#D41876}\\frac{\\partial H}{\\partial q_i}}\\\\ \u0026amp;=\\frac{\\partial F}{\\partial t} + \\{F, H\\} \\end{aligned} $$\nEt si $F$ n\u0026rsquo;est pas une fonction du temps¬†:\n$$ \\frac{dF}{dt} = \\{F, H\\} $$\nDonc si $\\{F, H\\}=0$, alors $F$ est une constante du mouvement¬†!\nIl y a une interpr√©tation g√©om√©trique √† cette relation.\nSi $F=F(\\boldsymbol{q},\\boldsymbol{p})$, alors le vecteur $(\\dot{\\boldsymbol{q}},\\dot{\\boldsymbol{p}})=\\left(\\frac{\\partial H}{\\partial\\boldsymbol{p}},-\\frac{\\partial H}{\\partial \\boldsymbol{q}}\\right)$ est tangent √† la surface $F(\\boldsymbol{q},\\boldsymbol{p})=$ constante. En effet $\\boldsymbol{\\nabla} F \\cdot (\\dot{\\boldsymbol{q}}, \\dot{\\boldsymbol{p}}) = \\left(\\frac{\\partial F}{\\partial \\boldsymbol{q}},\\frac{\\partial F}{\\partial \\boldsymbol{p}}\\right)\\cdot (\\dot{\\boldsymbol{q}},\\dot{\\boldsymbol{p}})= \\frac{\\partial F}{\\partial q_i} \\frac{\\partial H}{\\partial p_i} - \\frac{\\partial F}{\\partial p_i} \\frac{\\partial H}{\\partial q_i}=\\{F,H\\}=0$.\nEt de m√™me, $(\\dot{\\boldsymbol{q}},\\dot{\\boldsymbol{p}})$ est tangent √† la surface $H(\\boldsymbol{q},\\boldsymbol{p})=$ constante puisque $\\boldsymbol{\\nabla} H \\cdot (\\dot{\\boldsymbol{q}}, \\dot{\\boldsymbol{p}}) = \\left(\\frac{\\partial H}{\\partial \\boldsymbol{q}},\\frac{\\partial H}{\\partial \\boldsymbol{p}}\\right)\\cdot (\\dot{\\boldsymbol{q}},\\dot{\\boldsymbol{p}})= -\\dot{\\boldsymbol{p}}\\cdot\\dot{\\boldsymbol{q}}+\\dot{\\boldsymbol{q}}\\cdot\\dot{\\boldsymbol{p}}=0$\nLe lien entre crochets de Poisson et loi de conservation rappelle bien s√ªr le r√¥le du commutateur en m√©canique quantique.\nLe taux de variation de l\u0026rsquo;esp√©rance quantique de l\u0026rsquo;op√©rateur $\\hat{F}$ est en effet donn√© par¬†:\n$$ \\frac{d \\langle \\hat{F} \\rangle}{dt} = \\frac{1}{i \\hbar} \\langle [\\hat{F}, \\hat{H}] \\rangle $$\nLe parall√©lisme entre m√©canique quantique et formalisme hamiltonien pousse √† jeter un pont entre les deux¬†:\n$$ \\{ A, B \\} \\rightarrow \\frac{1}{\\mathrm{i} \\hbar} \\langle [\\hat{A}, \\hat{B}] \\rangle $$\nV√©rifions le parall√®le pour le couple (position, moment conjugu√©)¬†:\n$$ \\begin{aligned} \\{ q_j, p_k \\} \u0026amp;= \\frac{\\partial q_j}{\\partial q_i}\\frac{\\partial p_k}{\\partial p_i}-\\frac{\\partial p_k}{\\partial q_i}\\frac{\\partial q_j}{\\partial p_i}\\\\ \u0026amp;= \\delta_{ij}\\delta_{ik}-0\\times 0\\\\ \u0026amp;= \\delta_{jk}\\\\ \\end{aligned} $$\nCe qui donnerait avec le pont $[\\hat{q}_j,\\hat{p}_k]=\\mathrm{i}\\hbar\\delta_{jk}$ qui est bien le commutateur quantique entre position et impulsion.\nEn relativit√© restreinte Cherchons le Lagrangien d\u0026rsquo;une particule libre de masse $m$ dans le cadre de la relativit√© restreinte.\nL\u0026rsquo;action $S=\\int_{t_1}^{t_2} L \\mathrm{d}t$ de la particule se doit d\u0026rsquo;√™tre invariante de Lorentz. L\u0026rsquo;introduction du temps propre $\\mathrm{d}\\tau=\\frac{\\mathrm{d}t}{\\gamma}$ (le seul v√©ritable invariant) semble s\u0026rsquo;imposer¬†: $S = \\int_{\\tau_1}^{\\tau_2}L\\gamma\\mathrm{d}\\tau$.\n$L\\gamma$ doit alors √† son tour √™tre invariant de Lorentz et on ne voit pas vraiment d\u0026rsquo;autre possibilit√© que d\u0026rsquo;√™tre constant, d\u0026rsquo;o√π $L=\\frac{K}{\\gamma}$ o√π $K$ est une constante. Or on sait qu\u0026rsquo;aux petites vitesses, on doit retrouver $L=\\frac{1}{2}mv^2\\, (+\\text{cste})$, et comme $\\gamma^{-1} \\approx 1 - \\frac{1}{2} \\frac{v^2}{c^2} $ lorsque $v\\ll c$, on obtient $K\\left(1-\\frac{1}{2}\\left(\\frac{v^2}{c^2}\\right)\\right)=\\frac{1}{2}mv^2+\\text{cste}$. Cela impose $K=-mc^2$. Et finalement¬†:\n$$ S=-mc^2\\int_{\\tau_1}^{\\tau_2}\\mathrm{d}\\tau=-mc\\int_a^b \\mathrm{d}s $$\navec $\\mathrm{d}s=\\sqrt{c^2\\mathrm{d}t^2-\\mathrm{d}x^2-\\mathrm{d}y^2-\\mathrm{d}z^2}$\nPar principe de moindre action $\\delta S = 0$ or $\\delta \\int_a^b \\mathrm{d}s=0$ est maximum le long d\u0026rsquo;une ligne droite. On retrouve donc que la trajectoire d\u0026rsquo;une particule libre est une ligne droite.\nIl est par exemple facile de se convaincre math√©matiquement que le chemin purement temporel joignant A √† B est le plus long possible puisque $c\\mathrm{d}t\u0026gt;\\sqrt{c^2\\mathrm{d}t^2-\\mathrm{d}x^2}$ (c\u0026rsquo;est la magie de la m√©trique minkowskienne que de rendre plus long un chemin sans d√©tour).\nOn en d√©duit l\u0026rsquo;expression du moment conjugu√© d\u0026rsquo;une particule libre (sa quantit√© de mouvement)¬†:\n$$ \\boldsymbol{p}=\\frac{\\partial L}{\\partial \\boldsymbol{v}}=\\frac{\\partial }{\\partial \\boldsymbol{v}}\\left( -mc^2\\sqrt{1-v^2/c^2}\\right)=\\gamma m \\boldsymbol{v} $$\nEt son √©nergie¬†:\n$$ E=H=\\boldsymbol{p}\\cdot\\boldsymbol{v}-L=\\gamma m v^2 +\\frac{mc^2}{\\gamma}=\\gamma m c^2\\left[\\frac{v^2}{c^2}+\\left(1-\\frac{v^2}{c^2}\\right)\\right]=\\gamma m c^2 $$\nEn relativit√© restreinte, on assemble l\u0026rsquo;√©nergie et le moment en un quadrivecteur $p^\\mu=(\\frac{E}{c},\\boldsymbol{p})$ (ou $p_\\mu=(\\frac{E}{c},-\\boldsymbol{p})$).\nParticule charg√©e dans un champ √©lectromagn√©tique Donnons une charge $q$ √† notre particule et couplons-la √† un champ √©lectromagn√©tique. Ce dernier peut √™tre d√©crit par un champ quadrivectoriel $A^\\mu(x)=\\left(\\frac{V(x)}{c},\\boldsymbol{A}(x)\\right)$ o√π $V(x)$ est le potentiel √©lectrique (scalaire) et $\\boldsymbol{A}(x)$ est le potentiel magn√©tique (vectoriel). L\u0026rsquo;interaction avec le champ correspond √† une √©nergie $-qA_\\mu\\mathrm{d}x^\\mu$ et donc l\u0026rsquo;action s\u0026rsquo;√©crit¬†:\n$$ S = \\int_{t_1}^{t_2} \\left( - \\frac{mc^2}{\\gamma} + q\\boldsymbol{A} \\cdot \\boldsymbol{v} - qV \\right) dt $$\nLe Lagrangien est l\u0026rsquo;int√©grande et donc le moment canonique conjugu√© est donn√© par¬†:\n$$ \\boldsymbol{p} = \\frac{\\partial L}{\\partial \\boldsymbol{v}} = \\gamma m \\boldsymbol{v} + q\\boldsymbol{A} $$\nSyst√®me d‚Äôunit√©s utilis√©es en TQC¬†:\nunit√©s de Lorentz-Heaviside\nElles simplifient l\u0026rsquo;√©criture des √©quations de l\u0026rsquo;√©lectromagn√©tisme en posant¬†: $$ \\mu_0=\\epsilon_0=1 $$ Les √©quations de Maxwell deviennent¬†: $$ \\begin{array}{ll} \\boldsymbol{\\nabla} \\cdot \\boldsymbol{E} = \\rho \u0026amp;\\boldsymbol{\\nabla} \\times \\boldsymbol{E} = -\\frac{1}{c}\\frac{\\partial \\boldsymbol{B}}{\\partial t}\\\\ \\boldsymbol{\\nabla} \\cdot \\boldsymbol{B} = 0 \u0026amp;\\boldsymbol{\\nabla} \\times \\boldsymbol{B} = \\frac{1}{c}\\left( \\boldsymbol{J} + \\frac{\\partial \\boldsymbol{E}}{\\partial t}\\right) \\end{array} $$\nunit√©s naturelles\nComme les vitesses s\u0026rsquo;expriment en fraction de $c$ et les spins en unit√©s de $\\hbar$, il est plus commode de les utiliser comme √©talon de mesure en posant¬†: $$ \\hbar=c=1 $$\nOn d√©finit le tenseur antisym√©trique de second rang $F_{\\mu\\nu}$ comme¬†:\n$$ F_{\\mu \\nu} = \\partial_{\\mu} A_{\\nu} - \\partial_{\\nu} A_{\\mu} $$\nC\u0026rsquo;est le tenseur du champ √©lectromagn√©tique (il ressemble √† un rotationnel √† 4 dimensions). Ses √©l√©ments contiennent les composantes des champs $\\boldsymbol{E}$ et $\\boldsymbol{B}$.\nEn notant que $\\partial^\\mu=\\left(\\frac{\\partial}{\\partial t},-\\nabla\\right)$ et $\\partial_\\mu=\\left(\\frac{\\partial}{\\partial t},\\nabla\\right)$, on obtient les composantes du champ √† partir de¬†:\n$\\boldsymbol{B}=\\boldsymbol{\\nabla} \\times \\boldsymbol{A}$, qui donne ${\\color{#0076BA}B^i}=-\\varepsilon^{ijk}\\partial_jA_k=-\\frac{1}{2}\\varepsilon^{ijk}F^{jk}$ o√π $\\varepsilon^{ijk}$ est le symbole de Levi-Civita, $\\boldsymbol{E}=-\\frac{\\partial \\boldsymbol{A}}{\\partial t}-\\boldsymbol{\\nabla} V$, qui donne ${\\color{#D41876}E^i}=-\\partial^0 A^i+\\partial^i A^0=-F^{0i}=F^{i0}$. $$ F_{\\mu \\nu} = \\begin{pmatrix} 0 \u0026amp;\\color{#D41876} E_1 \u0026amp; \\color{#D41876}E_2 \u0026amp; \\color{#D41876}E_3 \\\\ \\color{#D41876}-E_1 \u0026amp; 0 \u0026amp; \\color{#0076BA}-B_3 \u0026amp; \\color{#0076BA}B_2 \\\\ \\color{#D41876}-E_2 \u0026amp;\\color{#0076BA} B_3 \u0026amp; 0 \u0026amp;\\color{#0076BA} -B_1 \\\\ \\color{#D41876}-E_3 \u0026amp;\\color{#0076BA} -B_2 \u0026amp;\\color{#0076BA} B_1 \u0026amp; 0 \\end{pmatrix} $$\n$$ F^{\\mu \\nu} = \\begin{pmatrix} 0 \u0026amp; \\color{#D41876}{-E^1} \u0026amp; \\color{#D41876}-E^2 \u0026amp; \\color{#D41876}-E^3 \\\\ \\color{#D41876}E^1 \u0026amp; 0 \u0026amp;\\color{#0076BA} -B^3 \u0026amp; \\color{#0076BA}B^2 \\\\ \\color{#D41876}E^2 \u0026amp;\\color{#0076BA} B^3 \u0026amp; 0 \u0026amp;\\color{#0076BA} -B^1 \\\\ \\color{#D41876}E^3 \u0026amp;\\color{#0076BA} -B^2 \u0026amp; \\color{#0076BA}B^1 \u0026amp; 0 \\end{pmatrix} $$\nOn cherche √† nouveau un invariant de Lorentz pour le Lagrangien du champ, ce qui nous am√®ne logiquement au produit scalaire du tenseur champ¬†:\n$$ F_{\\mu \\nu} F^{\\mu \\nu} = 2({\\color{#0076BA}\\boldsymbol{B}}^2 - {\\color{#D41876}\\boldsymbol{E}}^2) $$\nEt le Lagrangien peut s\u0026rsquo;√©crire ainsi¬†:\n$$ L = -\\frac{1}{4} \\int \\mathrm{d}^3 x \\, F_{\\mu \\nu} F^{\\mu \\nu} $$\nLe facteur $1/4$ se justifiera par la suite.\nEnfin, la conservation locale de la charge s\u0026rsquo;exprime par l\u0026rsquo;√©quation de continuit√©¬†:\n$$ \\frac{\\partial \\rho}{\\partial t} + \\boldsymbol{\\nabla} \\cdot \\boldsymbol{J} = \\partial_{\\mu} J^{\\mu} = 0 $$\nChamps classiques Un champ classique est une bestiole qui se nourrit d\u0026rsquo;une position dans l\u0026rsquo;espace-temps et qui pond l\u0026rsquo;amplitude du champ en ce point. La sortie peut √™tre un scalaire (ex¬†: temp√©rature), un nombre complexe, un vecteur (ex¬†: champ magn√©tique), un tenseur (ex¬†: $F_{\\mu\\nu}(x)$) ou tout objet plus complexe. On obtient alors respectivement un champ scalaire, un champ scalaire complexe, un champ vectoriel, un champ tensoriel, etc.\nLes champs sont d√©finis localement.\nLes valeurs du champ vivent dans un espace suppl√©mentaire \u0026ldquo;au-dessus\u0026rdquo; de l\u0026rsquo;espace-temps. Pour un champ scalaire, par exemple, l\u0026rsquo;amplitude pouvant prendre une valeur r√©elle en chaque point de l\u0026rsquo;espace-temps, c\u0026rsquo;est comme ci passait en se point un axe r√©el suppl√©mentaire. Et pour un champ dont l\u0026rsquo;amplitude s\u0026rsquo;√©bat dans un espace plus complexe, c\u0026rsquo;est comme ci on avait coll√© une copie de cet espace en tout point de l\u0026rsquo;espace-temps. L\u0026rsquo;espace total obtenu (espace de base + copies en tout point de l\u0026rsquo;espace des amplitudes) s\u0026rsquo;appelle un fibr√©.\nDensit√© lagrangienne et hamiltonienne Le but ici est de formuler des Lagrangiens et Hamiltoniens dans le langage des champs classiques.\nDans le cas d\u0026rsquo;un r√©seau lin√©aire discret de ressorts de constante de raideur $K$ et de masselottes de masses $m$ s√©par√©es d\u0026rsquo;une longueur $\\ell$, on avait √©crit l\u0026rsquo;Hamiltonien suivant¬†:\n$$ H = \\sum_j \\frac{p_j^2}{2m} + \\frac{1}{2} K(q_{j+1} - q_j)^2 $$\nEt le Lagrangien¬†:\n$$ L = \\sum_j \\frac{p_j^2}{2m} - \\frac{1}{2} K(q_{j+1} - q_j)^2 $$\nOn passe √† la limite continue¬†:\n$$ \\begin{aligned} \\ell \u0026amp;\\rightarrow 0\\\\ q_j \u0026amp;\\rightarrow \\phi(x,t)\\\\ \\sum_j \u0026amp;\\rightarrow \\frac{1}{\\ell}\\int\\mathrm{d}x\\\\ \\frac{q_{j+1}-q_j}{\\ell} \u0026amp;\\rightarrow \\frac{\\partial \\phi(x,t)}{\\partial x} \\end{aligned} $$\nOn obtient¬†:\n$$ H = \\int \\mathrm{d}^3x \\left[\\frac{1}{2} \\rho \\left( \\frac{\\partial \\phi}{\\partial t} \\right)^2 + \\frac{1}{2} \\mathcal{T} \\left(\\boldsymbol{\\nabla}\\phi \\right)^2 \\right] $$\n$$ L = \\int \\mathrm{d}^3x \\left[\\frac{1}{2} \\rho \\left( \\frac{\\partial \\phi}{\\partial t} \\right)^2 - \\frac{1}{2} \\mathcal{T} \\left( \\boldsymbol{\\nabla} \\phi \\right)^2 \\right] $$\nOn a introduit la masse lin√©ique $\\rho=m/\\ell$ et la tension du ressort $\\mathcal{T}=K\\ell$.\nD√©finissons les densit√©s hamiltonienne et lagrangienne comme¬†:\n$$ H = \\int \\mathrm{d}^3x \\, \\mathcal{H} $$\n$$ L = \\int \\mathrm{d}^3x \\, \\mathcal{L} $$\nL\u0026rsquo;int√©r√™t du passage √† cette densit√© devient plus claire si on regarde ce que devient la formule de l\u0026rsquo;action¬†: $S=\\int\\mathrm{d}^4x\\mathcal{L}$. Le temps et l\u0026rsquo;espace sont maintenant trait√©s sur un pied d\u0026rsquo;√©galit√©, ce qui sied beaucoup mieux √† une th√©orie relativiste.\n$\\mathcal{H}$ et $\\mathcal{L}$ sont g√©n√©ralement fonction de $\\phi$, $\\dot{\\phi}$ et $\\phi\u0026rsquo;$.\nD√©finissons le moment conjugu√© $\\pi(x)$ √† partir de la d√©riv√©e fonctionnelle¬†:\n$$ \\pi(x) = \\frac{\\delta L}{\\delta \\dot{\\phi}} = \\frac{\\partial \\mathcal{L}}{\\partial \\dot{\\phi}} $$\nCela permet de relier $\\mathcal{H}$ et $\\mathcal{L}$¬†:\n$$ \\mathcal{H} = \\pi \\dot{\\phi} - \\mathcal{L} $$\nDans le cas de l\u0026rsquo;exemple masselottes-ressorts, on obtient¬†:\n$$ \\mathcal{L}=\\frac{1}{2}\\rho\\left(\\frac{\\partial\\phi}{\\partial t}\\right)^2-\\frac{1}{2}\\mathcal{T}(\\boldsymbol{\\nabla}\\phi)^2 $$\n$$ \\mathcal{H}=\\frac{1}{2}\\rho\\left(\\frac{\\partial\\phi}{\\partial t}\\right)^2+\\frac{1}{2}\\mathcal{T}(\\boldsymbol{\\nabla}\\phi)^2 $$\n$$ \\pi=\\rho\\frac{\\partial\\phi}{\\partial t} $$\nLe principe de moindre action $\\delta S = 0$ sur $S=\\int\\mathrm{d}^4x\\mathcal{L}(\\phi,\\partial_\\mu\\phi)$ donne la version quadridimensionnelle des √©quations d\u0026rsquo;Euler-Lagrange¬†:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial \\phi} - \\partial_\\mu \\left( \\frac{\\partial \\mathcal{L}}{\\partial (\\partial_\\mu \\phi)} \\right) = 0 $$\nRegardons ce que cela donne pour le champ √©lectromagn√©tique.\n$$ \\mathcal{L} = -\\frac{1}{4}F_{\\mu\\nu}F^{\\mu\\nu}=\\frac{1}{2} (\\boldsymbol{E}^2 - \\boldsymbol{B}^2) $$\nEn l\u0026rsquo;absence de potentiel √©lectrique ($V=0$), on a $A^\\mu=(0,\\boldsymbol{A})$ et donc $E^i=F^{i0}=\\partial^iA^0-\\partial^0 A^i=-\\partial^0 A^i$.\nPar cons√©quent $\\mathcal{L} =\\frac{1}{2} (\\boldsymbol{E}^2 - \\boldsymbol{B}^2)=\\frac{1}{2}(\\dot{\\boldsymbol{A}}^2 - \\boldsymbol{B}^2)$.\nEt le moment conjugu√© est $\\pi^i=\\partial\\mathcal{L}/\\partial(\\partial_0A_i)$ (puisqu\u0026rsquo;ici $\\phi=A$) et donc $\\boldsymbol{\\pi}=-\\dot{\\boldsymbol{A}}=\\boldsymbol{E}$, ce qui donne¬†:\n$$ \\mathcal{H} = \\pi^i\\dot{A}_i-\\mathcal{L} = \\frac{1}{2} \\left( \\boldsymbol{E}^2 + \\boldsymbol{B}^2 \\right) $$\nEt les √©quations d\u0026rsquo;Euler-Lagrange donnent¬†:\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A_\\mu} - \\partial_\\lambda \\left( \\frac{\\partial \\mathcal{L}}{\\partial (\\partial_\\lambda A_\\mu)} \\right) = 0 $$\nLe premier terme est nul puisque $\\mathcal{L}=\\frac{1}{4}F_{\\mu\\nu}F^{\\mu\\nu}$ ne contient que des d√©riv√©es de $A_\\mu$. Et en r√©√©crivant le second terme comme $\\partial_\\lambda F^{\\lambda\\mu}$, on obtient¬†:\n$$ \\partial_\\lambda F^{\\lambda\\mu} = 0 $$\n√âcriture compacte des deux √©quations de Maxwell inhomog√®nes dans le vide ($\\boldsymbol{\\nabla}\\cdot\\boldsymbol{E}=0$ et $\\boldsymbol{\\nabla}\\times\\boldsymbol{B}=\\dot{\\boldsymbol{E}}$).\nEn couplant lin√©airement le quadrivecteur densit√© de courant $J^\\mu=(\\rho,\\boldsymbol{J})$ au champ √©lectromagn√©tique, on obtient un nouveau Lagrangien¬†:\n$$ \\mathcal{L} = -\\frac{1}{4} F_{\\mu\\nu}F^{\\mu\\nu} - J^\\mu A_\\mu $$\nOn a maintenant\n$$ \\frac{\\partial \\mathcal{L}}{\\partial A_\\mu} = - J^\\mu $$\nEt donc\n$$ \\partial_\\lambda F^{\\lambda\\mu} = J^\\mu $$\nOn retrouve les deux √©quations de Maxwell inhomog√®nes avec charges ($\\boldsymbol{\\nabla}\\cdot\\boldsymbol{E}=\\rho$ et $\\boldsymbol{\\nabla}\\times\\boldsymbol{B}=\\boldsymbol{J}+\\dot{\\boldsymbol{E}}$).\nLe facteur $1/4$ dans le Lagrangien se justifie donc a posteriori par le fait qu\u0026rsquo;il nous ait donn√© les bonnes √©quations.\nDans la suite, lorsqu\u0026rsquo;on parlera de Lagrangien, il s\u0026rsquo;agira le plus souvent en r√©alit√© de la densit√© lagrangienne.\nLa r√©solution des √©quations d\u0026rsquo;Euler-Lagrange va produire tous les modes d\u0026rsquo;oscillations et donc les vecteurs d\u0026rsquo;onde autoris√©s, c\u0026rsquo;est-√†-dire les valeurs particuli√®res $k_n$ qui survivent √† la dissipation. Par principe de superposition, l\u0026rsquo;onde la plus g√©n√©rale est faite de la somme pond√©r√©e des diff√©rents modes possibles¬†:\n$$ \\phi(x, t) = \\sum_{\\boldsymbol{k}_n} a_{\\boldsymbol{k}_n} \\mathrm{e}^{-\\mathrm{i}(\\omega t - \\boldsymbol{k}_n \\cdot \\boldsymbol{x})} $$\nOr on sait maintenant interpr√©ter ces modes normaux comme des oscillateurs harmoniques qui peuvent donc √™tre quantifi√©s, aboutissant √† des solutions sous forme de particules.\nM√©canique quantique relativiste, premi√®re tentative Equation de Klein-Gordon Rappelons le raisonnement permettant d\u0026rsquo;aboutir √† l\u0026rsquo;√©quation de Schr√∂dinger (cadre : particule libre en m√©canique quantique non-relativiste)¬†:\non part de la relation de dispersion qui lie √©nergie et impulsion de la particule¬†: $E=\\frac{\\boldsymbol{p}^2}{2m}$¬†; on transforme $E$ et $\\boldsymbol{p}$ en op√©rateurs¬†: $E\\rightarrow\\hat{E}$ et $\\boldsymbol{p}\\rightarrow \\hat{\\boldsymbol{p}}$¬†; on substitue $\\hat{E}=\\mathrm{i}\\hbar\\frac{\\partial}{\\partial t}$ et $\\hat{\\boldsymbol{p}}=-\\mathrm{i}\\hbar\\boldsymbol{\\nabla}$¬†; on obtient l\u0026rsquo;√©quation de Schr√∂dinger¬†: $$ \\mathrm{i} \\hbar \\frac{\\partial \\phi(x,t)}{\\partial t} = - \\frac{\\hbar^2}{2m} \\boldsymbol{\\nabla}^2 \\phi(\\boldsymbol{x},t) $$\n$\\phi(\\boldsymbol{x},t)$ est la fonction d\u0026rsquo;onde et les solutions de l\u0026rsquo;√©quation sont des ondes planes $\\phi(\\boldsymbol{x},t)=N\\mathrm{e}^{-\\mathrm{i}(\\omega t-\\boldsymbol{k}\\cdot\\boldsymbol{x})}$ o√π $N$ est une constante de normalisation (il s\u0026rsquo;agit ici d\u0026rsquo;une onde incidente). On peut r√©√©crire la solution sous forme quadrivectorielle $\\phi(x)=\\mathrm{e}^{-\\mathrm{i} p\\cdot x}$.\nAppliquons les op√©rateurs impulsion et √©nergie sur la solution¬†:\n$$ \\begin{aligned} \\hat{\\boldsymbol{p}}\\phi(\\boldsymbol{x},t)=\\hbar\\boldsymbol{k}\\phi(\\boldsymbol{x},t)\\\\ \\hat{E}\\phi(\\boldsymbol{x},t)=\\hbar\\omega\\phi(\\boldsymbol{x},t) \\end{aligned} $$\nUne onde incidente a donc une impulsion et une √©nergie positives.\nPour obtenir une √©quation d\u0026rsquo;onde relativiste, on va tenter le m√™me cheminement.\nL\u0026rsquo;√©quation de dispersion d\u0026rsquo;une particule relativiste est donn√©e par¬†:\n$$ E = \\left( \\boldsymbol{p}^2 c^2 + m^2 c^4 \\right)^{\\frac{1}{2}} $$\nEt avec les m√™me substitutions ($E\\rightarrow\\hat{E}=\\mathrm{i}\\hbar\\frac{\\partial}{\\partial t}$ et $\\boldsymbol{p}\\rightarrow \\hat{\\boldsymbol{p}}=-\\mathrm{i}\\hbar\\boldsymbol{\\nabla}$), on obtient¬†:\n$$ \\mathrm{i} \\hbar \\frac{\\partial \\phi}{\\partial t} = \\left( -\\hbar^2 c^2 \\boldsymbol{\\nabla}^2 + m^2 c^4 \\right)^{\\frac{1}{2}} \\phi $$\nDeux probl√®mes¬†:\nl\u0026rsquo;√©quation n\u0026rsquo;a pas l\u0026rsquo;air covariante, que faire avec la racine carr√©e¬†? Comment prend-on la racine carr√©e d\u0026rsquo;un op√©rateur diff√©rentiel¬†? Esquivons les deux probl√®me en partant du carr√© de la relation de dispersion. On obtient maintenant¬†:\n$$ -\\hbar^2 \\frac{\\partial^2 \\phi}{\\partial t^2} = \\left( -\\hbar^2 c^2 \\boldsymbol{\\nabla}^2 + m^2 c^4 \\right) \\phi $$\nIl s\u0026rsquo;agit de l\u0026rsquo;√©quation de Klein-Gordon. Par soucis de clart√©, on repart en unit√©s naturelles ($\\hbar=c=1$)¬†:\n$$ -\\frac{\\partial^2 \\phi(x, t)}{\\partial t^2} = \\left( -\\boldsymbol{\\nabla}^2 + m^2 \\right) \\phi(\\boldsymbol{x}, t) $$\nTout semble maintenant parfaitement covariant. En notant $\\partial^2=\\partial_\\mu\\partial^\\mu=\\frac{\\partial^2}{\\partial t^2}-\\boldsymbol{\\nabla}^2$, on peut r√©√©crire joliment l\u0026rsquo;√©quation de Klein-Gordon¬†:\n$$ (\\partial^2 + m^2)\\phi(x)=0 $$\nC\u0026rsquo;est Schr√∂dinger qui d√©couvrit le premier l\u0026rsquo;√©quation de Klein-Gordon\u0026hellip; mais il la vite rejet√©e car elle ne donnait pas la bonne structure fine pour l\u0026rsquo;atome d\u0026rsquo;Hydrog√®ne. Il n\u0026rsquo;a finalement gard√© que sa limite non-relativiste, l\u0026rsquo;√©quation de Schr√∂dinger (on verra plus loin comment on passe de l\u0026rsquo;une √† l\u0026rsquo;autre).\nPour r√©soudre l\u0026rsquo;√©quation, tentons la solution $\\phi(\\boldsymbol{x},t)=N\\mathrm{e}^{-\\mathrm{i}Et+\\mathrm{i}\\boldsymbol{p}\\cdot\\boldsymbol{x}}=N\\mathrm{e}^{-\\mathrm{i}p\\cdot x}$ (en unit√©s naturelles, $\\boldsymbol{k}=\\boldsymbol{p}$ et $\\omega=E$ et comme on est plus int√©ress√© par l\u0026rsquo;√©nergie et l\u0026rsquo;impulsion des particules, ce sont eux qu\u0026rsquo;on utilise).\nEn substituant dans l\u0026rsquo;√©quation, on retrouve la relation de dispersion $E^2=\\boldsymbol{p}^2+m^2$. $\\phi$ est donc bien une solution et tout semble parfait jusqu\u0026rsquo;au moment o√π on constate que pour obtenir l\u0026rsquo;√©nergie de la particule, il faut prendre la racine carr√©e de l\u0026rsquo;√©quation de dispersion\u0026hellip; Deux solutions coexistent¬†: $E=\\pm(\\boldsymbol{p}^2+m^2)^{\\frac{1}{2}}$. Peut-on juste ignorer la solution n√©gative en arguant qu\u0026rsquo;elle est non physique¬†?\nCourants de probabilit√© et densit√©s Les √©nergies n√©gatives, c\u0026rsquo;est d√©j√† pas mal incommodant\u0026hellip; mais elles engendrent quelque chose de peut-√™tre encore plus dur √† avaler¬†: des densit√©s de probabilit√© n√©gatives¬†!\nLa densit√© de probabilit√© $\\rho$ et le courant de probabilit√© $\\boldsymbol{j}$ ob√©issent √† l\u0026rsquo;√©quation de continuit√©¬†:\n$$ \\frac{\\partial \\rho}{\\partial t} +\\boldsymbol{\\nabla} \\cdot \\boldsymbol{j} = 0 $$\nD√©monstration\u0026nbsp;: On multiplie l\u0026rsquo;√©quation de Klein-Gordon pour $\\phi$ par $\\phi^*$¬†$$ \\begin{aligned} \\phi^*\\left(-\\hbar^2 \\frac{\\partial^2 \\phi}{\\partial t^2}\\right) \u0026amp;= \\phi^*\\left( -\\hbar^2 c^2 \\boldsymbol{\\nabla}^2 + m^2 c^4 \\right) \\phi\\\\ \u0026amp; = -\\hbar^2 c^2 \\phi^*\\boldsymbol{\\nabla}^2 \\phi+ m^2 c^4 |\\phi^2| \\end{aligned} $$\nPuis on multiplie l\u0026rsquo;√©quation de Klein-Gordon pour $\\phi^*$ par $\\phi$¬†:\n$$ -\\hbar^2 \\phi\\frac{\\partial^2 \\phi^*}{\\partial t^2} = -\\hbar^2 c^2 \\phi\\boldsymbol{\\nabla}^2 \\phi^*+ m^2 c^4 |\\phi^2| $$\nEn soustrayant membre √† membre les deux √©quations, on obtient¬†:\n$$ \\left( \\phi^*\\frac{\\partial^2 \\phi}{\\partial t^2}- \\phi\\frac{\\partial^2 \\phi^*}{\\partial t^2}\\right)=c^2\\left( \\phi^*\\boldsymbol{\\nabla}^2 \\phi- \\phi\\boldsymbol{\\nabla}^2 \\phi^*\\right) $$\nQu\u0026rsquo;on peut r√©√©crire¬†:\n$$ \\mathrm{i}\\hbar\\frac{\\partial}{\\partial t}\\left( \\phi^*\\frac{\\partial \\phi}{\\partial t}- \\phi\\frac{\\partial \\phi^*}{\\partial t}\\right)=-\\mathrm{i}\\hbar c^2\\boldsymbol{\\nabla}\\cdot\\left( \\phi^*\\boldsymbol{\\nabla} \\phi- \\phi\\boldsymbol{\\nabla}\\phi^*\\right) $$\nEn effet, $\\frac{\\partial \\phi^*}{\\partial t}\\frac{\\partial \\phi}{\\partial t}-\\frac{\\partial \\phi}{\\partial t}\\frac{\\partial \\phi^*}{\\partial t}=0$, et de m√™me $\\boldsymbol{\\nabla} \\phi^*\\cdot\\boldsymbol{\\nabla} \\phi-\\boldsymbol{\\nabla} \\phi\\cdot\\boldsymbol{\\nabla} \\phi^*=0$.\nOn retrouve bien l\u0026rsquo;√©quation de continuit√© en posant¬†:\n$$ \\rho=\\mathrm{i}\\hbar\\left(\\phi^* \\frac{\\partial \\phi}{\\partial t}-\\phi \\frac{\\partial \\phi^*}{\\partial t}\\right) $$\n$$ \\boldsymbol{J}=- \\mathrm{i}\\hbar c^2\\left(\\phi^* \\boldsymbol{\\nabla} \\phi-\\phi \\boldsymbol{\\nabla} \\phi^*\\right) $$\nqui devient en notation quadrivectorielle¬†:\n$$ \\partial_\\mu j^\\mu = 0 $$\nEt le courant de probabilit√© quadrivectoriel s\u0026rsquo;√©crit¬†:\n$$ j^\\mu(x) = \\mathrm{i} \\left[ \\phi^*(x) \\partial^\\mu \\phi(x) - \\phi(x) \\partial^\\mu \\phi^*(x) \\right] $$\nEn substituant la solution $\\phi(x)=N\\mathrm{e}^{-\\mathrm{i}p\\cdot x}$, on obtient une composante temporelle de la probabilit√© de courant, la densit√© de probabilit√©, √† l\u0026rsquo;allure inqui√©tante¬†:\n$$ j^0 = \\rho = 2|N|^2 E $$\nComme $E$ peut √™tre n√©gative, $\\rho$ aussi, et il faudrait alors donner un sens √† des probabilit√©s n√©gatives\u0026hellip;\nL\u0026rsquo;interpr√©tation de Feynman des √©nergies n√©gatives Feynman a propos√© une interpr√©tation audacieuse des √©tats √† √©nergie n√©gative solutions de l\u0026rsquo;√©quation de Klein-Gordon¬†: il s\u0026rsquo;agirait de particules remontant le temps, des particules antiparticules¬†!\nConsid√©rons l\u0026rsquo;√©quation classique gouvernant le mouvement d\u0026rsquo;une particule charg√©e dans un champ √©lectromagn√©tique¬†:\n$$ m \\frac{\\mathrm{d}^2 x^\\mu}{\\mathrm{d}\\tau^2} = q F^{\\mu\\nu} \\frac{\\mathrm{d}x_\\nu}{\\mathrm{d}\\tau}‚Äã $$\nChanger le signe du temps propre $\\tau$ dans l\u0026rsquo;√©quation a la m√™me cons√©quence que changer le signe de la charge $q$. Donc une particule remontant le temps est √©quivalente a une particule de charge oppos√©e avec un temps s\u0026rsquo;√©coulant normalement.\nPour nos solutions de la forme $\\mathrm{e}^{-\\mathrm{i}(Et-\\boldsymbol{p}\\cdot\\boldsymbol{x})}$ (avec $E\u0026lt;0$), changer $t\\rightarrow -t$ doit √™tre compens√© par $E\\rightarrow -E$ pour r√©tablir le sens normal d\u0026rsquo;√©coulement du temps. Mais le reversement du temps touche aussi l\u0026rsquo;impulsion $\\boldsymbol{p}\\rightarrow\\boldsymbol{-p}$. Cela donne au final $\\mathrm{e}^{-\\mathrm{i}(Et+\\boldsymbol{p}\\cdot\\boldsymbol{x})}$ avec $E\u0026gt;0$. La particule incidente d\u0026rsquo;√©nergie n√©gative devient une particule √©mise d\u0026rsquo;√©nergie positive.\nOn sait maintenant g√©rer les √©nergies n√©gatives¬†: une particule d\u0026rsquo;√©nergie n√©gative se transforme en antiparticule d\u0026rsquo;√©nergie positive en changeant le signe de la charge et de l\u0026rsquo;impulsion tridimensionnelle.\nUne solution g√©n√©rale de l\u0026rsquo;√©quation de Klein-Gordon pour une √©nergie positive particuli√®re est donn√©e par la superposition de deux √©tats¬†:\n$$ \\phi(x) = \\left[ \\begin{array}{c} \\text{Particule re√ßue} \\\\ \\text{d\u0026rsquo;√©nergie positive} \\\\ \\propto \\mathrm{e}^{-\\mathrm{i}(Et - \\boldsymbol{p} \\cdot \\boldsymbol{x})} \\end{array} \\right] + \\left[ \\begin{array}{c} \\text{Antiparticule √©mise} \\\\ \\text{d\u0026rsquo;√©nergie positive} \\\\ \\propto \\mathrm{e}^{+\\mathrm{i}(Et - \\boldsymbol{p} \\cdot \\boldsymbol{x})} \\end{array} \\right] $$\nQuelques Lagrangiens de champs classiques Champ scalaire sans masse Un champ scalaire $\\phi(x)$ assigne une amplitude scalaire √† toute position $x$ de l\u0026rsquo;espace-temps. Le Lagrangien ne d√©pend que du taux de variation temporel $\\partial_\\phi$ et spatial $\\boldsymbol{\\nabla}$ de $\\phi$. Et pour que $\\mathcal{L}$ respecte les canons relativistes, on choisit¬†:\n$$ \\mathcal{L}=\\frac{1}{2} \\partial^\\mu \\phi \\partial_\\mu \\phi=\\frac{1}{2}\\left(\\partial_\\mu \\phi\\right)^2 $$\nIl se d√©veloppe en $\\mathcal{L}=\\frac{1}{2}\\left(\\partial_0 \\phi\\right)^2-\\frac{1}{2} \\boldsymbol{\\nabla}^2 \\phi$ pour lui donner un air de $\\mathcal{L}=$ (√©nergie cin√©tique)- (√©nergie potentielle). Comme d\u0026rsquo;habitude, le facteur ($\\frac{1}{2}$, ici) est l√† pour faire coller les pr√©dictions du Lagrangien √† ce qu\u0026rsquo;on connait.\nComme $\\frac{\\partial \\mathcal{L}}{\\partial \\phi}=0$ et $\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_\\mu \\phi\\right)}=\\partial^\\mu \\phi$, l\u0026rsquo;√©quation d\u0026rsquo;Euler-Lagrange implique l\u0026rsquo;√©quation de mouvement suivante¬†:\n$$ \\partial_\\mu\\partial^\\mu\\phi =0 $$\nCe n\u0026rsquo;est autre que l\u0026rsquo;√©quation des ondes (√©quation de d\u0026rsquo;Alembert) $\\partial^2=0$ ou $\\frac{\\partial^2 \\phi}{\\partial t^2}-\\boldsymbol{\\nabla}^2 \\phi=0$.\nEt les solutions ondulent sous la forme¬†:\n$$ \\phi(x, t)=\\sum_{\\boldsymbol{p}} a_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i}\\left(E_{\\boldsymbol{p}} t-\\boldsymbol{p} \\cdot \\boldsymbol{x}\\right)} $$\navec une √©quation de dispersion donn√©e par¬†:\n$$ E_{\\boldsymbol{p}}=c|\\boldsymbol{p}| $$\n(ou $E_{\\boldsymbol{p}}=|\\boldsymbol{p}|$ avec $c=1$).\nComme $E_{\\boldsymbol{p}}=0$ en $|\\boldsymbol{p}|=0$, on dit que la relation de dispersion est sans gap.\nCette √©quation d√©crira plus loin l\u0026rsquo;√©nergie des excitations quantiques du syst√®me et puisque c\u0026rsquo;est la version $m=0$ de la relation de dispersion relativiste $E_p=\\sqrt{p^2+m^2}$, on parle de particules non massives ainsi que d\u0026rsquo;un champ scalaire sans masse.\nL\u0026rsquo;√©quation d\u0026rsquo;onde √©tant lin√©aire, les ondes solutions ob√©issent au principe de superposition. Les particules correspondantes sont alors dites libres ou sans interaction. Si on les envoie les unes contres les autres, elles se traversent sans se voir.\nChamp scalaire avec masse Pour inclure une masse, on fait d√©pendre $\\mathcal{L}$ non plus seulement de $\\partial_\\mu\\phi$ mais aussi du champ $\\phi$ lui-m√™me en introduisant un terme d\u0026rsquo;√©nergie potentielle $U(\\phi)\\propto\\phi^2$ qui va traduire le co√ªt d\u0026rsquo;avoir un champ plut√¥t que du vide √† cet endroit.\n$$ \\mathcal{L}=\\frac{1}{2}\\left(\\partial_\\mu \\phi\\right)^2-\\frac{1}{2} m^2 \\phi^2, $$\nMontrons √† partir des √©quations d\u0026rsquo;Euler-Lagrange que le param√®tre $m$ est bien une masse.\nComme on a $\\frac{\\partial \\mathcal{L}}{\\partial \\phi}=-m^2 \\phi$ et $\\frac{\\partial \\mathcal{L}}{\\partial\\left(\\partial_\\mu \\phi\\right)}=\\partial^\\mu \\phi$, on obtient¬†:\n$$ \\left(\\partial_\\mu \\partial^\\mu+m^2\\right) \\phi=0 $$\nL\u0026rsquo;√©quation du mouvement de ce champ est donc l\u0026rsquo;√©quation de Klein-Gordon¬†!\nSes solutions sont √† nouveau¬†:\n$$ \\phi(x, t)=\\sum_{\\boldsymbol{p}} a_{\\boldsymbol{p}} \\mathrm{e}^{-\\mathrm{i}\\left(E_{\\boldsymbol{p}} t-\\boldsymbol{p} \\cdot \\boldsymbol{x}\\right)} $$\navec la relation de dispersion¬†:\n$$ E^2_\\boldsymbol{p}=\\boldsymbol{p}^2+m^2 $$\nAvoir $m\\neq 0$ cr√©e un gap dans la relation de dispersion (√† $\\boldsymbol{p}=0$, $E_\\boldsymbol{p}=\\pm m$) correspondant √† la masse de la particule.\n√Ä nouveau, les √©quations du mouvement sont lin√©aires et donc les particules ainsi d√©crites n\u0026rsquo;interagissent pas.\nSource externe On veut maintenant introduire des interactions. Le plus simple est de faire interagir un champ scalaire avec un potentiel externe. On d√©crit le potentiel par une fonction $J(x)$ (source externe) qui interagit avec le champ via le terme $-J(x)\\phi(x)$ dans le potentiel. Cela donne le Lagrangien¬†:\n$$ \\mathcal{L}=\\frac{1}{2}\\left[\\partial_\\mu \\phi(x)\\right]^2-\\frac{1}{2} m^2[\\phi(x)]^2+J(x) \\phi(x) $$\nL\u0026rsquo;√©quation du mouvement devient¬†:\n$$ \\left(\\partial_\\mu \\partial^\\mu+m^2\\right) \\phi(x)=J(x) . $$\nOn a ainsi maintenant une √©quation diff√©rentielle inhomog√®ne.\nLa th√©orie $\\phi^4$ Comment faire interagir des particules les unes avec les autres (ou des champs avec des champs)¬†? La recette la plus simple consiste √† ajouter un terme d\u0026rsquo;√©nergie potentielle $U(\\phi)$ proportionnel √† $\\phi^4$ au Lagrangien scalaire. S\u0026rsquo;il cr√©e bien des interactions, ce terme emp√™che dans le m√™me temps de trouver des solutions aux √©quations autrement que par une th√©orie des perturbations\u0026hellip;\n$$ \\mathcal{L}=\\frac{1}{2} \\partial^\\mu \\phi \\partial_\\mu \\phi-\\frac{1}{2} m^2 \\phi^2-\\frac{1}{4!} \\lambda \\phi^4, $$\nEt cela donne une √©quation du mouvement pas tr√®s jolie¬†:\n$$ \\left(\\partial^2+m^2\\right) \\phi=-\\frac{\\lambda}{3!} \\phi^3 . $$\nDeux champs scalaires Pour faire interagir des particules, on peut aussi d√©crire deux types de particules via deux champs diff√©rents $\\phi_1(x)$ et $\\phi_2(x)$ et les faire interagir gr√¢ce au potentiel $U(\\phi_1,\\phi_2)=g(\\phi_1^2+\\phi_2)^2$ o√π $g$ param√©trise la force de l\u0026rsquo;interaction.\nD√©velopper le carr√© donne des termes d\u0026rsquo;interaction propre en $\\phi^4$ mais aussi un terme d\u0026rsquo;interaction mutuelle $2\\phi_1^2\\phi_2^2$.\nLa densit√© lagrangienne est donn√©e par¬†:\n$$ \\mathcal{L}=\\frac{1}{2}\\left(\\partial_\\mu \\phi_1\\right)^2-\\frac{1}{2} m^2 \\phi_1^2+\\frac{1}{2}\\left(\\partial_\\mu \\phi_2\\right)^2-\\frac{1}{2} m^2 \\phi_2^2-g\\left(\\phi_1^2+\\phi_2^2\\right)^2 $$\nOn remarque que certaines transformations des champs gardent invariant le Lagrangien. On peut ainsi op√©rer une rotation des champs dans l\u0026rsquo;espace abstrait $\\phi_1$-$\\phi_2$. On a alors $\\phi_1 \\rightarrow \\phi_1^{\\prime}$ et $\\phi_2 \\rightarrow \\phi_2^{\\prime}$ avec¬†:\n$$ \\binom{\\phi_1^{\\prime}}{\\phi_2^{\\prime}}=\\left(\\begin{array}{cc} \\cos \\theta \u0026amp; -\\sin \\theta \\\\ \\sin \\theta \u0026amp; \\cos \\theta \\end{array}\\right)\\binom{\\phi_1}{\\phi_2} $$\nOn dit que les particules d√©crites par ce Lagrangien ont un degr√© de libert√© interne. L\u0026rsquo;invariance de la physique par rapport aux rotations d\u0026rsquo;un angle $\\theta$ dans l\u0026rsquo;espace $\\phi_1$-$\\phi_2$ exprime une sym√©trie $SO(2)$ de la th√©orie.\n$SO(2)$ est le groupe sp√©cial orthogonal √† 2 dimensions. Il correspond aux matrices $2\\times2$ orthogonales avec un d√©terminant √©gal √† 1.\nC\u0026rsquo;est une sym√©trie continue et on va voir plus loin que les sym√©tries continues conduisent √† des quantit√©s conserv√©es.\nChamp scalaire complexe On peut simplifier le Lagrangien √† deux champs scalaires pr√©c√©dent en passant √† des champs scalaires complexes $\\psi$ et $\\psi^\\dagger$ d√©finis par¬†:\n$$ \\begin{aligned} \\psi \u0026amp; =\\frac{1}{\\sqrt{2}}\\left[\\phi_1+\\mathrm{i} \\phi_2\\right] \\\\ \\psi^{\\dagger} \u0026amp; =\\frac{1}{\\sqrt{2}}\\left[\\phi_1-\\mathrm{i} \\phi_2\\right] \\end{aligned} $$\nOn obtient¬†:\n$$ \\mathcal{L}=\\partial^\\mu \\psi^{\\dagger} \\partial_\\mu \\psi-m^2 \\psi^{\\dagger} \\psi-g\\left(\\psi^{\\dagger} \\psi\\right)^2 $$\nLe nouveau champ scalaire complexe $\\psi$ contient, comme avant, deux degr√©s de libert√©. Le nouveau Lagrangien se retrouve maintenant invariant par rapport aux rotations dans le plan complexe $\\psi \\rightarrow \\psi \\mathrm{e}^{\\mathrm{i} \\alpha}$ et $\\psi^{\\dagger} \\rightarrow \\mathrm{e}^{-\\mathrm{i} \\alpha} \\psi^{\\dagger}$ qui expriment une sym√©trie $U(1)$.\n$U(1)$ est le groupe des transformations unitaires √† 1 dimension.\nL\u0026rsquo;√©quivalence entre les deux descritptions pr√©c√©dentes d√©coulent de l\u0026rsquo;isomorphisme entre $SO(2)$ et $U(1)$ not√© $SO(2)\\simeq U(1)$.\nTh√©orie $\\psi^\\dagger\\psi\\phi$ Terminons par une th√©orie d√©crivant 3 types de particules. On additionne les Lagrangiens de champ scalaire complexe (avec une masse $m$) et de champ scalaire r√©el (avec une masse $\\mu$) et on ajoute un terme d\u0026rsquo;interaction entre les deux $g\\psi^\\dagger\\psi\\phi$¬†:\n$$ \\mathcal{L}= \\partial^\\mu \\psi^{\\dagger} \\partial_\\mu \\psi-m^2 \\psi^{\\dagger} \\psi+\\frac{1}{2}\\left(\\partial_\\mu \\phi\\right)^2-\\frac{1}{2} \\mu^2 \\phi^2-g \\psi^{\\dagger} \\psi \\phi $$\nComme on le verra plus tard, cette th√©orie ressemble beaucoup √† l\u0026rsquo;√©lectrodynamique quantique.\nDans le chapitre suivant, on va passer de ces champs classiques √† des champs quantiques et v√©rifier que les particules √©mergent bien comme leurs excitations.\nChapitre suivant\nSommaire\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/pca/",
	"title": "Analyse en composantes principales",
	"tags": [],
	"description": "",
	"content": " Analyse en composantes principales Les axes qui tiraillent territorialement la France D√©taillons un exemple sur le mod√®le de celui des pr√©noms dans la vid√©o.\nL\u0026rsquo;id√©e ici est de r√©cup√©rer plein de s√©ries de donn√©es d√©partementales disparates sur le site de l\u0026rsquo;INSEE pour voir si les composantes principales font ou non ressurgir une g√©ographie identifiable de ce brouillard de chiffres. On peut alors s\u0026rsquo;amuser √† balbutier des interpr√©tations pour ces axes de diff√©rentiation territoriaux et probablement irriter ainsi fortement tout sociologue un tant soit peu √©duqu√©. C\u0026rsquo;est la magie m√™me de l\u0026rsquo;ACP de permettre √† un quidam de se prendre pour un sociologue\u0026hellip;\nCherchant √† √©tudier la variabilit√© entre d√©partements, √† faire ressortir leurs sp√©cificit√©s, on choisit ici de n\u0026rsquo;avoir que des donn√©es sous forme de taux quitte √† rendre relative certaines donn√©es brutes en divisant par la population du d√©partement. Certes, un vecteur associant les variables non relatives aurait probablement constitu√© la 1re composante principale puisque la d√©mographie explique alors une large part de la variance, et comme chaque composante est orthogonale aux suivantes, cela aurait peut-√™tre √©limin√© ce biais d√®s la 2e composante\u0026hellip; Mais autant s\u0026rsquo;en d√©barrasser dans les donn√©es pour √©viter toute confusion.\nUn tableau de 96 lignes (les d√©partements m√©tropolitains) et 51 colonnes (code du d√©partement + nom du d√©partement + 49 variables statistiques) constitue les donn√©es de base. Un export en .csv (acc√®s au fichier) et direction Pandas.\nimport pandas as pd file_path = \u0026#34;tableauINSEE.csv\u0026#34; data = pd.read_csv(file_path, delimiter=\u0026#39;;\u0026#39;) On nettoie un peu en changeant les virgules fran√ßaises en points am√©ricains comme s√©parateurs d√©cimaux afin que Pandas daigne associer des nombres (des flottants) aux donn√©es.\ndepartements = data.columns[2:] data_cleaned = data[columns_to_use].replace(\u0026#39;,\u0026#39;, \u0026#39;.\u0026#39;, regex=True).apply(pd.to_numeric, errors=\u0026#39;coerce\u0026#39;) Puis on standardise les vecteurs : pour chaque s√©rie de 96 valeurs (les valeurs d√©partementales de chaque cat√©gorie statistique), on centre les donn√©es en soustrayant la moyenne et on les met √† l‚Äô√©chelle en divisant par l‚Äô√©cart type, de sorte que les coordonn√©es des vecteurs aient tous une moyenne de 0 et un √©cart type de 1.\nfrom sklearn.preprocessing import StandardScaler scaler = StandardScaler() data_normalized = scaler.fit_transform(data_cleaned) data_normalized_df = pd.DataFrame(data_normalized, columns= departements) Voil√† par exemple la carte de la variable \u0026ldquo;cheptel bovin\u0026rdquo;. Une carte = un vecteur √† 96 dimensions.\nTra√ßons la matrice de corr√©lation de ces 49 variables pour faire connaissance.\nimport plotly.express as px # Calculer la matrice de corr√©lation correlation_matrix = data_normalized_df.corr() # Cr√©er un heatmap de la matrice de corr√©lation fig = px.imshow( correlation_matrix, labels=dict(x=\u0026#34;Variables\u0026#34;, y=\u0026#34;Variables\u0026#34;, color=\u0026#34;Corr√©lation\u0026#34;), x=correlation_matrix.columns, y=correlation_matrix.columns, color_continuous_scale=\u0026#39;Temps\u0026#39; ) Enfin, on applique l\u0026rsquo;analyse en composantes principales sur les vecteurs normalis√©s.\nRq¬†: les fonctions utilis√©es pour la normalisation et l\u0026rsquo;ACP viennent de diff√©rents modules de la biblioth√®que Scikit-learn sp√©cialis√©e dans le machine learning.\nfrom sklearn.decomposition import PCA pca = PCA() pca_components = pca.fit_transform(data_normalized) pca_df = pd.DataFrame(pca_components, columns=[f\u0026#39;Composante {i+1}\u0026#39; for i in range(pca_components.shape[1])]) Tra√ßons le \u0026ldquo;scree plot\u0026rdquo; qui repr√©sente le pourcentage de variance expliqu√©e en fonction des composantes.\nimport numpy as np import plotly.graph_objs as go # R√©cup√©rer la variance expliqu√©e pour les 20 premi√®res composantes principales explained_variance = pca.explained_variance_ratio_[:20] # Calculer la variance expliqu√©e cumul√©e explained_variance_cumulative = np.cumsum(explained_variance) # Cr√©ation du scree plot fig = go.Figure() # Ajouter les barres pour la variance expliqu√©e individuelle fig.add_trace(go.Bar( x=[f\u0026#39;Composante {i+1}\u0026#39; for i in range(len(explained_variance))], y=explained_variance, name=\u0026#39;Variance expliqu√©e individuelle\u0026#39;, marker=dict(color=\u0026#39;rgba(55, 128, 191, 0.7)\u0026#39;), )) # Ajouter la ligne pour la variance expliqu√©e cumul√©e fig.add_trace(go.Scatter( x=[f\u0026#39;Composante {i+1}\u0026#39; for i in range(len(explained_variance))], y=explained_variance_cumulative, name=\u0026#39;Variance expliqu√©e cumul√©e\u0026#39;, mode=\u0026#39;lines+markers\u0026#39;, line=dict(color=\u0026#39;rgb(255, 0, 0)\u0026#39;), )) # Mise en page du graphique fig.update_layout( title=\u0026#39;Scree Plot\u0026#39;, xaxis_title=\u0026#39;Nombre de composantes principales\u0026#39;, yaxis_title=\u0026#39;Pourcentage de variance expliqu√©e\u0026#39;, showlegend=True, template=\u0026#39;plotly_white\u0026#39; ) # Afficher le graphique fig.show() On constate que la 1re composante explique presque 40% de la variance et qu\u0026rsquo;arriv√© √† la 6e, on d√©passe 80% d\u0026rsquo;explication.\nAffichons les valeurs de la premi√®re composante sur une carte de France. Les d√©partements scorant positivement fort sur cette composante apparaisse jaune et ceux scorant le plus n√©gativement sont en bleu.\n# Ajouter les codes d√©partements et leurs noms √† la dataframe des composantes principales pca_df[\u0026#39;Code\u0026#39;] = data[\u0026#39;Code\u0026#39;] pca_df[\u0026#39;D√©partement\u0026#39;] = data[\u0026#39;D√©partement\u0026#39;] # Choisissez la composante √† afficher composante = \u0026#39;Composante 1\u0026#39; # Cr√©er la carte fig = px.choropleth_mapbox( pca_df, geojson=\u0026#34;https://france-geojson.gregoiredavid.fr/repo/departements.geojson\u0026#34;, # GeoJSON pour les d√©partements fran√ßais locations=\u0026#39;Code\u0026#39;, featureidkey=\u0026#34;properties.code\u0026#34;, color=composante, color_continuous_scale=\u0026#34;Viridis\u0026#34;, mapbox_style=\u0026#34;white-bg\u0026#34;, # Fond de carte blanc zoom=5, center={\u0026#34;lat\u0026#34;: 46.603354, \u0026#34;lon\u0026#34;: 1.888334}, # Centre de la France opacity=1, labels={composante: composante}, hover_name=\u0026#39;D√©partement\u0026#39;, # Afficher le nom du d√©partement hover_data={\u0026#39;Code\u0026#39;: False, composante: True}, # Supprimer le code du d√©partement et afficher la composante ) fig.show() Cette composante semble opposer les d√©partements dont la population vit pour une large part en ville aux d√©partements plus ruraux.\nD√©voilons la recette de cette 1re composante en d√©taillant les proportions dans lesquelles interviennent chaque ingr√©dient (les variables statistiques). Pour cela, on trace l\u0026rsquo;histogramme des diff√©rentes charges (ou loadings) tri√©es. Les coefficients de charges correspondent aux poids respectifs de chaque variable dans le vecteur.\nnumero_composante = 1 composante = f\u0026#39;Composante {numero_composante}\u0026#39; # R√©cup√©rer les loadings pour la composante choisie loadings_composante = loadings[composante] # Trier les loadings par valeur d√©croissante loadings_composante_sorted = loadings_composante.sort_values(ascending=False) # Cr√©er un DataFrame pour Plotly Express loadings_df = pd.DataFrame({ \u0026#39;Variable\u0026#39;: loadings_composante_sorted.index, \u0026#39;Loading\u0026#39;: loadings_composante_sorted.values }) # Cr√©er un histogramme des loadings avec une coloration en fonction des valeurs fig = px.bar( loadings_df, x=\u0026#39;Variable\u0026#39;, y=\u0026#39;Loading\u0026#39;, color=\u0026#39;Loading\u0026#39;, # Utilisation des valeurs pour la coloration color_continuous_scale=\u0026#39;Viridis\u0026#39;, # Choix de l\u0026#39;√©chelle de couleurs template=\u0026#39;plotly_white\u0026#39;, title=f\u0026#39;Loadings tri√©s pour la {composante}\u0026#39;, height=600, ) # Supprimer la colorbar si vous ne souhaitez pas l\u0026#39;afficher fig.update_coloraxes(showscale=False) # Afficher le graphique fig.show() Les marqueurs de l‚Äôurbanit√© sont bien oppos√©s √† ceux de la ruralit√©.\nLes diff√©rences de mode de vie entre ces environnements semblent donc responsables du plus gros de la distinction entre d√©partements (du moins pour le lot de variables choisis qui sont malgr√© tout tr√®s disparates).\nOn peut renforcer cette interpr√©tation en v√©rifiant comment une nouvelle variable, la \u0026ldquo;part de la population vivant dans une unit√© urbaine en 2017\u0026rdquo; score sur un plan form√© par les deux premi√®res composantes.\nLa variable \u0026ldquo;population urbaine\u0026rdquo; repr√©sent√©e par un segment rouge a comme pr√©vu une direction proche de la composante 1 et lui est donc fortement corr√©l√©e (on a aussi plac√© les d√©partements dans ce plan en les coloriant en fonction de leur valeur de la variable population urbaine).\nUne fois cette opposition ville-campagne retir√©e, que reste-t-il pour expliquer les 60% de variance r√©siduelle entre d√©partements¬†?\nD\u0026rsquo;apr√®s la deuxi√®me composante, l\u0026rsquo;axe sur lequel les valeurs des d√©partements s\u0026rsquo;√©talent le plus semble correspondre √† une dimension socio-√©conomique. Cela semble montrer que la r√©partition des richesses sur le territoire est loin d\u0026rsquo;√™tre homog√®ne √† l\u0026rsquo;√©chelle des d√©partements¬†; certains concentrent les marqueurs de richesse et d\u0026rsquo;autres accumulent les stigmates de la pauvret√©.\nLa troisi√®me composante mat√©rialise une opposition g√©ographique entre le Nord et le Sud (avec Paris dans le camp du Sud) gr√¢ce √† un axe allant des ouvriers aux artisans/commer√ßants.\nLa quatri√®me composante distingue principalement positivement Paris avec de forts taux de fonctionnaires, de m√©decins et de mariages de m√™me sexe, et des petits taux de voitures par m√©nage et d\u0026rsquo;employ√©s. Le contraste est maximale avec la Corse, le nord des Alpes et la grande couronne.\nLa cinqui√®me composante semble tr√®s politique avec un axe allant de M√©lenchon-Lassalle √† Zemmour-Le Pen. Le vote protestataire semble ainsi se polariser territorialement. L\u0026rsquo;urbanit√© de la Seine-Saint-Denis en soutien de M√©lenchon alli√© au Sud-Ouest rural de Lassalle contre un vote d\u0026rsquo;extr√™me-droite plus √† l\u0026rsquo;Est.\nParis score bizarrement tr√®s fort n√©gativement bien que le vote pour l\u0026rsquo;extr√™me droite y soit plus rare qu\u0026rsquo;ailleurs.\nD√©taillons les valeurs pour chaque cat√©gorie statistique normalis√©e √† Paris (on a conserv√© l\u0026rsquo;ordre des charges de la composante 5).\nVoil√† ce que deviennent ces valeurs apr√®s multiplication par les charges de la composante 5.\nEt en sommant toutes ces valeurs, on obtient la projection de Paris et la composante 5. Le r√©sultat d\u0026rsquo;environ -3,8 est bien celui attribu√©e √† Paris dans la carte de la composante. On voit ainsi que les variables politiques ne sont pas celles qui participent au score de Paris.\nLa sixi√®me composante oppose bizarrement le vote P√©cresse aux mariages\u0026hellip; Et fait ressortir g√©ographiquement le centre par rapport aux bords.\nEn comparant la carte du vote P√©cresse √† celle du taux de mariage, on observe bien une tr√®s l√©g√®re compl√©mentarit√© mais cela n\u0026rsquo;a rien de concluant et c\u0026rsquo;est confirm√© par la corr√©lation de -0,063 entre les deux variables. On ne peut pas vraiment se contenter des extr√©mit√©s de l\u0026rsquo;axe pour tenter une interpr√©tation.\nL\u0026rsquo;attelage leader de cette composante est pour le moins curieux : les Hauts-de-Seine et la Creuse¬†! Mais les variables qui tirent leurs scores sont en bonne partie diff√©rentes. Un vote P√©cresse en 2022 relativement plus fort qu\u0026rsquo;ailleurs semble d\u0026rsquo;ailleurs le seul vrai point commun.\nLe semblant d\u0026rsquo;homog√©n√©it√© g√©ographique avec un centre qui s\u0026rsquo;oppose aux bords laisse cependant supposer que quelque chose se trame dans cette composante et on peut hasarder quelques hypoth√®ses sociologiques de bistrot. La composante semble agglom√©rer une France rurale √¢g√©e au tissu sociale distendue (plus de morts, moins de mariages) votant plus qu\u0026rsquo;ailleurs pour la droite historique. Et elle lui adjoint deux d√©partements (Hauts-de-Seine et Yvelines) abritant une haute bourgeoisie plus dense qu\u0026rsquo;ailleurs ayant pour seul point commun notable de voter pour cette m√™me droite.\nLa septi√®me composante a pour p√¥les la proportion d\u0026rsquo;√©coles priv√©es d\u0026rsquo;un c√¥t√© et les mariages de m√™me sexe de l\u0026rsquo;autre. L\u0026rsquo;opposition est amusante mais le reste du vecteur ne semble pas vouloir se pr√™ter √† des interpr√©tations faciles pas plus que la r√©partition g√©ographique (au-del√† de la zone bretonne au sens large ou le priv√© est surrepr√©sent√©).\n√Ä la louche ou plut√¥t au godet d\u0026rsquo;une pelleteuse, on a donc les axes de s√©paration territoriaux suivant (√† l\u0026rsquo;√©chelle des d√©partements), par ordre de pertinence :\nVille contre campagne Riches contre pauvres Ouvriers contre artisans/commer√ßants Paris contre le reste Vote radical de gauche et protestataire rural contre vote d\u0026rsquo;extr√™me droite Droite historique contre le reste Religieux contre mariage pour tous "
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/geometrie/geo2/",
	"title": "Cercle et Trigo",
	"tags": [],
	"description": "",
	"content": " Cercle et triangle Trigonom√©trie La loi des sinus a contribu√© √† la d√©finition du m√®tre, la premi√®re unit√© \u0026ldquo;universelle\u0026rdquo; lors de la r√©volution fran√ßaise.\nDelambre et M√©chain furent missionner pour mesurer le plus pr√©cis√©ment possible un bout de m√©ridien entre Dunkerque et Barcelone, la toute neuve d√©finition du m√®tre √©tant¬†:\nun m√®tre est la dix millioni√®me partie d\u0026rsquo;un quart de m√©ridien terrestre\nLe cercle r√©p√©titeur de Borda fut leur arme secr√®te. Il permettait alors de mesurer des angles avec une incertitude de quelques secondes seulement. Mesurer des distances avec cette pr√©cision est beaucoupl plus difficile¬†! L\u0026rsquo;id√©e fut alors de r√©aliser des triangulations¬†: on vise des sommets faciles √† rep√©rer et on note les angles entre eux. Ils r√©alis√®rent des dizaines de vis√©es tout le long du bout de m√©ridien √©tudi√© et le recouvrit de triangles.\nUne seule distance mesur√©e avec une grande pr√©cision permit alors de d√©terminer toutes les autres. En effet, gr√¢ce √† la loi des sinus, la connaissance de la longueur d\u0026rsquo;un c√¥t√© et de deux angles dans un triangle permet de d√©terminer les longueurs des deux autres c√¥t√©s.\nThal√®s l\u0026rsquo;avait semble-t-il d√©j√† r√©alis√© puisqu\u0026rsquo;il aurait mis au point la m√©thode suivante pour conna√Ætre la distance entre un navire et la c√¥te¬†: deux observateurs $A$ et $C$ s\u0026rsquo;√©loignent en direction oppos√©e le long du trait de c√¥te et mesurent les angles $\\alpha = \\widehat{CAB}$ et $\\gamma = \\widehat{ACB}$ entre la direction du bateau et la c√¥te ainsi que la distance $AC$ qui les s√©pare l\u0026rsquo;un de l\u0026rsquo;autre. On a alors $d=AC\\times\\frac{\\sin\\left(\\gamma\\right)\\sin\\left(\\alpha\\right)}{\\sin\\left(\\pi-(\\alpha+\\gamma)\\right)}$¬†!\nPreuve :\n$$\\beta=180^\\circ - (\\alpha+\\gamma)$$ $$AB = AC\\times\\frac{\\sin\\gamma}{\\sin\\beta}$$ $$d=AB\\sin\\alpha$$\nTh√©or√®me de l\u0026rsquo;angle inscrit et de l\u0026rsquo;angle au centre Nom d\u0026rsquo;un th√©or√®me et syst√®me √©ducatif¬†:\nL\u0026rsquo;√©ducation secondaire se d√©mocratise au cours du xixe si√®cle. La g√©om√©trie, qui a souvent pour but d\u0026rsquo;exercer √† la logique, en est un √©l√©ment important. Les manuels se r√©pandent, et leurs auteurs se mettent √† introduire plus ou moins syst√©matiquement des √©l√©ments historiques, en particulier en attribuant des noms de math√©maticien aux th√©or√®mes. L\u0026rsquo;√©poque est √©galement au renouveau des recherches historiques sur les math√©matiques de la Gr√®ce antique. C\u0026rsquo;est dans ce contexte qu\u0026rsquo;√† partir de la fin du xixe si√®cle le nom de Thal√®s est attribu√© √† deux th√©or√®mes distincts. En France puis en Italie c\u0026rsquo;est √† un th√©or√®me du livre VI des √âl√©ments d\u0026rsquo;Euclide, soit celui sur la proportionnalit√© des segments d√©coup√©s par une ligne parall√®le √† un c√¥t√© dans un triangle, soit sa g√©n√©ralisation pour deux s√©cantes. En Allemagne c\u0026rsquo;est au th√©or√®me de l\u0026rsquo;angle inscrit dans un demi-cercle. Ces d√©nominations sont bien √©tablies dans les ann√©es 1920, √©poque √† laquelle d\u0026rsquo;autres pays adoptent l\u0026rsquo;une ou l\u0026rsquo;autre (mais pas l\u0026rsquo;Angleterre ni les √âtats-Unis).\nCes choix correspondent √† des traditions diff√©rentes d\u0026rsquo;enseignement de la g√©om√©trie, plut√¥t fid√®le √† Euclide et √† l\u0026rsquo;ordre d\u0026rsquo;exposition des √âl√©ments en Allemagne, alors qu\u0026rsquo;une tradition plus anti-euclidienne s\u0026rsquo;est d√©velopp√©e en France depuis La Ram√©e. Le choix s\u0026rsquo;est port√© en France et en Italie, sur un th√©or√®me qui met en √©vidence un invariant de la g√©om√©trie affine, le rapport de mesures alg√©briques sur une droite, ce qui correspond au d√©veloppement en Europe de la g√©om√©trie projective et affine au xixe si√®cle. Ce th√©or√®me est mis en avant dans les d√©buts de l\u0026rsquo;apprentissage de la g√©om√©trie, alors que l\u0026rsquo;Allemagne privil√©gie le th√©or√®me de Pythagore, et choisit de donner le nom de Thal√®s √† un th√©or√®me li√© au triangle rectangle.\nDans un cas comme dans l\u0026rsquo;autre, le choix du nom n\u0026rsquo;a finalement pas ob√©i a des consid√©rations strictement historiques. L\u0026rsquo;histoire est instrumentalis√©e au service d\u0026rsquo;un choix didactique¬†: il s\u0026rsquo;agit de mettre un th√©or√®me en avant, en lui attribuant le nom d\u0026rsquo;un math√©maticien c√©l√®bre, d\u0026rsquo;o√π des choix diff√©rents dans des traditions d\u0026rsquo;enseignement diff√©rentes.\nSource : Wikip√©dia "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/meca/coriolis/",
	"title": "Coriolis",
	"tags": [],
	"description": "",
	"content": " Coriolis Simulations de Troyens dans un syst√®me o√π la grosse masse $M$ et la petite masse $m$ sont dans les proportions $m=M\\frac{\\mu}{1-\\mu} $ avec $\\mu=1/200$\nExplication des bandes de vents dominants (th√®se + plan√®tes).\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/quantique/epistovsonto/",
	"title": "√âpist√©mique vs. ontique",
	"tags": [],
	"description": "",
	"content": " √âpist√©mique vs. ontiques En m√©canique quantique, l‚Äô√©volution d‚Äôun syst√®me isol√© est unitaire, c‚Äôest‚Äë√†‚Äëdire d√©crite par un op√©rateur qui conserve la norme (ou la probabilit√© totale) du vecteur d‚Äô√©tat‚ÄØ; cette dynamique, enti√®rement d√©terministe, est gouvern√©e par l‚Äô√©quation de Schr√∂dinger. D√®s qu‚Äôune mesure intervient, toutefois, la r√®gle de Born (postulat de projection) impose une seconde loi¬†: l‚Äô√©tat s‚Äôeffondre de fa√ßon non unitaire et al√©atoire pour livrer l‚Äôun des r√©sultats possibles, puis se fige afin que le m√™me r√©sultat soit retrouv√© si l‚Äôon r√©p√®te imm√©diatement la mesure. Malgr√© ce clivage, ce cadre th√©orique a r√©volutionn√© notre compr√©hension du monde et propuls√© des avanc√©es technologiques spectaculaires1.\nR√©concilier ces deux modes d‚Äô√©volution‚ÄØ‚Äî‚ÄØl‚Äôun continu, r√©versible et probabilistiquement conservatif, l‚Äôautre discontinu, irr√©versible et intrins√®quement stochastique‚ÄØ‚Äî‚ÄØreste l‚Äôun des d√©fis conceptuels majeurs de la physique moderne. C\u0026rsquo;est le probl√®me de la mesure. D\u0026rsquo;ailleurs, la simple question \u0026ldquo;C\u0026rsquo;est quoi mesurer un syst√®me¬†?\u0026rdquo; n\u0026rsquo;a pas √† ce jour de r√©ponse claire, loin de l√†.\nDistinction de Harrigan et Spekkens Une partie de la communaut√© scientifique a rapidement adopt√© une attitude un peu relou en d√©cr√©tant que l\u0026rsquo;appareil th√©orique n\u0026rsquo;√©tait qu\u0026rsquo;une machinerie ultra performante pour pr√©dire les r√©sultats des exp√©riences, rien de plus. Le reste n\u0026rsquo;est que balivernes philosophiques. Cette approche instrumentiste n\u0026rsquo;attribue d√®s lors aucune r√©alit√© √† l\u0026rsquo;√©tat quantique.\nLes instrumentistes font partie du camp œà-√©pist√©mique pour lesquels l\u0026rsquo;√©tat quantique œà n\u0026rsquo;est que porteur d\u0026rsquo;information sans r√©alit√© physique. Ils se sont tr√®s t√¥t r√©unis dans l\u0026rsquo;\u0026ldquo;√©cole de Copenhague\u0026rdquo; qui a longtemps servi de canon √† l\u0026rsquo;enseignement de la quantique. Leur mantra un peu caricatural est le c√©l√®bre \u0026ldquo;shut up and calculate\u0026rdquo;.\nDe l\u0026rsquo;autre c√¥t√© du spectre, on a les interpr√©tations œà-ontiques pour lesquelles l\u0026rsquo;√©tat quantique traduit une r√©alit√© physique.\nIl y a malgr√© tout des œà‚Äë√©pist√©miques moins hardcores qui imaginent une r√©alit√© cach√©e derri√®re l\u0026rsquo;√©tat quantique. L\u0026rsquo;√©tat quantique repr√©sente alors notre connaissance (incompl√®te) de cette r√©alit√© de la m√™me fa√ßon qu\u0026rsquo;une distribution de probabilit√© dans l\u0026rsquo;espace des phases repr√©sente notre connaissance de la position ponctuelle d\u0026rsquo;une particule. Le mod√®le jouet de Spekkens illustre √ßa tr√®s bien.\nSupposons qu\u0026rsquo;une th√©orie ou mod√®le associe au syst√®me une propri√©t√© physique $\\lambda$ (la r√©alit√©) qui d√©termine la probabilit√© des diff√©rents r√©sultats obtenus lors de la mesure du syst√®me.\nLa th√©orie quantique va, elle, associer un √©tat quantique $|\\psi\\rangle$ √† un syst√®me pr√©par√© d\u0026rsquo;une certaine fa√ßon. Mais l\u0026rsquo;√©tat physique $\\lambda$ n\u0026rsquo;est pas n√©cessairement fix√© de mani√®re unique par la pr√©paration qui a produit $|\\psi\\rangle$. On dira que la pr√©paration a produit un √©tat physique $\\lambda$ selon une distribution de probabilit√© $\\mu_\\psi(\\lambda)$.\nSupposons maintenant que pour toute paire d\u0026rsquo;√©tats quantiques $|\\psi_1\\rangle$ et $|\\psi_2\\rangle$, les distributions $\\mu_1(\\lambda)$ et $\\mu_2(\\lambda)$ ne se recouvrent jamais. L\u0026rsquo;√©tat quantique $|\\psi\\rangle$ peut alors √™tre inf√©r√© de mani√®re unique √† partir de l\u0026rsquo;√©tat physique du syst√®me. √áa d√©finit bien au final une \u0026ldquo;propri√©t√© physique\u0026rdquo; pour l\u0026rsquo;√©tat quantique (puisqu\u0026rsquo;il y a une correspondance un pour un entre √©tat quantique et propri√©t√© physique $\\lambda\\leftrightarrow\\psi$). C\u0026rsquo;est la vision œà-ontique.\n√Ä l\u0026rsquo;inverse, s\u0026rsquo;il y a recouvrement pour au moins une paire d\u0026rsquo;√©tats quantiques, alors $|\\psi\\rangle$ peut √™tre vu que comme un simple porteur d\u0026rsquo;information sans r√©alit√©. Comment l\u0026rsquo;√©tat quantique pourrait √™tre l\u0026rsquo;incarnation d\u0026rsquo;une r√©alit√© si la m√™me r√©alit√© physique peut se cacher derri√®re diff√©rents √©tats quantiques¬†? C\u0026rsquo;est bien la possibilit√© m√™me du chevauchement qui rend un œà‚Äë√©pist√©mique œà‚Äë√©pist√©mique (o√π est l\u0026rsquo;incertitude si √ßa ne se chevauche pas¬†?).\nCette distinction entre √©tat √©pist√©mique et √©tat ontique reposant sur le non‚Äërecouvrement a √©t√© d√©velopp√©e par Harrigan et Spekkens dans un article de 2010¬†:\nHarrigan, N. \u0026amp; Spekkens, R. W. ‚ÄúEinstein, Incompleteness, and the Epistemic View of Quantum States‚Äù, Foundations of Physics 40 (2), 125‚Äì157 (f√©vrier 2010) üåê\nAttraits de la vision œà-√©pist√©mique Tous les œà‚Äë√©pist√©miques s\u0026rsquo;accordent sur le statut probabiliste de la fonction d\u0026rsquo;onde. Comme dans le mod√®le jouet de Spekkens, ce point de vue permet de d√©mystifier certaines bizarreries quantiques et c\u0026rsquo;est une approche de ce type que j\u0026rsquo;utilise dans la vid√©o ci-dessous pour tenter de rendre plus accessible l\u0026rsquo;√©tat quantique.\nUn argument fort pour cette vision probabiliste de la fonction d\u0026rsquo;onde est justement le fait que la th√©orie quantique peut √™tre consid√©r√©e comme une g√©n√©ralisation non‚ÄØcommutative de la th√©orie classique des probabilit√©s comme l\u0026rsquo;a montr√© von Neumann.\nMais un argument √† mes yeux bien plus convaincant pour la œà-√©pist√©mologie est la dissolution qu\u0026rsquo;elle entra√Æne du probl√®me de la mesure. Rappelons-nous que le probl√®me nait de la double r√®gle d\u0026rsquo;√©volution du formalisme quantique¬†: l\u0026rsquo;une douce et continue lorsque le syst√®me est isol√© et non observ√© et l\u0026rsquo;autre instantan√©e et discontinue lors d\u0026rsquo;une mesure. Comme une mesure n\u0026rsquo;est th√©oriquement qu\u0026rsquo;une interaction physique, on peut se demander pourquoi elle ne pourrait pas √™tre mod√©lis√©e par l\u0026rsquo;√©quation de Schr√∂dinger. Mais c\u0026rsquo;est en suivant cette voie qu\u0026rsquo;on se retrouve avec des situations apparemment absurdes comme celle du chat mort et vivant.\nLe probl√®me ne se pose en fait que pour des √©tats œà-ontiques¬†; seulement alors se retrouve-t-on avec un chat r√©ellement dans deux √©tats superpos√©s. Pour un œà‚Äë√©pist√©mique, la fonction d\u0026rsquo;onde n\u0026rsquo;a jamais repr√©sent√© que l\u0026rsquo;√©tendue de nos connaissances, le chat peut tr√®s bien √™tre mort ou vivant avant qu\u0026rsquo;on regarde. La description par une superposition ne refl√®te que notre ignorance de la possibilit√© qui s\u0026rsquo;est r√©alis√©e. L\u0026rsquo;√©tendue de la fonction d\u0026rsquo;onde dans l\u0026rsquo;espace des phases n\u0026rsquo;est pour un œà‚Äë√©pist√©mique qu\u0026rsquo;un brouillard d\u0026rsquo;incertitude.\nTh√©or√®me PBR et n√©cessit√© de l\u0026rsquo;ontologie Les œà-ontiques associent, eux, une r√©alit√© √† la fonction d\u0026rsquo;onde. On peut les classer en deux cat√©gories (en appelant $\\lambda$ la propri√©t√© physique associ√©e au syst√®me, cf. plus haut)¬†:\nles purs et durs pour lesquels la fonction d\u0026rsquo;onde est la r√©alit√© fondamentale ($\\lambda\\equiv\\psi$),\net ceux qui suppl√©mentent $\\psi$ avec des variables additionnelles $\\xi$ ($\\lambda=(\\psi,\\xi)$).\nA priori, les œà-ontistes n\u0026rsquo;ont pas grand-chose pour eux¬†; le probl√®me de la mesure redevient un probl√®me s√©rieux, la fonction d\u0026rsquo;onde vit dans un espace √† grande dimension, les in√©galit√©s de Bell imposent une non localit√© des variables cach√©es et le th√©or√®me de Kochen‚ÄëSpecker impose leur contextualit√©. Pour les œà-ontistes purs et durs, c\u0026rsquo;est moins compliqu√© √† avaler puisqu\u0026rsquo;ils ont sign√© en connaissance de cause (la quantique est non locale et contextuelle), mais pour ceux qui esp√©raient cacher des propri√©t√©s plus sympathiques dans $\\xi$, c\u0026rsquo;est r√¢p√©.\nLes œà‚Äëontiques devaient passer pour de sacr√©s masos aux yeux des œà‚Äë√©pist√©miques jusqu\u0026rsquo;au jour o√π le th√©or√®me PBR leur est tomb√© dessus.\nPusey, M. F. ; Barrett, J. ; Rudolph, T. (2012). \u0026ldquo;On the reality of the quantum state\u0026rdquo;. Nature Physics. 8 (6): 475‚Äì478 üåê\nRevenons √† la distinction entre œà‚Äëontiques et œà‚Äë√©pist√©miques r√©sum√©e par ce sch√©ma¬†:\nLes trois physiciens Pusey, Barrett et Rudolph ont r√©ussi √† montrer que si les distributions de probabilit√© d\u0026rsquo;une propri√©t√© physique (ontologiques) se recouvrent ($\\mu_1(\\lambda)\\cdot\\mu_2(\\lambda)=q\u0026gt;0$) pour deux √©tats quantiques distincts $|\\psi_1\\rangle$ et $|\\psi_2\\rangle$ (plus pr√©cis√©ment si la mesure de l\u0026rsquo;intersection $\\Delta$ des supports n\u0026rsquo;est pas nulle), alors il y a une contradiction avec les pr√©dictions de la th√©orie quantique.\nPBR est donc un no-go theorem. Pour des hypoth√®ses somme-toute assez raisonnables, il prouve qu\u0026rsquo;une interpr√©tation √©pist√©mique de la fonction d\u0026rsquo;onde n\u0026rsquo;est pas compatible avec des propri√©t√©s physiques r√©elles. Et c\u0026rsquo;est maintenant au tour des œà-√©pist√©miques de se lancer dans des circonvolutions pour √©chapper aux conclusions du th√©or√®me (en cherchant des loopholes)¬†!\nIl aurait suffit pour prouver que $|\\psi\\rangle$ est √©pist√©mique de trouver une seule paire d\u0026rsquo;√©tats dont les densit√©s de probabilit√© se recouvrent alors que pour prouver qu\u0026rsquo;il est ontique, il faut s\u0026rsquo;assurer que c\u0026rsquo;est le cas d\u0026rsquo;aucune paire\u0026hellip; Mais c\u0026rsquo;est bien ce que PBR a r√©ussi √† faire¬†!\nL\u0026rsquo;absence de recouvrement quand les deux √©tats quantiques sont orthogonaux n\u0026rsquo;est pas vraiment surprenante. Par contre, le fait que cela tienne pour des √©tats non orthogonaux est plus troublant ($\\mu_1(\\lambda) \\mu_2(\\lambda)=0$ pour tout $\\lambda$ m√™me si $\\left\\langle\\psi_1 \\mid \\psi_2\\right\\rangle \\neq 0$).\nPourquoi c\u0026rsquo;est √©tonnant¬†?\nSi on a $\\left\\langle\\psi_1 \\mid \\psi_2\\right\\rangle \\neq 0$, on obtient en introduisant la r√©solution de l\u0026rsquo;identit√© pour une base quelconque de l\u0026rsquo;espace ($1=\\int \\mathrm{d} a|a\\rangle\\langle a|$) $\\int \\mathrm{d} a\\left\\langle\\psi_1 \\mid a\\right\\rangle\\left\\langle a \\mid \\psi_2\\right\\rangle \\neq 0 $. Donc pour au moins un $a$, $\\left\\langle\\psi_1 \\mid a\\right\\rangle\\left\\langle a \\mid \\psi_2\\right\\rangle \\neq 0$. Et comme $\\rho_i(a)=\\left|\\left\\langle a \\mid \\psi_i\\right\\rangle\\right|^2$, √ßa donne finalement $\\rho_1(a) \\rho_2(a) \\neq 0$ pour au moins un $a$.\nMoralit√©¬†: les distributions de probabilit√©s quantiques se recouvrent sans que cela ne soit le cas des distributions de $\\lambda$¬†!\n‚ñ∫\u0026nbsp;Id√©e de la preuve de PBR\u0026nbsp;: Brossons la preuve sur l\u0026rsquo;exemple d\u0026rsquo;une paire d\u0026rsquo;√©tats non orthogonaux (PBR le g√©n√©ralise √† une paire arbitraire).\nSupposons un espace de Hilbert √† 2 dimensions avec une base orthogonale $|0\\rangle$, $|1\\rangle$.\nEt soit une autre base orthogonale $|+\\rangle$, $|-\\rangle$, avec $|\\pm\\rangle=\\frac{|0\\rangle \\pm |1\\rangle}{\\sqrt{2}}$.\nPrenons la paire non orthogonale $|0\\rangle$, $|+\\rangle$ ($\\langle 0 \\mid+\\rangle=1 / \\sqrt{2}$).\nLe but est de d√©montrer que $\\mu_0(\\lambda) \\mu_{+}(\\lambda)=0$ pour tout $\\lambda$. On va le prouver par l\u0026rsquo;absurde en supposant qu\u0026rsquo;il y a une probabilit√© $q\u0026gt;0$ telle que les deux √©tats quantiques r√©sultent d\u0026rsquo;un $\\lambda$ dans la r√©gion $\\Delta$ de recouvrement.\nConsid√©rons maintenant deux syst√®mes dont les √©tats physique ne sont pas corr√©l√©s. On peut les obtenir par exemple en op√©rant deux copies d\u0026rsquo;un dispositif de pr√©paration ind√©pendamment. Chaque syst√®me peut √™tre pr√©par√© tel que son √©tat quantique soit $|0\\rangle$ ou $|+\\rangle$.\nLes √©tats physiques $\\lambda_1$ et $\\lambda_2$ ont alors la probabilit√© $q^2\u0026gt;0$ d\u0026rsquo;√™tre tous les deux dans la r√©gion de chevauchement $\\Delta$.\nCela signifie qu\u0026rsquo;il y a une probabilit√© non nulle que l\u0026rsquo;√©tat physique des deux syst√®mes soit compatible avec n\u0026rsquo;importe lequel des quatre √©tats quantiques possibles $|0\\rangle \\otimes|0\\rangle$, $|0\\rangle \\otimes|+\\rangle$, $|+\\rangle \\otimes|0\\rangle$ et $|+\\rangle \\otimes|+\\rangle$.\nLes deux syst√®mes sont ramen√©s ensemble et mesur√©s dans une base orthogonale particuli√®re¬†: $\\left|\\phi_1\\right\\rangle=\\frac{1}{\\sqrt{2}}[|0\\rangle|1\\rangle+|1\\rangle|0\\rangle]$, $\\left|\\phi_2\\right\\rangle=\\frac{1}{\\sqrt{2}}[|0\\rangle|-\\rangle+|1\\rangle|+\\rangle]$, $\\left|\\phi_3\\right\\rangle=\\frac{1}{\\sqrt{2}}[|+\\rangle|1\\rangle+|-\\rangle|0\\rangle]$ et $\\left|\\phi_4\\right\\rangle=\\frac{1}{\\sqrt{2}}[|+\\rangle|-\\rangle+|-\\rangle|+\\rangle]$.\nCette base a √©t√© choisie pour avoir $\\left\\langle\\phi_1 \\mid 00\\right\\rangle=0$, $\\left\\langle\\phi_2 \\mid 0+\\right\\rangle=0$, $\\left\\langle\\phi_3 \\mid+0\\right\\rangle=0$ et $\\left\\langle\\phi_4 \\mid++\\right\\rangle=0$.\nQuel que soit le r√©sultat de la mesure ($|\\phi_1\\rangle$, $|\\phi_2 \\rangle$, $|\\phi_3 \\rangle$ ou $|\\phi_4 \\rangle$), il √©limine forc√©ment l\u0026rsquo;une des possibilit√©s ($|00\\rangle$, $|0+\\rangle$, $|+0\\rangle$ ou $|++\\rangle$). Cela entra√Æne que la probabilit√© de recouvrement des quatre √©tats physiques est nulle. Contradiction¬†!\nIl y a $q^2$ chance que l\u0026rsquo;appareil de mesure ne sache pas laquelle des quatre m√©thode de pr√©paration a √©t√© utilis√©e, et lorsque √ßa arrive, il prend le risque de produire un r√©sultat qui n\u0026rsquo;est sens√© jamais arriver.\nLes hypoth√®ses du th√©or√®me sont¬†:\nil existe une r√©alit√© fondamentale $\\lambda$, des $\\lambda$ pr√©par√©s s√©par√©ment sont statistiquement ind√©pendants, les pr√©dictions statistiques de la m√©canique quantique sont correctes, la r√©alit√© de l\u0026rsquo;√©tat quantique (œà-ontique) est d√©finie comme le non recouvrement des distributions de probabilit√© de $\\lambda$. Selon ces hypoth√®ses, $|\\psi\\rangle$ est r√©el¬†! Quelle que soit la propri√©t√© physique fondamentale $\\lambda$, pour un $\\lambda$ donn√©, $|\\psi\\rangle$ peut √™tre d√©termin√© de mani√®re unique.\nnote\nParmi les puristes ($\\lambda\\equiv|\\psi\\rangle$), des extra puristes ont d√©cid√© d\u0026rsquo;avaler la pilule des dimensions √©lev√©es pour regagner la localit√© et la s√©parabilit√© des √©tats. En admettant que notre espace √† trois dimensions n\u0026rsquo;est que l\u0026rsquo;ombre d\u0026rsquo;un espace multidimensionnelle dans lequel √©volue la fonction d\u0026rsquo;onde, on √©vacue en effet toutes les gal√®res (extra dimensions mises √† part), du moins jusqu\u0026rsquo;√† la mesure\u0026hellip;\nCette petite page explique l\u0026rsquo;attrait de ce compromis.\nvoir ici pour une description succincte des fondations de la m√©canique quantique dans son interpr√©tation \u0026ldquo;classique\u0026rdquo;).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/info/ia/",
	"title": "IA",
	"tags": [],
	"description": "",
	"content": " Intelligence Artificielle Avant de parler d\u0026rsquo;intelligence artificielle, il convient de s\u0026rsquo;interroger sur le terme intelligence.\nL\u0026rsquo;intelligence arificielle (IA) est une discipline scientifique qui a vu officiellement le jour en 1956. Elle repose sur la conjecture selon laquelle toutes les fonctions cognitives, en particulier l\u0026rsquo;apprentissage, le raisonnement, le calcul, la perception, la m√©morisation, voire la d√©couverte scientifique ou la cr√©ativit√© artistique, peuvent √™tre d√©crites avec une pr√©cision telle qu\u0026rsquo;il serait possible de les reproduire sur des ordinateurs.\nHistoire de l\u0026rsquo;IA La premi√®re programmeuse : Ada Lovelace De Turing au Big Data et Deep Learning Apprentissage Machine : apprendre √† pr√©dire L\u0026rsquo;apprentissage automatique (Machine Learning) est √† l\u0026rsquo;intersection de l\u0026rsquo;IA et d\u0026rsquo;un autre champ scientifique : la science des donn√©es (data science).\nArthur Samuel d√©finit l\u0026rsquo;apprentissage automatique ainsi en 1959 :\nLa capacit√© √† apprendre sans avoir √©t√© sp√©cifiquement programm√© pour.\nEn pratique, il s\u0026rsquo;agit de produire des r√©ponses adapt√©es aux donn√©es fournies en entr√©e (identifier des motifs, des tendances, construire des mod√®les, faire des pr√©dictions). L\u0026rsquo;apprentissage automatique n\u0026rsquo;est donc ni plus ni moins que du traitement de donn√©es visant √† pr√©dire des r√©sultats en fonction des donn√©es entrantes.\nLe diagramme suivant1 survole le domaine.\nOn va s\u0026rsquo;int√©resser √† l\u0026rsquo;apprentissage automatique dit \u0026ldquo;classique\u0026rdquo;2 qui regroupe des algorithmes tr√®s simples n√©s dans les ann√©es 50 et toujours utilis√©s aujourd\u0026rsquo;hui √† peu pr√®s partout. Cette branche de l\u0026rsquo;apprentissage automatique se d√©compose prinicipalement en deux familles d\u0026rsquo;algorithmes¬†: l\u0026rsquo;apprentissage supervis√© et son pendant, l\u0026rsquo;apprentissage non supervis√©. Nous allons √©tudier un algorithme star de chacune de ces familles. Algorithme des k plus proches voisins \u0026ndash; Exemple d\u0026rsquo;apprentissage supervis√© L\u0026rsquo;algorithme des k plus proches voisin (k-nearest neighbors ou KNN) est une des techniques les plus simples en apprentissage automatique. Sa facilit√© d\u0026rsquo;utilisation et sa rapidit√© en font un outil de choix dans l\u0026rsquo;industrie.\nKNN est un algorithme d\u0026rsquo;apprentissage supervis√©¬†; cela signifie que l\u0026rsquo;algorithme n√©cessite des donn√©es classifi√©es en amont qui vont lui servir √† trouver la bonne √©tiquette pour d\u0026rsquo;autres donn√©es non encore classifi√©es.\nSuivant la nature de l\u0026rsquo;√©tiquette, KNN peut servir √†¬†:\nune classification des nouvelles donn√©es si les √©tiquettes sont des catagories ; une r√©gression si les √©tiquettes sont des nombres. Principe KNN enregistre, dans un premier temps, tous les points de donn√©es √©tiquet√©es qui vont lui servir √† l\u0026rsquo;apprentissage (c\u0026rsquo;est le training set). Puis, quand arrive un point de donn√©e non √©tiquet√©, l\u0026rsquo;algorithme calcule sa distance aux autres points et s√©lectionne les k plus proches. On a alors deux cas possibles :\nsi les √©tiquettes sont des cat√©gories, l\u0026rsquo;algorithme calcule le mode des cat√©gories des voisins s√©lectionn√©s (cat√©gorie la plus repr√©sent√©e). si les √©tiquettes sont des nombres, l\u0026rsquo;algorithme calcule la moyenne des √©tiquettes des voisins s√©lectionn√©s. L\u0026rsquo;algorithme des k plus proches voisins est non param√©trique dans le sens o√π aucun mod√®le math√©matique de classification ou r√©gression n\u0026rsquo;est construit √† partir des donn√©es (pas de param√®tre √† ajuster) puisque toutes les donn√©es d\u0026rsquo;apprentissage sont enregistr√©es telles quelles.\nCela signifie qu\u0026rsquo;on ne pr√©suppose rien de particulier sur les donn√©es (√† part que des points proches appartiennent √† la m√™me cat√©gorie). L\u0026rsquo;algorithme est donc particuli√®rement robuste (les donn√©es parlent d\u0026rsquo;elles-m√™me) et simple √† mettre √† jour (suffit d\u0026rsquo;ajouter les nouvelles donn√©es d\u0026rsquo;apprentissage).\nChoix de k Comme le montre la petite animation ci-dessus, le choix de k modifie le r√©sultat obtenu.\nSi k est trop petit, le moyennage est faible et donc la variabilit√© va √™tre tr√®s grande. On parle alors de surapprentissage (overfitting). En augmentant k, les r√©sultats obtenus se stabilisent (vote de la majorit√©) et les erreurs diminuent, jusqu\u0026rsquo;au moment o√π la boule √† l\u0026rsquo;int√©rieur de laquelle se fait le moyennage devient trop grosse, amenant in fine l\u0026rsquo;algorithme a choisir syst√©matiquement la cat√©gorie majoritaire, quel que soit le point\u0026hellip; On augmente alors le biais (ici, le biais est le pr√©judice en faveur du plus grand nombre). L\u0026rsquo;ajustement ne suit plus les variations, on parle de sous-apprentissage (underfitting). Pour r√©sumer :\nvariance biais Cas d\u0026rsquo;une r√©gression Cas d\u0026rsquo;une classification k trop petit $\\rightarrow$ overfitting forte faible k trop grand $\\rightarrow$ underfitting faible fort tip\nLorsqu\u0026rsquo;on ne conna√Æt rien sur les donn√©es, on peut toujours commencer par prendre la racine carr√©e du nombre de points dans l\u0026rsquo;ensemble d\u0026rsquo;entra√Ænement comme k de d√©part.\nLe choix de k est donc affaire de compromis. Pour le rendre plus scientifique, on peut chercher √† mesurer la performance de l\u0026rsquo;algorithme pour diff√©rentes valeurs de k.\nMais comment mesure-t-on la performance d\u0026rsquo;un algorithme d\u0026rsquo;apprentissage automatique¬†?\nValidation \u0026ndash; Matrice de confusion La matrice de confusion permet d\u0026rsquo;√©valuer la qualit√© des pr√©dictions d\u0026rsquo;un algorithme.\nPrenons l\u0026rsquo;exemple de l\u0026rsquo;utilisation de KNN sur une banque d\u0026rsquo;images de chiffres √©crits √† la main et plus sp√©cifiquement concentrons-nous sur sa capacit√© √† reconna√Ætre des \u0026ldquo;3\u0026rdquo;.\nOn d√©coupe l\u0026rsquo;espace en 4 cadrans. Sur une dimension, on regroupe d\u0026rsquo;un c√¥t√© les donn√©es pertinentes (les 3) et de l\u0026rsquo;autre le reste des donn√©es (les non 3), et on d√©compose l\u0026rsquo;autre dimension en pr√©dictions positives (les 3 pr√©dits) et n√©gatives (les non 3 pr√©dits).\nPuis on compte dans chaque cadran le nombre de donn√©es correspondant au recouvrement des pr√©dictions et de la r√©alit√©. Un nom issu du vocabulaire des diagnostics m√©dicaux est attribu√© √† chacun de ces cadrans¬†:\nles vrais positifs VP (les 3 identifi√©s comme des 3), les vrais n√©gatifs VN (les non 3 identifi√©s comme des non 3), les faux positifs FP (les non 3 identifi√©s comme des 3), les faux n√©gatifs FN (les 3 identifi√©s comme des non 3). √Ä partir de ces effectifs, on peut calculer 3 grandeurs permettant d\u0026rsquo;√©valuer la qualit√© de la pr√©diction¬†:\nPr√©cision\nNombre de donn√©es bien pr√©dites parmi les pr√©dictions positives¬†: $$\\frac{VP}{VP+FP}$$\nRappel ou sensibilit√©\nNombre de donn√©es bien pr√©dites parmi les donn√©es positives¬†: $$\\frac{VP}{VP+FN}$$\nExactitude (accuracy)\n$$\\frac{VP+VN}{VP+VN+FP+FN}$$\ninfo\nUn algorithme peut tr√®s bien √™tre tr√®s pr√©cis (les pr√©dictions positives sont bien des 3), mais peu sensible, avec un faible taux de rappel (parmi tous les 3, peu ont √©t√© identifi√©s).\n√Ä l\u0026rsquo;inverse, on peut avoir une bonne sensibilit√© (la plupart des vrais 3 ont √©t√© identifi√©s comme tel), mais peu pr√©cis (beaucoup de chiffres identifi√©s comme des 3 sont en fait d\u0026rsquo;autres chiffres).\ntip\nOn peut tout aussi bien d√©finir la matrice de confusion avec les pr√©dictions sur les lignes et la r√©alit√© sur les colonnes.\nMaintenant qu\u0026rsquo;on sait √©valuer l\u0026rsquo;algorithme, cherchons la valeur de k qui maximise l\u0026rsquo;exactitude.\nDans le graphe ci-dessous, on a trac√© l\u0026rsquo;exactitude de l\u0026rsquo;algorithme pour la reconnaissance des \u0026ldquo;9\u0026rdquo; en fonction de la valeur de k. Si notre but est de reconna√Ætre le mieux possible les 9 manuscrits, il semblerait que la valeur de k optimale soit 18.\nExemple d'utilisation de KNN pour reconna√Ætre un chiffre Algorithme des k-moyennes \u0026ndash; Exemple d\u0026rsquo;apprentissage non supervis√© Le boulot de l\u0026rsquo;algorithme des k-moyennes (k-means) n\u0026rsquo;est pas d\u0026rsquo;√©tiqueter les donn√©es, mais de les regrouper par famille. C\u0026rsquo;est donc un algorithme de partitionnement des donn√©es (clustering).\nContrairement √† KNN, l\u0026rsquo;algorithme des k-moyennes ne n√©cessite pas de donn√©es pr√©√©tiquet√©es. Il fait ainsi parti des algorithmes d\u0026rsquo;apprentissage automatique non-supervis√© (il se d√©brouille tout seul avec les donn√©es myst√®res).\nPar contre, l\u0026rsquo;algorithme partage avec KNN sa grande simplicit√© d\u0026rsquo;emploi et son efficacit√© qui le rendent lui aussi tr√®s populaire dans l\u0026rsquo;industrie.\nPrincipe L\u0026rsquo;algorithme d√©pend d\u0026rsquo;un seul param√®tre en plus des donn√©es¬†: le nombre de partitions (clusters) k.\nOn commence par choisir k points au hasard dans l\u0026rsquo;espace des donn√©es (il peut s\u0026rsquo;agir de k points de donn√©es ou de k autres points). Ce sont les k centres (ou centro√Ødes).\ntip\nPlus les points choisis au d√©part sont √©loign√©s les uns des autres, mieux c\u0026rsquo;est. Une am√©lioration de l\u0026rsquo;algorithme de base propos√©e en 2007, k-means++, s\u0026rsquo;en assure.\nOn attribue ensuite √† chaque centre tous les points de donn√©es qui lui sont le plus proches, formant ainsi k groupes.\nEnfin, on d√©place chaque centre au barycentre de son groupe.\nOn r√©p√®te les deux derni√®res op√©rations (attribution des points les plus pr√®s et d√©placement des centres) tant que les centres bougent d\u0026rsquo;une it√©ration √† l\u0026rsquo;autre.\nL\u0026rsquo;algorithme vise √† r√©soudre au final un probl√®me d\u0026rsquo;optimisation ; son but est en effet de trouver le minimum de la distance entre les points √† l\u0026rsquo;int√©rieur de chaque partition.\nMath√©matiquement, √©tant donn√© un ensemble de points $(x_1,x_2,\\ldots,x_n)$, on cherche √† partitionner les $n$ points en $k$ ensembles $S=\\{S_1,S_2,\\ldots,S_k\\}$ en minimisant la grandeur : $$I = \\sum_{i=1}^{k}\\sum_{x_j \\in S_i}||x_i-\\mu_i||^2$$ o√π $\\mu_i$ est le barycentre des points dans $S_i$.\n$I$ est la variance intra-classe ou inertie intra-classe (terme surtout utilis√© en anglais).\nChoix de k Choisir le bon nombre de clusters est crucial pour l\u0026rsquo;algorithme des k-moyennes, comme l\u0026rsquo;illustre l\u0026rsquo;exemple suivant¬†:\nMais ce n\u0026rsquo;est pas toujours simple (contrairement √† l\u0026rsquo;exemple) de deviner le bon nombre de clusters juste en inspectant les donn√©es. Alors comment faire¬†?\nOn pourrait se dire qu\u0026rsquo;il suffit de prendre le mod√®le avec la plus faible inertie. Mais malheureusement, l\u0026rsquo;inertie n\u0026rsquo;est pas une m√©trique adapt√©e au choix de k puisqu\u0026rsquo;elle ne fait que descendre quand k augmente\u0026hellip; Logique¬†: plus il y a de clusters, plus la distance intra-cluster diminue\u0026amp;nbsp!\nTra√ßons l\u0026rsquo;inertie en fonction de k pour y voir plus clair¬†: On remarque qu\u0026rsquo;ici, le nombre de clusters id√©al correspond au point d\u0026rsquo;inflexion de la courbe (ou, si on imagine un bras, au coude).\nConfirmons en simulant des donn√©es s√©par√©es en 5 tas et en retra√ßant la courbe. L√† encore, le coude indique le nombre k id√©al.\nOn semble donc avoir trouver une tactique utilisable lorsqu\u0026rsquo;on n\u0026rsquo;a pas d\u0026rsquo;autres indices.\ntip\nIl existe des m√©thodes plus pr√©cises pour d√©terminer k, mais elles sont aussi plus gourmandes en calcul. La plus r√©pandue utilise les coefficients de silhouette de chaque point (diff√©rence entre la distance moyenne avec les points du m√™me groupe (coh√©sion) et la distance moyenne avec les points des autres groupes voisins (s√©paration)).\nLimites L\u0026rsquo;algorithme des $k$-moyennes se confronte √† une difficult√© classique en apprentissage automatique, et plus g√©n√©ralement pour tout probl√®me d\u0026rsquo;optimisation : obtenir un minimum global plut√¥t qu\u0026rsquo;un minimum local.\nLa convergence vers un des minima locaux d√©pend crucialement de la position initiale des centres.\nDans l\u0026rsquo;exemple suivant, on obtient 3 partitionnements diff√©rents pour 3 initialisations diff√©rentes des centres.\nOn v√©rifie que les centres sont bien bloqu√©s sur leur position dans les deux premiers cas puisqu\u0026rsquo;aucun changement d\u0026rsquo;attribution n\u0026rsquo;est possible.\nDans l\u0026rsquo;algo classique, pour pallier au mieux ce probl√®me, on initialise les centres al√©atoirement et on relance l\u0026rsquo;algorithme un certain nombre de fois pour ne garder au final que la solution qui minimise l\u0026rsquo;inertie intra-classe.\nAutre souci des k-moyennes¬†: des difficult√©s pour partitionner des clusters de diff√©rentes tailles, diff√©rentes densit√©s ou des formes non sph√©riques.\nApplications L\u0026rsquo;algorithme des k-moyennes ne pr√©suppose rien sur les donn√©es et peut s\u0026rsquo;av√©rer, par le fait, tr√®s utile en premi√®re approche dans un r√¥le de d√©fricheur.\nL\u0026rsquo;algorithme permet aussi de trancher des d√©bats de la plus haute importance sur les couleurs comme \u0026ldquo;est-ce plus vert que bleu¬†?\u0026rdquo; en organisant un combat entre les centro√Ødes de chaque couleur.\nDans la m√™me veine, on peut utiliser k-moyennes pour segmenter une image par couleur, ce qui peut s\u0026rsquo;av√©rer int√©ressant pour identfier des zones (comme des for√™ts) sur des donn√©es satellite ou pour compresser des images. Le choix de k correspond alors au nombre de couleurs qu\u0026rsquo;on veut garder.\nLa simplicit√© de k-moyennes en fait un bon outil de d√©grossissage des donn√©es, y compris sur des donn√©es d√©j√† √©tiquet√©es. Cela permet de r√©duire leur dimensionnalit√©, avant d\u0026rsquo;utiliser des algorithmes plus complexes d\u0026rsquo;apprentissage supervis√©.\nApprentissage profond L\u0026rsquo;apprentissage profond (deep learning) r√©volutionne le secteur de l\u0026rsquo;IA dans les ann√©es 2010. Il consiste √† entra√Æner un ordinateur √† ‚Äúapprendre‚Äù en analysant un grand nombre d‚Äôexemples. Il fait cela √† l‚Äôaide de structures math√©matiques appel√©es r√©seaux de neurones, qui s‚Äôinspirent vaguement du fonctionnement du cerveau humain. L‚Äôid√©e de ‚Äúprofondeur‚Äù vient du fait que les r√©seaux de neurones utilis√©s dans le deep learning ont de nombreuses couches. Chaque couche effectue une partie de l‚Äôanalyse et transmet ses r√©sultats √† la suivante. C‚Äôest un peu comme si un probl√®me complexe √©tait r√©solu par une s√©rie d‚Äô√©tapes simples, chacune se concentrant sur un d√©tail particulier.\nQuoi de mieux que la le√ßon inaugurale au Coll√®ge de France d\u0026rsquo;un de ses fondateurs, Yann Le Cun, pour nous expliquer de quoi il retourne.\nMalheureusement, le m√®me qui suit r√©sume aujourd\u0026rsquo;hui assez bien notre compr√©hension fine du fonctionnement des mod√®les de deep learning¬†: on est devant une boite noire qui fait le job demand√© sans que l\u0026rsquo;on comprenne trop comment\u0026hellip;\nAv√®nement des grands mod√®les de langage Les grands mod√®les de langage (LLM en anglais) r√©volutionnent √† leur tour l\u0026rsquo;IA dans les ann√©es 2020. Avec leur architecture non-r√©currente, le transformeur, bas√©e sur un m√©canisme dit d\u0026rsquo;attention, ils peuvent avec succ√®s traiter des donn√©es s√©quentielles tout en √©tant parall√©lisable lors de l\u0026rsquo;entrainement (cela permet des gains en performance √©normes et ainsi d\u0026rsquo;augmenter de plusieurs ordres de grandeur le nombre de param√®tres de leurs mod√®les, d\u0026rsquo;o√π le qualificatif \u0026ldquo;grand\u0026rdquo;).\nStephen Wolfram a √©crit un long article p√©dagogique sur le fonctionnement de ChatGPT (l\u0026rsquo;agent conversationnel bas√© sur un LLM qui a sid√©r√© le grand public lors de sa mise √† disposition en 2022).\nLe g√©nial 3Blue1Brown a r√©alis√© une s√©rie de vid√©os sur les LLM, mais il a aussi pens√© aux gens press√©s avec cette vid√©o introductive¬†:\nUn champ de recherche immense s\u0026rsquo;est ouvert pour tenter de comprendre le fonctionnement interne des LLM. Des chercheurs d\u0026rsquo;Anthropic (l\u0026rsquo;entreprise √† l\u0026rsquo;origine de Claude) ont trouver une m√©thode pour aller en quelque sorte sonder le \u0026ldquo;cerveau\u0026rdquo; de Claude jusqu\u0026rsquo;√† localiser l\u0026rsquo;emplacement de diff√©rents concepts et r√©ussissant m√™me √† booster l\u0026rsquo;activation d\u0026rsquo;un concept par rapport aux autres. C\u0026rsquo;est ainsi qu\u0026rsquo;est n√© Golden Gate Claude qui pu, pendant 24h, interagir avec les utilisateurs et partager sa totale obsession pour le pont de San Francisco. Dans l\u0026rsquo;exemple d\u0026rsquo;interaction suivant, √† la fois dr√¥le et douloureux, Claude semble lutter contre sa psychose¬†:\nIA et √©thique L\u0026rsquo;intelligence artificielle repose en grande partie sur la collecte de donn√©es et leur traitement. Or des biais tr√®s importants aux cons√©quences potentiellement dramatique peuvent s\u0026rsquo;immiscer lors de cette √©tape cruciale.\nPrenons l\u0026rsquo;exemple du biais des sruvivants qui est un des plus importants biais de s√©lection.\nBiais des survivants Il tire son nom des efforts du statisticien Abraham Wald du Statistical Research Group qui analysait les impacts sur les bombardiers am√©riacains revenus de mission pendant la seconde guerre mondiale afin de d√©terminer les zones o√π il fallait am√©liorer le blindage. En voyant cette image l\u0026rsquo;erreur de logique consisterait √† vouloir blinder les zones les plus impact√©es. C\u0026rsquo;est tomber dans le biais des survivants¬†! En effet, les avions √©tudi√© sont ceux qui sont revenus, les \u0026ldquo;survivants\u0026rdquo;, ce qui signifie que les impacts re√ßus ne les ont pas d√©truit. Par contre, l\u0026rsquo;absence d\u0026rsquo;impact dans certaines zones pourrait indiquer qu\u0026rsquo;il s\u0026rsquo;agit l√† d\u0026rsquo;endroits critiques o√π un tir provoque plus certainement une avarie grave et donc l\u0026rsquo;absence de survivants y pr√©sentant des impacts. Il faut en conclusion blinder d\u0026rsquo;avantage les zones sans impact¬†!\nC\u0026rsquo;est le biais des survivants qui nous fait dire par exemple dire que les constructions anciennes √©taient plus solides ou la musique meilleure il y a quelques d√©cennies.\nLe biais du survivant s\u0026rsquo;immisce partout o√π il y a s√©lection puisque le crit√®re de s√©lection laisse fatalement de c√¥t√© les donn√©es ne respectant pas les crit√®res et il faut donc se garder de conclusions g√©n√©rales ne prenant pas en compte la population √©cart√©e.\nEt au final, le biais des survivants n\u0026rsquo;est bien qu\u0026rsquo;une forme de biais de s√©lection. En effet, un biais de s√©lection d√©signe g√©n√©ralement toute situation o√π l‚Äô√©chantillon utilis√© pour une analyse ou une prise de d√©cision n‚Äôest pas repr√©sentatif de la population globale en raison de la m√©thode utilis√©e pour s√©lectionner les donn√©es. Avec le biais des survivants, l\u0026rsquo;erreur de repr√©sentativit√© consiste √† consid√©rer, comme son nom l\u0026rsquo;indique, uniquement les survivants, mais ce n\u0026rsquo;est qu\u0026rsquo;un exemple parmi d\u0026rsquo;autre de s√©lection non repr√©sentative de donn√©es. L\u0026rsquo;entra√Ænement des IA, en particulier celui des IA g√©n√©ratives, est tr√®s sensible √† ces d√©fauts de repr√©sentation de la m√™me fa√ßon qu\u0026rsquo;un enfant √©lev√© dans un milieu particulier (secte par exemple) aura beaucoup de mal √† se faire une repr√©sentation adapt√©e du reste du monde.\nUne fois r√©colt√©es, les donn√©es sont tra√Æt√©es et en particulier, on cherche √† √©tablir des corr√©lations entre des groupes de donn√©es. En effet, ces corr√©lations permettent d\u0026rsquo;extrapoler dans des zones de l\u0026rsquo;espace vides de donn√©es et ainsi pr√©dire les valeurs ou cat√©gories de donn√©es manquantes. Par contre, on doit alors faire attention √† un autre biais classique r√©sum√© par un des mantras de la statistique : \u0026ldquo;La corr√©lation n\u0026rsquo;implique pas la causalit√©\u0026rdquo;.\nLa corr√©lation n\u0026rsquo;implique pas la causalit√© Pour expliquer la corr√©lation entre deux √©v√®nements, on a trop vite fait de supposer une relation de cause √† effet. C\u0026rsquo;est une possibilit√© mais ce n\u0026rsquo;est pas la seule¬†!\nAutres explications possibles de la corr√©lation¬†?\nR√©ponse (cliquer pour afficher) Hasard. Un troisi√®me facteur cause les deux autres (exemple classique de la corr√©lation entre la vente de glace et les attaques de requins ou entre le nombre d'enfants et le nombre de cigognes - on parle d'ailleurs parfois d'effet cigogne). Peut √™tre que la relation de cause √† effet est dans l'autre sens que le sens envisag√©. "
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/godel/",
	"title": "Logique modale",
	"tags": [],
	"description": "",
	"content": " Il y a diff√©rentes logiques de la prouvabilit√© dont celle de G√∂del-L√∂b. $\\Box\\phi$ y est interpr√©t√©e comme \u0026ldquo;la formule $\\phi$ est prouvable\u0026rdquo;.\nToute formule peut √™tre cod√©e par un entier (il suffit par exemple de regarder le code binaire du fichier de traitement de texte o√π cette formule est √©crite et d\u0026rsquo;ajouter \u0026ldquo;1\u0026rdquo; √† gauche du code pour √©viter la perte √©ventuelle de z√©ros non significatifs). De m√™me, on peut coder un arbre de preuve (puisqu\u0026rsquo;il peut lui-m√™me s\u0026rsquo;√©crire avec un logiciel de traitement de texte). D√®s lors, une formule peut tr√®s bien parler du fait qu\u0026rsquo;une autre formule est d√©montrable puisqu\u0026rsquo;il s\u0026rsquo;agit de dire qu\u0026rsquo;il existe un entier poss√©dant les propri√©t√©s qui le caract√©risent comme √©tant le code d\u0026rsquo;une preuve de cette formule. C\u0026rsquo;est ce que fait la formule $\\Box\\phi$.\nL\u0026rsquo;arithm√©tique de Peano $\\text{PA}$ est une th√©orie - ensemble d\u0026rsquo;axiomes - en logique du premier ordre dont l\u0026rsquo;arithm√©tique des entiers est un mod√®le. L\u0026rsquo;id√©e d\u0026rsquo;associer un nombre entier √† une formule ou √† une preuve a permis √† G√∂del d\u0026rsquo;injecter dans la th√©orie de l\u0026rsquo;arithm√©tique des d√©clarations sur la th√©orie elle-m√™me. Les nombres gagnent ainsi la capacit√© m√©ta de parler d\u0026rsquo;eux-m√™mes.\nLe premier th√©or√®me d\u0026rsquo;incompl√©tude de G√∂del s\u0026rsquo;appuie sur une formule affirmant sa propre non prouvabilit√© $\\neg\\Box\\phi\\leftrightarrow \\phi$. Si on peut d√©montrer la formule $\\neg\\Box\\phi\\leftrightarrow \\phi$ dans un syst√®me donn√©, alors on prouve qu\u0026rsquo;il existe une formule non d√©montrable et pourtant vraie, ce qui impose l\u0026rsquo;incompl√©tude du syst√®me.\nLa d√©monstration de G√∂del est un impresssionnant tour de force de 30 pages, mais elle devient bien plus triviale avec la notion de machine de Turing. La plupart des langages de programmation (comme Python) sont Turing-complet, ce qui signifie qu\u0026rsquo;ils sont √©quivalents √† une machine de Turing universelle capable de calculer tout ce qui est calculable. Or Turing a aussi d√©couvert ce qu\u0026rsquo;une telle machine ne sait pas faire¬†: d√©terminer (en un temps fini) si un programme (une suite d\u0026rsquo;instructions) va tourner √† l\u0026rsquo;infini ou finir par s\u0026rsquo;arr√™ter. C\u0026rsquo;est le probl√®me de l\u0026rsquo;arr√™t.\nPour prouver qu'aucun programme ne peut r√©soudre le probl√®me de l'arr√™t, Turing a raisonn√© par l'absurde\u0026nbsp;:\nsupposons qu'un tel programme $\\text{P}$ existe. On peut alors construire un programme $\\text{P'}$ √† partir de $\\text{P}$ qui, lorsqu'on lui fournit le code d'un programme $\\text{Q}$ en entr√©e Le th√©or√®me de L√∂b stipule que les formules dont on peut prouver que $\\Box\\phi\\rightarrow\\phi$ sont les formules prouvables¬†:\n$\\Box(\\Box\\phi\\rightarrow\\phi)\\rightarrow\\Box\\phi$. La logique de la prouvabilit√© de G√∂del-L√∂b consiste √† partir du syst√®me $\\text{\\bf K}$ (d\u0026rsquo;ailleurs la formule $\\text{K}$ ne d√©crit ici rien d\u0026rsquo;autre que le modus ponens¬†; si j\u0026rsquo;ai une preuve de $\\phi\\rightarrow\\psi$ et une preuve de $\\phi$, alors j\u0026rsquo;ai une preuve de $\\psi$) auquel on ajoute l\u0026rsquo;axiome $\\text{(4)}=\\Box P\\rightarrow \\Box\\Box P$ (si j\u0026rsquo;ai une preuve de $P$, alors j\u0026rsquo;ai une preuve du fait que j\u0026rsquo;ai une preuve de $P$) ainsi que le th√©or√®me de L√∂b ($\\text{L}$).\nEn appelant $\\mathscr{C}_{tr.,b.d.}$ la classe des mod√®les de Kripke √† la fois transitifs et born√©s √† droite (sans suite infinie de mondes), alors on a le r√©sultat suivant¬†:\n$ \\Gamma\\vdash_\\text{\\bf GL} \\phi$ si et seulement si $\\Gamma\\Vdash_{\\mathscr{C}_{tr.,b.d.}}\\phi$ "
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique4/",
	"title": "Logique modale",
	"tags": [],
	"description": "",
	"content": " info\nNotes de lecture du livre La logique pas √† pas de Jacques Duparc que je paraphrase all√©grement.\nLogique modale Syntaxe et s√©mantique Syst√®mes logiques Diff√©rentes logiques modales Une petite communaut√© de dragons vivent sur une √Æle aux r√®gles sp√©ciales. Si un dragon apprend qu\u0026rsquo;il a les yeux bleus, c\u0026rsquo;est la honte et il doit partir le soir m√™me. Mais les autres dragons n\u0026rsquo;ont pas le droit de lui dire et aucun de ces dragons n\u0026rsquo;a de reflet et ne peut donc constater la couleur de ses propres yeux.\nTrois dragons peuplent l\u0026rsquo;√Æle, Apophis, Bahamut et Carax√®s, lorsqu\u0026rsquo;un jour, un √©tranger d√©barque.\nL\u0026rsquo;√©tranger d√©clare¬†:\nl\u0026rsquo;un de vous au moins a les yeux bleus.\nLe premier soir, aucun dragon ne part.\nLe deuxi√®me soir, toujours rien.\nLe troisi√®me soir, ils partent tous les trois\u0026hellip;\nLa logique modale va permettre de mod√©liser ce petit probl√®me.\nSyntaxe La syntaxe de la logique modale est celle du calcul des propositions √† laquelle on ajoute deux op√©rateurs unaires, les op√©rateurs de modalit√©¬†:\nLa boite $\\Box$ Le diamant $\\Diamond$ On va √™tre amen√© √† non pas d√©finir un seul $\\Box$ et $\\Diamond$ mais potentiellement une infinit√© (d√©nombrable).\nOn √©crira alors $[i]$ et $\\langle i \\rangle$, o√π $i$ varie dans un ensemble $I$.\nLe langage $\\mathcal{L}$ de la logique modale est l\u0026rsquo;ensemble suivant¬†:\n$\\mathcal{L}=VAR\\cup\\set{\\top,\\bot,\\neg,\\lor,\\land\\rightarrow,\\leftrightarrow,(,)}\\cup\\set{[i]:i\\in\\mathbb{N}}\\cup\\set{\\langle i\\rangle:i\\in\\mathbb{N}}$\nSoient $\\phi$, $\\psi$, $\\theta$ trois formules, on note $\\phi[\\theta/\\psi]$ la substitution uniforme de la formule $\\theta$ √† toutes les occurrences de $\\psi$ dans $\\phi$.\nUne substitution uniforme consiste dans un premier temps √† retirer toutes les occurrences de $\\psi$ puis √† remplacer chacune par $\\theta$.\nS√©mantique Syst√®me de transition On va utiliser les mod√®les de Kripke, c\u0026rsquo;est-√†-dire des graphes dirig√©s, pour interpr√©ter les formules de la logique modale. Mais on se cantonnera ici √† la logique classique (les interpr√©tations de la n√©gation et de l\u0026rsquo;implication seront traditionnels et non intuitionnistes). Chaque n≈ìud va correspondre √† des mondes possibles qui sont ou non li√©s entre eux par des arcs.\nOn appelle ces graphes dirig√©s des syst√®mes de transition (frame en anglais)¬†:\nPour $I$ un ensemble non vide, un syst√®me de transition √©tiquet√© par $I$ (ou de signature $I$) est une structure relationnelle (un graphe dirig√© √©tiquet√©) $$\\mathcal{S}=(N,A)$$ o√π $N$ est un ensemble non vide dont les √©l√©ments sont appel√©s n≈ìuds et $A$ est un ensemble de ralations binaires sur $\\mathbb{N}$ indic√©es par $I$ ($A=\\set{A_i\\subseteq N\\times N|i\\in I}$).\nUn syst√®me de transition $\\mathcal{S}(N,A)$ (avec $A=\\set{A_i\\subseteq N\\times N|i\\in I}$) est dit\nr√©flexif si, pour tout $i\\in I$ et pour tout $a\\in N$, il v√©rifie $a\\xrightarrow{i}a$\u0026nbsp;; sym√©trique si, pour tout $i\\in I$ et pour tout $a,b\\in N$, d√®s qu'il v√©rifie $a\\xrightarrow{i}b$, il v√©rifie aussi $b\\xrightarrow{i}a$\u0026nbsp;; transitif si, pour tout $i\\in I$ et pour tout $a,b,c\\in N$, d√®s qu'il v√©rifie √† la fois $a\\xrightarrow{i}b$ et $b\\xrightarrow{i}c$, il v√©rifie aussi $a\\xrightarrow{i}c$\u0026nbsp;; fam√©lique si, pour tout $i\\in I$ et pour tout $a,b\\in N$, d√®s qu'il v√©rifie $a\\xrightarrow{i} b$ alors $a=b$\u0026nbsp;; dense si, pour tout $i\\in I$ et pour tout $a,c\\in N$, d√®s qu'il v√©rifie $a\\xrightarrow{i} c$, alors il existe $b$ tel que $a\\xrightarrow{i} b$ et $b\\xrightarrow{i} c$\u0026nbsp;; non born√© √† droite si, pour tout $i\\in I$ et pour tout $a\\in N$, il existe un n≈ìud $b\\in N$ tel que $a\\xrightarrow{i}b$\u0026nbsp;; euclidien si, pour tout $i\\in I$ et pour tout $a,b,c\\in N$, d√®s qu'il v√©rifie √† la fois $a\\xrightarrow{i} b$ et $a\\xrightarrow{i} c$, alors il v√©rifie aussi $b\\xrightarrow{i} c$. Satisfaction Un syst√®me de transition se mue en mod√®le de la logique modale (mod√®le de Kripke) d√®s qu\u0026rsquo;il est √©quip√© d\u0026rsquo;une valuation portant sur les variables √©tudi√©es. Une valuation indique pour chaque n≈ìud du graphe les variables qui sont forc√©es en ce n≈ìud.\nSoient $I$ un ensemble non vide et $\\mathcal{S}=(N,A)$ un syst√®me de transition √©tiquet√© par $I$.\nUne valuation sur ce syst√®me de transition $\\mathcal{S}$ est une fonction¬†: $$ \\begin{array}{ccc} \\mathcal{V}: VAR \u0026amp; \\to \u0026amp; \\mathcal{P}(N)\\\\ \\phantom{\\mathcal{V}:}P \u0026amp; \\mapsto \u0026amp; \\set{a|a\\Vdash P} \\end{array} $$\nLa satisfaction d\u0026rsquo;une formule $\\phi$ au n≈ìud $a$ (au sein de $\\mathcal{S}$ et pour la valuation $\\mathcal{V}$) est not√©e $a\\Vdash\\phi$ et se d√©finit par induction sur la hauteur de $\\phi$¬†:\n$a\\Vdash\\top$ et $a\\nVdash\\bot$ $a\\Vdash P$ ssi $a\\in\\mathcal{V}(P)$ (pour une variable $P$ quelconque) $a\\Vdash\\neg\\phi$ ssi $a\\nVdash\\phi$ $a\\Vdash\\phi\\lor\\psi$ ssi ($a\\Vdash\\phi$ ou $a\\Vdash\\psi$) $a\\Vdash\\phi\\land\\psi$ ssi ($a\\Vdash\\phi$ et $a\\Vdash\\psi$) $a\\Vdash\\phi\\rightarrow\\psi$ ssi ($a\\nVdash\\phi$ ou $a\\Vdash\\psi$) $a\\Vdash\\phi\\leftrightarrow\\psi$ ssi (($a\\Vdash\\phi$ et $a\\Vdash\\psi$) ou ($a\\nVdash\\phi$ et $a\\nVdash\\psi$)) $a\\Vdash[i]\\phi$ ssi pour tout $b$, si $a\\xrightarrow{i}b$, alors $b\\Vdash\\phi$ $a\\Vdash\\langle i\\rangle\\phi$ ssi il existe $b$ tel que $a\\xrightarrow{i}b$ et $b\\Vdash\\phi$ S\u0026rsquo;il n\u0026rsquo;existe pas de n≈ìud $b$ tel que $a\\xrightarrow{i}b$, alors quelle que soit la formule $\\phi$, $a\\Vdash\\langle i\\rangle\\phi$ est toujours fausse. Autrement dit, $a\\nVdash \\langle i\\rangle\\phi$.\nPar contre, s\u0026rsquo;il n\u0026rsquo;existe pas de n≈ìud $b$ tel que $a\\xrightarrow{i}b$, alors $a\\Vdash[i]\\phi$ est toujours vraie quelle que soit la formule $\\phi$. En effet pour que $a\\Vdash[i]\\phi$ soit v√©rifi√©e il faut que si $a\\xrightarrow{i}b$, alors $b\\Vdash\\phi$. Mais s\u0026rsquo;il n\u0026rsquo;y a aucun $a\\xrightarrow{i}b$ alors l\u0026rsquo;implication est tojours v√©rifi√©e.\nLa satisfaction d\u0026rsquo;une formule en un n≈ìud $a$ d\u0026rsquo;un syst√®me de transition $\\mathcal{S}$ √©quip√© d\u0026rsquo;une valuation $\\mathcal{V}$ d√©pend de ces trois ingr√©dients et on notera donc $\\langle \\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\phi$.\nRevenons √† nos dragons¬†:\nExemple des dragons aux yeux bleus les mondes possibles (les n≈ìuds du graphe) vont correspondre aux diff√©rentes combinaisons possibles de couleurs d\u0026rsquo;yeux pour les trois dragons $A$, $B$ et $C$. On a deux possibilit√©s (yeux bleus ou non) pour chacun des trois dragons. Cela fait 8 mondes possibles.\nLes arcs entre mondes vont √™tre √©tiquet√©s chacun par un des trois dragons et vont correspondre aux diff√©rents mondes que ce dragon pense possible depuis le monde o√π il est.\nPar exemple, dans le monde o√π seul $B$ a les yeux bleus (le monde 3) $A$ imagine aussi possible le monde o√π $A$ et $B$ ont tous les deux les yeux bleus (le monde 5). Une fl√®che √©tiquet√©e par $A$ rejoint donc ces deux mondes possibles. Et bien s√ªr, dans le monde o√π $A$ et $B$ ont les yeux bleus, $A$ imagine possible le monde o√π seul $B$ a les yeux bleus (il y a donc une fl√®che de 5 vers 3). Et comme c\u0026rsquo;est vrai dans chaque cas et pour les trois dragons, cela montre que ce syst√®me de transition est sym√©trique. Il est de plus √©vident que chaque monde semble possible pour les dragons qui s\u0026rsquo;y trouvent. Le syst√®me est donc aussi r√©flexif.\nImaginons que l\u0026rsquo;on soit dans le monde 2. L\u0026rsquo;ensemble des mondes possibles se simplifient alors grandement.\nLa d√©claration de l\u0026rsquo;√©tranger n\u0026rsquo;est pas une surprise pour $B$ et $C$ qui voient bien que le pauvre $A$ a les yeux bleus et est donc sans effet pour eux. Par contre $A$ comprend que l\u0026rsquo;autre monde qu\u0026rsquo;il pensait possible, le monde b√©ni o√π il n\u0026rsquo;avait pas des yeux infames (le monde 1) est en fait impossible.\nAvant la d√©claration de l\u0026rsquo;√©tranger, $2\\Vdash\\color{#00AB8E}\\langle A\\rangle \\color{#000}\\neg A$ puisqu\u0026rsquo;il y a une fl√®che √©tiquet√©e par $A$ qui rejoint 1 (qui r√©alise $\\neg A$). On peut traduire √ßa par¬†: \u0026ldquo;dans le monde 2, Apophis croit possible qu\u0026rsquo;il n\u0026rsquo;ait pas les yeux bleus\u0026rdquo;. De m√™me $2\\Vdash\\color{#FEAE00}\\langle B\\rangle \\color{#000}\\neg B$ √† cause de la fl√®che √©tiquet√©e par $B$ vers le monde 5 et $2\\Vdash\\color{#FF42A1}\\langle C\\rangle \\color{#000}\\neg C$ √† cause de la fl√®che √©tiquet√©e par $C$ vers le monde 6. Donc chaque dragon croit encore possible d\u0026rsquo;avoir les yeux bleus. D\u0026rsquo;autre part, on peut √©crire que $2\\Vdash\\color{#FF42A1}[C] \\color{#000} A$ puisque les trois arcs √©tiquet√©s par $C$ qui partent de 2 vont dans des mondes (2, 5 et 6) o√π $A$ est r√©alis√©e. √áa peut se lire¬†: \u0026ldquo;dans le monde 2, Carax√®s sait qu\u0026rsquo;Apophis a les yeux bleus\u0026rdquo;.\nOn peut aussi compliquer les √©nonc√©s avec par exemple¬†: $6 \\Vdash \\color{#FF42A1}\\langle C\\rangle \\color{#FEAE00}\\langle B\\rangle \\color{#000} (B\\land\\neg C)$ (une fl√®che rose part de 6 vers 2 et une fl√®che jaune part de 2 vers 5 o√π $B$ a les yeux bleus mais pas $C$).\nApr√®s la d√©claration de l\u0026rsquo;√©tranger, le monde 1 dispara√Æt¬†!\nSi $B$ et $C$ sont toujours dans le doute quant √† la couleur de leurs yeux, pour $A$, les jeux sont faits. En effet, on a dor√©navant $2\\Vdash\\color{#00AB8E}[A] \\color{#000} A$. Et le soir m√™me, Apophis s\u0026rsquo;en va tout penaud.\nEn g√©n√©ralisant, dans les mondes o√π seul un dragon a les yeux bleus (2, 3 et 4), ce dragon le comprend au moment de la d√©claration de l\u0026rsquo;√©tranger et se retire le soir m√™me.\nImaginons maintenant que l\u0026rsquo;on soit dans le monde 5 o√π Apophis et Bahamut ont les yeux bleus.\nLa d√©claration de l\u0026rsquo;√©tranger ne fait pas beaucoup d\u0026rsquo;effet car ils √©taient d√©j√† tous les trois au courant qu\u0026rsquo;au moins l\u0026rsquo;un d\u0026rsquo;eux avait les yeux bleus\u0026hellip; Par cons√©quent, le soir m√™me, personne ne part. Et √ßa par contre, √ßa change pas mal de choses¬†! En effet, les mondes 2 et 3 deviennet d\u0026rsquo;un coup impossibles puisque on a vu que les mondes o√π seul un des dragon a les yeux bleus voient ce dragon d√©camper d√®s le premier soir.\nOn a maintenant¬†: $5\\Vdash (\\color{#00AB8E}[A] \\color{#000} A)\\land (\\color{#FEAE00}[B] \\color{#000} B)$. $A$ et $B$ ont acquis la certitude d\u0026rsquo;avoir les yeux bleus et s\u0026rsquo;envolent au deuxi√®me soir.\nLes deux dragons aux yeux bleus des mondes 7 ou 6 auraient fait de m√™me.\nLa derni√®re situation possible correspond au monde 8.\nOn reprend le raisonnement pr√©c√©dent. Personne ne part au premier soir, mais cette fois-ci, √ßa ne permet d\u0026rsquo;√©liminer aucun monde possible\u0026hellip; Et par cons√©quent, au deuxi√®me soir, personne ne s\u0026rsquo;envole. Mais l√†, bingo¬†! √áa √©limine les mondes o√π seuls deux dragons ont les yeux piscine. Finalement, ils comprennent tous les trois qu\u0026rsquo;ils sont banis et s\u0026rsquo;envolent au troisi√®me soir.\nLa situation de d√©part correspondait donc au monde 8.\nOn peut remarquer en passant diff√©rentes √©quivalences¬†:\n$a\\Vdash\\neg\\color{#00AB8E}[A] \\color{#000} \\phi$ ssi $a\\Vdash \\color{#00AB8E}\\langle A\\rangle \\color{#000} \\neg \\phi$\nPour A, ne pas savoir quelque chose est √©quivalent √† croire possible sont contraire. $a\\Vdash\\color{#00AB8E}[A] \\color{#000} \\phi$ ssi $a\\Vdash \\neg\\color{#00AB8E}\\langle A\\rangle \\color{#000} \\neg \\phi$\nPour A, savoir quelque chose est identique √† ne pas croire possible le contraire de cette chose. $a\\Vdash\\neg\\color{#00AB8E}[A] \\color{#000} \\neg\\phi$ ssi $a\\Vdash \\color{#00AB8E}\\langle A\\rangle \\color{#000} \\phi$\nPour A, croire possible quelque chose revient √† ne pas savoir le contraire de cette chose. Mod√®les et classes Soient $\\phi$ une formule, $I$ un ensemble non vide, $\\mathcal{S}=(N,A)$ un syst√®me de transition √©tiquet√© par $I$, $a$ un n≈ìud de $\\mathcal{S}$ et $\\mathcal{V}$ une valuation,\n$\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}\\phi$ ssi pour tous n≈ìuds $a'$, $\\langle\\mathcal{S},\\mathcal{V},a'\\rangle\\Vdash\\phi$ $\\langle\\mathcal{S},a\\rangle\\Vdash^{\\forall \\mathcal{V}}\\phi$ ssi pour toutes valuations $\\mathcal{V'}$, $\\langle\\mathcal{S},\\mathcal{V'},a\\rangle\\Vdash\\phi$ $\\mathcal{S}\\Vdash^{\\forall a,\\forall \\mathcal{V}}\\phi$ ssi pour tous n≈ìuds $a'$ et toutes valuations $\\mathcal{V'}$, $\\langle\\mathcal{S},\\mathcal{V'},a'\\rangle\\Vdash\\phi$ Un mod√®le de la logique modale (mod√®le de Kripke) est un syst√®me de transition √©quip√© d\u0026rsquo;une valuation¬†: $$\\mathcal{M}=\\langle\\mathcal{S},\\mathcal{V}\\rangle$$ Une formule $\\phi$ est vraie (ou satisfaite) dans le mod√®le $\\langle\\mathcal{S},\\mathcal{V}\\rangle$ si elle est forc√©e en chacun des n≈ìuds du syst√®me de transition¬†: $$\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}\\phi$$\nLa notion de v√©rit√© dans un mod√®le de Kripke est donc une notion de v√©rit√© globale puisqu\u0026rsquo;elle prend en compte l\u0026rsquo;ensemble des n≈ìuds du syst√®me de transition.\nPrenons l\u0026rsquo;exemple du syst√®me de transition $\\mathcal{S}$ √©quip√© de la valuation $\\mathcal{V}$ ci-dessous¬†:\nOn a bien $\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}Q\\lor\\neg Q$. Par contre, comme $b\\Vdash Q$ alors que $a\\Vdash\\neg Q$, $\\langle\\mathcal{S},\\mathcal{V}\\rangle\\nVdash^{\\forall a}Q$ et $\\langle\\mathcal{S},\\mathcal{V}\\rangle\\nVdash^{\\forall a}\\neg Q$.\nOn peut donc avoir une formule $\\phi\\lor\\psi$ satisfaite sans que ni $\\phi$ ni $\\psi$ ne le soient¬†!\ninfo\nOn retrouve les mod√®les du calcul des propositions en se restreignant aux syst√®mes de transition contenant un seul n≈ìud. Dit autrement, une formule $\\phi$ de profondeur modale nulle est une formule du calcul des propositions.\nSoit $\\mathcal{M}$ un mod√®le du calcul des propositions d√©fini par la distribution de valeur de v√©rit√© $\\delta_\\mathcal{M}$ qui associe aux variables apparaissant dans $\\phi$ la valeur 1 lorsqu\u0026rsquo;elles sont vraies dans $\\mathcal{M}$ et 0 sinon ($\\delta_\\mathcal{M}(P)=1$ ssi $\\mathcal{M}\\models P$). La formule $\\phi$ st vraie dans $\\mathcal{M}$ (not√© $\\mathcal{M}\\models\\phi$) si et seulement si cette m√™me formule $\\phi$ est vraie dans le syst√®me de transition $\\mathcal{S}_\\mathcal{M}=(N,A)$, o√π $N=\\set{a}$ et $A=\\empty$, √©quip√© de la valuation $\\mathcal{V}_{\\delta_\\mathcal{M}}$ d√©finie par¬†: $\\mathcal{V}_{\\delta_\\mathcal{M}}(P)=\\set{a}$ si et seulement si $\\delta_\\mathcal{M}(P)=1$ et $\\mathcal{V}_{\\delta_\\mathcal{M}}(P)=\\empty$ si et seulement si $\\delta_\\mathcal{M}(P)=0$.\nCela s\u0026rsquo;√©crit¬†: $\\mathcal{M}\\models\\phi$ ssi $\\langle\\mathcal{S},\\mathcal{V}_{\\delta_\\mathcal{M}}\\rangle\\Vdash^{\\forall a}\\phi$\nSoit $\\phi$ une formule de la logique modale et $\\mathcal{S}$ un syst√®me de transition.\nLa formule $\\phi$ est valide dans $\\mathcal{S}$ si¬†: $$\\mathcal{S}\\Vdash^{\\forall a,\\forall \\mathcal{V}} \\phi$$\nLa notion de validit√© est globale, elle aussis. Et √† nouveau, une formule $\\phi\\lor\\psi$ peut √™tre valide dans un syst√®me de transition sans que ni $\\phi$, ni $\\psi$ ne le soit.\nSoient $\\phi$ une formule de profondeur modale nulle et $\\mathcal{S}=(N,A)$ un syst√®me de transition.\nLes affirmations suivantes sont √©quivalentes¬†:\n$\\mathcal{S}\\Vdash^{\\forall a,\\forall\\mathcal{V}} \\phi$ Il existe $a\\in N$ tel que $\\langle\\mathcal{S},a\\rangle\\Vdash^{\\forall \\mathcal{V}}\\phi$ Pour $\\mathcal{T}=(\\set{a},\\empty)$, $\\langle\\mathcal{T},a\\rangle\\Vdash^{\\forall \\mathcal{V}}\\phi$ $\\phi$ est une tautologie du calcul des propositions On va montrer la suite des implications suivante $(1)\\rightarrow(2)$, $(2)\\rightarrow(3)$, $(3)\\rightarrow(4)$ et $(4)\\rightarrow(1)$. La circularit√© nous permettra alors d\u0026rsquo;obtenir toutes les autres implications. Par exemple, $(2)\\rightarrow(1)$ d√©coule de $(2)\\rightarrow(3)\\rightarrow(4)\\rightarrow(1)$.\n$(1)\\rightarrow(2)$ et $(2)\\rightarrow(3)$ sont imm√©diates. Pour montrer $(3)\\rightarrow(4)$, nous allons montrer la contrapos√©e $\\neg(4)\\rightarrow\\neg(3)$.\nSupposons que $\\phi$ n'est pas une tautologie du calcul des propositions. Il existe donc un mod√®le $\\mathcal{M}$ pour lequel la formule est fausse. √âquipons alors le syst√®me de transition $\\mathcal{T}=(\\set{a},\\empty)$ de la valuation o√π pour chacune des variables propositionnelles $P$ apparaissant dans $\\phi$, $a\\in\\mathcal{V}(P)$ si et seulement si $\\mathcal{M}\\models P$.\nPar d√©finition de la satisfaction locale d'une formule $\\langle \\mathcal{T},\\mathcal{V},a\\rangle\\Vdash \\phi$ ssi $\\mathcal{M}\\models\\phi$.\nComme notre hypoth√®se est que $\\mathcal{M}\\not\\models\\phi$, on en d√©duit que $\\langle \\mathcal{T},\\mathcal{V},a\\rangle\\nVdash \\phi$. On a donc trouv√© une valuation qui contredit $(3)$. Pour montrer $(4)\\rightarrow(1)$, on passe √©galement par la contrapos√©e $\\neg(1)\\rightarrow\\neg(4)$.\nOn suppose que $\\mathcal{S}\\Vdash^{\\forall a,\\forall \\mathcal{V}}\\phi$ n'est pas satisfaite, et donc qu'il existe une valuation $\\mathcal{V}$ et un n≈ìud $a$ tels que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\phi$. On d√©finit le mod√®le du calcul des propositions $\\mathcal{M}$ par $\\mathcal{M}\\models P$ si et seulement si $a\\in\\mathcal{V}(P)$, et ce pour chaque variable propositionnelle apparaissant dans $\\phi$. On a donc $\\mathcal{M}\\not\\models\\phi$. Par cons√©quent, $\\phi$ n'est pas une tautologie. Soit $\\phi$ une formule et $\\mathscr{C}$ une classe de syst√®mes de transition.\n$\\mathscr{C}\\Vdash\\phi$ ssi pour tout $\\mathcal{S}\\in\\mathscr{C}, \\mathcal{S}\\Vdash^{\\forall a,\\forall\\mathcal{V}}\\phi$ ($\\phi$ est valide dans tout syst√®me de la classe $\\mathscr{C}$).\nSoit $\\mathscr{C}$ la classe de tous les syst√®mes de transition.\n$\\mathscr{C}\\Vdash\\Box(P\\rightarrow Q)\\rightarrow(\\Box P\\rightarrow \\Box Q)$\nSoient $\\mathcal{S}$ un syst√®me de transition, $a$ un n≈ìud de $\\mathcal{S}$ et $\\mathcal{V}$ une valuation quelconque.\nSi $\\langle \\mathcal{S},\\mathcal{V}, a\\rangle\\Vdash\\Box(P\\rightarrow Q)$, alors pour tout $b$ tel que $a\\longrightarrow b$, $\\langle \\mathcal{S},\\mathcal{V}, b\\rangle\\Vdash P\\rightarrow Q$.\nPour montrer $\\langle \\mathcal{S},\\mathcal{V}, a\\rangle\\Vdash\\Box P\\rightarrow \\Box Q$, il suffit de supposer $\\langle \\mathcal{S},\\mathcal{V}, a\\rangle\\Vdash\\Box P$ et de montrer $\\langle \\mathcal{S},\\mathcal{V}, a\\rangle\\Vdash\\Box Q$.\nOr, si $\\langle \\mathcal{S},\\mathcal{V}, a\\rangle\\Vdash\\Box P$, alors $\\langle \\mathcal{S},\\mathcal{V}, b\\rangle\\Vdash P$ pour tout $b$ tel que $a\\longrightarrow b$. Et comme par hypoth√®se $\\langle \\mathcal{S},\\mathcal{V}, b\\rangle\\Vdash P\\rightarrow Q$, on en d√©duit $\\langle \\mathcal{S},\\mathcal{V}, b\\rangle\\Vdash Q$.\nEt comme c\u0026rsquo;est vrai pour tout $b$ successeur de $a$, on en d√©duit $\\langle \\mathcal{S},\\mathcal{V}, a\\rangle\\Vdash\\Box Q$.\nPour tout syst√®me de transition $\\mathcal{S}$, les affirmations suivantes sont √©quivalents¬†:\n$\\mathcal{S}$ est r√©flexif Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\Box\\phi\\rightarrow\\phi)$ $(1)\\Rightarrow(2)$\nSi un n≈ìud $a$ quelconque, pour une valuation quelconque, v√©rifie $a\\Vdash\\Box \\phi$, alors par d√©finition, pour tout $b$ tel que $a\\longrightarrow b$, $b\\Vdash\\phi$. Puisque $\\mathcal{S}$ est r√©flexif, $a$ est lui-m√™me un de ces $b$ et donc $a\\Vdash\\phi$. $(2)\\Rightarrow(1)$\nSupposons que $\\mathcal{S}$ ne soit pas r√©flexif. Il s'en suit qu'il existe un n≈ìud $a$ tel que $a\\,\\,\\not\\!\\!\\longrightarrow a$. Soit $\\phi=P$ une formule de hauteur nulle et $\\mathcal{V}$ une valuation telle que $b\\in\\mathcal{V}(P)$ si et seulement si $a\\longrightarrow b$. Par d√©finition de $\\mathcal{V}$, on a √† la fois $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box P$ et $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash P$, donc $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash (\\Box P\\rightarrow P)$. Cela prouve ainsi que $\\mathcal{S}\\nvdash ^{\\forall a,\\forall\\mathcal{V}}(\\Box P\\rightarrow P)$, ce qui contredit l'hypoth√®se. On note $\\mathscr{C}_{ref.}$ la classe de tous les syst√®mes de transition r√©flexifs.\nPour tout syst√®me de transition $\\mathcal{S}$, les affirmations suivantes sont √©quivalents¬†:\n$\\mathcal{S}$ est fam√©lique Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\phi\\rightarrow\\Box\\phi)$ $(1)\\Rightarrow(2)$\nUn syst√®me de transition $\\mathcal{S}$ muni de la valuation $\\mathcal{V}$ satisfait la formule $\\phi\\rightarrow\\Box\\phi$ au n≈ìud $a$ si, $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\phi$ entra√Æne $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\phi$. Autrement dit, d√®s qu'une formule est satisfaite en un n≈ìud, elle doit √™tre satisfaite en chacun des successeurs de ce n≈ìud.\nSi le seul successeur possible d'un n≈ìud est lui-m√™me (syst√®me fam√©lique), l'implication tient toujours. $(2)\\Rightarrow(1)$\nDans le cas o√π il existe un n≈ìud $a$ poss√©dant un successeur $b$ diff√©rent de lui-m√™me, il suffit de consid√©rer la formule $\\phi=P$ et la valuation $\\mathcal{V}$ qui v√©rifie $a\\in\\mathcal{V}(P)$ et $b\\not\\in\\mathcal{V}(P)$ pour obtenir\u0026nbsp;:\n$\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\phi$ $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\Box\\phi$ $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\phi\\rightarrow\\Box\\phi$ Pour tout syst√®me de transition $\\mathcal{S}$, les affirmations suivantes sont √©quivalents¬†:\n$\\mathcal{S}$ est transitif Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\Box\\phi\\rightarrow \\Box\\Box\\phi )$ $(1)\\Rightarrow(2)$\nSoient $\\mathcal{S}$ un syst√®me de transition transitif, $\\mathcal{V}$ une valuation et $a$ un n≈ìud.\nSi $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\phi$ alors pour tout n≈ìud $b$ tel que $a\\longrightarrow b$, $\\langle\\mathcal{S},\\mathcal{V},b\\rangle\\Vdash\\phi$.\nPour montrer $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\Box\\phi$, il suffit de montrer que pour tout $c$ tel qu'il existe $b$ v√©rifiant $a\\longrightarrow b$ et $b\\longrightarrow c$, on a $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\Vdash\\phi$.\nOr puisque $\\mathcal{S}$ est transitif, $a\\longrightarrow b$ et $b\\longrightarrow c$ impliquent $a\\longrightarrow c$. D'o√π $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\phi$ entra√Æne $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\Vdash\\phi$. $(2)\\Rightarrow(1)$\nSoit un syst√®me de transition non transitif $\\mathcal{S}$ et soient $a$, $b$ et $c$ trois n≈ìuds de $\\mathcal{S}$ tels que $a\\longrightarrow b$, $b\\longrightarrow c$ et $a\\not\\!\\!\\longrightarrow c$. Soit $\\mathcal{V}$ une valuation qui v√©rifie $d\\in\\mathcal{V}(P)$ si et seulement si $a\\rightarrow d$.\nPour $\\phi=P$, nous obtenons donc √† la fois $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\phi$ et $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\nVdash\\phi$ ce qui montre que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\Box\\Box\\phi$. On note $\\mathscr{C}_{tr.}$ la classe de tous les syst√®mes de transition transitifs.\nPour tout syst√®me de transition $\\mathcal{S}$, les affirmations suivantes sont √©quivalents¬†:\n$\\mathcal{S}$ est sym√©trique Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\phi\\rightarrow \\Box\\Diamond\\phi)$ On peut traduire $\\Box\\Diamond\\phi$ par \u0026ldquo;pour tout successeur possible, $\\phi$ est vrai dans au moins un successeur\u0026rdquo;.\n$(1)\\Rightarrow(2)$\nSoient $\\mathcal{S}$ un syst√®me de transition sym√©trique, $\\mathcal{V}$ une valuation et $a$ un n≈ìud.\nOn suppose $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash \\phi$. Pour tout n≈ìud $a$, soit $a$ n'a pas de successeur et la formule $\\phi\\rightarrow \\Box\\Diamond\\phi$ est trivialement v√©rifi√©e (toute formule commen√ßant par $\\Box$ est vraie en un n≈ìud sans successeur), soit $a$ poss√®de un ou plusieurs successeurs et chacun d'entre eux poss√®de alors $a$ comme successeur par sym√©trie du graphe. Tous ces n≈ìuds ont donc au moins un successeur, $a$, o√π $\\phi$ est vraie. Par cons√©quent, tous les successeurs de $a$ forcent $\\Diamond \\phi$. Et finalement, on a bien $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash \\Box\\Diamond\\phi$. $(2)\\Rightarrow(1)$\nS'il existe deux n≈ìuds $a$ et $b$ tels que $a\\longrightarrow b$ mais $b\\,\\,\\not\\!\\!\\longrightarrow a$, il suffit de consid√©rer une valuation telle que $a\\Vdash P$ et $c\\Vdash\\neg P$ pour tout n≈ìud $c$ tel que $b\\longrightarrow c$ pour obtenir $a\\nVdash P \\rightarrow\\Box\\Diamond P$. On note $\\mathscr{C}_{sym.}$ la classe de tous les syst√®mes de transition sym√©triques.\nPour tout syst√®me de transition $\\mathcal{S}$, les affirmations suivantes sont √©quivalents¬†:\n$\\mathcal{S}$ est dense Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\Box\\Box\\phi\\rightarrow \\Box\\phi)$ $(1)\\Rightarrow(2)$\nSoient $\\mathcal{S}$ un syst√®me de transition dense, $\\mathcal{V}$ une valuation et $a$ un n≈ìud.\nSupposons que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\Box\\phi$ est v√©rifi√© et montrons que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\phi$ l'est √©galement.\nSoit $c$ un n≈ìud v√©rifiant $a\\longrightarrow c$. $\\mathcal{S}$ √©tant dense, il existe $b$ tel que $a\\longrightarrow b$ et $b\\longrightarrow c$. Par cons√©quent, l'hypoth√®se $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\Box\\phi$ entra√Æne aussit√¥t $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\Vdash \\phi$. $(2)\\Rightarrow(1)$\nSoit un syst√®me de transition non dense $\\mathcal{S}$ qui v√©rifie $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\Box\\Box\\phi\\rightarrow \\Box\\phi)$ pour n'importe quelle formule $\\phi$.\nSi $\\mathcal{S}$ n'est pas dense, il existe deux n≈ìuds $a$ et $c$ tels que $a\\longrightarrow c$ sans qu'aucun $b$ ne v√©rifie √† la fois $a\\longrightarrow b$ et $b\\longrightarrow c$.\nConsid√©rons la valuation $\\mathcal{V}$ d√©finie par $d\\in\\mathcal{V}(P)$ si et seulement si $d\\neq c$.\nIl s'en suit que si $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\nVdash P$ alors $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\Box P$, et pourtant $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Box\\Box P$. D'o√π $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\Box\\Box P \\rightarrow \\Box P$. Pour tout syst√®me de transition $\\mathcal{S}$, les affirmations suivantes sont √©quivalents¬†:\n$\\mathcal{S}$ est non born√© √† droite Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\Box\\phi\\rightarrow \\Diamond\\phi)$ $(1)\\Rightarrow(2)$\nSoient $\\mathcal{S}$ un syst√®me de transition non born√© √† droite, $\\mathcal{V}$ une valuation et $a$ un n≈ìud.\nOn suppose que $a\\Vdash\\Box P$. La seule possibilit√© pour que $a\\nVdash\\Diamond P$, c'est que le n≈ìud $a$ n'est aucun successeur or l'hypoth√®se que $\\mathcal{S}$ est non bron√© √† droite nous l'interdit, donc n√©cessairement $a\\Vdash\\Diamond P$. $(2)\\Rightarrow(1)$\nSi un syst√®me de transition n'est pas born√© √† droite, il existe un n≈ìud $a$ sans successeur. Supposons une valuation $\\mathcal{V}$ telle que seul $a\\in \\mathcal{V}(P)$. On a alors $a\\Vdash \\Box P$ mais aussi $a\\nvdash \\Diamond P$. On note $\\mathscr{C}_{n.b.d.}$ la classe de tous les syst√®mes de transition non born√©s √† droite.\nPour tout syst√®me de transition $\\mathcal{S}$, les affirmations suivantes sont √©quivalents¬†:\n$\\mathcal{S}$ est euclidien Pour toute formule $\\phi$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall \\mathcal{V}}(\\Diamond\\phi\\rightarrow \\Box\\Diamond\\phi)$ $(1)\\Rightarrow(2)$\nSoient $\\mathcal{S}$ un syst√®me de transition euclidien, $\\mathcal{V}$ une valuation et $a$ un n≈ìud.\nS'il existe un n≈ìud $c$ tel que $a\\longrightarrow c$ et $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\Vdash \\phi$, alors pour tout n≈ìud $b$ tel que $a\\longrightarrow b$, il existe un n≈ìud $d$, tel que $b\\longrightarrow d$ et $\\langle\\mathcal{S},\\mathcal{V},d\\rangle\\Vdash \\phi$. En effet, il suffit de prendre $d=c$ puisque le caract√®re euclidien du graphe assure que de $a\\longrightarrow b$ et $a\\longrightarrow c$, nous obtenons $b\\longrightarrow c$. $(2)\\Rightarrow(1)$\nSi un syst√®me de transition n'est pas euclidien, il existe des n≈ìuds $a$, $b$, $c$, avec $a\\longrightarrow b$ et $a\\longrightarrow c$, mais sans arc $b\\longrightarrow c$, alors il suffit de consid√©rer une valuation telle que $\\langle\\mathcal{S},\\mathcal{V},c\\rangle\\Vdash P$ et pour tout n≈ìud $d\\neq c$, $\\langle\\mathcal{S},\\mathcal{V},d\\rangle\\nVdash P$. On obtient ainsi $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash \\Diamond P$, mais comme il n'y a pas d'arc de $b$ vers $c$, $\\langle\\mathcal{S},\\mathcal{V},b\\rangle\\nVdash \\Diamond P$ et par cons√©quent $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash \\Box\\Diamond P$. Soit $\\Gamma$ un ensemble de formules. On √©crit¬†:\n$\\mathcal{S}\\Vdash^{\\forall a, \\forall\\mathcal{V}} \\Gamma$ si et seulement si pour toute formule $\\phi\\in\\Gamma$, $\\mathcal{S}\\Vdash^{\\forall a, \\forall\\mathcal{V}} \\phi$\n$\\mathcal{S}$ est un syst√®me de transition tel qu\u0026rsquo;en tout n≈ìud et quelle que soit la valuation consid√©r√©e, il satisfait tout ce que \u0026ldquo;raconte\u0026rdquo; $\\Gamma$. C\u0026rsquo;est donc par sa seule forme que ce syst√®me de transition v√©rifie l\u0026rsquo;ensemble des formules de $\\Gamma$.\n$\\mathscr{C} \\Vdash\\Gamma$ si et seulement si pour toute formule $\\phi\\in\\Gamma$, $\\mathscr{C}\\Vdash \\phi$\nM√™me affirmation que la pr√©c√©dente mais pour une classe de syst√®mes de transition toute enti√®re plut√¥t que pour un seul repr√©sentant. En ce sens, la plus grande classe $\\mathscr{C}$ qui v√©rifie $\\mathscr{C}\\Vdash\\Gamma$ octroie une sorte de caract√©risation par la seule forme des graphes de ce que raconte $\\Gamma$.\nCons√©quences s√©mantiques Soient $\\Gamma$ un ensemble de formules et $\\mathscr{C}$ une classe de syst√®mes de transition.\n$\\phi$ est une cons√©quence s√©mantique globale de $\\Gamma$ (not√© $\\Gamma\\models^{\\forall a}_\\mathscr{C}\\phi$) si et seulement si pour tout syst√®me de transition $\\mathcal{S}\\in \\mathscr{C}$, et toute valuation $\\mathcal{V}$, si $\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}\\Gamma$ alors $\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}\\phi$\nPlus simplement, une formule est cons√©quence d\u0026rsquo;un ensemble d\u0026rsquo;hypoth√®ses si chaque fois que ces hypoth√®ses sont vraies dans un mod√®le de la classe consid√©r√©e, alors cette formule est √©galement vraie dans ce mod√®le.\nSoient $\\Gamma$ un ensemble de formules et $\\mathscr{C}$ une classe de syst√®mes de transition.\n$\\phi$ est une cons√©quence s√©mantique locale de $\\Gamma$ (not√© $\\Gamma\\models_\\mathscr{C}\\phi$) si et seulement si pour tout syst√®me de transition $\\mathcal{S}\\in \\mathscr{C}$, toute valuation $\\mathcal{V}$, et tout n≈ìud $a\\in N$, si $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Gamma$ alors $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\phi$\nLa cons√©quence s√©mantique locale est plus forte que la globale au sens o√π si $\\Gamma\\models_\\mathscr{C}\\phi$ alors $\\Gamma\\models^{\\forall a}_\\mathscr{C}\\phi$. L\u0026rsquo;inverse est g√©n√©ralement faux comme le montre l\u0026rsquo;exemple suivant¬†:\nSoient $\\mathscr{C}$ la classe de tous les syst√®mes de transition, $\\Gamma=\\set{P}$ et $\\phi=\\Box P$.\nOn a $\\Gamma\\models^{\\forall a}_\\mathscr{C}\\phi$.\nEn effet, consid√©rons un mod√®le quelconque $\\langle\\mathcal{S},\\mathcal{V}\\rangle$ qui v√©rifie $\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}\\Gamma$, ce qui revient ici √† affirmer que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash P$ est vraie pour tout n≈ìud $a$. En particulier, pour tout n≈ìud $b$ tel que $a\\longrightarrow b$, la relation $\\langle\\mathcal{S},\\mathcal{V},b\\rangle\\Vdash P$ est v√©rifi√©e, ce qui signifie que tout n≈ìud $a$ v√©rifie $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash \\Box P$.\nPar contre, il est facile de trouver un mod√®le o√π $\\Gamma\\models_\\mathscr{C}\\phi$ est faux¬†:\nDans ce mod√®le, on a $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash P$ et $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash \\Box P$ tip\nPlus prosa√Øquement, la cons√©quence locale dit que partout o√π il y a $\\Gamma$, il y a $\\phi$, alors que la cons√©quence globale dit que s\u0026rsquo;il y a $\\Gamma$ partout alors il y a $\\phi$ partout.\nIl existe un lien entre les deux cons√©quences s√©mantiques¬†:\n$\\Gamma\\models^{\\forall a}_\\mathscr{C}\\phi$ si et seulement si $\\set{\\Box^n\\psi|n\\in\\mathbb{N},\\psi\\in\\Gamma}\\models_\\mathscr{C}\\phi$ (o√π $\\Box^n\\psi = \\underbrace{\\Box\\Box\\ldots\\Box}_{n}\\psi$ si $n\u0026gt;0$ et $\\psi$ si $n =0$)\n$\\Box^n$ permet en quelque sorte de g√©n√©raliser la v√©rit√© d\u0026rsquo;un n≈ìud puisque dire qu\u0026rsquo;on a $\\Box^n\\psi$ en $a$ implique que $\\psi$ est vraie en tout n≈ìud atteignable depuis $a$.\n$\\Leftarrow$\nSi on a $\\Gamma$ en tout n≈ìud, alors en un n≈ìud $a$ quelconque, on a n√©cessairement $\\Box^n\\psi$ vraie pour tout $n$ et tout $\\psi$ dans $\\Gamma$. Or par hypoth√®se, cela implique que $\\phi$ est vraie en ce n≈ìud. Et comme la raisonnement ne d√©pend pas du n≈ìud $a$, cela montre que $\\phi$ est vraie en tout n≈ìud. $\\Rightarrow$\nD√©montrons la contrapos√©e\u0026nbsp;: si $\\set{\\Box^n\\psi|n\\in\\mathbb{N},\\psi\\in\\Gamma}\\not\\models_\\mathscr{C}\\phi$ alors $\\Gamma\\not\\models^{\\forall a}_\\mathscr{C}\\phi$.\nSoient $\\mathcal{S}=(N,A)$ un syst√®me de transition, $\\mathcal{V}$ une valuation et $a\\in N$ un n≈ìud, tels que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\set{\\Box^n\\psi|n\\in\\mathbb{N},\\psi\\in\\Gamma}$ alors que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\phi$.\nFabriquons le syst√®me de transition $\\mathcal{T}=(N',A')$ qui correspond √† la restriction de $\\mathcal{S}$ √† l'ensemble des n≈ìuds accessibles depuis $a$ et munissons-le d'une valuation $\\mathcal{V}'$ identique √† $\\mathcal{V}$ pour ces n≈ìuds.\nPar construction, dans le mod√®le $\\langle\\mathcal{T},\\mathcal{V}'\\rangle$ ainsi form√©, $\\psi$ est vraie en tout n≈ìud\u0026nbsp;: $\\langle\\mathcal{T},\\mathcal{V}'\\rangle\\Vdash^{\\forall a}\\psi$. Cela entra√Æne que $\\langle\\mathcal{T},\\mathcal{V}'\\rangle\\Vdash^{\\forall a}\\Gamma$, et pourtant $\\langle\\mathcal{T},\\mathcal{V}',a\\rangle\\nVdash\\phi$ par hypoth√®se, ce qui implique $\\Gamma\\not\\models^{\\forall a}_\\mathscr{C}\\phi$. Si $\\mathscr{C}_\\phi$ et $\\mathscr{C}_\\psi$ d√©signent respectivement tous les syst√®mes dans lesquels $\\phi$ est vraie et tous ceux o√π $\\psi$ est vraie, alors $\\phi\\models^{\\forall a}_\\mathscr{C}\\psi$ est √©quivalent √† l\u0026rsquo;inclusion $\\mathscr{C}_\\phi\\subseteq\\mathscr{C}_\\psi$.\nDonc si on a √† la fois $\\phi\\models^{\\forall a}_\\mathscr{C}\\psi$ et $\\psi\\models^{\\forall a}_\\mathscr{C}\\phi$, alors $\\mathscr{C}_\\phi = \\mathscr{C}_\\psi$.\nTh√©ories, √©quivalence et bisimulation On appelle th√©orie du mod√®le $\\langle\\mathcal{S},\\mathcal{V}\\rangle$ l\u0026rsquo;ensemble de toutes les formules qui sont vraies dans ce mod√®le¬†: $$Th_\\mathcal{M}=\\set{\\phi|\\langle\\mathcal{S},\\mathcal{V}\\rangle\\Vdash^{\\forall a}\\phi}$$\nLa th√©orie d\u0026rsquo;un mod√®le est compos√©e de toutes les formules d√©crivant ce mod√®le.\nDeux mod√®les sont distinguables par la logique modale s\u0026rsquo;ils n\u0026rsquo;ont pas la m√™me th√©orie.\nOn appelle th√©orie locale d\u0026rsquo;un mod√®le $\\langle\\mathcal{S},\\mathcal{V}\\rangle$ au n≈ìud $a$ l\u0026rsquo;ensemble de toutes les formules qui sont vraies dans ce mod√®le¬†: $$Th_{(\\mathcal{M},a)}=\\set{\\phi|\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\phi}$$\nSoient $\\mathcal{M}$ et $\\mathcal{N}$ deux mod√®les de la logique modale.\n$\\mathcal{M}$ et $\\mathcal{N}$ sont √©quivalents s'ils ont la m√™me th√©orie\u0026nbsp;:\n$\\mathcal{M} \\equiv \\mathcal{N}$ si et seulement si $Th_\\mathcal{M}=Th_\\mathcal{N}$ $\\mathcal{M}$ et $\\mathcal{N}$ sont localement √©quivalents s'ils partagent la m√™me th√©orie locale\u0026nbsp;:\n$(\\mathcal{M},a) \\equiv (\\mathcal{N},b)$ si et seulement si $Th_{(\\mathcal{M},a)}=Th_{(\\mathcal{N},b)}$ Les deux mod√®les suivants sont √©quivalents ($\\mathcal{M} \\equiv \\mathcal{N}$).\nEt par ailleurs, $(\\mathcal{M},a) \\equiv (\\mathcal{N},a')$, $(\\mathcal{M},b) \\equiv (\\mathcal{N},b')$, $(\\mathcal{M},b) \\equiv (\\mathcal{N},c')$.\nLa bisimulation va nous permettre de reconna√Ætre que ces mod√®les sont localement √©quivalents. Deux mod√®les correspondent en quelque sorte √† deux univers parall√®les. Mais il se peut que depuis un des mondes du premier univers, tout paraisse exactement identique √† ce que l\u0026rsquo;on per√ßoit depuis un monde du deuxi√®me univers. Bien que les univers soient diff√©rents, ses \u0026ldquo;habitants\u0026rdquo; ne peuvent pas les diff√©rentier. On dit qu\u0026rsquo;ils sont indiscernables ou bisimilaires. Deux mondes sont indiscernables s\u0026rsquo;il n\u0026rsquo;existe pas de formule vraie dans l\u0026rsquo;un et fausse dans l\u0026rsquo;autre.\nUn jeu √† deux joueurs √† information parfaite, le jeu de bisimulation, est d√©fini de telle sorte que le joueur appel√© Duplicateur poss√®de une strat√©gie gagnante si et seulement si les deux mondes (associ√©s √† leurs mod√®les respectifs) qui interviennent dans le jeu sont indiscernables.\nPlus besoin de passer en revue toutes les formules possibles pour d√©terminer si deux mondes sont ou non discernables\u0026hellip;\nSoient $\\mathcal{M}=\\langle\\mathcal{S}=(N,A) , \\mathcal{V}\\rangle$ et $\\mathcal{N}=\\langle\\mathcal{T}=(M,B),\\mathcal{W}\\rangle$ deux mod√®les de la logique modale et soient $a\\in N$ et $b\\in M$ deux n≈ìuds.\nLe jeu de bisimulation $\\mathbb{Bis}((\\mathcal{M},a),(\\mathcal{N},b))$ est d√©fini comme suit¬†:\nil comprend deux joueurs appel√©s Duplicateur et Corrupteur.\nLe Duplicateur ($\\text{D}_{up}$) cherche √† montrer que les deux mod√®les sont localement bisimilaires, alors que le Corrupteur ($\\text{C}_{or}$) au contraire veut montrer qu'ils ne le sont pas.\nUn coup, pour chacun des joueurs, consiste √† choisir un n≈ìud dans un syst√®me de transition. Les n≈ìuds de d√©part √©tant $a$ dans $\\mathcal{S}$ et $b$ dans $\\mathcal{T}$. √Ä chaque tour, pour une position $a'$ dans $\\mathcal{M}$ et une position $b'$ dans $\\mathcal{N}$, $\\text{C}_{or}$ choisit d'abord de jouer dans $\\mathcal{S}$ ou dans $\\mathcal{T}$ puis il choisit un arc permettant d'arriver √† un successeur du n≈ìud $a'$ ou $b'$ et $\\text{D}_{up}$ r√©pond en choisissant un successeur dans l'autre mod√®le tel que les n≈ìuds choisit dans chaque mod√®le satisfont les m√™mes variables propositionnelles. Fin de la partie\u0026nbsp;: S'il existe une variable $P$ telle que la position initiale v√©rifie $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash P$ si et seulement si $\\langle\\mathcal{T},\\mathcal{W},b\\rangle\\nVdash P$, alors $\\text{C}_{or}$ l'emporte. Si un joueur ne peut plus avancer alors il perd la partie. Si la partie est infinie, $\\text{D}_{up}$ l'emporte. Exemple 1¬†:\nLe Corrupteur poss√®de une strat√©gie gagnante\u0026nbsp;: $\\text{C}_{or}$ choisit de joueur dans $\\mathcal{N}$ puis va en $b'$. $\\text{D}_{up}$ n'a pas d'autre choix que d'aller en $b$. $\\text{C}_{or}$ choisit √† nouveau de joueur dans $\\mathcal{N}$ et va en $c'$. $\\text{D}_{up}$ perd car il ne peut plus avancer. Exemple 2¬†:\nLe Duplicateur poss√®de une strat√©gie gagnante\u0026nbsp;: Si $\\text{C}_{or}$ choisit d'aller en $b'$ ou en $c'$, $\\text{D}_{up}$ va en $b$ (il n'a pas trop le choix en m√™me temps...). Si $\\text{C}_{or}$ choisit d'aller en $b$, $\\text{D}_{up}$ va en $b'$ ou en $c'$ (c'est sans importance). Le Corrupteur ne peut pas jouer son deuxi√®me coup et perd la partie. Exemple 3¬†:\nLe mod√®le $\\mathcal{N}$ est d√©fini par\u0026nbsp;: l'ensemble des n≈ìuds est $\\mathbb{N}$, de tout n≈ìud $n\\in\\mathbb{N}$ part deux arcs\u0026nbsp;: $n\\longrightarrow n+1$ et $n\\longrightarrow n+2$ $2n\\Vdash P$ et $2n+1\\nVdash P$ Le Duplicateur poss√®de une strat√©gie gagnante qui consiste √† tout moment de la partie\u0026nbsp;: si $\\text{C}_{or}$ joue dans $\\mathcal{N}$ et va sur $n$, √† aller sur $a$ si $n$ est pair et sur $b$ sinon, si $\\text{C}_{or}$ joue dans $\\mathcal{M}$ et va sur $a$, √† aller sur $n+2$ si $n$ est pair et sur $n+1$ sinon, si $\\text{C}_{or}$ joue dans $\\mathcal{M}$ et va sur $b$, √† aller sur $n+1$ si $n$ est pair et sur $n+2$ sinon, De la sorte, le jeu continue ind√©finiment. Th√©or√®me de Henessy-Milner¬†: $(\\mathcal{M},a)\\equiv(\\mathcal{N},b)$ si et seulement si $\\text{D}_{up}$ a une strat√©gie gagnante dans $\\mathbb{Bis}((\\mathcal{M},a),(\\mathcal{N},b))$.\nSuite : Syst√®mes logiques "
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique5/",
	"title": "Logique modale",
	"tags": [],
	"description": "",
	"content": " info\nNotes de lecture du livre La logique pas √† pas de Jacques Duparc que je paraphrase all√©grement.\nLogique modale Syntaxe et s√©mantique Syst√®mes logiques Diff√©rentes logiques modales Axiomatique¬†: les syst√®mes logiques Nous cherchons l\u0026rsquo;√©quivalent en logique modale des tautologies pour le calcul des propositions¬†; des formules vraies dans tous les mod√®les, ou plut√¥t dans toute une classe de mod√®les car comme on va le voir, la topologie d\u0026rsquo;un mod√®le contraint la v√©rit√© √† l\u0026rsquo;int√©rieur de celui-ci.\nLa logique de la classe $\\mathscr{C}$ est la classe de toutes les formules qui sont valides dans tous les syst√®mes de transition de $\\mathscr{C}$¬†: $\\mathbb{Log}_\\mathscr{C}=\\set{\\phi|\\text{pour tout }\\mathcal{S}\\in\\mathscr{C},\\mathcal{S}\\Vdash^{\\forall a,\\forall \\mathcal{V}} \\phi}$\nCette logique doit contenir toutes les formules qui sont des tautologies du calcul des propositions puisque ces formule de profondeur modale nulle (pas d\u0026rsquo;op√©rateurs modaux) sont n√©cessairement forc√©es en tout n≈ìud de n\u0026rsquo;importe quel mod√®le.\nSi la logique contient la formule $\\phi$, elle doit aussi contenir $\\Box\\phi$. En effet, si $\\phi$ est forc√©e en tout n≈ìud, alors le successeur de tout n≈ìud force aussi $\\phi$. On appelle √ßa la n√©cessitation (ou g√©n√©ralisation) d'une formule. De plus, si $\\phi$ appartient √† la logique, la satisfaction de $\\phi$ en un n≈ìud $a$ donn√© est ind√©pendante de la valuation $\\mathcal{V}$ (que $P$ soit vraie ou fausse, $P$ est forc√©e au n≈ìud $a$). Par cons√©quent, que $\\psi$ soit vraie ou fausse, la satisfaction de la formule $\\phi[\\psi/P]$ ne varie pas. Toutes substitutions uniformes op√©r√©es sur les variables d'une formule appartenant √† la logique sera aussi dans la logique. Enfin, la logique de la classe de tous les syst√®mes de transition est close par modus ponens\u0026nbsp;; si $\\phi \\rightarrow\\psi$ et $\\psi$ appartiennent toutes deux √† la logique, alors $\\psi$ √©galement. En effet, le modus ponens pr√©serve la validit√© puisque si un mod√®le v√©rifie en un noud √† la fois $\\psi$ et $\\psi\\rightarrow\\phi$, alors (par d√©finition de l'interpr√©tation de l'implication) il v√©rifie √©galement $\\phi$. Pour d√©terminer des nouvelles tautologies en calcul des propositions, on a utilis√© des syst√®mes de d√©duction (√† la Hilbert, d√©duction naturelle, calcul des s√©quents). La modalit√© complique un peu les choses et seul le syst√®me √† la Hilbert va pouvoir s\u0026rsquo;adapter facilement.\nUn axiome est une formule. Un syst√®me formel est un ensemble d'axiomes. Soient $I$ un ensemble non vide, $\\phi$ une formule et $\\text{\\bf S}$ un syst√®me formel.\nUne preuve de la formule $\\varphi$ dans le syst√®me formel $\\text{\\bf S}$ est une suite finie de formule $\\langle\\varphi_1,\\varphi_2,\\ldots,\\varphi_n\\rangle$ telle que¬†:\n$\\varphi_n=\\phi$ chaque $\\varphi_l$ v√©rifie l'une des quatre conditions suivantes\u0026nbsp;: $\\varphi_l \\in \\text{\\bf S}$ ($\\varphi_l$ est un axiome) $\\varphi_l$ est obtenue par application de la r√®gle du modus ponens √† deux formules d'indice inf√©rieurs $\\varphi_j$ et $\\varphi_k$ o√π $j , k \u003c l$ ($\\varphi_j = \\varphi_k\\rightarrow\\varphi_l$) $\\varphi_l = \\varphi_k[\\theta /P]$ o√π $k \u003c l$ (substitution d'une variable $P$ quelconque par une formule $\\theta$ quelconque dans une formule d'indice inf√©rieure) $\\varphi_l = [i]\\varphi_j$ o√π $j \u003c l$ ($\\varphi_l$ est obtenue √† partir de l'application de la r√®gle de n√©cessitation appliqu√©e √† une formule d'indice inf√©rieure pour une √©tiquette $i\\in I$ quelconque) Exemple 1¬†:\nSi $\\text{\\bf S}$ d√©signe l\u0026rsquo;ensemble des tautologies du calcul des propositions, alors la suite $\\langle P\\rightarrow(P\\lor\\neg P),\\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q),\\Box (\\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q)),\\Box (\\Diamond \\phi\\rightarrow(\\Diamond \\phi\\rightarrow\\lor\\neg\\Diamond \\phi))\\rangle$ est une preuve de $\\Box (\\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q)),\\Box (\\Diamond \\phi\\rightarrow(\\Diamond \\phi\\rightarrow\\lor\\neg\\Diamond \\phi)$ dans ce syst√®me.\nEn effet¬†:\n$\\varphi_1=P\\rightarrow(P\\lor\\neg P)$ est une tautologie du calcul des propositions et donc un axiome du sydt√®me formel $\\text{\\bf S}$, $\\varphi_2 = \\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q) = P\\rightarrow(P\\lor\\neg P)[\\Diamond Q /P]$ est la substitution de la formule $\\Diamond Q$ √† $P$ dans $\\varphi_0$, $\\varphi_3 = \\Box (\\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q))$ est obtenu par n√©cessitation de $\\varphi_1$, $\\varphi_4 = \\Box (\\Diamond \\phi\\rightarrow(\\Diamond \\phi\\rightarrow\\lor\\neg\\Diamond \\phi) = \\varphi_3[\\phi/Q]$ C\u0026rsquo;est plus √©l√©gant de repr√©senter l\u0026rsquo;encha√Ænement sous forme d\u0026rsquo;arbre¬†:\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$P\\rightarrow(P\\lor\\neg P)$} \\RightLabel{$\\scriptsize\\;sub.$} \\UnaryInfC{$\\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q)$} \\RightLabel{$\\scriptsize\\;nec.$} \\UnaryInfC{$\\Box (\\Diamond Q\\rightarrow(\\Diamond Q\\rightarrow\\lor\\neg\\Diamond Q))$} \\RightLabel{$\\scriptsize\\;sub.$} \\UnaryInfC{$\\Box (\\Diamond \\phi\\rightarrow(\\Diamond \\phi\\rightarrow\\lor\\neg\\Diamond \\phi)$} \\end{prooftree} $$ Exemple 2¬†:\nSi $\\text{\\bf S}$ d√©signe l\u0026rsquo;ensemble des tautologies du calcul des propositions auquel on ajoute la formule $P\\rightarrow\\Diamond P$, alors l\u0026rsquo;arbre ci-dessous d√©crit une preuve de $\\Box(\\phi\\rightarrow\\Diamond\\Diamond\\phi)$ dans ce syst√®me¬†:\nSoient $\\text{\\bf S}$ un syst√®me formel et $\\Gamma$ un ensemble de formules appel√©es hypoth√®ses.\nOn √©crit $\\Gamma\\vdash_\\text{\\bf S} \\phi$ s\u0026rsquo;il existe un ensemble fini de formules $\\set{\\psi_1,\\psi_2,\\ldots,\\psi_n}\\subseteq\\Gamma$ tel que la formule $(\\psi_1\\land\\psi_2\\land\\ldots\\land\\psi_n)\\rightarrow\\phi$ puisse √™tre prouv√©e dans le syst√®me formel $\\text{\\bf S}$. Autrement dit le s√©quent $\\vdash_\\text{\\bf S}(\\psi_1\\land\\psi_2\\land\\ldots\\land\\psi_n)\\rightarrow\\phi$ est v√©rifi√©e pour un nombre fini de formules bien choisies de $\\Gamma$.\ninfo\nAttention, ici les hypoth√®ses n\u0026rsquo;ont pas le m√™me sens qu\u0026rsquo;en logique des propositions. En effet, pour prouver le s√©quent $\\phi\\vdash_\\text{\\bf S}\\psi$, on ne peut pas ajouter $\\phi$ aux axiomes et tenter d\u0026rsquo;atteindre $\\psi$ √† partir des r√®gles (modus ponens, substitution uniforme et n√©cessitation). Il faut ici r√©ussir √† prouver la formule $\\phi\\rightarrow\\psi$ sur la base des seuls axiomes. Si on pouvait faire comme en logique des propositions, prouver $\\phi\\vdash_\\text{\\bf S}\\Box\\phi$ deviendrait √©vident¬†: on place $\\phi$ dans les axiomes et on applique la r√®gle de n√©cessitation. Mais non, il faut pouvoir prouver $\\vdash_\\text{\\bf S}\\phi\\rightarrow\\Box\\phi$ et comme on va le voir, dans le syst√®me $\\text{\\bf K}$, la formule $P\\rightarrow\\Box P$ n\u0026rsquo;est pas prouvable ($P\\nvdash_\\text{\\bf K}\\Box P$).\nLe syst√®me K Le syst√®me $\\text{\\bf K}$ est le plus petit syst√®me formel qui contient¬†:\nchaque tautologie du calcul des propositions $\\text{(K)}$\u0026nbsp;: $\\Box(P\\rightarrow Q)\\rightarrow (\\Box P\\rightarrow \\Box Q)$ $\\text{(dual)}$\u0026nbsp;: $\\Diamond P\\leftrightarrow\\neg\\Box\\neg P$ Le syst√®me $\\text{\\bf K}$ est minimal en ce sens qu\u0026rsquo;il est valid√© dans tout mod√®le.\nPour tout mod√®le $\\mathcal{M}=\\langle\\mathcal{S},\\mathcal{V}\\rangle$ et tout n≈ìud $a$, si $a\\Vdash\\Box(P\\rightarrow Q)$ et $a\\Vdash\\Box P$, cela signifie que pour tout n≈ìud $b$ tel que $a\\longrightarrow b$, $b\\Vdash P\\rightarrow Q$ et $b\\Vdash P$, par cons√©quent $b\\Vdash Q$ et donc $a\\Vdash \\Box Q$.\nCela montre que l\u0026rsquo;axiome $\\text{(K)}$ est valide dans tous les syst√®mes de transition.\nPour montrer que $P\\rightarrow\\Box P$ n\u0026rsquo;est pas prouvable dans $\\text{\\bf K}$, il suffit donc de pr√©senter un mod√®le (et le syst√®me $\\text{\\bf K}$ autorise tous les syst√®mes de transition) o√π l\u0026rsquo;implication est fausse pour au moins un n≈ìud. Or on a d√©j√† vu un tel mod√®le¬†:\nDans ce mod√®le, on a $a\\Vdash P$ mais aussi $a\\nVdash \\Box P$ et par cons√©quent $a\\nVdash P\\rightarrow\\Box P$. Cet axiome $\\text{(K)}$ (K en l\u0026rsquo;honneur de Kripke) est appel√© axiome de distribution car il permet de distribuer l\u0026rsquo;op√©rateur modal $\\Box$ √† l\u0026rsquo;int√©rieur de l\u0026rsquo;implication. On l\u0026rsquo;appelle aussi parfois axiome d\u0026rsquo;omniscience logique car il stipule que si un agent sait que $P$ implique $Q$, alors s‚Äôil sait √©galement que $P$ il doit savoir que $Q$. En d‚Äôautres termes, l\u0026rsquo;agent doit conna√Ætre toutes les cons√©quences logiques de ce qu\u0026rsquo;il sait d√©j√†.\nL\u0026rsquo;axiome $\\text{(dual)}$ permet, lui, de passer d\u0026rsquo;un op√©rateur modal √† l\u0026rsquo;autre et est aussi valide dans tout syt√®me de transition (√™tre s√ªr de la pr√©sence d\u0026rsquo;un truc revient √† ne pas √™tre s√ªr de son absence, ou pour rester sur la structure du syst√®me de transition, dire qu\u0026rsquo;il existe un chemin vers $P$ revient √† ne pas dire que tous les chemins vont vers $\\neg P$).\nLa relation s≈ìur permettant de passer de l\u0026rsquo;op√©rateur boite √† l\u0026rsquo;op√©rateur diamant $\\Box P \\leftrightarrow \\neg\\Diamond\\neg P$ (si tous les chemins m√®nent √† $P$, il n\u0026rsquo;existe pas de chemin menant √† $\\neg P$) est elle aussi vraie partout.\nCes deux relations soulignent bien la nature duale des op√©rateurs $\\Diamond$ et $\\Box$.\nMais ce syst√®me formel n\u0026rsquo;est pas encore suffisamment riche pour contenir toutes les formules valides dans tous les mod√®les quels qu\u0026rsquo;ils soient. Pour cela, il faudrait √©galement consid√©rer toutes les substitutions uniformes possibles (entre autre dans la formule $\\text{(K)}$). On passe alors aux logiques modales.\nLogique modale normale Une logique modale normale $\\mathbb{L}$ est un ensemble de formules qui contient le syst√®me formel $\\text{\\bf K}$ et qui est clos par¬†:\nmodus ponens $$ \\begin{prooftree} \\AxiomC{$\\phi$} \\AxiomC{$\\phi\\rightarrow\\psi$} \\RightLabel{$\\scriptsize\\;mod.p.$} \\BinaryInfC{$\\psi$} \\end{prooftree} $$ substitution uniforme $$ \\begin{prooftree} \\AxiomC{$\\phi$} \\RightLabel{$\\scriptsize\\;sub.$} \\UnaryInfC{$\\phi[\\theta/P]$} \\end{prooftree} $$ n√©cessitation $$ \\begin{prooftree} \\AxiomC{$\\phi$} \\RightLabel{$\\scriptsize\\;nec.$} \\UnaryInfC{$\\Box\\phi$} \\end{prooftree} $$ Toute logique modale normale contient chaque formule $\\phi$ prouvable dans le syst√®me formel $\\text{\\bf K}$ (si $\\vdash_\\text{\\bf K}\\phi$ alors $\\phi$ fait partie de toute logique modale normale).\nLa plus petite logique modale normale est construite sur la base du syst√®me formel $\\text{\\bf K}$. On la note $\\mathbb{L}_\\text{\\bf K}$. Le lien entre la logique normale $\\mathbb{L}_\\text{\\bf K}$ et le syst√®me formel $\\text{\\bf K}$ est le suivant¬†: $\\phi \\in \\mathbb{L}_\\text{\\bf K}$ ssi $\\vdash_\\text{\\bf K}\\phi$.\nQuelques axiomes usuels $\\text{(K) :}$ $\\Box(P\\rightarrow Q)\\rightarrow(\\Box P\\rightarrow \\Box Q)$\nOn l\u0026rsquo;a vu, l\u0026rsquo;axiome $\\text{(K)}$ est √† la base des logiques modales normales.\n$\\text{(D) :}$ $\\Box P\\rightarrow \\Diamond P$\nLa formule $\\text{(D)}$ dit que si quelque chose est n√©cessaire, alors elle doit √™tre possible. On peut aussi l\u0026rsquo;interpr√©ter diff√©remment dans d\u0026rsquo;autres types de logiques que nous allons entrevoir ensuite¬†: en logique d√©ontique, la formule dit que quelque chose d\u0026rsquo;obligatoire doit √™tre permis¬†; en logique √©pist√©mique, elle dit que si je sais $P$, alors il est faux que je sache $\\neg P$¬†; et en logique doxastique, que si je crois que $P$, alors il est faux que je crois que $\\neg P$.\n$\\text{(D)}$ est parfois appel√© axiome de consistance.\nComme on l\u0026rsquo;a vu plus t√¥t, un syst√®me de transition qui valide $\\text{(D)}$ n\u0026rsquo;admet pas d\u0026rsquo;impasse. Autrement dit, les arcs doivent √™tre non born√©s √† droite.\n$\\text{(T) :}$ $\\Box P\\rightarrow P$\nLa formule $\\text{(T)}$ dit que si quelque chose est n√©cessaire, alors elle est. On l\u0026rsquo;appelle parfois axiome de factivit√©.\nEn logique d√©ontique¬†: ce qui est obligatoire est.\nEn logique √©pist√©mique¬†: si je sais $P$, alors $P$ est av√©r√©.\nEn logique doxastique¬†: si je crois que $P$, alors $P$ est √©galement av√©r√©.\nEt nous avons vu qu\u0026rsquo;un syst√®me de transition valide $\\text{(T)}$ si et seulement si ce syst√®me est r√©flexif. $\\text{(T)}$ est donc aussi l\u0026rsquo;axiome de r√©flexivit√©.\n$\\text{(B) :}$ $P\\rightarrow \\Box\\Diamond P$\nLa formule $\\text{(B)}$ dit que si quelque chose est vraie, alors il est n√©cessaire qu\u0026rsquo;elle soit possible.\nEn logique d√©ontique¬†: ce qui est, est obligatoirement permis.\nEn logique √©pist√©mique¬†: si je $P$ est av√©r√©, alors je sais que je ne sais pas $\\neg P$.\nEn logique doxastique¬†: si $P$ est av√©r√©, alors je crois que je ne crois pas $\\neg P$.\nEt nous avons vu qu\u0026rsquo;un syst√®me de transition valide $\\text{(B)}$ si et seulement si ce syst√®me est sym√©trique. $\\text{(B)}$ est donc aussi l\u0026rsquo;axiome de sym√©trie.\n$\\text{(4) :}$ $\\Box P\\rightarrow \\Box\\Box P$\nLa formule $\\text{(4)}$ dit que si quelque chose est n√©cessaire, alors elle est n√©cessairement n√©cessaire.\nEn logique d√©ontique¬†: si quelque chose est obligatoire, c\u0026rsquo;est obligatoire qu\u0026rsquo;elle le soit.\nEn logique √©pist√©mique¬†: si je sais $P$, alors je sais que je sais $P$. Et transitivement, on sait que l\u0026rsquo;on sait que l\u0026rsquo;on sait\u0026hellip; Autrement dit, avec la formule (4) nous d√©crivons des agents parfaitement rationnel.\nOn appelle $\\text{(4)}$ l\u0026rsquo;axiome d\u0026rsquo;introspection positive dans le cadre de la logique √©pist√©mique.\nEt nous avons vu qu\u0026rsquo;un syst√®me de transition valide $\\text{(4)}$ si et seulement si ce syst√®me est transitif. $\\text{(4)}$ est donc aussi l\u0026rsquo;axiome de transitivit√©.\n$\\text{(5) :}$ $\\Diamond P\\rightarrow \\Box\\Diamond P$\nLa formule $\\text{(5)}$ dit que s\u0026rsquo;il n\u0026rsquo;est pas n√©cessaire que quelque chose ne soit pas, alors elle est n√©cessaire.\nEn logique d√©ontique¬†: si quelque chose est permise, c\u0026rsquo;est obligatoire qu\u0026rsquo;elle soit permise.\nEn logique √©pist√©mique¬†: si je ne sais pas quelque chose, alors je sais que je ne le sais pas. L\u0026rsquo;interpr√©tation √©pist√©mique d√©crit un agent parfaitement conscient de ce qu\u0026rsquo;il ne sait pas.\nEn logique doxastique¬†: si je ne crois pas quelsue chose, alors je crois que je ne le crois pas. L\u0026rsquo;interpr√©tation doxastique d√©crit un agent parfaitement conscient de ce qu\u0026rsquo;il ne croit pas.\nOn appelle $\\text{(5)}$ l\u0026rsquo;axiome d\u0026rsquo;introspection n√©gative dans le cadre de la logique √©pist√©mique.\nEt nous avons vu qu\u0026rsquo;un syst√®me de transition valide $\\text{(5)}$ si et seulement si ce syst√®me est euclidien. $\\text{(5)}$ est donc aussi l\u0026rsquo;axiome d\u0026rsquo;euclidit√©.\nEn prenant ensemble les formules $\\text{(5)}$ et $\\text{(T)}$, on regarde des syst√®mes de transition √† la fois euclidiens et r√©flexifs. Or la conjonction de ces deux propri√©t√©s est √©quivalentes √† la conjonction des trois propri√©t√©s d\u0026rsquo;une relation d\u0026rsquo;√©quivalence¬†!\nUn syst√®me de transition est √† la fois euclidien et r√©flexif si et seulement si il est √† la fois r√©flexif, sym√©trique et transitif.\nAutrement dit, un graphe euclidien et r√©flexif est en fait ni plus ni moins qu\u0026rsquo;un graphe dans lequel la relation d\u0026rsquo;accessibilit√© est une relation d\u0026rsquo;√©quivalence (relation d√©finie par les trois propri√©t√©s r√©flexivit√©, sym√©trie et transitivit√©).\n$\\Rightarrow$\u0026nbsp;:\nr√©flexif\u0026nbsp;: c'est imm√©diat sym√©trique\u0026nbsp;: si on a un arc $a\\longrightarrow b$, alors par r√©flexivit√©, on a aussi $a\\longrightarrow a$, et le caract√®re euclidien impose donc $b\\longrightarrow a$. transitif\u0026nbsp;: supposons qu'on ait deux arcs $a\\longrightarrow b$ et $b\\longrightarrow c$. Comme on a montr√© la sym√©trie du graphe, on a aussi un arc $b\\longrightarrow a$ et le caract√®re euclidien impose donc un arc $a\\longrightarrow c$. $\\Leftarrow$\u0026nbsp;:\nr√©flexif\u0026nbsp;: c'est imm√©diat euclidien\u0026nbsp;: supposons qu'on ait deux arcs $a\\longrightarrow b$ et $a\\longrightarrow c$. La sym√©trie donne un arc $b\\longrightarrow a$ et la transitivit√© impose alors un arc $b\\longrightarrow c$. Quelques syst√®mes formels et logiques usuels Par la suite, nous confonderons un syst√®me formel et la logique qu\u0026rsquo;il induit par cl√¥ture. Les syst√®mes formels qu\u0026rsquo;on va introduire sont tous des extensions du syst√®me $\\text{\\bf K}$.\n$\\text{\\bf K}$ $\\text{\\bf KD}$\u0026nbsp;: $\\text{\\bf K}\\cup\\set{\\text{(D)}}$ $\\text{\\bf KB}$\u0026nbsp;: $\\text{\\bf K}\\cup\\set{\\text{(B)}}$ $\\text{\\bf KT}$\u0026nbsp;: $\\text{\\bf K}\\cup\\set{\\text{(T)}}$ $\\text{\\bf K4}$\u0026nbsp;: $\\text{\\bf K}\\cup\\set{\\text{(4)}}$ $\\text{\\bf KB4}$\u0026nbsp;: $\\text{\\bf K}\\cup\\set{\\text{(B),(4)}}$ $\\text{\\bf KD4}$\u0026nbsp;: $\\text{\\bf K}\\cup\\set{\\text{(D),(4)}}$ $\\text{\\bf KDB}$\u0026nbsp;: $\\text{\\bf K}\\cup\\set{\\text{(D),(B)}}$ $\\text{\\bf KTB}$\u0026nbsp;: $\\text{\\bf K}\\cup\\set{\\text{(T),(B)}}$ $\\text{\\bf S4} = \\text{\\bf KT4}$\u0026nbsp;: $\\text{\\bf K}\\cup\\set{\\text{(T),(4)}}$ $\\text{\\bf S5} = \\text{\\bf KT5}$\u0026nbsp;: $\\text{\\bf K}\\cup\\set{\\text{(T),(5)}}$ Convainquons-nous dans un premier temps que ces axiomes apportent bien quelque chose de plus en montrant par exemple que la logique $\\text{\\bf KB}$ ne permet pas de prouver l\u0026rsquo;axiome $\\text{(4)}$.\nComme $\\text{(B)}$ est vraie dans tout syst√®me de transition sym√©trique, il suffit de trouver un mod√®le sym√©trique o√π au moins un n≈ìud ne satisfait pas $\\text{(4)}=\\Box P\\rightarrow\\Box\\Box P$ pour montrer que $\\text{\\bf K4}\\not\\subseteq \\text{\\bf KB}$.\nComme $P$ est satisfait dans tout les successeurs de $a$, $a\\Vdash\\Box P$. Mais pour avoir $a\\Vdash\\Box\\Box P$, il faudrait que tous les successeurs de $a$ satisfassent $\\Box P$, or ce n'est pas le cas de $b$ (qui a pour successeur $a$ lui-m√™me par sym√©trie et $a\\nVdash P$). D'o√π $a\\nVdash \\Box\\Box P$. De m√™me, montrons que $\\text{\\bf KTB}$ ne peut prouver ni $\\text{(4)}$ ni $\\text{(5)}$.\n$\\text{(T)}$ et $\\text{(B)}$ seront vraies ensemble dans tout syst√®me de transition √† la fois r√©flexif et sym√©trique. Exhibons un tel mod√®le o√π au moins un n≈ìud ne satisfait ni $\\text{(4)}=\\Box P\\rightarrow\\Box\\Box P$ ni $\\text{(5)}=\\Diamond P\\rightarrow\\Box\\Diamond P$.\nOn constate que dans ce mod√®le r√©flexif et sym√©trique, $a$ ne satisfait pas $\\text{(4)}$ et $b$ ne satisfait pas une instance de $\\text{(5)}$ o√π $P=\\neg Q$. Mais on va pr√©f√©rer dans la suite mettre au jour les inclusions inverses (qui contient qui) gr√¢ce √† la notion de r√©duction.\nSoient $\\text{\\bf S}$ et $\\text{\\bf S\u0026rsquo;}$ deux syst√®mes formels et $\\phi$ une formule quelconque,\n$\\text{\\bf S}‚â§\\text{\\bf S'}$ si et seulement si $\\vdash_\\text{\\bf S}\\phi\\rightarrow \\,\\vdash_\\text{\\bf S'}\\phi$ Cette relation est r√©flexive ($\\text{\\bf S}‚â§\\text{\\bf S}$) et transitive (si $\\text{\\bf S}‚â§\\text{\\bf S\u0026rsquo;}$ et $\\text{\\bf S\u0026rsquo;}‚â§\\text{\\bf S\u0026rsquo;\u0026rsquo;}$ alors $\\text{\\bf S}‚â§\\text{\\bf S\u0026rsquo;\u0026rsquo;}$).\n$\\text{\\bf S}‚â§\\text{\\bf S\u0026rsquo;}$ se lit $\\text{\\bf S}$ se r√©duit √† $\\text{\\bf S\u0026rsquo;}$. Tout ce qui est prouv√© √† l\u0026rsquo;aide de $\\text{\\bf S}$ peut √©galement l\u0026rsquo;√™tre √† l\u0026rsquo;aide de $\\text{\\bf S\u0026rsquo;}$.\nSi $\\text{\\bf S}$ se r√©duit √† $\\text{\\bf S\u0026rsquo;}$, cela signifie que $\\text{\\bf S\u0026rsquo;}$ permet de prouver au moins autant de formules (appel√©es th√©or√®mes) que $\\text{\\bf S}$ et par cons√©quent, que la classe des syst√®mes de transition dans lesquels tous les th√©or√®mes de $\\text{\\bf S\u0026rsquo;}$ sont valides est incluse dans celle des syst√®mes de transition qui tous valident les th√©or√®mes de $\\text{\\bf S}$ (c\u0026rsquo;est plus dur de raconter plus de choses, √ßa restreint l\u0026rsquo;ensemble des mod√®les possibles).\nAutrement dit,\nsi $\\mathscr{C}_\\text{\\bf S}$ et $\\mathscr{C}_\\text{\\bf S\u0026rsquo;}$ d√©signent respectivement la classe de tous les syst√®mes de transition qui valident $\\text{\\bf S}$ et $\\text{\\bf S\u0026rsquo;}$¬†:\n$\\text{\\bf S}‚â§\\text{\\bf S'}$ si et seulement si $\\mathscr{C}_\\text{\\bf S'}\\subseteq\\mathscr{C}_\\text{\\bf S}$. Par contre, les logiques modales normales √©tant des ensembles de formules satisfaites, pour elles l\u0026rsquo;inclusion est dans l\u0026rsquo;autre sens¬†:\nLorsque $\\text{\\bf S}$ et $\\text{\\bf S\u0026rsquo;}$ contiennent tous les deux le syst√®me formel $\\text{\\bf K}$, les logiques modales normales qu\u0026rsquo;ils engendrent ($\\mathbb{L}_\\text{\\bf S}$ et $\\mathbb{L}_\\text{\\bf S\u0026rsquo;}$) v√©rifient la relation suivante¬†:\n$\\text{\\bf S}‚â§\\text{\\bf S'}$ si et seulement si $\\mathbb{L}_\\text{\\bf S}\\subseteq\\mathbb{L}_\\text{\\bf S'}$. On obtient alors imm√©diatement les r√©ductions suivantes¬†:\n$\\text{\\bf K}‚â§\\text{\\bf K4}$ $\\text{\\bf K}‚â§\\text{\\bf KB}$ $\\text{\\bf K}‚â§\\text{\\bf KD}$ $\\text{\\bf K4}‚â§\\text{\\bf KB4}$ $\\text{\\bf K4}‚â§\\text{\\bf KD4}$ $\\text{\\bf KB}‚â§\\text{\\bf KB4}$ $\\text{\\bf KB}‚â§\\text{\\bf KDB}$ $\\text{\\bf KD}‚â§\\text{\\bf KDB}$ $\\text{\\bf KD}‚â§\\text{\\bf KD4}$ $\\text{\\bf KT}‚â§\\text{\\bf KTB}$ $\\text{\\bf KT}‚â§\\text{\\bf KT4}$ Montrons maintenant que¬†:\n$\\text{\\bf KD}‚â§\\text{\\bf KT}$ Il suffit de montrer que la formule $\\text{(D)}$ peut √™tre prouv√©e gr√¢ce au syst√®me $\\text{\\bf KT}$, c\u0026rsquo;est-√†-dire¬†: $\\vdash_\\text{\\bf KT} \\Box P\\rightarrow\\Diamond P$.\nOn en d√©duit imm√©diatement¬†:\n$\\text{\\bf KDB}‚â§\\text{\\bf KTB}$ $\\text{\\bf KD4}‚â§\\text{\\bf KT4=S4}$ On peut aussi montrer que¬†:\n$\\text{\\bf KTB}‚â§\\text{\\bf S5}$ Il suffit de montrer que la formule $\\text{(B)}$ peut √™tre prouv√©e gr√¢ce au syst√®me $\\text{\\bf KT5}$, c\u0026rsquo;est-√†-dire¬†: $\\vdash_\\text{\\bf S5} P\\rightarrow\\Box\\Diamond P$.\nMontrons d\u0026rsquo;abord que $\\vdash_\\text{\\bf KT}P\\rightarrow\\Diamond P$, ce qui montre √©galement que $\\vdash_\\text{\\bf KT5}P\\rightarrow\\Diamond P$.\nPuis utilisons ce r√©sultat pour montrer que $\\vdash_\\text{\\bf S5}P\\rightarrow\\Box\\Diamond P$ $\\text{\\bf S4}‚â§\\text{\\bf S5}$ Il suffit de montrer que la formule $\\text{(4)}$ peut √™tre prouv√©e gr√¢ce au syst√®me $\\text{\\bf KT5}$, c\u0026rsquo;est-√†-dire¬†: $\\vdash_\\text{\\bf S5} \\Box P\\rightarrow\\Box\\Box P$.\nOn commence par montrer $\\vdash_\\text{\\bf S5} \\Box P\\rightarrow\\Box\\Diamond\\Box P$ en ajoutant juste une substitution au th√©or√®me pr√©c√©dant¬†:\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;th.$} \\UnaryInfC{$ P\\rightarrow\\Box\\Diamond P$} \\RightLabel{$\\scriptsize\\;sub.$} \\UnaryInfC{$ \\Box P \\rightarrow \\Box\\Diamond \\Box P$} \\end{prooftree} $$ On montre ensuite $\\vdash_\\text{\\bf S5} \\Diamond\\Box P\\rightarrow \\Box P$ Puis on montre $\\vdash_\\text{\\bf S5} \\Box\\Diamond\\Box P\\rightarrow \\Box\\Box P$ Pour enfin pouvoir conclure\u0026nbsp;: $\\text{\\bf KB4}‚â§\\text{\\bf S5}$ Il faut montrer √† la fois que¬†:\nla formule $\\text{(4)}$ peut √™tre prouv√©e dans le syst√®me $\\text{\\bf S5}$ et on l'a fait dans la d√©monstration de la r√©duction de $\\text{\\bf S4}$ √† $\\text{\\bf S5}$, la formule $\\text{(B)}$ peut √™tre prouv√©e dans le syst√®me $\\text{\\bf S5}$ et on l'a fait dans la d√©monstration de la r√©duction de $\\text{\\bf KTB}$ √† $\\text{\\bf S5}$, Repr√©sentons tout √ßa graphiquement en utilisant le code suivant¬†: une fl√®che allant de $\\text{\\bf S}$ √† $\\text{\\bf S\u0026rsquo;}$ signifie que la relation $\\text{\\bf S}‚â§\\text{\\bf S\u0026rsquo;}$ est av√©r√©e ($\\text{\\bf S}$ se r√©duit √† $\\text{\\bf S\u0026rsquo;}$).\nEt comme on l\u0026rsquo;a vu, la r√©duction d\u0026rsquo;un syst√®me formel √† un autre implique l\u0026rsquo;inclusion inverse entre les classes de tous les syst√®mes de transition qui valident ces syst√®mes formels.\nDans le graphique suivant, une fl√®che allant de $\\mathscr{C}$ √† $\\mathscr{C\u0026rsquo;}$ d√©signe donc l\u0026rsquo;inclusion inverse $\\mathscr{C}\\supseteq\\mathscr{C\u0026rsquo;}$.\nLe projet est maintenant de pr√©ciser le rapport entre axiomatique et s√©mantique avec l\u0026rsquo;espoir que l\u0026rsquo;ensemble des formules valides sur les structures d\u0026rsquo;un certain type soit identique √† l\u0026rsquo;ensemble des axiomes et th√©or√®mes d\u0026rsquo;un certain syst√®me (ce qu\u0026rsquo;on a appel√© une logique modale), car ainsi la validit√© formera une interpr√©tation du syst√®me. Mais pour s\u0026rsquo;assurer de cette ad√©quation entre s√©mantique et axiomatique, il faut d\u0026rsquo;une part obtenir la correction (ou solidit√©, soundness en anglais) du syst√®me (toutes les formules prouvables sont vraies), et sa r√©ciproque, la compl√©tude (toutes les formules vraies sont prouvables).\nDit autrement, on doit pouvoir d√©duire du syst√®me formel tout le vrai (compl√©tude) et rien que le vrai (correction).\nOn va d\u0026rsquo;abord chercher √† faire correspondre une logique modale normale √† chaque classe de syst√®mes de transitions.\nSoit $\\mathscr{C}$ une classe de syst√®mes de transition et $\\mathbb{L}$ une logique modale normale, $\\mathbb{L}$ est correcte par rapport √† la classe $\\mathscr{C}$ (not√© $\\mathscr{C}$-correcte) si pour toute formule $\\phi$ et pour tout syst√®me de transition $\\mathcal{S}$ de la classe $\\mathscr{C}$, si $\\vdash_\\mathbb{L}\\phi$ alors $\\mathcal{S}\\Vdash^{\\forall a,\\forall \\mathcal{V}}\\phi$.\nCe qu\u0026rsquo;on peut aussi √©crire¬†:\n$$\\text{si } \\vdash_\\mathbb{L}\\phi \\text{ alors }\\models_{\\mathscr{C}}\\phi$$\n√Ä chacune des onze classes, on peut faire correspondre une logique modale normale qui se trouve √™tre correcte pour cette classe.\nsi $\\vdash_\\text{\\bf K}\\phi$ alors $\\models_{\\mathscr{C}}\\phi$ si $\\vdash_\\text{\\bf K4}\\phi$ alors $\\models_{\\mathscr{C}_{tr.}}\\phi$ si $\\vdash_\\text{\\bf KD}\\phi$ alors $\\models_{\\mathscr{C}_{n.b.d.}}\\phi$ si $\\vdash_\\text{\\bf KD4}\\phi$ alors $\\models_{\\mathscr{C}_{tr.,n.b.d.}}\\phi$ si $\\vdash_\\text{\\bf KT}\\phi$ alors $\\models_{\\mathscr{C}_{ref.}}\\phi$ si $\\vdash_\\text{\\bf S4}\\phi$ alors $\\models_{\\mathscr{C}_{ref.,tr.}}\\phi$ si $\\vdash_\\text{\\bf KB}\\phi$ alors $\\models_{\\mathscr{C}_{sym.}}\\phi$ si $\\vdash_\\text{\\bf KB4}\\phi$ alors $\\models_{\\mathscr{C}_{sym.,tr.}}\\phi$ si $\\vdash_\\text{\\bf KDB}\\phi$ alors $\\models_{\\mathscr{C}_{sym.,n.b.d.}}\\phi$ si $\\vdash_\\text{\\bf KTB}\\phi$ alors $\\models_{\\mathscr{C}_{ref.,sym.}}\\phi$ si $\\vdash_\\text{\\bf S5}\\phi$ alors $\\models_{\\mathscr{C}_{ref.,sym.,tr.}}\\phi$ On a d√©j√† en fait d√©j√† tout d√©montr√©.\nEn effet, on a montr√© que les axiomes de la logique normale minimale (tautologies du calcul des propositions, $\\text{(K)}$ et $\\text{(dual)}$) sont satisfaites en tout n≈ìud et pour toute valuation de tous les syst√®mes de transitions. Et on a montr√© que les trois op√©rations utilis√©s pour les preuves (la n√©c√©ssitation, la substitution uniforme et le modus ponens) pr√©servent la validit√©. Donc une formule prouvable dans $\\text{\\bf K}$ est valide dans tout syst√®me de transition ( $\\text{\\bf K}$ est $\\mathscr{C}$-correcte).\nEnsuite, on a montr√© que chacun des axiomes modaux suppl√©mentaires ajout√©s √† $\\text{\\bf K}$ pour obtenir les autres logiques normales ont une validit√© limit√©e √† une classe sp√©cifique de syst√®mes de transition.\n$\\text{(T)}$ est valide dans tout syst√®me de transition r√©flexif. $\\text{(B)}$ est valide dans tout syst√®me de transition sym√©trique. $\\text{(4)}$ est valide dans tout syst√®me de transition transitif. $\\text{(D)}$ est valide dans tout syst√®me de transition non born√© √† droite. $\\text{(4)}$ est valide dans tout syst√®me de transition euclidien. Et on a montr√© qu\u0026rsquo;un graphe est √† la fois r√©flexif et euclidien si et seulement si il est r√©flexif, sym√©trique et transitif.\nOn repr√©sente graphiquement par une fl√®che \u0026ldquo;$\\text{\\bf X}\\longrightarrow\\mathscr{C}_x$\u0026rdquo; la relation de correction¬†: \u0026ldquo;si $\\vdash_\\text{\\bf X}\\phi$, alors $\\models_{\\mathscr{C}_x}\\phi$\u0026rdquo;.\nOn va essayer maintenant d\u0026rsquo;associer les classe aux logiques, la s√©mantique √† la syntaxe.\nSoit $\\mathscr{C}$ une classe de syst√®mes de transition et $\\mathbb{L}$ une logique modale normale.\n$\\mathbb{L}$ est faiblement compl√®te par rapport √† la classe $\\mathscr{C}$ (not√© $\\mathscr{C}$-faiblement compl√®te) si pour toute formule $\\phi$\u0026nbsp;:\n$$\\text{si }\\models_\\mathscr{C}\\phi \\text{ alors } \\vdash_\\mathbb{L}\\phi$$ $\\mathbb{L}$ est fortement compl√®te par rapport √† la classe $\\mathscr{C}$ (not√© $\\mathscr{C}$-fortement compl√®te) si pour toute formule $\\phi$ et tout ensemble de formules $\\Gamma$\u0026nbsp;:\n$$\\text{si } \\Gamma \\models_\\mathscr{C}\\phi \\text{ alors } \\Gamma \\vdash_\\mathbb{L}\\phi$$ On a alors le th√©or√®me de forte compl√©tude suivant¬†:\nsi $\\Gamma\\models_{\\mathscr{C}}\\phi$ alors $\\Gamma\\vdash_\\text{\\bf K}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{tr.}}\\phi$ alors $\\Gamma\\vdash_\\text{\\bf K4}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{n.b.d.}}\\phi$ alors $\\Gamma\\vdash_\\text{\\bf KD}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{tr.,n.b.d.}}\\phi$ alors $\\Gamma\\vdash_\\text{\\bf KD4}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{ref.}}\\phi$ alors $\\Gamma\\vdash_\\text{\\bf KT}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{ref.,tr.}}\\phi$ alors $\\Gamma\\vdash_\\text{\\bf S4}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{sym.}}\\phi$ alors $\\Gamma\\vdash_\\text{\\bf KB}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{sym.,tr.}}\\phi$ alors $\\Gamma\\vdash_\\text{\\bf KB4}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{sym.,n.b.d.}}\\phi$ alors $\\Gamma\\vdash_\\text{\\bf KDB}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{ref.,sym.}}\\phi$ alors $\\Gamma\\vdash_\\text{\\bf KTB}\\phi$ si $\\Gamma\\models_{\\mathscr{C}_{ref.,sym.,tr.}}\\phi$ alors $\\Gamma\\vdash_\\text{\\bf S5}\\phi$ Pour le d√©montrer on va avoir besoin de nouvelles d√©finitions et de plusieurs lemmes.\nSoient $\\text{\\bf S}$ un syst√®me formel et $\\Gamma$ un ensemble de formules,\n$\\Gamma$ est $\\text{\\bf S}$-consistant si et seulement si $\\Gamma\\nvdash_\\text{\\bf S}\\bot$. Dans le cas contraire, $\\Gamma$ est dit $\\text{\\bf S}$-inconsistant. Lemme d'existence Soient $\\mathscr{C}$ une classe de syst√®me de transition et $\\mathbb{L}$ une logique modale normale,\n$\\mathbb{L}$ est $\\mathscr{C}$-(fortement) compl√®te si et seulement si pour tout $\\Gamma\\subseteq\\mathbb{L}$ qui est $\\mathbb{L}$-consistant, il existe $\\mathcal{S}\\in\\mathscr{C},\\mathcal{V}$ et $a$ tels que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Gamma$. $\\Rightarrow$\u0026nbsp;:\npuisque $\\mathbb{L}$ est $\\mathscr{C}$-compl√®te, un ensemble de formules quelconque $\\mathbb{L}$-consistant $\\Gamma\\subseteq\\mathbb{L}$ est n√©cessairement satisfaisable pour un syst√®me de transition de la classe $\\mathscr{C}$. En effet, si ce n'√©tait pas le cas, l'expression $\\Gamma\\models_\\mathscr{C}\\bot$ serait vraie, mais alors nous pourrions en d√©duire $\\Gamma\\vdash_\\mathbb{L}\\bot$ en utilisant la $\\mathscr{C}$-compl√©tude de $\\mathbb{L}$. $\\Leftarrow$\u0026nbsp;:\nmontrons la contrapos√©e. On suppose donc que $\\mathbb{L}$ n'est pas $\\mathscr{C}$-compl√®te. Soit donc $\\Gamma$ un ensemble de formules et $\\phi$ une formule tels que $\\Gamma\\models_\\mathscr{C}\\phi$ mais $\\Gamma\\nvdash_\\mathbb{L}\\phi$. Il appara√Æt d√®s lors que $\\Gamma\\cup\\set{\\neg\\phi}\\nvdash_\\mathbb{L}\\bot$ est v√©rifi√©e, car $\\Gamma\\cup\\set{\\neg\\phi}\\vdash_\\mathbb{L}\\bot$ entra√Ænerait $\\Gamma\\vdash_\\mathbb{L}\\neg\\phi\\rightarrow\\bot$ puis, en passant par la contrapos√©e, $\\Gamma\\vdash_\\mathbb{L}\\phi$. Par cons√©quent $\\Gamma\\cup\\set{\\neg\\phi}$ est $\\mathbb{L}$-consistant et pourtant, pour tout syst√®me de transition $\\mathcal{S}\\in\\mathscr{C}$, $\\mathcal{V}$ et $a$, $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\nVdash\\Gamma\\cup\\set{\\neg\\phi}$. L\u0026rsquo;id√©e pour d√©montrer la compl√©tude est de chercher un mod√®le $\\langle\\mathcal{S},\\mathcal{V}\\rangle$ dont le syst√®me de transition fait partie de la classe consid√©r√©e et un n≈ìud $a$ tels que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Gamma$ o√π $\\Gamma$ est un ensemble de formules $\\mathbb{L}$-consistant.\nOn va mettre au point un tel mod√®le en modelant les n≈ìuds de son syst√®me de distribution directement √† partir d\u0026rsquo;ensembles de formules¬†! La s√©mantique d√©tourne ainsi la syntaxe √† son profit en donnant vie √† une sorte de Golem syntaxique.\nSoient $\\mathbb{L}$ une logique modale normale et $\\Gamma$ un ensemble de formules,\n$\\Gamma$ est maximal $\\mathbb{L}$-consistant (MC) si et seulement si $\\Gamma\\nvdash_\\mathbb{L}\\bot$ et pour tout ensemble $\\Gamma \\subsetneq \\Gamma'$, $\\Gamma'\\vdash_\\mathbb{L}\\bot$. Remarques¬†:\nsi $\\Gamma$ est maximal $\\mathbb{L}$-consistant, alors\u0026nbsp;:\n$\\mathbb{L}\\subseteq\\Gamma$ $\\Gamma$ est clos par modus ponens pour toute formule $\\phi$, $\\phi\\in\\Gamma$ ou $\\neg\\phi\\in\\Gamma$ pour toutes formules $\\phi$ et $\\psi$, $\\phi\\lor\\psi\\in\\Gamma$ si et seulement si $\\phi\\in\\Gamma$ ou $\\psi\\in\\Gamma$ Lemme de Lindenbaum Si $\\Gamma$ est $\\mathbb{L}$-consistant, alors il existe $\\Gamma_{max}$ maximal $\\mathbb{L}$-consistant tel que $\\Gamma\\subseteq\\Gamma_{max}$. Soit $(\\phi_n)_{n\\in\\mathbb{N}}$ une √©num√©ration de toutes les formules. On d√©finit¬†:\n$\\Gamma_0=\\Gamma$ $\\Gamma_{n+1}=\\begin{cases}\\Gamma_n\\cup\\set{\\phi_n} \\text{ si } \\Gamma_n\\cup\\set{\\phi_n}\\nvdash_\\mathbb{L}\\bot\\\\ \\Gamma_n\\cup\\set{\\neg\\phi_n} \\text{ sinon}\\end{cases}$ $\\displaystyle \\Gamma_{max}=\\bigcup_{n\\in\\mathbb{N}}\\Gamma_n$ $\\Gamma_{max}$ est $\\mathbb{L}$-consistant car sinon il existerait un plus petit entier $n$ tel que $\\Gamma_{n+1}\\vdash_\\mathbb{L}\\bot$. Nous aurions alors $\\Gamma_n\\cup\\set{\\phi_n}\\vdash_\\mathbb{L}\\bot$ et $\\Gamma_n\\cup\\set{\\neg\\phi_n}\\vdash_\\mathbb{L}\\bot$ par construction de $\\Gamma_{max}$. En utilisant le raisonnement par l'absurde, nous obtenons √† la fois $\\Gamma_n\\vdash_\\mathbb{L}\\phi_n$ et $\\Gamma_n\\vdash_\\mathbb{L}\\neg\\phi_n$, ce qui entra√Æne $\\Gamma_n\\vdash_\\mathbb{L}\\bot$, contredisant la fait que par minimalit√© de l'entier $n$, $\\Gamma_n$ est $\\mathbb{L}$-consistant. $\\Gamma_{max}$ est maximal car sinon il existerait $\\Gamma\u0026rsquo;$ $\\mathbb{L}$-consistant tel que $\\Gamma_{max}\\subsetneq\\Gamma\u0026rsquo;$. Dans ce cas, une formule quelconque $\\psi$ telle que $\\psi\\in\\Gamma\u0026rsquo;\\setminus\\Gamma_{max}$ appara√Ætrait dans l\u0026rsquo;√©num√©ration $(\\phi_n)_{n\\in\\mathbb{N}}$ de toutes les formules et il existerait ainsi un entier $n$ tel que $\\psi=\\phi_n$. Par construction de $\\Gamma_{max}$, comme $\\phi_n\\not\\in\\Gamma_{max}$, $\\Gamma_n\\cup\\set{\\phi_n}\\vdash_\\mathbb{L}\\bot$ est v√©rifi√©e. Donc $\\Gamma_{max}\\cup\\set{\\phi_n}\\vdash_\\mathbb{L}\\bot$ ce qui entra√Æne $\\Gamma\u0026rsquo;\\vdash_\\mathbb{L}\\bot$, contredisant la $\\mathbb{L}$-consistance de $\\Gamma\u0026rsquo;$.\nSoit $\\mathbb{L}$ une logique modale normale, le mod√®le canonique $\\mathcal{M}_\\mathbb{L}$ est d√©fini par $\\mathcal{M}_\\mathbb{L}=\\langle N_\\mathbb{L},A_\\mathbb{L},\\mathcal{V}_\\mathbb{L}\\rangle$ avec\u0026nbsp;: $N_\\mathbb{L}=\\set{\\Gamma|\\Gamma\\text{ maximal }\\mathbb{L}\\text{-consistant}}$ $A_\\mathbb{L}$ d√©fini par $\\Gamma\\longrightarrow\\Gamma'$ si et seulement si pour toute formule $\\phi$, si $\\phi\\in\\Gamma'$ alors $\\Diamond\\phi\\in\\Gamma$ (c.-√†-d. $\\Gamma\\supseteq\\set{\\Diamond\\phi : \\phi\\in\\Gamma'}$) $\\mathcal{V}_\\mathbb{L}$ est d√©fini par $\\mathcal{V}_\\mathbb{L}(P)=\\set{\\Gamma\\in\\N_\\mathbb{L}|P\\in\\Gamma}$ (c.-√†-d. $\\Gamma\\Vdash P$ si et seulement si $P\\in\\Gamma$) Lemme ($\\longrightarrow$ et $\\Box$) Soit $\\mathbb{L}$ une logique modale normale et $\\mathcal{M}_\\mathbb{L}=\\langle N_\\mathbb{L},A_\\mathbb{L},\\mathcal{V}_\\mathbb{L}\\rangle$ le mod√®le canonique associ√© √† $\\mathbb{L}$. Pour tout n≈ìud $\\Gamma,\\Gamma\u0026rsquo;\\in N_\\mathbb{L}$, on a¬†:\n$\\Gamma\\longrightarrow\\Gamma'$ si et seulement si pour toute formule $\\phi$, si $\\Box\\phi\\in\\Gamma$ alors $\\phi\\in\\Gamma'$. $\\Rightarrow$\u0026nbsp;:\nRaisonnons par l'absurde en supposant $\\Box\\phi\\in\\Gamma$ et $\\phi\\not\\in\\Gamma'$. Par maximale $\\mathbb{L}$-consistance de $\\Gamma'$, $\\neg\\phi\\in\\Gamma'$. Par d√©finition de l'arc $\\Gamma\\longrightarrow\\Gamma'$, $\\Diamond\\neg\\phi\\in\\Gamma$ et puisque $\\Gamma$ est consistant, $\\neg\\Diamond\\neg\\phi\\not\\in\\Gamma$, d'o√π $\\Box\\phi\\not\\in\\Gamma$ (o√π on a utilis√© la dualit√© entre $\\Diamond$ et $\\Box$). Contradiction. $\\Leftarrow$\u0026nbsp;:\nSoit $\\phi\\in\\Gamma'$. Raisonnons par l'absurde en supposant $\\Diamond\\phi\\not\\in\\Gamma$ (ce qui signifie que $\\Gamma\\,\\,\\not\\!\\!\\longrightarrow\\Gamma'$). Alors par maximale $\\mathbb{L}$-consistance $\\neg\\Diamond\\phi\\in\\Gamma$, par cons√©quent $\\neg\\Diamond\\neg\\neg\\phi\\in\\Gamma$ et donc $\\Box\\neg\\phi\\in\\Gamma$ (o√π on a utilis√© la dualit√© entre $\\Diamond$ et $\\Box$). Par hypoth√®se, $\\Box\\neg\\phi\\in\\Gamma$ entra√Æne $\\neg\\phi\\in\\Gamma'$, ce qui contredit la $\\mathbb{L}$-consistance de $\\Gamma'$. Lemme ($\\longrightarrow$ et $\\Diamond$) Soit $\\mathbb{L}$ une logique modale normale et $\\mathcal{M}_\\mathbb{L}=\\langle N_\\mathbb{L},A_\\mathbb{L},\\mathcal{V}_\\mathbb{L}\\rangle$ le mod√®le canonique associ√© √† $\\mathbb{L}$. Pour tout n≈ìud $\\Gamma,\\Gamma\u0026rsquo;\\in N_\\mathbb{L}$¬†:\nSi $\\Diamond\\phi\\in\\Gamma$ alors il existe $\\Gamma'$ tel que $\\Gamma\\longrightarrow\\Gamma'$ et $\\phi\\in\\Gamma'$. Supposons $\\Diamond\\phi\\in\\Gamma$. Pour obtenir $\\Gamma\u0026rsquo;$, on construit d\u0026rsquo;abord $\\Theta=\\set{\\phi}\\cup\\set{\\psi|\\Box\\psi\\in\\Gamma}$. Cet ensemble est consistant car sinon il existerait $\\psi_1,\\ldots,\\psi_k\\in\\Theta$ tels que $\\vdash_\\mathbb{L}(\\psi_1\\land\\ldots\\land\\psi_k)\\rightarrow\\neg\\phi$. Par n√©cessitation, $\\vdash_\\mathbb{L}\\Box((\\psi_1\\land\\ldots\\land\\psi_k)\\rightarrow\\neg\\phi)$. Et par distributivit√© (utilisation de l\u0026rsquo;axiome $\\text{(K)}$) $\\vdash_\\mathbb{L}\\Box(\\psi_1\\land\\ldots\\land\\psi_k)\\rightarrow\\Box\\neg\\phi$. Et puisque dans toute logique normale $\\vdash_\\mathbb{L}(\\Box\\psi_1\\land\\ldots\\land\\Box\\psi_k)\\rightarrow\\Box(\\psi_1\\land\\ldots\\land\\psi_k)$, on obtient finalement $\\vdash_\\mathbb{L}(\\Box\\psi_1\\land\\ldots\\land\\Box\\psi_k)\\rightarrow\\Box\\neg\\phi$. Par cons√©quent $\\Gamma\\vdash_\\mathbb{L}\\Box\\neg\\phi$, ce qui implique que $\\Box\\neg\\phi\\in\\Gamma$ par maximalit√©, et donc √©galement $\\neg\\Diamond\\phi\\in\\Gamma$ (par dualit√©). Or par hypoth√®se, $\\Diamond\\phi\\in\\Gamma$, ce qui entra√Æne l\u0026rsquo;inconsistance de $\\Gamma$. Contradiction.\nComme on vient de montrer que l\u0026rsquo;ensemble $\\Theta=\\set{\\phi}\\cup\\set{\\psi|\\Box\\psi\\in\\Gamma}$ est $\\mathbb{L}$-consistant, il suffit de prendre pour $\\Gamma\u0026rsquo;$ l\u0026rsquo;ensemble $\\Theta_{max}$ maximal $\\mathbb{L}$-consistant donn√© par le lemme de Lindenbaum tel que $\\Theta\\subseteq\\Theta_{max}$. On a bien ainsi un n≈ìud de $N_\\mathbb{L}$ contenant $\\phi$.\nLemme de v√©rit√© Soit $\\mathbb{L}$ une logique modale normale et $\\mathcal{M}_\\mathbb{L}=\\langle N_\\mathbb{L},A_\\mathbb{L},\\mathcal{V}_\\mathbb{L}\\rangle$ le mod√®le canonique associ√© √† $\\mathbb{L}$. Pour tout n≈ìud $\\Gamma\\in N_\\mathbb{L}$¬†:\n$\\langle\\mathcal{M}_\\mathbb{L},\\Gamma\\rangle\\Vdash\\phi$ si et seulement si $\\phi\\in\\Gamma$ La d√©monstration se fait par induction sur la hauteur de la formule $\\phi$.\nsi $ht(\\phi)=0$, alors $\\phi=\\top$ ou $\\phi=\\bot$ ou $\\phi$ est une variable propositionnelle et $\\Gamma\\Vdash\\phi$ ssi $\\phi\\in\\Gamma$ est la d√©finition m√™me de la valuation $\\mathcal{V}_\\mathbb{L}$. si $ht(\\phi)=n+1$ si $\\phi=\\neg\\psi$, alors $\\Gamma\\Vdash\\neg\\phi$ ssi $\\Gamma\\nVdash\\psi$. Or par hypoth√®se d'induction $\\Gamma\\nVdash\\psi$ ssi $\\psi\\not\\in\\Gamma$. Par maximalit√© de $\\Gamma$, on obtient $\\neg\\psi\\in\\Gamma$. si $\\phi=(\\psi_1\\lor\\psi_2)$, alors $\\Gamma\\Vdash(\\psi_1\\lor\\psi_2)$ ssi $\\Gamma\\Vdash\\psi_1$ ou $\\Gamma\\Vdash\\psi_2$.\nPar hypoth√®se d'induction, $\\Gamma\\Vdash\\psi_1$ ssi $\\psi_1\\in\\Gamma$ et $\\Gamma\\Vdash\\psi_2$ ssi $\\psi_2\\in\\Gamma$. Et par maximalit√© de $\\Gamma$, $\\psi_1\\in\\Gamma$ ou $\\psi_2\\in\\Gamma$ ssi $(\\psi_1\\lor\\psi_2)\\in\\Gamma$. si $\\phi=(\\psi_1\\land\\psi_2)$, alors $\\Gamma\\Vdash(\\psi_1\\lor\\psi_2)$ ssi $\\Gamma\\Vdash\\psi_1$ et $\\Gamma\\Vdash\\psi_2$.\nPar hypoth√®se d'induction, $\\Gamma\\Vdash\\psi_1$ ssi $\\psi_1\\in\\Gamma$ et $\\Gamma\\Vdash\\psi_2$ ssi $\\psi_2\\in\\Gamma$. Or par $\\mathbb{L}$-consistance, $\\psi_1\\in\\Gamma$ et $\\psi_2\\in\\Gamma$ ssi $(\\psi_1\\land\\psi_2)\\in\\Gamma$. si $\\phi=(\\psi_1\\rightarrow\\psi_2)$. On d√©duit ce cas de $\\neg\\psi_1\\lor\\psi_2$. si $\\phi=(\\psi_1\\leftrightarrow\\psi_2)$. On d√©duit ce cas de $(\\psi_1\\rightarrow\\psi_2)\\land(\\psi_2\\rightarrow\\psi_1)$. si $\\phi=\\Diamond\\psi$, alors $\\Gamma\\Vdash\\Diamond \\psi$ ssi il existe $\\Gamma',\\Gamma\\longrightarrow\\Gamma'$ et $\\Gamma'\\Vdash\\psi$. Or par hypoth√®se d'induction $\\Gamma'\\Vdash\\psi$ ssi $\\psi\\in\\Gamma'$. Finalement, par d√©finition de la relation $\\Gamma\\longrightarrow\\Gamma'$ et par le lemme ($\\longrightarrow$ et $\\Diamond$) $\\psi\\in\\Gamma'$ ssi $\\Diamond\\psi\\in\\Gamma$. si $\\phi=\\Box\\psi$, alors $\\Gamma\\Vdash\\Box\\phi$ ssi $\\Gamma\\Vdash\\neg\\Diamond\\neg\\psi$ ssi $\\neg\\Diamond\\neg\\psi\\in\\Gamma$ ssi $\\Box\\psi\\in\\Gamma$ (par maximal $\\mathbb{L}$-consistance). Nous voil√† par√©s pour d√©montrer la compl√©tude forte des onze logiques normales d√©crites gr√¢ce au mod√®le canonique de chacune.\nLe lemme d\u0026rsquo;existence nous dit que $\\mathbb{L}$ est $\\mathscr{C}$-fortement compl√®te ssi pour tout $\\Gamma\\subseteq\\mathbb{L}$, $\\mathbb{L}$-consistant, il existe $\\mathcal{S}\\in\\mathscr{C}$, $\\mathcal{V}$ et $a$ tels que $\\langle\\mathcal{S},\\mathcal{V},a\\rangle\\Vdash\\Gamma$.\nPour chaque logique normale, nous allons choisir le mod√®le canonique $\\mathcal{M}_\\mathbb{L}=\\langle N_\\mathbb{L},A_\\mathbb{L},\\mathcal{V}_\\mathbb{L}\\rangle$ associ√©. Et pour le n≈ìud $a$, nous allons prendre $\\Gamma_{max}$. Le lemme de v√©rit√© nous dit alors que $\\langle\\mathcal{M}_\\mathbb{L},\\Gamma_{max}\\rangle\\Vdash\\phi$ ssi $\\phi\\in\\Gamma_{max}$. En cons√©quence, puisque $\\Gamma\\subseteq\\Gamma_{max}$, il ressort que $\\langle\\mathcal{M}_\\mathbb{L},\\Gamma_{max}\\rangle\\Vdash\\Gamma$.\nIl ne reste plus qu\u0026rsquo;√† v√©rifier √† chaque fois que le mod√®le canonique est bien construit sur la base d\u0026rsquo;un syst√®me de transition de la classe $\\mathscr{C}$ consid√©r√©e.\n$\\mathcal{M}_\\text{\\bf K}\\in\\mathscr{C}$\u0026nbsp;: comme $\\mathscr{C}$ d√©signe la classe de tous les syst√®mes de transition, il n'ya rien √† montrer. $\\mathcal{M}_\\text{\\bf K4}\\in\\mathscr{C}_{tr.}$\u0026nbsp;:\nil faut montrer que pour tout n≈ìud $\\Xi_1$, $\\Xi_2$ et $\\Xi_3$ de $\\mathcal{M}_\\text{\\bf K4}$, si $\\Xi_1\\longrightarrow \\Xi_2$ et $\\Xi_2\\longrightarrow \\Xi_3$, alors $\\Xi_1\\longrightarrow \\Xi_3$.\nLa formule $\\text{(4)}=\\Box p\\rightarrow\\Box\\Box P$ est √©quivalente (par dualit√©) √† la formule $\\neg\\Diamond\\neg P\\rightarrow\\neg\\Diamond\\neg\\neg\\Diamond\\neg P$ elle m√™me √©quivalente √† $\\Diamond\\Diamond\\neg P\\rightarrow\\Diamond\\neg P$ (en passant par la contrapos√©e et en √©liminant les doubles n√©gations). Comme $\\Diamond\\Diamond\\neg P\\rightarrow\\Diamond\\neg P\\in\\Gamma$, on a aussi $\\Diamond\\Diamond\\neg P\\rightarrow\\Diamond\\neg P\\in\\Xi_1$ et puisque $\\Xi_1$ est clos par substitution uniforme, $\\Diamond\\Diamond\\neg \\neg\\phi\\rightarrow\\Diamond\\neg \\neg\\phi\\in\\Xi_1$. Et donc chaque formule $\\Diamond\\Diamond\\rightarrow\\Diamond$ appartient √† $\\Xi_1$.\nPar d√©finition de $\\Xi_1\\longrightarrow \\Xi_2$ et $\\Xi_2\\longrightarrow \\Xi_3$, pour une formule $\\phi$ quelconque, si $\\phi\\in\\Xi_3$, alors $\\Diamond\\phi\\in\\Xi_2$ et $\\Diamond\\Diamond\\phi\\in\\Xi_1$. Et comme $\\Xi_1$ est clos par modus ponens, si √† la fois $\\Diamond\\Diamond\\phi\\in\\Xi_1$ et $\\Diamond\\Diamond\\phi\\rightarrow\\Diamond\\phi\\in\\Xi_1$, alors $\\Diamond\\phi\\in\\Xi_1$. Cela prouve que pour toute formule $\\phi$, si $\\phi\\in\\Xi_3$, alors $\\Diamond\\phi\\in\\Xi_1$, et par cons√©quent $\\Xi_1\\longrightarrow\\Xi_3$. $\\mathcal{M}_\\text{\\bf KD}\\in\\mathscr{C}_{n.b.d.}$\u0026nbsp;:\nil faut montrer que pour tout n≈ìud $\\Xi_1$ de $\\mathcal{M}_\\text{\\bf KD}$, il existe $\\Xi_2$ tel que $\\Xi_1\\longrightarrow \\Xi_2$.\nD'apr√®s le lemme ($\\longrightarrow$ et $\\Diamond$), si $\\Diamond\\phi\\in\\Xi_1$, alors il existe $\\Xi_2$ tel que $\\Xi_1\\longrightarrow \\Xi_2$ et $\\phi\\in\\Xi_2$. Or l'axiome $\\text{(D)}$ dit pr√©cis√©ment $\\Box P\\rightarrow\\Diamond P$. Comme $\\Xi_1$ est clos par substitution uniforme, la formule $\\Box\\top\\rightarrow\\Diamond\\top$ est dans $\\Xi_1$. Or $\\Xi_1$ v√©rifie n√©cessairement $\\Box\\top$ (car une tautologie est vraie partout) et comme $\\Xi_1$ est clos par modus ponens, $\\Diamond\\top\\in\\Xi_1$, et donc d'apr√®s le lemme ($\\longrightarrow$ et $\\Diamond$), il existe $\\Xi_2$ tel que $\\Xi_1\\longrightarrow \\Xi_2$ et $\\top\\in\\Xi_2$. $\\mathcal{M}_\\text{\\bf KD4}\\in\\mathscr{C}_{tr.,n.b.d.}$\u0026nbsp;:\nCons√©quence de 2. et 3. $\\mathcal{M}_\\text{\\bf KT}\\in\\mathscr{C}_{ref.}$\u0026nbsp;:\nIl faut montrer que $\\Xi\\longrightarrow \\Xi$ est v√©rifi√© pour tout n≈ìud $\\Xi$ de $\\mathcal{M}_\\text{\\bf KT}$.\nLa formule $\\text{(T)}=\\Box P\\rightarrow P$ est √©quivalente √† $\\neg\\Diamond\\neg P\\rightarrow P$ et par contraposition √† $\\neg P\\rightarrow\\diamond\\neg P$. Comme $\\Xi$ est clos par substitution uniforme, la formule $\\neg \\neg \\phi\\rightarrow\\diamond\\neg \\neg\\phi$ est dans $\\Xi$ et donc $\\phi\\rightarrow\\Diamond\\phi$ est dans $\\Xi$. La cloture par modus ponens permet alors de d√©duire que pour toute formule $\\phi\\in\\Xi$, $\\Diamond\\phi\\in\\Xi$, ce qui est la condition n√©cessaire √† la relation $\\Xi\\longrightarrow \\Xi$. $\\mathcal{M}_\\text{\\bf S4}\\in\\mathscr{C}_{ref.,tr.}$\u0026nbsp;:\nCons√©quence de 2. et 5. $\\mathcal{M}_\\text{\\bf KB}\\in\\mathscr{C}_{sym.}$\u0026nbsp;:\nil faut montrer que pour tout couple de n≈ìuds $\\Xi_1$ et $\\Xi_2$ de $\\mathcal{M}_\\text{\\bf KB}$, $\\Xi_1\\longrightarrow \\Xi_2$ alors $\\Xi_2\\longrightarrow \\Xi_1$.\n$\\text{(B)}= P\\rightarrow \\Box\\Diamond P$ . Comme $\\Xi_1$ est clos par substitution uniforme, les formules $\\phi\\rightarrow\\Box\\Diamond\\phi$ sont dans $\\Xi_1$. La cloture par modus ponens de $\\Xi_1$ nous dit alors que si $\\phi\\in\\Xi_1$, alors $\\Box\\Diamond\\phi\\in\\Xi_1$. Or d'apr√®s le lemme ($\\longrightarrow$ et $\\Box$), si $\\Xi_1\\longrightarrow\\Xi_2$ alors pour toute formule $\\phi$, si $\\Box\\phi\\in\\Xi_1$, alors $\\phi\\in\\Xi_2$. Par cons√©quent, pour tout $\\phi\\in\\Xi_1$, $\\Diamond\\phi\\in\\Xi_2$, ce qui est la condition n√©cessaire pour d√©finir $\\Xi_2\\longrightarrow\\Xi_1$. $\\mathcal{M}_\\text{\\bf KB4}\\in\\mathscr{C}_{sym.tr.}$\u0026nbsp;:\nCons√©quence de 2. et 7. $\\mathcal{M}_\\text{\\bf KDB}\\in\\mathscr{C}_{sym.,n.b.d.}$\u0026nbsp;:\nCons√©quence de 3. et 7. $\\mathcal{M}_\\text{\\bf KTB}\\in\\mathscr{C}_{ref.sym.}$\u0026nbsp;:\nCons√©quence de 5. et 7. $\\mathcal{M}_\\text{\\bf S5}\\in\\mathscr{C}_{ref.,sym.tr.}$\u0026nbsp;:\nCons√©quence de 2. et 5. et 7. On repr√©sente graphiquement par une fl√®che \u0026ldquo;$\\mathscr{C}_x\\longrightarrow\\text{\\bf X}$\u0026rdquo; la relation de forte-compl√©tude¬†: \u0026ldquo;si $\\Gamma\\models_{\\mathscr{C}_x}\\phi$, alors $\\Gamma\\vdash_\\text{\\bf X}\\phi$\u0026rdquo;.\nnote\nLa notion de cons√©quence s√©mantique $\\models$ demande une satisfaction sur tous les mod√®le alors que la notion de cons√©quence syntaxique $\\vdash$ demande seulement l\u0026rsquo;existence d\u0026rsquo;une preuve.\nDit autrement, $\\models$ est d√©finie avec un quantificateur universel $\\forall$ et $\\vdash$ avec un quantificateur existentiel $\\exists$ or la n√©gation du quantificateur universel est le quantificateur existentiel et inversement.\nD√©montrer qu\u0026rsquo;une formule est non d√©montrable est donc tr√®s difficile du c√¥t√© de la th√©orie de la d√©monstration puisque cela oblige √† montrer que toutes les preuves √©chouent. Par contre, sur le versant s√©mantique, il suffit de trouver un mod√®le o√π la n√©gation de la formule est satisfaite (un contre-exemple suffit).\nC\u0026rsquo;est la correction qui nous permet le changement de versant (on passe de la syntaxe √† la s√©mantique).\nOn peut trouver d\u0026rsquo;ailleurs un peu douloureux que la correspondance de loin la plus difficile √† d√©montrer (la compl√©tude) apporte si peu d\u0026rsquo;applications concr√®tes.\nSuite : Diff√©rentes logiques modales "
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique6/",
	"title": "Logique modale",
	"tags": [],
	"description": "",
	"content": " Logique modale Syntaxe et s√©mantique Syst√®mes logiques Diff√©rentes logiques modales Petit condens√© de logique modale\u0026nbsp;: En logique modale, un \u0026ldquo;monde possible\u0026rdquo; contient les propositions qui sont vraies dans ce monde. Il correspond √† un mod√®le du calcul des propositions (√† une ligne d\u0026rsquo;une table de v√©rit√©). Ces mondes possibles sont connect√©s entre eux par des relations d\u0026rsquo;accessibilit√©, et les op√©rateurs de modalit√© permettent d\u0026rsquo;explorer ces relations. Si $P$ est vraie dans tous les mondes accessibles depuis le monde $a$, alors on √©crit $a\\Vdash \\Box P$ et si $P$ est vraie dans au moins un de ces mondes, on utilise $a\\Vdash \\Diamond P$. Un mod√®le de la logique modale est alors un graphe orient√©, appel√© syst√®me de transition ou structure de Kripke.\nL'un des objectifs fondamentaux des syst√®mes logiques est de fournir un cadre o√π les propositions peuvent √™tre syst√©matiquement et formellement prouv√©es ou r√©fut√©es. On cherche au final une recette pour mettre √† jour les tautologies de la th√©orie, les v√©rit√©s absolues, les formules valid√©es dans tout mod√®le. Cette exigence conduit √† une axiomatisation, l'√©tablissement d'un ensemble de r√®gles d'inf√©rences et de principes de base pour forger une machine √† produire du vrai.\nAvec pour axiomes les tautologies du calcul des propositions auxquelles on ajoute l'axiome modal $\\text{(K)}$ (qui distribue $\\Box$ dans une implication), et les r√®gles de n√©cessitation (si $\\vdash\\phi$, alors on a aussi $\\vdash\\Box\\phi$), de substitution uniforme (une formule prouvable o√π toute occurence d'une proposition est remplac√©e par une m√™me formule quelconque est tout autant prouvable) et de modus ponens pour assurer la coh√©rence de la machinerie, on se retrouve avec la logique modale normale de base $\\mathbf{K}$ qui permet de produire l'ensemble des formules universellement vraies. Toute formule obtenue dans $\\mathbf{K}$ est vraie dans tout monde possible de toute structure de Kripke, et inversement. On dit alors qu'il y a correction et compl√©tude entre le syst√®me formel et les mod√®les associ√©s\u0026nbsp;: $\\vdash_\\text{\\bf K}\\phi$ ssi $\\Vdash_\\mathscr{C}\\phi$ o√π $\\vdash_\\text{\\bf K}\\phi$ dit que $\\phi$ a √©t√© prouv√©e dans $\\mathbf{K}$, et $\\Vdash_\\mathscr{C}\\phi$ dit que $\\phi$ est valide (vraie dans tout monde) pour tout mod√®le de la classe $\\mathscr{C}$ qui d√©signe l'ensemble des syst√®mes de transition.\nEn ajoutant de nouveaux axiomes modaux au syst√®me formel, on ajoute par l√† m√™me des contraintes aux mod√®les associ√©s (les graphes gagnent de nouvelles propri√©t√©s). √Ä chaque jeu d'axiomes va correspondre une certaine classe de syst√®mes de transition o√π la v√©rit√© pourra se d√©ployer. En ajoutant l'axiome $\\text{(B)}=P\\rightarrow \\Box\\Diamond P$, par exemple, la validit√© des formules obtenues sera limit√©e aux graphes non orient√©s $\\mathscr{C}_{sym.}$ puisque l'axiome $\\text{(B)}$ ajoute la sym√©trie aux structures. Les diff√©rentes logiques modales On cherche maintenant √† incarner la logique modale avec des interpr√©tations pr√©cises des op√©rateurs modaux. La bo√Æte peut servir √† exprimer qu\u0026rsquo;un agent sait quelque chose et son dual, le diamant, exprime alors le fait que cet agent croit seulement possible cette m√™me chose. √áa nous place dans une logique √©pist√©mique (logique de la connaissance) et c\u0026rsquo;est principalement cette version qu\u0026rsquo;on a utilis√©e jusqu\u0026rsquo;ici pour donner corps √† la th√©orie. Mais d\u0026rsquo;autres emplois sont courants et ne rattachent pas forc√©ment les op√©rateurs modaux √† un agent, comme c\u0026rsquo;est le cas de la logique al√©thique et la logique d√©ontique. On verra aussi qu\u0026rsquo;en fonction des interpr√©tations, certains axiomes sont plus naturels que d\u0026rsquo;autres et donc que ces diff√©rentes logiques modales s\u0026rsquo;√©panouissent dans des univers aux topologies diff√©rentes.\nLogique al√©thique La logique modale est dite al√©thique lorsque les op√©rateurs modaux traduisent la n√©cessit√© et la possibilit√©.\nLes trois cellules rectangulaires sont destin√©es √† √™tre conjointement exhaustives et mutuellement exclusives¬†: chaque proposition peut √™tre n√©cessaire, contingente (possiblement vraie mais aussi possiblement fausse), ou encore impossible, mais jamais plus qu\u0026rsquo;un de ces trois choix.\nOn peut en d√©duire l\u0026rsquo;hexagone logique suivant, extension du carr√© des oppositions d\u0026rsquo;Aristote, d√ª √† Sesmat et Blanch√©¬†:\nDeux propositions contradictoires ne sont jamais ni vraies, ni fausses en m√™me temps. Deux propositions contraires ne sont jamais vraies en m√™me temps, mais peuvent √™tre fausses en m√™me temps. Deux propositions sous-contraires ne sont jamais fausses en m√™me temps, mais peuvent √™tre vraies en m√™me temps. L\u0026rsquo;axiome $\\text{(K)}=\\Box(P\\rightarrow Q)\\rightarrow(\\Box P\\rightarrow\\Box Q)$, qui rappelons-le est vrai dans tout syst√®me, se traduit en logique al√©thique par \u0026ldquo;s\u0026rsquo;il est n√©cessaire que $P$ entra√Æne $Q$, alors si $P$ est n√©cessaire, $Q$ est n√©cessaire √©galement.\u0026rdquo;\nParmi les autres axiomes qu\u0026rsquo;on a pu rencontrer, deux semblent plus adapt√©s √† cette interpr√©tation de la modalit√©¬†:\nl'axiome $\\text{(T)}=\\Box P\\rightarrow P$\u0026nbsp;: \"s'il est n√©cessaire que $P$, alors $P$ est r√©alis√©e\" (les logiciens du Moyen √Çge, d√©j√†, employaient la formule ab necesse esse). l'axiome $\\text{(B)}= P\\rightarrow \\Box\\Diamond P$\u0026nbsp;: \"si $P$ est r√©alis√©e, alors il est n√©cessaire que $P$ soit possible\" (cela semble en effet assez naturel). Le syst√®me logique normale adapt√© √† la logique al√©thique semble donc √™tre $\\text{\\bf KBT}$ et la classe de mod√®les adapt√©e est donc celle des syst√®mes de transition √† la fois r√©flexifs et sym√©triques.\nAristote est √† l\u0026rsquo;origine de la logique al√©thique et il en donne un exemple c√©l√®bre dans De l\u0026rsquo;interpr√©tation¬†:\nN√©cessairement il y aura demain une bataille navale ou il n\u0026rsquo;y en aura pas. Mais il n\u0026rsquo;est pas n√©cessaire qu\u0026rsquo;il y ait demain une bataille navale, pas plus qu\u0026rsquo;il n\u0026rsquo;est n√©cessaire qu\u0026rsquo;il n\u0026rsquo;y en ait pas. Mais qu\u0026rsquo;il y ait ou qu\u0026rsquo;il n\u0026rsquo;y ait pas demain une bataille navale, voil√† qui est n√©cessaire.\nEn appelant $P$ l\u0026rsquo;affirmation \u0026ldquo;il y aura une bataille navale demain\u0026rdquo;, la premi√®re et troisi√®me phrase se traduisent par la m√™me formule, $\\Box(P\\lor\\neg P)$, alors que la deuxi√®me phrase se traduit par $\\neg\\Box P\\land\\neg\\Box\\neg P$.\nLe mod√®le ci-dessous repr√©sente les deux mondes possibles (avec ou sans bataille navale). Les deux formules sont v√©rifi√©es en chacun des mondes.\nLa logique modale \u0026ldquo;moderne\u0026rdquo; a elle aussi d\u0026rsquo;abord √©t√© al√©thique lorsqu\u0026rsquo;elle n√©e du d√©sir de C.I. Lewis, en 1913, de renforcer l\u0026rsquo;implication mat√©rielle $P\\rightarrow Q$. Il a voulu lui substituer une implication stricte qui s\u0026rsquo;av√©rera finalement une implication mat√©rielle renforc√©e du caract√®re de la n√©cessit√©. L\u0026rsquo;√©tude de l\u0026rsquo;implication stricte revenait donc √† celle de la logique al√©thique. La logique modale s\u0026rsquo;est ensuite d√©velopp√©e de mani√®re purement axiomatique jusqu\u0026rsquo;au d√©but des ann√©es 60, quand S. Kripke introduit l\u0026rsquo;id√©e des mondes possibles pour cr√©er une s√©mantique √† la logique modale.\nMais revenons √† l\u0026rsquo;implication. La formule $(P\\rightarrow Q)\\lor(Q\\rightarrow P)$ est une tautologie du calcul des propositions. Pour deux propositions quelconques, il y en a donc toujours une qui implique l\u0026rsquo;autre, ce qui para√Æt absurde puisque les propositions peuvent √™tre totalement ind√©pendantes¬†!\nAvec l\u0026rsquo;implication stricte (implication ordinaire renforc√©e de la n√©cessit√©), le paradoxe dispara√Æt car $\\Box(P\\rightarrow Q)\\lor\\Box(Q\\rightarrow P)$ n\u0026rsquo;est pas valide. En effet, aucun des mondes du mod√®le ci-dessous ne satisfait la formule.\nLe m√©taphysicien Charles Hartshorne a produit une \u0026ldquo;preuve\u0026rdquo; de l\u0026rsquo;existence de Dieu qui repose sur la logique $\\text{\\bf{KB}}$ et qui n√©cessite un th√©or√®me de la logique modale $\\vdash_\\text{\\bf K} \\Box(P\\rightarrow Q)\\rightarrow(\\Diamond P\\rightarrow\\Diamond Q)$, l\u0026rsquo;axiome d\u0026rsquo;Anselme $G\\rightarrow\\Box G$ (si Dieu existe, il est n√©cessaire que Dieu existe) et l\u0026rsquo;axiome de Leibniz $\\Diamond G$ (il est possible que Dieu existe).\nMonsieur Phi a une chouette vid√©o sur ce genre d'argument ontologique. Logique d√©ontique Cette logique porte sur l\u0026rsquo;obligation ou la permission.\nLes deux principes de la logique al√©thique (les deux axiomes choisis) deviennent clairement faux en logique d√©ontique\u0026nbsp;:\n$\\Box P\\rightarrow P$\u0026nbsp;: si $P$ est obligatoire, alors $P$ est vraie. √áa n'a ici pas de sens puisque c'est le caract√®re propre de l'obligation que de pouvoir √™tre transgress√©e\u0026nbsp;! Il ne s'agirait pas sinon d'une obligation mais bien d'une n√©cessit√©. $P\\rightarrow \\Box\\Diamond P$\u0026nbsp;: si $P$ est vraie, alors $P$ doit √™tre permise. C'est √† nouveau d√©nu√© de sens puisque des choses non permises arrivent. Consid√©rer la propositions vraies revient d'ailleurs √† tomber dans le sophisme naturaliste qui stipule que tout ce qui est naturel est bon (\"il n'y a pas de v√™tements dans la nature, donc il ne faut pas autoriser les v√™tements\"). Mais quelle formule s\u0026rsquo;adapte √† la logique d√©ontique¬†? L\u0026rsquo;axiome $\\text{(D)}=\\Box P\\rightarrow\\Diamond P$ affirmant que \u0026ldquo;si quelque chose est obligatoire, alors elle est permise\u0026rdquo;, convient naturellement. En triturant un peu la formule, on obtient $\\Box P\\rightarrow\\Diamond P\\equiv\\neg\\Box P\\lor\\Diamond P\\equiv\\Diamond\\neg P\\lor\\Diamond P$ \u0026ldquo;soit c\u0026rsquo;est permis de faire une chose, soit c\u0026rsquo;est permis de ne pas la faire\u0026rdquo;. Ou encore¬†: $\\Diamond\\neg P\\lor\\Diamond P\\equiv \\neg\\Box P\\lor\\neg\\Box\\neg P\\equiv\\neg (\\Box P\\land\\Box\\neg P)$ \u0026ldquo;ne peuvent √™tre obligatoire √† la fois une chose et son contraire\u0026rdquo;. Tout cela semble plut√¥t √©vident.\nLe syst√®me formel correspondant est donc $\\text{\\bf KD}$ et la classe de syst√®mes de transition associ√©e est $\\mathscr{C}_{n.b.d.}$. La raison de cette topologie ouverte, avec des mondes qui ont tous un √©chappatoire est qu\u0026rsquo;une impasse est par construction un monde ou rien n\u0026rsquo;est permis¬†!\nPlusieurs paradoxes d√©coulent de cette formalisation de l\u0026rsquo;obligation et de la permission. Penchons-nous d\u0026rsquo;abord sur le paradoxe de Ross (d√ª √†\u0026hellip; Ross).\nL\u0026rsquo;√©nonc√© \u0026ldquo;Il est obligatoire de poster la lettre\u0026rdquo; ($\\Box P$ o√π $P$ correspond √† poster la lettre) entra√Æne \u0026ldquo;il est obligatoire de poster la lettre ou de la br√ªler\u0026rdquo; ($\\Box(P\\lor B)$ o√π $B$ correspond √† br√ªler la lettre). En effet $\\Box P\\rightarrow \\Box(P\\lor B)$ est prouvable sans hypoth√®se et est donc v√©rifi√©e en tout n≈ìud de tout mod√®le de Kripke d\u0026rsquo;une logique modale normale (puisque l\u0026rsquo;axiome $\\text{(D)}$ n\u0026rsquo;est m√™me pas n√©cessaire √† la preuve).\nOn part de la tautologie $P\\rightarrow (P\\lor Q)$ et de l\u0026rsquo;axiome $\\text{(K)}$¬†:\nSi l\u0026rsquo;obligation de poster une lettre implique l\u0026rsquo;obligation de la poster ou de la br√ªler, alors on peut satisfaire l\u0026rsquo;obligation d√©coulant de la premi√®re en br√ªlant la lettre. Certes, la lettre est loin d\u0026rsquo;√™tre post√©e et on a d\u0026rsquo;ailleurs probablement enfreint un interdit, mais on a pourtant bien aussi satisfait une obligation\u0026hellip;\nL\u0026rsquo;implication a permis une sorte de r√©duction du poids de l\u0026rsquo;obligation par affaiblissement de $P$ en $P\\lor B$.\nOn rencontre un autre probl√®me en affaiblissant une conjonction comme le montre le paradoxe du Bon Samaritain (d√ª √† Prior)¬†: l\u0026rsquo;√©nonc√© \u0026ldquo;il est obligatoire qu\u0026rsquo;Alice aide Bob qui a √©t√© vol√©\u0026rdquo; peut se traduire par $\\Box(A\\land B)$ o√π $A$ est \u0026ldquo;Alice aide Bob\u0026rdquo; et $B$ est \u0026ldquo;Bob a √©t√© vol√©\u0026rdquo;. Le probl√®me est que $\\Box(A\\land B)\\rightarrow\\Box B$ est prouvable dans le syst√®me $\\text{\\bf K}$ ($\\text{(D)}$ n\u0026rsquo;est toujours pas n√©cessaire √† la preuve).\nOn part de la tautologie $(A\\land B)\\rightarrow B$ et de l\u0026rsquo;axiome $\\text{(K)}$¬†:\nDonc si aider Bob lors d\u0026rsquo;un vol est obligatoire, il devient alors tout aussi obligatoire que Bob soit vol√©. Bob n\u0026rsquo;est pas ravi de la situation.\nUne fa√ßon de se sortir de ces pi√®ges est de maintenir une s√©paration stricte entre les descriptions des faits et les prescriptions d√©ontiques. Cela √©vite d\u0026rsquo;inf√©rer des faits (comme l\u0026rsquo;existence d\u0026rsquo;une victime) √† partir des obligations morales ou l√©gales.\nMais plus fondamentalement, en habillant modalement les deux tautologies ($P\\rightarrow(P\\lor Q)$ et $(A\\land B\\rightarrow B$) qui ont servi d\u0026rsquo;axiome dans les d√©monstrations, on permet de les interpr√©ter sous un jour plus concret r√©v√©lant leur bizarrerie. Le ver √©tait dans le fruit d√®s l\u0026rsquo;utilisation de l\u0026rsquo;implication mat√©rielle classique (celle que Lewis cherchait justement √† dompter avec la modalit√©).\nPourquoi ces paradoxes sont-ils attach√©s √† la logique d√©ontique alors qu\u0026rsquo;on peut tr√®s bien obtenir les m√™mes formules dans les autres logiques modales (puisqu\u0026rsquo;aucun axiome n\u0026rsquo;est n√©cessaire)¬†?\nLa logique d√©ontique vise √† proposer un guide √©thique, des normes √† suivre, l√† o√π la logique al√©thique, par exemple, n\u0026rsquo;√©nonce qu\u0026rsquo;un √©tat de fait. Devant l\u0026rsquo;affirmation que la n√©cessit√© du postage d\u0026rsquo;une lettre rend n√©cessaire son postage ou sa destruction par le feu, on a moins envie de monter au rideau (m√™me si √ßa reste un peu bizarre). Une chose n√©cessaire s\u0026rsquo;impose physiquement et non moralement, il n\u0026rsquo;y a pas le choix, et l\u0026rsquo;application brute de la logique \u0026ldquo;math√©matique\u0026rdquo; fait donc plus sens. En tant que prescripteur de comportement, la logique d√©ontique a besoin d\u0026rsquo;une plus grande incarnation et de plus d\u0026rsquo;attention sur l\u0026rsquo;interpr√©tation.\nLe dilemme du libre choix (aussi d√ª √† Ross) est une autre curiosit√© qui voit une hypoth√®se d\u0026rsquo;allure b√©nigne se transformer en bo√Æte de Pandore.\n\u0026ldquo;Tu peux soit dormir sur le sofa soit dans la chambre d\u0026rsquo;amis\u0026rdquo; semble impliquer que \u0026ldquo;tu peux dormir sur le sofa et tu peux dormir dans la chambre d\u0026rsquo;amis\u0026rdquo;. On imagine ainsi que l\u0026rsquo;implication $\\Diamond(A\\lor B)\\rightarrow(\\Diamond A\\land\\Diamond B)$ est prouvable dans le syst√®me. Mais ce n\u0026rsquo;est pas le cas (voir ci-dessous). Qu\u0026rsquo;√† cela ne tienne¬†! Ajoutons-l√† aux hypoth√®ses. Cela serait dommage de se passer d\u0026rsquo;une implication si naturelle\u0026hellip; Mais l√†, patatras, tout devient possible¬†! Imaginons en effet qu\u0026rsquo;il existe une quelconque possibilit√© $\\Diamond A$, alors $\\Diamond (A\\lor B)$ aussi est vraie, et ce quel que soit $B$ (s\u0026rsquo;il ya un monde accessible o√π $A$ est vraie, alors dans ce m√™me monde, $A\\lor B$ est vraie) et gr√¢ce √† notre pernicieux nouvel axiome, on d√©duit la v√©rit√© de $\\Diamond A\\land\\Diamond B$ qui se r√©duit √† celle de $\\Diamond B$ (puisque par hypoth√®se $\\Diamond A$ est vraie). On a finalement $\\Diamond A\\rightarrow \\Diamond B$ pour tout $B$. La possibilit√© de $A$ rend tout possible gr√¢ce √† cet axiome diabolique.\nOn prouve que $\\Diamond(A\\lor B)\\rightarrow(\\Diamond A\\land\\Diamond B)$ n\u0026rsquo;est pas prouvable dans le syst√®me $\\text{\\bf KD}$ en trouvant un mod√®le appartenant √† $\\mathscr{C}_{n.b.d.}$ et tel qu\u0026rsquo;au moins un de ses n≈ìuds ne satisfait pas la formule.\nUn dernier pour la route¬†: le dilemme de Sartre (d√ª √† Lemmon). Il d√©crit l\u0026rsquo;impossibilit√© de repr√©senter un conflit d\u0026rsquo;obligations en logique d√©ontique standard (√† cause de l\u0026rsquo;axiome $\\text{(D)}$). La situation dans laquelle se retrouve Jepht√© dans la Bible en est une illustration parfaite. Jepht√© promet √† Dieu de sacrifier le premier √™tre vivant qu\u0026rsquo;il croise s\u0026rsquo;il gagne la guerre. Il gagne la guerre et croise sa fille (c\u0026rsquo;est ballot). On a bien conflit d\u0026rsquo;obligations puisque l\u0026rsquo;obligation de ne pas tuer sa fille doit bien figurer quelque part aussi\u0026hellip; On a donc une situation o√π il est obligatoire de faire une chose $\\Box A$ et aussi obligatoire de ne pas la faire $\\Box \\neg A$. Or $\\Box A\\rightarrow \\Diamond A$ d\u0026rsquo;apr√®s $\\text{(D)}$ et par dualit√©, on obtient $\\Box A\\rightarrow \\neg \\Box\\neg A$. La situation de d√©part (la satisfaction de $\\Box A$ et $\\neg\\Box A$), bien qu\u0026rsquo;envisageable, aboutit √† une inconsistance interdisant donc m√™me son existence dans le syst√®me formel $\\text{\\bf KD}$ (qui commet s√ªrement l√† un p√©cher).\nMalgr√© ses \u0026ldquo;petits\u0026rdquo; d√©fauts, la logique d√©ontique connait diff√©rentes applications en informatique. En autorisant l\u0026rsquo;expression de raisonnements normatifs, elle permet de sp√©cifier la marche √† suivre en cas de comportement anormal. De mani√®re plus g√©n√©ral, elle propose un cadre pour g√©rer les autorisations et politiques d\u0026rsquo;acc√®s, et on la retrouve dans le secteur de l\u0026rsquo;automatisation juridique ou encore celui des contraintes sur les bases de donn√©es.\nLogique √©pist√©mique Elle permet de raisonner √† propos de la connaissance d\u0026rsquo;un ou plusieurs agents.\nC\u0026rsquo;est elle qui est au c≈ìur de notre histoire introductive de dragons ou de cette blague sur des logiciens assoiff√©s.\nAppelons $1$, $2$ et $3$ les trois logiciens de gauche √† droite et $B_i$ l'affirmation \"le logicien $i$ veut une bi√®re\".\nAu d√©part, $1$ veut une bi√®re (et elle le sait\u0026hellip;), mais elle ne peut pas savoir dans lequel des 4 mondes restant possibles elle se trouve¬†: $[1]B_1\\land\\langle1\\rangle B_2\\land\\langle1\\rangle B_3$. Comme le monde o√π $B_1\\land B_2\\land B_3$ fait partie des mondes possibles, elle r√©pond \u0026ldquo;je ne sais pas\u0026rdquo;. $2$ en d√©duit $B_1$. Car si $\\neg B_1$, alors $[1]\\neg B_1$ et donc le monde $B_1\\land B_2\\land B_3$ o√π tous veulent picoler n\u0026rsquo;est plus accessible et elle aurait r√©pondu non. Voulant elle aussi une bi√®re, $2$ h√©site encore entre deux mondes $[2]B_1\\land[2]B_2\\land\\langle2\\rangle B_3$. $3$ en d√©duit $B_2$ et peut maintenant r√©pondre puisque $[3]B_1\\land[3]B_2\\land[3] B_3$ ($3$ aussi veut une bi√®re bien s√ªr). Plus qu\u0026rsquo;un monde possible, celui qui satisfait $B_1\\land B_2\\land B_3$. C\u0026rsquo;est un grand \u0026ldquo;Oui !\u0026rdquo;. La r√®gle de n√©cessitation associ√©e √† l\u0026rsquo;axiome $\\text{(K)}$ (valables dans tout n≈ìud de tout syst√®me d\u0026rsquo;une logique normale) impliquent l\u0026rsquo;omniscience logique de l\u0026rsquo;agent $i$. En effet, par n√©cessitation $A\\rightarrow B$ devient $[i](A\\rightarrow B)$ et $\\text{(K)}=[i](A\\rightarrow B)\\rightarrow([i]A\\rightarrow [i]B)$. Par cons√©quent, l\u0026rsquo;agent connait toutes les cons√©quences logique ($[i]B$) de ce qu\u0026rsquo;il sait ($[i]A$).\nEn logique modale √©pist√©mique, la relation binaire entre deux mondes est une relation d\u0026rsquo;indistinguabilit√©¬†; l\u0026rsquo;agent ne sait pas dans quel monde il se trouve. Cette relation est n√©cessairement sym√©trique par la notion m√™me d\u0026rsquo;indistinguabilit√©. Et comme un monde ne pourra jamais √™tre distingu√© de lui-m√™me, il faut ajouter une relation r√©flexive de chaque monde √† lui-m√™me. Le dernier ingr√©dient est la transitivit√©. Si $a$ est indistinguable de $b$ et que $b$ est indistinguable de $c$, alors $a$ est aussi indistinguable de $c$. Une telle relation, sym√©trique, r√©flexive et transitive est une relation d\u0026rsquo;√©quivalence. Et on aurait pu partir de l√† finalement puisque c\u0026rsquo;est bien l\u0026rsquo;√©quivalence qui se cache derri√®re la notion d\u0026rsquo;indistinguabilit√©.\nLa classe de syst√®mes de transition assurant une relation d\u0026rsquo;√©quivalence entre les mondes possibles correspond √† la logique $\\text{\\bf S5}$.\nDeux jeux d\u0026rsquo;axiomes √©quivalents permettent d\u0026rsquo;obtenir $\\text{\\bf S5}$¬†: $\\set{\\text{(\\text{K}),(\\text{T}),(\\text{B}),(\\text{4})}}$ o√π $\\text{(T)}$ apporte la r√©flexivit√©, $\\text{(B)}$ la sym√©trie et $\\text{(4)}$ la transitivit√©, ou bien le jeu $\\set{(\\text{K}),(\\text{T}),(\\text{5})}$ o√π $\\text{(5)}$ apporte l\u0026rsquo;euclidanit√© sachant qu\u0026rsquo;un syst√®me de transition euclidien est √† la fois sym√©trique et transitif.\nLes axiomes doivent √™tre v√©rifi√©s dans tous les mondes possibles. Ainsi $\\text{(T)}=[i]P\\rightarrow P$ signifie que dans un monde quelconque, si l\u0026rsquo;agent $i$ sait que $P$, alors $P$ est vrai dans ce monde (l\u0026rsquo;agent ne sait que des choses vraies). Et il faut aussi que soit r√©alis√©e $\\text{(5)}=\\neg[i]P\\rightarrow[i]\\neg[i]P$, ce qui signifie que si l\u0026rsquo;agent $i$ ne sait pas quelque chose, alors il sait qu\u0026rsquo;il ne le sait pas. C\u0026rsquo;est le principe d\u0026rsquo;introspection n√©gative.\nL\u0026rsquo;agent de la logique $\\text{\\bf S5}$ est une sorte d\u0026rsquo;agent id√©al pour lequel l\u0026rsquo;acc√®s √† la connaissance est maximal (sans qu\u0026rsquo;il ne sache tout pour autant). Il n\u0026rsquo;est tellement pas humain qu\u0026rsquo;il ne peut pas croire\nOn place parfois l\u0026rsquo;agent dans une logique inf√©rieure, $\\text{\\bf S4}$, qui perd la sym√©trie (et donc l\u0026rsquo;indistinguabilit√©¬†!). L\u0026rsquo;introspection n√©gative dispara√Æt alors et est remplac√©e par l\u0026rsquo;introspection positive $\\text{(4)}:[i]P\\rightarrow [i][i] P$ o√π si l\u0026rsquo;agent sait quelque chose, alors il sait qu\u0026rsquo;il sait. Mine de rien, √ßa lui limite pas mal l\u0026rsquo;acc√®s √† la connaissance par rapport √† l\u0026rsquo;autre introspection. L\u0026rsquo;agent reste id√©al en ce qu\u0026rsquo;il conserve une omniscience logique totale, mais il peut vivre dans un monde ou il peut croire possible qu\u0026rsquo;il sait quelque chose qu\u0026rsquo;il ne sait pas (la honte).\nDans $\\text{\\bf S4}$, l\u0026rsquo;accessibilit√© n\u0026rsquo;est plus n√©cessairement r√©ciproque entre les mondes. Cela introduit une hi√©rarchie, un ordre o√π certains mondes peuvent √™tre consid√©r√©s comme des extensions ou des raffinements d\u0026rsquo;autres, des mondes \u0026ldquo;plus inform√©s\u0026rdquo;. C\u0026rsquo;est pertinent pour mod√©liser la communication dans des r√©seau o√π l\u0026rsquo;information n\u0026rsquo;est pas toujours r√©ciproquement √©chang√©e (d√®s que les canaux sont asym√©triques comme c\u0026rsquo;est le cas dans la plupart des structures hi√©rarchiques).\nCoupl√© √† une logique temporelle, cela permet de mod√©liser un apprentissage progressif o√π l\u0026rsquo;acquisition d\u0026rsquo;information se fait de mani√®re s√©quentielle ou cumulative. En robotique ou en intelligence artificielle, cela permet dans la m√™me veine de mod√©liser des processus de planification o√π chaque d√©cision ou action m√®ne √† un nouvel √©tat du monde qui int√®gre toutes les connaissances et cons√©quences des actions pr√©c√©dentes.\nLogique temporelle Il y a beaucoup de logiques temporelles qui multiplient g√©n√©ralement les op√©rateurs modaux. Mais de base, une logique modale temporelle peut exprimer ce qu\u0026rsquo;on trouve dans le carr√© des oppositions ci-dessous.\nLa logique temporale peut aussi bien s\u0026rsquo;int√©resser au pass√© qu\u0026rsquo;au futur.\nLe temps dont il est question ici est discret, chaque monde possible correspondant √† un instant $t$.\nPas de raison particuli√®re pour que la relation d\u0026rsquo;accessibilit√© entre ces mondes soit r√©flexive ou sym√©trique, par contre, elle se doit d\u0026rsquo;√™tre transitive pour conserver l\u0026rsquo;ordonnancement entre les diff√©rents instants consid√©r√©s. L\u0026rsquo;axiome cl√© est donc le $\\text{(4)}$ et le futur d\u0026rsquo;un monde correspond alors √† l\u0026rsquo;ensemble des mondes qui lui sont accessibles (l\u0026rsquo;absence de r√©flexivit√© entra√Æne que le monde actuel ne fasse pas partie du futur).\nOn peut alors traduire les op√©rateurs modaux par\u0026nbsp;:\n$\\Box \\phi$¬†: $\\phi$ sera toujours v√©rifi√©e, $\\Diamond \\phi$¬†: $\\phi$ sera v√©rifi√©e √† un certain moment. Cela permet d'exprimer des √©nonc√©s tels que\u0026nbsp;:\n$\\Box\\neg(\\texttt{started}\\land\\neg\\texttt{ready})$¬†: il est impossible d\u0026rsquo;atteindre un √©tat o√π $\\texttt{started}$ est activ√©, mais pas $\\texttt{ready}$, $\\Box(\\texttt{request}\\rightarrow\\Diamond\\texttt{acknowledged})$¬†: pour tout √©tat, si une demande (de quelque ressource que ce soit) est effectu√©e, alors elle sera finalement reconnue, Le temps des logique temporelles peut prendre une forme lin√©aire ou arborescente.\nDans la logique temporelle lin√©aire (LTL), les instants s\u0026rsquo;encha√Ænent en un collier de perles infini. C\u0026rsquo;est id√©al pour explorer un syst√®me d√©terministe ou une seule branche d\u0026rsquo;ex√©cution et v√©rifier efficacement que toutes les sp√©cifications sont satisfaites.\nDans une logique temporelle arborescente (CTL), le champ des possibles s\u0026rsquo;ouvrent. Elle permet de mod√©liser des syst√®mes non d√©terministes ou des choix amenant sur des branches diff√©rentes peuvent √™tre faits.\nLa logique temporelle est tr√®s utilis√©e en v√©rification formelle, o√π la technique de base est essentiellement le model checking.\nLogique de la prouvabilit√© Souhaitant √©prouver la stabilit√© de l\u0026rsquo;√©difice math√©matique, en particulier l\u0026rsquo;arithm√©tique (la science des nombres, le c≈ìur des maths), G√∂del a imagin√© une m√©thode astucieuse, une sorte de hack qui lui permit de plonger les √©nonc√©s logiques dans le champ de l\u0026rsquo;arithm√©tique (en particulier sur l\u0026rsquo;Arithm√©tique de Peano $\\text{PA}$, une th√©orie - ensemble d\u0026rsquo;axiomes - en logique du premier ordre dont l\u0026rsquo;arithm√©tique des entiers est un mod√®le). En associant √† chaque formule un num√©ro (le num√©ro de G√∂del ou code de G√∂del), not√© $\\ulcorner\\phi\\urcorner$, il a permis aux nombres de parler d\u0026rsquo;eux-m√™mes dans une sorte d\u0026rsquo;√©panchement introspectif.\nCe qui semblait √† l\u0026rsquo;√©poque un processus √©minemment complexe est devenu trivial √† l\u0026rsquo;√®re de l\u0026rsquo;informatique. Pour associer un nombre unique √† une formule, il suffit par exemple de regarder le code binaire du fichier de traitement de texte o√π la formule est √©crite (et d\u0026rsquo;ajouter √©ventuellement un \u0026ldquo;1\u0026rdquo; au d√©but si √ßa commence par des \u0026ldquo;0\u0026rdquo;).\nOn peut m√™me coder sans probl√®me un arbre de preuve complet (tout ce qu\u0026rsquo;on peut √©crire dans un traitement de texte finalement). D√®s lors, une formule peut tr√®s bien parler du fait qu\u0026rsquo;une autre formule est d√©montrable puisqu\u0026rsquo;il s\u0026rsquo;agit de dire qu\u0026rsquo;il existe un entier poss√©dant les propri√©t√©s qui le caract√©risent comme √©tant le code d\u0026rsquo;une preuve de cette formule. C\u0026rsquo;est ce que fait la formule $\\Box\\phi$ o√π l\u0026rsquo;op√©rateur de modalit√© $\\Box$ prend alors le sens de \u0026ldquo;est prouvable\u0026rdquo; (elle √©quivaut √† la fameuse formule de G√∂del $G\\leftrightarrow\\neg\\text{Bew}(\\ulcorner G\\urcorner)$, $\\Box$ correspondant √† $\\text{Bew}(x)$, \u0026ldquo;bew\u0026rdquo; √©tant le d√©but du mot allemand beweisbar qui signifie prouvable).\nLe premier th√©or√®me d\u0026rsquo;incompl√©tude de G√∂del s\u0026rsquo;appuie sur une formule affirmant sa propre non prouvabilit√© $\\neg\\Box\\phi\\leftrightarrow \\phi$. Si on peut d√©montrer la formule $\\neg\\Box\\phi\\leftrightarrow \\phi$ dans un syst√®me donn√©, alors on prouve qu\u0026rsquo;il existe une formule non d√©montrable et pourtant vraie, ce qui impose l\u0026rsquo;incompl√©tude du syst√®me.\nLemme de diagonalisation¬†:\nPour toute formule arithm√©tique $f(x)$, il existe une formule arithm√©tique $P$ telle que $P\\leftrightarrow f(P)$.\nC\u0026rsquo;est l\u0026rsquo;outil permettant √† G√∂del de construire des formules autor√©f√©rentes comme $\\neg\\Box\\phi\\leftrightarrow \\phi$.\nLe lemme dit que pour toute formule $\\phi$, il existe une formule $S(\\ulcorner\\phi\\urcorner)$ qui parle de $\\phi$ (une formule m√©ta en somme) telle qu\u0026rsquo;on peut prouver $\\phi$ si et seulement si on peut prouver $S(\\ulcorner\\phi\\urcorner)$¬†: $\\vdash\\phi\\leftrightarrow S(\\ulcorner\\phi\\urcorner)$.\nPour d√©montrer le lemme, on va supposer que $n$ est le num√©ro d\u0026rsquo;une formule de la forme $A(x)$ o√π $x$ est une variable et on d√©finit $F(n)=\\ulcorner A(\\ulcorner A(x)\\urcorner)\\urcorner$ et si $n$ ne correspond pas √† une formule sous cette forme, alors $F(n)=0$.\nNotons que $F$ d√©pend du choix de la variable $x$. Ainsi, $F(\\ulcorner y=0 \\urcorner)=\\ulcorner(\\ulcorner y=0 \\urcorner) = 0\\urcorner$ et $F(\\ulcorner x=0 \\urcorner)=\\ulcorner(\\ulcorner x=0 \\urcorner) = 0\\urcorner$.\nSoit $g(w)\\equiv S(F(w))$ et $\\phi\\equiv g(\\ulcorner g(x)\\urcorner)$.\nOn obtient¬†:\n$$ \\begin{aligned} \\phi \u0026amp;= g(\\ulcorner g(x)\\urcorner)\\\\ \u0026amp;= S(F(\\ulcorner g(x)\\urcorner) \\\\ \u0026amp;= S(\\ulcorner g(\\ulcorner g(x)\\urcorner)\\urcorner)\\\\ \u0026amp;= S(\\ulcorner\\phi\\urcorner) \\end{aligned} $$\nTr√®s informellement, le lemme de diagonalisation exprime l\u0026rsquo;√©chec de l\u0026rsquo;argument de la diagonale de Cantor pour construire une formule $S(x)$ qui ne ferait pas partie de l\u0026rsquo;ensemble d√©nombrable des formules prouvables. La cl√¥ture de l\u0026rsquo;ensemble des formules prouvables par action d\u0026rsquo;une formule sur les √©l√©ments de cet ensemble emp√™che en effet de \u0026ldquo;diagonaliser √† l\u0026rsquo;ext√©rieur\u0026rdquo; du domaine.\nLa formule autor√©f√©rente de G√∂del est proche du fameux paradoxe du menteur \u0026ldquo;cette phrase est fausse\u0026rdquo;.\nPremier th√©or√®me d\u0026rsquo;incompl√©tude¬†:\nDans un syst√®me formel consistant (= coh√©rent = syst√®me dans lequel on ne peut pas prouver le faux) suffisamment expressif pour d√©crire l\u0026rsquo;arithm√©tique, on ne peut pas prouver toutes les formules vraies (comme la formule qui affirme sa non prouvabilit√©). La prouvabilit√© est plus faible que la v√©rit√©.\nMontrons dans la logique $\\text{\\bf K4}$ que $\\vdash\\phi\\leftrightarrow\\neg\\Box\\phi$ permet d\u0026rsquo;arriver √† $\\vdash\\neg\\Box\\bot\\rightarrow\\neg\\Box\\phi$¬†; la formule n\u0026rsquo;est pas prouvable si aucune contradiction n\u0026rsquo;est prouvable. On obtient donc bien l\u0026rsquo;essence du premier th√©or√®me d\u0026rsquo;incompl√©tude.\n$\\phi\\leftrightarrow\\neg\\Box\\phi$ $\\Box\\phi\\rightarrow\\neg\\phi$ (√† partir de 1.) $\\Box(\\Box\\phi\\rightarrow\\neg\\phi)$ (n√©cessitation) $\\Box\\Box\\phi\\rightarrow\\Box\\neg\\phi$ (distribution de la boite gr√¢ce √† $\\text{(K)}$) $\\Box\\phi\\rightarrow\\Box\\neg\\phi$ (par l'axiome $\\text{(4)}= \\Box \\phi\\rightarrow\\Box\\Box\\phi$ et avec l'√©tape 4., on d√©duit $\\Box\\phi\\rightarrow\\Box\\Box\\phi\\rightarrow\\Box\\neg\\phi$) $\\Box\\phi\\rightarrow\\Box\\phi\\land\\Box\\neg\\phi$ (√† partir de 5. et $\\Box\\phi\\rightarrow\\Box\\phi$) $\\Box\\phi\\rightarrow\\Box(\\phi\\land \\neg\\phi)$ $\\Box\\phi\\rightarrow\\Box\\bot$ $\\neg\\Box\\bot\\rightarrow \\neg\\Box\\phi$ (contrapos√©e de 8.) Le th√©or√®me de L√∂b utilise un autre formule autor√©f√©rente, $\\Box(\\Box\\phi\\rightarrow\\phi)\\rightarrow\\Box\\phi$, s\u0026rsquo;apparentant plut√¥t, dans son cas, au paradoxe d\u0026rsquo;un diseur de v√©rit√© \u0026ldquo;ce que je dis l√† est vrai\u0026rdquo; (aucune valeur de v√©rit√© ne semble applicable puisqu\u0026rsquo;un menteur dirait la m√™me chose).\nEt la surprise est que de tels points fixes sont prouvables et par cons√©quent vrais.\nLe th√©or√®me de L√∂b stipule que les formules dont on peut prouver que $\\Box\\phi\\rightarrow\\phi$ sont les formules prouvables¬†:\nSi $\\vdash\\Box\\phi\\rightarrow\\phi$, alors $\\vdash \\phi$. Le th√©or√®me affirme donc que si on peut prouver une formule en admettant comme axiome qu\u0026rsquo;on peut la prouver, alors on peut la prouver sans l\u0026rsquo;axiome. Le th√©or√®me est donc une sorte de m√©thode Cou√© logique, une proph√©tie autor√©alisatrice¬†; \u0026ldquo;si tu crois en toi, √ßa va le faire\u0026rdquo;. C\u0026rsquo;est bizarre, mais √ßa se d√©montre.\nPreuve du th√©or√®me de L√∂b¬†:\nOn suppose $\\vdash \\Box\\phi\\rightarrow\\phi$.\nGr√¢ce au lemme de diagonalisation, on construit la formule arithm√©tique autor√©f√©rente suivante\u0026nbsp;:\n$\\vdash\\sigma\\leftrightarrow (\\Box\\sigma\\rightarrow\\phi)$ (\"ma propre prouvabilit√© implique $\\phi$\").\n$\\vdash\\Box(\\sigma\\rightarrow (\\Box\\sigma\\rightarrow\\phi))$ (par n√©cessitation) $\\vdash\\Box\\sigma\\rightarrow \\Box(\\Box\\sigma\\rightarrow\\phi)$ puis $\\vdash\\Box\\sigma\\rightarrow (\\Box\\Box\\sigma\\rightarrow\\Box\\phi)$ (distribution de la boite gr√¢ce √† $\\text{(K)}$) On a donc $\\vdash \\Box\\sigma\\rightarrow\\Box\\Box\\sigma$ et $\\vdash \\Box\\sigma\\rightarrow\\Box\\phi$ $\\vdash \\Box\\sigma\\rightarrow\\phi$ (car $\\vdash \\Box\\sigma\\rightarrow\\Box\\phi\\rightarrow\\phi$ puisqu'on suppose $\\vdash \\Box\\phi\\rightarrow\\phi$) $\\vdash (\\Box\\sigma\\rightarrow\\phi)\\rightarrow\\sigma$ (sens r√©ciproque de la formule autor√©f√©rente) $\\vdash\\sigma$ par modus ponens de 4. et 5. $\\vdash\\Box\\sigma$ par n√©cessitation. $\\vdash \\phi$ par modus ponens de 4. et 7. Si un syst√®me formel est consistant alors il ne peut pas prouver sa propre consistance. Et donc les seuls syst√®mes qui peuvent prouver leur consistance sont ceux qui sont inconsistants\u0026hellip;\nC\u0026rsquo;est le second th√©or√®me d\u0026rsquo;incompl√©tude de G√∂del.\nCe th√©or√®me n\u0026rsquo;est autre que la contrapos√©e du th√©or√®me de L√∂b¬†!\nAvec $\\phi=\\bot$, on obtient¬†:\nsi $\\vdash\\neg\\Box\\bot$, alors $\\vdash\\neg\\Box(\\Box\\bot\\rightarrow\\bot)$ et comme $\\Box\\bot\\rightarrow\\bot\\equiv\\neg\\Box\\bot\\lor\\bot\\equiv\\neg\\Box\\bot$, on a finalement si $\\vdash\\neg\\Box\\bot$, alors $\\vdash\\neg\\Box(\\neg\\Box\\bot)$\nPar dualit√©, $\\Diamond P = \\neg\\Box\\neg P$. Donc $\\Diamond P$ exprime qu\u0026rsquo;on ne peut pas prouver que $P$ est faux. Autrement dit, $\\Diamond P$ affirme que $P$ est consistant (coh√©rent) dans le syst√®me formel.\n$\\neg\\Box\\bot\\rightarrow\\neg\\Box(\\neg\\Box\\bot)$ devient alors $\\Diamond\\top\\rightarrow \\neg\\Box(\\Diamond \\top)$ (\u0026ldquo;si une th√©orie est consistante, alors on ne peut pas prouver que la th√©orie est consistante\u0026rdquo;).\nLa logique de la prouvabilit√© de G√∂del-L√∂b ($\\text{\\bf GL}$) est form√©e √† partir du syst√®me $\\text{\\bf K}$ auquel on ajoute l\u0026rsquo;axiome de L√∂b $\\text{(L)}=\\Box(\\Box\\phi\\rightarrow\\phi)\\rightarrow\\Box\\phi$ (c\u0026rsquo;est la version formalis√©e du th√©or√®me de L√∂b).\n$\\vdash_\\text{\\bf GL}\\text{(4)}=\\Box\\phi\\rightarrow\\Box\\Box \\phi$\nEt donc $\\text{\\bf K4}‚â§\\text{\\bf KL}$ ($\\text{\\bf K4}$ se r√©duit √† $\\text{\\bf KL}$).\nObtention d\u0026rsquo;un premier th√©or√®me¬†: $\\vdash_\\text{\\bf K}\\Box P\\rightarrow \\Box ((\\Box P\\land \\Box\\Box P)\\rightarrow(P\\land \\Box P))$\nPuis d'un deuxi√®me\u0026nbsp;: $\\vdash_\\text{\\bf K}\\Box (P\\land\\Box P)\\rightarrow \\Box\\Box P$ On les imbrique √† l'aide de l'axiome de L√∂b $\\text{(L)}$\u0026nbsp;: La classe des syst√®mes de transition qui valide $\\text{\\bf GL}$ est donc transitive¬†!\nMontrons de plus que cette classe est irr√©flexive et asym√©trique.\nIl suffit de trouver un mod√®le qui valide $\\color{#1DB100}\\text{(L)}$ sans valider ni $\\color{#FF42A1}\\text{(B)}=\\Box P\\rightarrow \\Diamond\\Box P$, ni $\\color{#F27200}\\text{(T)}=\\Box P \\rightarrow P$.\nOr un graphe transitif et non r√©flexif, ni sym√©trique n\u0026rsquo;est autre qu\u0026rsquo;un arbre. On va aller un peu plus loin en montrant que ces arbres doivent √™tre en plus finis. Et c\u0026rsquo;est plut√¥t g√©nial que cette logique de la prouvabilit√© s\u0026rsquo;exprime naturellement sur des arbres finis qui repr√©sentent intuitivement la notion de preuve (la racine repr√©sentant la formule prouv√©e, et les feuilles les axiomes).\nLa logique $\\text{\\bf GL}$ est correcte et compl√®te par rapport √† la classe $\\mathscr{C}_{\\text{arbres finis}}$ des mod√®les de Kripke √† la fois transitifs, irr√©flexifs, asym√©triques et born√©s √† droite (sans suite infinie de mondes).\nCorrection¬†:\nSi $ \\vdash_\\text{\\bf GL} \\phi$, alors $\\models_{\\mathscr{C}_{\\text{arbres finis}}}\\phi$.\nComme $\\text{(L)}$ est le seule axiome ajout√© √† $\\text{\\bf K}$, il faut montrer que l\u0026rsquo;ensemble des syst√®mes de transition qui valide l\u0026rsquo;axiome de L√∂b est restreint aux seuls arbres finis (l\u0026rsquo;axiome $\\text{(K)}$ est valide dans $\\mathscr{C}$ et les r√®gles de la logique modale normale conserve la validation).\nSupposons que l\u0026rsquo;axiome ne soit pas satisfait en un n≈ìud $a$ d\u0026rsquo;un arbre fini $\\mathcal{T}=(N,A)$. On a donc $\\langle \\mathcal{T},a\\rangle \\not\\models \\Box(\\Box P\\rightarrow P)\\rightarrow \\Box P$.\nCela implique¬†:\n$\\langle \\mathcal{T},a\\rangle \\models \\Box(\\Box P\\rightarrow P)$, et $\\langle \\mathcal{T},a\\rangle \\not\\models \\Box P$. La deuxi√®me affirmation entra√Æne qu'il existe un n≈ìud $a'\\in N$ tel que $a\\longrightarrow a'$ et $\\langle \\mathcal{T},a\\rangle \\not\\models P$\nPar contre, la premi√®re affirmation nous assure que $\\langle \\mathcal{T},a'\\rangle \\models \\Box P\\rightarrow P$.\nPar cons√©quent $\\langle \\mathcal{T},a'\\rangle \\not\\models \\Box P$. Cela signifie qu'il existe un n≈ìud $a''\\in N$, successeur de $a'$ ($a'\\longrightarrow a''$) tel que $\\langle \\mathcal{T},a''\\rangle \\not\\models P$.\nPar transitivit√©, $a \\longrightarrow a''$. Or si on retourne √† l'affirmation $1.$, on a aussi $\\langle \\mathcal{T},a''\\rangle \\models \\Box P\\rightarrow P$, ce qui va conduire de fil en aiguille √† une s√©quence infinie de successeurs de $a$, contredisant ainsi que $\\mathcal{T}$ soit fini.\nOn en conclut que l\u0026rsquo;axiome de L√∂b et donc $\\text{\\bf KL}$ est valide dans tout arbre fini.\nCompl√©tude¬†:\nSi $\\models_{\\mathscr{C}_{\\text{arbres finis}}}\\phi$, alors $\\vdash_\\text{\\bf GL} \\phi$.\nComme entrevu plus haut, l\u0026rsquo;op√©rateur de modalit√© peut s\u0026rsquo;interpr√©ter comme la formule arithm√©tique de G√∂del $\\text{Bew}(x)$. Il s\u0026rsquo;agit plus pr√©cis√©ment d\u0026rsquo;une formule de l\u0026rsquo;Arithm√©tique de Peano $\\mathbf{PA}$. Il se trouve qu\u0026rsquo;on peut traduire toute formule de la logique modale dans $\\text{\\bf PA}$ et on montre alors que tout th√©or√®me de $\\text{\\bf GL}$ est prouvable dans $\\text{\\bf PA}$. C\u0026rsquo;est ce qu\u0026rsquo;on peut appeler la correction arithm√©tique de $\\text{\\bf GL}$. Et mieux, Solovay a prouv√© aussi sa compl√©tude arithm√©tique¬†; toute formule prouvable de l\u0026rsquo;arithm√©tique est un th√©or√®me de $\\text{\\bf GL}$.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/",
	"title": "Maths",
	"tags": [],
	"description": "",
	"content": " Un peu de maths G√©om√©trie Le triangle Cercle et triangle Aires et volumes Dimensions Poursuites Pythagore Thal√®s Droite d'Euler Trigonom√©trie Angle inscrit Alg√®bre √âquations Identit√©s remarquables Alg√®bre de Boole Alg√®bre lin√©aire Combinatoire et graphes Th√©orie des groupes Multiplication de polyn√¥mes et FFT Arithm√©tique Infinis d√©nombrables et ind√©nombrables Num√©ration Nombres alg√©briques Fractions continues Algorithme d'Euclide Triplets pythagoriciens Divisibilit√© et nombres premiers Probabilit√©s Loi binomiale Loi normale Densit√© de probabilit√©s Probabilit√©s conditionnelles et Bayes Cha√Ænes de Markov Curiosit√©s et √©nigmes Spaghetti et in√©galit√© triangulaire Anniversaires simultan√©s Paradoxe des deux enfants Paradoxe de Cover Statistiques Les diff√©rentes moyennes Monte Carlo Paradoxe de Simpson Test d'hypoth√®se Analyse en composante principale p-hacking √âcart-type exp√©rimental Inf√©rences bayesiennes Th√©orie des jeux Attaque/D√©fense "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/meca/action/",
	"title": "Moindre action",
	"tags": [],
	"description": "",
	"content": " Principe de moindre action Maupertuis formule ainsi le principe de moindre action en 1744¬†:\nLorsqu‚Äôil arrive quelque changement dans la nature, la quantit√© d‚Äôaction, n√©cessaire pour ce changement, est la plus petite qui soit possible.\nUne formulation plus moderne serait la minimisation du transfert d\u0026rsquo;√©nergie entre les r√©servoirs cin√©tique et potentiel au cours du temps d√©volu au trajet entre deux points.\nMaupertuis reprenait ainsi le flambeau de Fermat qui avait introduit d√®s 1657 le principe de moindre temps pour expliquer les trajets des rayons lumineux en optique g√©om√©trique.\nDe tels principes sont dits variationnels car il s\u0026rsquo;agit d\u0026rsquo;√©tudier et minimiser les variations entre deux points.\nLeur motivation originelle √©tait m√©taphysique¬†: par sa perfection, la nature se devait d\u0026rsquo;√™tre optimale. On trouva ensuite des justifications plus scientifiques\u0026hellip;\nConceptuellement, il s\u0026rsquo;agit d\u0026rsquo;une d√©marche descendante (top-down) consistant √† fixer un but global et √† en d√©duire ensuite les √©quations locales qui ont √©t√© n√©cessaires pour l\u0026rsquo;atteindre. C\u0026rsquo;est l\u0026rsquo;inverse de la d√©marche ascendante (bottom-up) plus famili√®re (car c\u0026rsquo;est celle enseign√©e au lyc√©e) qui part des √©quations locales pour aboutir de proche en proche au global (par un processus it√©ratif).\nEn m√©canique classique par exemple, le local, c\u0026rsquo;est Newton¬†: on construit la trajectoire compl√®te en int√©grant une √©quation diff√©rentielle depuis une position et une vitesse initiale. Et le global, c\u0026rsquo;est Lagrange ou Hamilton¬†: on fixe deux points et on cherche la bonne trajectoire parmi l\u0026rsquo;ensemble des trajectoires possibles (celle pour laquelle l\u0026rsquo;action est stationnaire).\nLes th√©ories m√©caniques sont progressivement pass√©es d\u0026rsquo;ascendantes √† descendantes. Pour la lumi√®re, c\u0026rsquo;est l\u0026rsquo;inverse.\nL\u0026rsquo;√©volution des id√©es en optique est passionnante en soi, mais si on s\u0026rsquo;y attarde ici, c\u0026rsquo;est surtout pour l\u0026rsquo;influence importante qu\u0026rsquo;elles ont eue sur certains architectes de la m√©canique classique puis quantique. Des √©l√©ments propres aux th√©ories optiques vont ainsi progressivement percoler en m√©canique.\nOptique Principe de moindre temps de Fermat Reprenons l\u0026rsquo;id√©e de Fermat pour d√©terminer le chemin d\u0026rsquo;un rayon lumineux r√©fract√© d\u0026rsquo;un milieu √† l\u0026rsquo;autre. L\u0026rsquo;id√©e cl√© de cette mani√®re de calculer par minimisation est de se donner un point de d√©part I et un point d\u0026rsquo;arriv√©e F et de chercher, parmi toutes les trajectoires possibles les rejoignant, la plus courte.\nOn constate d√©j√† que si I et F sont dans le m√™me milieu homog√®ne, la trajectoire la plus courte sera le segment de droite [IF] et donc le principe de Fermat pr√©dit bien que la lumi√®re se propage rectilignement dans un milieu homog√®ne.\nEt quand le point d\u0026rsquo;arriv√©e F est dans un autre milieu que le point de d√©part I, la prise en compte de l\u0026rsquo;effet du milieu comme un ralentissement apparent de la vitesse de la lumi√®re (dans un milieu d\u0026rsquo;indice optique $n$, la lumi√®re voyage √† la vitesse $c/n$ o√π $c$ est sa c√©l√©rit√© dans le vide, soit $300\\,000$ km/s) permet de retrouver le ph√©nom√®ne de r√©fraction¬†: la lumi√®re change de direction en passant d\u0026rsquo;un milieu √† l\u0026rsquo;autre. La d√©viation est telle que le trajet global sera le moins long possible et donc la partie du trajet dans le milieu d\u0026rsquo;indice optique sup√©rieur sera plus petite que l\u0026rsquo;autre.\nDans le programme ci-dessous, les temps de parcours de diff√©rents trajets pour rejoindre un point dans l\u0026rsquo;air √† un point dans l\u0026rsquo;eau sont compar√©s. Le trajet minimal est d√©termin√© et on montre qu\u0026rsquo;il respecte la loi de la r√©fraction de Snell-Descartes.\nC\u0026rsquo;est tr√®s √©l√©gant, mais n\u0026rsquo;est-ce pas que cela¬†? un raffinement calculatoire¬†? Comment la lumi√®re pourrait-elle aller explorer les diff√©rents chemins avant de choisir le moins long¬†? Cela para√Æt saugrenu\u0026hellip; Du moins du c√¥t√© particulaire de la description de la lumi√®re, car pour une onde, ce n\u0026rsquo;est pas si bizarre. Une onde est en effet intrins√®quement la mod√©lisation d\u0026rsquo;un comportement collectif. Consid√©rer une onde, c\u0026rsquo;est donc bien consid√©rer diff√©rents trajets pris \u0026ldquo;√† la fois\u0026rdquo;.\nOndelettes de Huygens Pour apprivoiser ce mouvement collectif, Huygens a l\u0026rsquo;id√©e en 1678 de d√©crire tout point atteint par la lumi√®re comme la source d\u0026rsquo;une nouvelle ondelette sph√©rique qui va se propager depuis ce point. Reconstituer le trajet de la lumi√®re revient alors √† regarder o√π les fronts d\u0026rsquo;onde de ces ondelettes s\u0026rsquo;accumulent pour former une enveloppe, le rayon per√ßant cette enveloppe perpendiculairement.\nHuygens parvient avec son principe de superposition √† retrouver l\u0026rsquo;ensemble des pr√©dictions de Fermat. L\u0026rsquo;optique g√©om√©trique tenait donc l√† deux explications alternatives¬†; l\u0026rsquo;une variationnelle et l\u0026rsquo;autre √† base de fronts d\u0026rsquo;onde.\nPour √™tre plus pr√©cis et faciliter le parall√®le avec Hamilton ensuite, explicitons les d√©finitions.\nSoit l\u0026rsquo;ensemble des points $q$ jusqu\u0026rsquo;o√π la lumi√®re issue d\u0026rsquo;un point $q_0$ peut traverser en un temps inf√©rieur ou √©gal √† $t$. La fronti√®re de cet ensemble, $\\Phi_{q_0}(t)$ est appel√© le front d\u0026rsquo;onde du point $q_0$ apr√®s un temps $t$ et regroupe les points que la lumi√®re peut atteindre en un temps $t$ mais pas avant.\nLe principe de Huygens peut alors s\u0026rsquo;exprimer ainsi¬†:\nsoit $\\Phi_{q_0}(t)$ le front d\u0026rsquo;onde issu du point $q_0$ apr√®s un temps $t$. Pour chaque point $q$ de ce front, on consid√®re le front d\u0026rsquo;onde au temps $s$, $\\Phi_{q}(s)$. Alors le front d\u0026rsquo;onde issu du point $q_0$ apr√®s le temps $t+s$, $\\Phi_{q_0}(t+s)$ sera l\u0026rsquo;enveloppe des fronts $\\Phi_{q}(s),q\\in \\Phi_{q_0}(t)$.\nEn fixant un point $q_0$, on appelle fonction caract√©ristique $S_{q_0}(q)$ la fonction qui associe √† tout point $q$ le longueur du chemin optique de $q_0$ √† $q$.\nLes courbes de niveau de la fonction caract√©ristique ne sont autre que les fronts d\u0026rsquo;onde.\nL\u0026rsquo;animation ci-dessous montre comment l\u0026rsquo;enveloppe des fronts d\u0026rsquo;onde est naturellement d√©vi√©e lors d\u0026rsquo;un changement de milieu si la vitesse de propagation change, expliquant ainsi la r√©fraction.\nM√™me si ses \u0026ldquo;ondelettes\u0026rdquo; pr√©figurent √©videmment de v√©ritables ondes, Huygens se contente de mod√©liser l\u0026rsquo;optique g√©om√©trique avec sa th√©orie et il s\u0026rsquo;imagine d\u0026rsquo;ailleurs les ondelettes s\u0026rsquo;√©vanouir partout ailleurs que sur l\u0026rsquo;enveloppe.\nVers l\u0026rsquo;optique ondulatoire En 1803, Young publie un dessin expliquant comment la lumi√®re passant √† travers deux fentes √©troites donne naissance √† des franges alternativement brillantes et sombres sur un √©cran.\nOn est encore tr√®s proche des ondelettes de Huygens sauf qu\u0026rsquo;on tourne notre attention sur l\u0026rsquo;ensemble du r√©seau plut√¥t qu\u0026rsquo;exclusivement sur les enveloppes.\nLa th√©orie de Huygens n√©cessite une superposition de suffisamment de sources pour clairement d√©finir ses fronts d\u0026rsquo;onde.\nDans l\u0026rsquo;image pr√©c√©dente, on a additionn√© des sources sur un petit segment vertical. L\u0026rsquo;effet de moir√© nous donne un aper√ßu du ph√©nom√®ne de diffraction.\nEn √©tirant le segment avec de nouvelles sources, les enveloppes finissent par mieux se d√©finir¬†; on retrouve une propagation rectiligne (du moins autour de l\u0026rsquo;axe de sym√©trie perpendiculaire aux sources).\nSi on peut comprendre g√©om√©triquement l\u0026rsquo;apparition de zone brillante par superposition des r√©seaux d\u0026rsquo;ondelettes, l\u0026rsquo;existence de zone parfaitement sombre est plus myst√©rieuse. Comment des zones √©clair√©es plusieurs fois pourraient-elles se trouver obscurcies¬†?\nFresnel et la phase L\u0026rsquo;ingr√©dient manquant pour expliquer les interf√©rences est apport√© en 1818 par Fresnel : il donne une phase aux ondelette¬†! Un traitement math√©matique complet devenait possible et Fresnel fut ainsi capable de mettre au point la th√©orie de la diffraction.\nFresnel s\u0026rsquo;imagine que la lumi√®re est une vibration transversale de l\u0026rsquo;√©ther analogue aux ondes m√©caniques d√©crites au si√®cle pr√©c√©dent par d\u0026rsquo;Alembert (1747).\nIl donne alors aux ondelettes de Huygens le pouvoir d\u0026rsquo;interf√©rer gr√¢ce √† la phase $\\phi=\\omega t-\\vec{k}\\cdot\\vec{r}$ (avec $\\|\\vec{k}\\|=k=\\frac{n \\omega}{c}$).\nArago √©crira¬†:\nQuel est donc le proc√©d√© magique qui permet de transformer √† volont√© la lumi√®re en ombre, le jour en nuit¬†? Ce proc√©d√© excitera plus de surprise encore que le fait en lui-m√™me¬†; ce proc√©d√© consiste √† diriger sur le papier, mais par une route l√©g√®rement diff√©rente, un second rayon lumineux qui, pris isol√©ment aussi, l\u0026rsquo;aurait fortement √©clair√©. Les deux rayons en se m√™lant semblaient devoir produire une illumination plus vive¬†; le doute √† cet √©gard ne semblait point permis¬†; eh bien¬†! ils se d√©truisent quelquefois tout √† fait et l\u0026rsquo;on se se trouve avoir cr√©√© les t√©n√®bres en ajoutant de la lumi√®re √† la lumi√®re.\nUn fait neuf exige un mot nouveau. Ce ph√©nom√®ne dans lequel des rayons, en se m√™lant, se d√©truisent tout √† fait ou seulement en partie, s\u0026rsquo;appelle une interf√©rence.\nLumi√®re ou obscurit√© se d√©duisent d√©sormais du terme d\u0026rsquo;interf√©rence entre les diff√©rentes sources secondaires en un endroit donn√©. Et gr√¢ce aux interf√©rences, le principe de superposition de Huygens peut maintenant expliquer la diffraction¬†! En sommant continument les multiples d√©phasages des ondelettes sur les fronts d\u0026rsquo;onde1, Fresnel est capable de pr√©dire pr√©cis√©ment les figures de diffraction obtenues dans plusieurs situations.\nAujourd\u0026rsquo;hui, le principe de superposition moderne mis au point par Fresnel porte le nom de ses deux papas¬†: principe de Huygens-Fresnel.\nFresnel est le grand artisan du basculement de paradigme d\u0026rsquo;une description corpusculaire de la lumi√®re (dont le g√©ant Newton √©tait le fer de lance) vers une description ondulatoire. Un fervent partisan des particules, Poisson, joua aussi un r√¥le amusant\u0026hellip;\nLorsqu‚Äôil √©plucha le m√©moire de concours de Fresnel, Poisson reprit ses int√©grales de diffraction et pr√©senta ses r√©sultats en 1819 devant la commission du ¬´ Grand Prix de la Diffraction ¬ª de l‚ÄôAcad√©mie des sciences. Les calculs pr√©disent un point lumineux au centre de l‚Äôombre d‚Äôun disque √©clair√© par une source ponctuelle. Totalement absurde pour un partisan des particules¬†! Poisson est convaincu qu\u0026rsquo;il tient l√† l\u0026rsquo;argument d√©finitif contre la th√©orie ondulatoire. Arago qui pr√©sidait la commission voulut en avoir le c≈ìur net et monta l\u0026rsquo;exp√©rience. Il observa alors bien ce qui sera d√©sormais appel√©e t√¢che de Fresnel (Arago spot en anglais)2¬†! L\u0026rsquo;argument d√©finitif de Poisson fit finalement triompher les ondes3.\nHelmholtz En 1859, Helmholtz r√©sume quasiment toute l\u0026rsquo;optique en une unique √©quation d\u0026rsquo;onde scalaire (d\u0026rsquo;abord pens√©e pour l\u0026rsquo;acoustique avant d\u0026rsquo;√™tre adapt√©e √† l\u0026rsquo;optique).\n$$\\nabla^2 \\Psi(\\vec{r})+k^2 n^2(\\vec{r}) \\Psi(\\vec{r})=0$$\nL\u0026rsquo;animation ci-dessous est calcul√©e √† partir de cette seule √©quation. Une s√©rie verticale de sources sur la gauche, un obstacle sph√©rique o√π l\u0026rsquo;onde vaut z√©ro et on laisse la magie (ou la physique, c\u0026rsquo;est selon) op√©rer.\nVotre navigateur ne prend pas en charge la balise video. De l\u0026rsquo;optique ondulatoire √† l\u0026rsquo;optique g√©om√©trique L\u0026rsquo;optique g√©om√©trique n\u0026rsquo;est maintenant plus que la limite aux courtes longueurs d\u0026rsquo;onde de l\u0026rsquo;optique ondulatoire.\nEn injectant dans l\u0026rsquo;√©quation de Helmholtz une solution de la forme $\\Psi(\\vec{r})=A(\\vec{r}) e^{i k_0 S(\\vec{r})}$ (o√π la phase r√©duite $S$ et l\u0026rsquo;amplitude $A$ changent peu sur des distances de l\u0026rsquo;ordre de $\\lambda$) et en ne gardant que le plus grand ordre en $k_0$ (ordre 2), on obtient l\u0026rsquo;√©quation √©ikonale¬†: $|\\vec{\\nabla} S|^2=n^2(\\mathbf{r})$.\nLa phase totale √©tant $k_0S(\\vec{r})$, on d√©finit le vecteur d\u0026rsquo;onde $\\vec{k}(\\vec{r})=\\vec{\\nabla}\\phi=k_0\\vec{\\nabla}S$. En prenant son carr√© $|\\vec{k}|^2$, on obtient $k_0^2|\\vec{\\nabla} S|^2=k_0^2 n^2(\\vec{r})$. Et donc $|\\vec{k}|=\\frac{2 \\pi}{\\lambda_0} n(\\vec{r})=\\frac{2 \\pi}{\\lambda(\\vec{r})}$. C\u0026rsquo;est bien la norme attendue pour un vecteur d\u0026rsquo;onde dans un milieu d\u0026rsquo;indice $n$ (o√π la longueur d\u0026rsquo;onde locale vaut $\\lambda=\\lambda_0/n$).\nLes surfaces de phase constante correspondent √† $S(\\vec{r})=\\text{cste}$. Et comme le gradient d‚Äôune fonction scalaire est orthogonal aux surfaces de niveau, $\\vec{n} =\\vec{\\nabla} S$ est la normale au front d‚Äôonde.\nPuisqu\u0026rsquo;on a d√©fini $\\vec{k}$ comme proportionnel √† $\\vec{\\nabla} S$, le vecteur d\u0026rsquo;onde est colin√©aire √† la normale. Sa direction est celle du rayon en optique g√©om√©trique.\nEn param√©trant le trajet par l\u0026rsquo;abscisse curviligne $s$, on obtient l\u0026rsquo;√©quation d\u0026rsquo;un rayon lumineux¬†: $\\frac{d \\vec{r}}{d s}=\\frac{\\vec{k}}{|\\vec{k}|}=\\frac{\\vec{\\nabla} S}{n}$.\nFaraday et Maxwell D√®s 1845, Faraday a l\u0026rsquo;intuition que la lumi√®re est une onde √©lectromagn√©tique. Mais c\u0026rsquo;est 20 ans plus tard que Maxwell cl√¥t d√©finitivement le chapitre de l\u0026rsquo;optique classique dans un bouquet final.\nAvec lui, la lumi√®re est promue onde vectorielle et elle int√®gre officiellement le spectre du rayonnement du champ √©lectromagn√©tique (dont elle ne constitue qu\u0026rsquo;une toute petite partie), et sa vitesse est maintenant d√©termin√©e th√©oriquement ($c=\\frac{1}{\\sqrt{\\epsilon_0\\mu_0}}$).\nD\u0026rsquo;un point de vue th√©orique, la compr√©hension scientifique de la lumi√®re est donc partie d\u0026rsquo;un principe variationnel global pour terminer sur des √©quations locales (celles d\u0026rsquo;Helmholtz ou de Maxwell).\nLa m√©canique a fait le chemin inverse, on a d\u0026rsquo;abord eu les √©quations de Newton (locales) avant de se tourner vers les principes variationnels.\nM√©canique Bernoulli et la brachystochrone En 1696, Jean Bernoulli utilise l\u0026rsquo;optique pour r√©soudre un probl√®me de m√©canique qui chiffonnait les physiciens de l\u0026rsquo;√©poque, le probl√®me de la brachystochrone.\nQuelle courbe d√©val√©e par un point mat√©riel pesant sans frottement ni vitesse initiale permet de rejoindre le plus vite le point de d√©part √† un point d\u0026rsquo;arriv√©e fix√© (en aval par rapport au champ de pesanteur suppos√© uniforme)¬†?\nBernoulli a en t√™te le principe de Fermat¬†: le trajet le plus court entre les deux points est celui que suivrait la lumi√®re. Et pour la trajectoire courb√©e, il fait l\u0026rsquo;analogie avec les mirages¬†: il d√©coupe verticalement l\u0026rsquo;espace en strates d\u0026rsquo;indices diff√©rents faisant varier la vitesse. La conservation de l\u0026rsquo;√©nergie lui donne une vitesse $v=\\sqrt{2gy}$ √† la strate d\u0026rsquo;altitude $y$ et la loi de r√©fraction selon le principe de Fermat lui assure que $\\frac{\\sin\\theta}{v}=\\text{cste}$ et donc $\\frac{\\theta}{\\sqrt{y}}=\\text{cste}$ o√π $\\theta$ est l\u0026rsquo;angle par rapport √† la verticale. Il r√©ussit ainsi √† d√©montrer que la courbe optimale est une cyclo√Øde dont la tangente initiale est verticale au d√©part et qui est engendr√©e par un cercle dont le diam√®tre est la descente maximale du point.\nLa raison pour laquelle $\\frac{\\theta}{\\sqrt{y}}=\\text{cste}$ correspond √† une cyclo√Øde est donn√©e dans cette vid√©o de 3Blue1Brown.\nEt le petit programme ci-dessous permet de tester manuellement diff√©rentes courbes et v√©rifier que le temps minimal est obtenu quand on s\u0026rsquo;approche de la cyclo√Øde (courbe jaune).\nLa formulation de ce probl√®me est typiquement variationnelle et cela en fait un tremplin (cyclo√Ødique) vers les th√©ories √† venir (surtout que toutes les stars du moment s\u0026rsquo;y confrontent et proposent une solution¬†: le fr√®re de Jean, Jacques Bernoulli, L\u0026rsquo;H√¥pital, Leibniz, Newton, etc.).\nEn jetant un pont entre l\u0026rsquo;optique et la m√©canique pour r√©soudre la brachistochrone, Bernoulli pr√©figure les futurs travaux de Hamilton.\nMaupertuis En 1744, Maupertuis porte dans le domaine de m√©canique l\u0026rsquo;id√©e de Fermat d\u0026rsquo;un principe variationnel permettant de d√©terminer le trajet des rayons lumineux. Il s\u0026rsquo;agit maintenant d\u0026rsquo;obtenir la trajectoire d\u0026rsquo;un syst√®me entre deux points en extr√©misant une certaine grandeur¬†: l\u0026rsquo;action.\nPour Maupertuis, l\u0026rsquo;action c\u0026rsquo;est $S_M=\\int_{A\\rightarrow B}mv\\mathrm{d}s$ o√π $s$ est l\u0026rsquo;abscisse curviligne sur la trajectoire. Anachroniquement, on peut transformer $S_M$ en $\\int_{A\\rightarrow B}2T\\mathrm{d}t$ o√π $T$ est l\u0026rsquo;√©nergie cin√©tique et minimiser l\u0026rsquo;action de Maupertuis revient alors √† minimiser l\u0026rsquo;√©nergie cin√©tique au cours du temps √† √©nergie totale fix√©e.\nL\u0026rsquo;action trouve d√©j√† l√† sa dimension d√©finitive d\u0026rsquo;une √©nergie multipli√©e par une dur√©e, ou une quantit√© de mouvement multipli√©e par une distance.\nLagrange Un √©l√®ve de Jean Bernoulli, Euler (le üêê des math√©matiques), met au point le calcul des variations √† partir de consid√©rations g√©om√©triques et g√©n√©ralise ainsi les m√©thodes mises au point pour r√©soudre le probl√®me de la brachistochrone.\nLagrange donne sa forme actuelle √† ce nouveau type de calcul avec une approche purement analytique et il introduit les coordonn√©es g√©n√©ralis√©es $q_i$ (qui en plus des positions habituelles peuvent √™tre des positions relatives, des angles, etc.) et la variable temps que n\u0026rsquo;utilisait pas Maupertuis.\nLa d√©finition lagrangienne de l\u0026rsquo;action g√©n√©ralise celle de Maupertuis¬†: $S_L=\\int_{t_1}^{t_2} L(q, \\dot{q}, t) \\mathrm{d} t$ o√π $L$, le lagrangien, vaut $T-V$, la diff√©rence entre l\u0026rsquo;√©nergie cin√©tique et l\u0026rsquo;√©nergie potentielle.\nLe principe de stationnarit√© $\\delta S_L= 0$ pour des extr√©mit√©s fix√©es en temps conduit aux √©quations d\u0026rsquo;Euler-Lagrange¬†:\n$$\\frac{\\mathrm{d}}{\\mathrm{d} t}\\frac{\\partial L}{\\partial \\dot{q}_i} - \\frac{\\partial L}{\\partial q_i} = 0$$\nDor√©navant, le lagrangien sera au c≈ìur de la physique moderne¬†: toute th√©orie commence par se chercher un lagrangien puisque tout d√©coule de lui. Un des plus grands ach√®vements humains est ainsi l\u0026rsquo;√©tablissement du lagrangien du mod√®le standard qui r√©unit toute la physique hormis la gravit√©.\nLe lagrangien de l\u0026rsquo;optique g√©om√©trique Le principe de moindre temps et le principe de moindre action sont deux principes variationnels. Dans l\u0026rsquo;optique g√©om√©trique de Fermat, l\u0026rsquo;action correspond √† la dur√©e du trajet entre le point de d√©part et le point d\u0026rsquo;arriv√©e. Mais qui joue le r√¥le du temps chez Fermat¬†? R√©pondre revient √† d√©terminer le \u0026ldquo;lagrangien\u0026rdquo; de l\u0026rsquo;optique g√©om√©trique.\nSupposons que le rayon se d√©place dans un plan $(x,z)$ o√π $z$ serait l\u0026rsquo;axe optique ou l\u0026rsquo;axe de propagation. Le temps de parcours vaut alors $T=\\frac1c \\int_1^2 n(x,z)\\mathrm{d}\\ell=\\frac1c \\int_1^2 n(x,z)\\sqrt{\\mathrm{d}x^2+\\mathrm{d}z^2}=\\frac1c \\int_1^2 n(x,z)\\sqrt{1+\\frac{\\mathrm{d}x^2}{\\mathrm{d}z^2}}\\,\\mathrm{d}z=\\int_1^2\\mathcal{L}(x,\\tilde{x},z)\\,\\mathrm{d}z$ o√π on on note $\\tilde{x}\\equiv\\frac{\\mathrm{d}x}{\\mathrm{d}z}$.\nLe r√¥le du temps de la m√©canique est donc jou√© par la coordonn√©e $z$ selon l\u0026rsquo;axe de propagation, $x$ garde son r√¥le et la vitesse $\\dot{x}$ devient l\u0026rsquo;inclinaison du rayon par rapport √† la direction de propagation $\\mathrm{d}x/\\mathrm{d}z$. Notons que le lagrangien de l\u0026rsquo;optique a la dimension de l\u0026rsquo;inverse d\u0026rsquo;une vitesse plut√¥t que d\u0026rsquo;une √©nergie.\nL\u0026rsquo;√©quation d\u0026rsquo;Euler-Lagrange s\u0026rsquo;√©crit $\\frac{\\mathrm{d}}{\\mathrm{d}z}\\left(\\frac{\\partial\\mathcal{L}}{\\partial \\tilde{x}}\\right)=\\frac{\\partial\\mathcal{L}}{\\partial x}$ et on tire du lagrangien $\\frac{\\partial\\mathcal{L}}{\\partial \\tilde{x}}=\\frac{n(x,z)\\tilde{x}}{c\\sqrt{1+\\tilde{x}^2}}$.\nSupposons que l\u0026rsquo;indice ne d√©pende pas de $x$¬†: $n(x,z)=n(z)$. On a alors $\\frac{\\partial\\mathcal{L}}{\\partial x}=0$ et donc $\\frac{\\partial\\mathcal{L}}{\\partial \\tilde{x}}$ est une constante en $z$ conserv√©e le long de l\u0026rsquo;axe de propagation.\nOn constate sur le sch√©ma que $\\tilde{x}=\\tan\\theta$ et donc $\\frac{\\tilde{x}}{\\sqrt{1+\\tilde{x}^2}}=\\sin\\theta$. La constance de $\\frac{\\partial\\mathcal{L}}{\\partial \\tilde{x}}$ selon $z$ implique donc celle de $n(z)\\sin\\theta$. On a retrouv√© la loi de Snell-Descartes de la r√©fraction¬†!\nHamilton Hamilton a pav√© la voie pour la transition de la m√©canique classique √† la m√©canique quantique.\nOn passe de la formulation de Lagrange √† celle d\u0026rsquo;Hamilton en d√©finissant les moments $p_i=\\partial L / \\partial \\dot{q}_i$ et en utilisant la transform√©e de Legendre $H(q, p, t)=\\sum_i p_i \\dot{q}_i-L$. Cela permet de remplacer la variable $\\dot{q}$ par le moment conjugu√© $p$ (ou impulsion).\nL\u0026rsquo;espace des configurations (l\u0026rsquo;espace des coordonn√©es ${q}$) o√π prend racine la m√©canique lagrangienne est ainsi d√©m√©nag√©e vers l\u0026rsquo;espace des phases $(q,p)$ o√π l\u0026rsquo;impulsion $p$ est consid√©r√©e comme une coordonn√©e libre au m√™me titre que $q$ (ce qui ne pouvait pas √™tre le cas de $\\dot{q}=\\frac{\\mathrm{d}q}{\\mathrm{d}t}$)4. L\u0026rsquo;action devient pour Hamilton $S_H=\\int_{t_1}^{t_2}\\left(\\sum_i p_i \\dot{q}_i-H\\right) \\mathrm{d} t$.\nLa stationnarit√© $\\delta S_H = 0$, pour des extr√©mit√©s fixes en $(q,t)$ fournit les √©quations canoniques¬†:\n$\\dot{q}_i=\\frac{\\partial H}{\\partial p_i}, \\quad \\dot{p}_i=-\\frac{\\partial H}{\\partial q_i}$.\nL\u0026rsquo;espace des phases est d√©j√† \u0026ldquo;quantum-ready\u0026rdquo;¬†: les transformations canoniques entre $q$ et $p$ seront les m√™mes, les crochets de Poisson (oui le m√™me que la t√¢che) pr√©figurent les commutateurs, les sym√©tries d\u0026rsquo;√©volution √† la Noether s\u0026rsquo;expriment naturellement, etc.\nHamilton-Jacobi Le principe de moindre action √† la Lagrange consistant √† chercher un chemin dans l\u0026rsquo;espace des configurations qui rende stationnaire l\u0026rsquo;action est tout √† fait analogue au principe de Fermat.\nHuygens ayant r√©ussi √† reproduire les r√©sultats de Fermat avec ses ondelettes, Hamilton se demande si on ne pourrait pas trouver une formulation √©quivalente en m√©canique. Une m√©canique √† base d\u0026rsquo;ondelettes et de fronts d\u0026rsquo;onde¬†!\nHamilton y parvient en envisageant l\u0026rsquo;action sous un nouvel aspect. Plut√¥t que de fixer les points de d√©part ($q_0,t_0$) et d\u0026rsquo;arriv√©e ($q_F,t_F$), il laisse ce dernier libre ($q,t$). Il obtient alors¬†:\n$\\displaystyle\\frac{\\partial S}{\\partial q}=p$ $\\displaystyle\\frac{\\partial S}{\\partial t}=-H$ (√©quation de Hamilton-Jacobi) ‚ñ∫ d√©monstration : Imaginons qu'on fasse varier la position finale en gardant $t$ fix√©\u0026nbsp;:\n$q(t) \\rightarrow q(t)+\\delta q$ et $\\delta t =0$ On obtient $\\delta S =\\delta \\int_{t_0}^t L d t=\\int_{t_0}^t \\delta L d t$.\nAvec $L=p \\dot{q}-H(q,p)$, $\\delta L=\\delta p \\dot{q}+p \\delta \\dot{q}-\\frac{\\partial H}{\\partial q} \\delta q -\\frac{\\partial H}{\\partial p} \\delta p$.\nRegroupons les termes en $\\delta p$¬†: $\\delta L=\\left(\\cancel{\\dot{q}-\\frac{\\partial H}{\\partial p}}\\right) \\delta p + p \\delta \\dot{q} - \\frac{\\partial H}{\\partial q} \\delta q= p \\delta \\dot{q} + \\dot{p} \\delta q$ car $\\dot{q}=\\frac{\\partial H}{\\partial p}$ et $\\dot{p}=-\\frac{\\partial H}{\\partial q}$ d\u0026rsquo;apr√®s les √©quations canoniques.\nEt comme $p \\delta \\dot{q}=\\frac{d}{d t}\\left(p \\delta q\\right)-\\dot{p} \\delta q$, $\\delta L = \\frac{d}{d t}\\left(p_i \\delta q_i\\right)$. D\u0026rsquo;o√π $\\int_{t_0}^t \\delta L d t=\\left[p_i \\delta q_i\\right]_{t_0}^t$.\nEt comme on ne touche pas aux conditions initiales ($\\delta q(t_0)=0$)¬†:$\\int_{t_0}^t \\delta L d t=p(t) \\delta q(t)$.\nOr, on peut aussi √©crire que $\\delta S = \\frac{\\partial S}{\\partial q} \\delta q$.\nPar identification, $\\frac{\\partial S}{\\partial q}=p(t)$.\nPour d√©terminer la variation de $S$ en fonction du temps d\u0026rsquo;arriv√©e, laissons la trajectoire √©voluer sur son chemin optimal. On a alors $\\mathrm{d}S=\\mathcal{L}\\mathrm{d}t$.\nD\u0026rsquo;autre part, $\\mathrm{d}S=\\mathcal{L}\\mathrm{d}t=\\frac{\\partial S}{\\partial q}\\mathrm{d}q+\\frac{\\partial S}{\\partial t}\\mathrm{d}t$.\nOr on a vu que $\\frac{\\partial S}{\\partial q}=p$, d\u0026rsquo;o√π $\\mathcal{L}\\mathrm{d}t=\\left(p \\dot{q}+\\frac{\\partial S}{\\partial t}\\right) \\mathrm{d} t$.\nOn en d√©duit donc que $\\frac{\\partial S}{\\partial t}=\\mathcal{L}-p \\dot{q}=-H$.\nDans le cas d\u0026rsquo;un syst√®me conservatif, on d√©duit de l\u0026rsquo;√©quation de Hamilton-Jacobi5¬†: $$S(q, t)=S_{0}(q)-E t$$ Hamilton a alors l\u0026rsquo;intuition g√©niale d\u0026rsquo;y voir une phase $\\phi(\\vec{r},t) = \\vec{k} \\cdot\\vec{r}-\\omega t$. Mais sa qu√™te n\u0026rsquo;est pas celle d\u0026rsquo;une m√©canique purement ondulatoire (il faudra attendre Schr√∂dinger pour √ßa), il souhaite la \u0026ldquo;forme Huygens\u0026rdquo; de l\u0026rsquo;√©quivalence entre la m√©canique et l\u0026rsquo;optique g√©om√©trique.\nPar analogie avec la limite de l\u0026rsquo;optique ondulatoire aux petites longueurs d\u0026rsquo;onde et le principe de Huygens, on peut identifier $S_{0}(q)$ avec la fonction caract√©ristique donnant la longueur du chemin optique de $q_0$ √† $q$.\nComme $\\partial S_0(q) / \\partial q=p$, le vecteur normal √† la surface $S_0(q)=\\text{cst}$, $\\vec{\\nabla} S_0(q)$ n\u0026rsquo;est autre que le vecteur impulsion $\\vec{p}$.\nLa trajectoire m√©canique suit les rayons (donn√©s par l\u0026rsquo;impulsion) normaux aux fronts d\u0026rsquo;onde constitu√©s par les surfaces d\u0026rsquo;action r√©duite constante¬†!\nSchr√∂dinger r√©sume les travaux d\u0026rsquo;Hamilton ainsi¬†:\nLe principe variationnel de Hamilton se trouve correspondre au principe de Fermat pour la propagation d‚Äôune onde dans l‚Äôespace des configurations (espace des q), et l‚Äô√©quation de Hamilton‚ÄìJacobi exprime le principe de Huygens pour cette propagation. Malheureusement, cette conception puissante et d‚Äôune importance majeure de Hamilton est, dans la plupart des relectures modernes, d√©pouill√©e de son magnifique habit ‚Äî jug√© superflu ‚Äî au profit d‚Äôune repr√©sentation plus terne de la correspondance analytique.\nL\u0026rsquo;admiration que voue Schr√∂dinger √† Hamilton a √©t√© r√©compens√©e puisque les travaux d\u0026rsquo;Hamilton se sont r√©v√©l√©s essentiels dans la qu√™te de Schr√∂dinger d\u0026rsquo;une √©quation d\u0026rsquo;onde quantique.\nSi Hamilton jette les base de la r√©volution quantique, les lubies g√©om√©triques de Jacobi pr√©figurent une autre r√©volution du 20e si√®cle, celle de la relativit√© g√©n√©rale.\nIl transforme en effet la recherche d\u0026rsquo;une trajectoire minimisant l\u0026rsquo;action en la recherche d\u0026rsquo;une g√©od√©sique dans l\u0026rsquo;espace des configurations¬†! Pour un syst√®me √† √©nergie fix√©e, il r√©√©crit ainsi l\u0026rsquo;action r√©duite comme $W=\\int_{q_1}^{q_2}p^i\\mathrm{d}q_i$ en $\\int_{q_1}^{q_2}ds_J$ en introduisant la m√©trique de Jacobi¬†: $\\mathrm{d}s_J^2 = 2(E-V(q)) g_{i j}(q) \\mathrm{d} q^i \\mathrm{d} q^j$ (o√π $g_{ij}$ est une m√©trique riemannienne). Le principe de Maupertuis devient alors un th√©or√®me g√©om√©trique.\nL\u0026rsquo;av√®nement de la m√©canique quantique Planck √Ä la fin du 19e si√®cle, r√©ussir √† mod√©liser l\u0026rsquo;√©nergie lib√©r√©e par un corps noir (une cavit√© dont les seules √©missions de rayonnement sont thermiques) en fonction de la fr√©quence est une √©nigme qui r√©siste au physicien.\nLe nombre de modes de vibration augment avec la fr√©quence et √† l\u0026rsquo;√©quilibre thermique, chaque mode doit recevoir la m√™me √©nergie, proportionnelle √† la temp√©rature. Moralit√©, la densit√© d\u0026rsquo;√©nergie aux grandes fr√©quences explose, c\u0026rsquo;est la catastrophe ultraviolette¬†! Bien s√ªr, la th√©orie est en violente contradiction avec l\u0026rsquo;exp√©rience qui pr√©sente z√©ro catastrophe.\nPour tenter de r√©soudre le probl√®me, Planck utilise ce qu\u0026rsquo;il pense √™tre un exp√©diant provisoire¬†: il introduit une discr√©tisation, un pas $h$ dont il compte bien se d√©barrasser ensuite en le faisant tendre vers z√©ro.\nEt √ßa marche¬†! Il obtient la courbe attendue du corps noir. Mais √† son grand d√©sarroi, il n\u0026rsquo;arrive pas √† se d√©barrasser de $h$. En le faisant tendre vers z√©ro, il perd la courbe. Et l\u0026rsquo;accord avec les r√©sultats exp√©rimentaux lui fixent m√™me une valeur pour son petit pas¬†: $\\pu{6,63E-34 J*s}$. Car oui, ce pas de discr√©tisation (qui deviendra la constante de Planck $h$) a la dimension d\u0026rsquo;une action, c\u0026rsquo;est un quantum d\u0026rsquo;action¬†!\nPour la premi√®re fois, une mod√©lisation de la nature s\u0026rsquo;av√®re fondamentalement discontinue et c\u0026rsquo;est l\u0026rsquo;action qui refuse de se faire d√©couper infiniment.\nde Broglie Dans son √©lan fabuleux de 1905, Einstein explique l\u0026rsquo;effet photo√©lectrique en discr√©tisant la lumi√®re en photons d\u0026rsquo;√©nergie $E=h\\nu$, g√©n√©ralisant l\u0026rsquo;id√©e de Planck au rayonnement.\nDe Broglie comprend alors que la formule $E=h\\nu$ n\u0026rsquo;est pas r√©serv√©e aux photons puisque le rayonnement √©change de l\u0026rsquo;√©nergie avec la mati√®re.\nIl √©crira¬†:\nApr√®s une longue m√©ditation et r√©flexion solitaire, j‚Äôai subitement eu l‚Äôid√©e durant l‚Äôann√©e 1923, que la d√©couverte faite par Einstein en 1905 doit √™tre g√©n√©ralis√©e par extension √† toute particule de mati√®re et singuli√®rement l‚Äô√©lectron.\nAvec ce quantum d\u0026rsquo;√©nergie proportionnel √† la fr√©quence, on associe de fait une caract√©ristique ondulatoire √† la mati√®re. De Broglie pousse cette dualit√© onde-corpuscule un cran plus loin en lui donnant aussi une longueur d\u0026rsquo;onde qu\u0026rsquo;il relie √† la quantit√© de mouvement¬†: $\\lambda=\\frac{h}{p}$.\nOn a maintenant un lien direct entre le $\\int n\\,\\mathrm{d}q$ de Fermat et le $\\int p\\,\\mathrm{d}q$ de Maupertuis (l\u0026rsquo;action r√©duite d\u0026rsquo;Hamilton)¬†: en appelant $v$ la vitesse de l\u0026rsquo;onde dans le milieu, $n=\\frac{c}{v}=\\frac{c}{\\lambda\\nu}=\\frac{c}{\\frac{h}{p}\\frac{E}{h}}=\\frac{pc}{E}$. Et donc $\\int n\\,\\mathrm{d}q=\\frac{c}{E}\\int p\\,\\mathrm{d}q$.\nDe Broglie parvient avec sa relation √† donner du sens aux orbites quantifi√©es de l\u0026rsquo;atome de Bohr¬†: l\u0026rsquo;√©lectron en orbite √©volue sur une trajectoire ferm√©e et maintenant qu\u0026rsquo;on peut lui associer une longueur d\u0026rsquo;onde, un syst√®me d\u0026rsquo;ondes stationnaires se met en place.\nEn supposant que la longueur d\u0026rsquo;onde reste la m√™me sur l\u0026rsquo;orbite, on doit avoir¬†: $n=\\oint\\frac{\\mathrm{d}\\ell}{\\lambda}=\\oint\\frac{p}{h}\\mathrm{d}\\ell$ (o√π $n$ est un entier, plus rien √† voir avec l\u0026rsquo;indice optique). Et en supposant une orbite circulaire de rayon $r$, on obtient un moment cin√©tique $L$ valant¬†: $L=mvr=\\frac{nh}{2\\pi}=n\\hbar$, ce qui est exactement la relation ad hoc introduite par Bohr.\nIl ne restait plus qu\u0026rsquo;√† faire le m√™me pas pour la m√©canique qu\u0026rsquo;entre Huygens et Fresnel pour l\u0026rsquo;optique. Ou comme le dit de Broglie¬†:\nLa nouvelle dynamique du point mat√©riel (incluant le photon d‚ÄôEinstein) est √† l‚Äôancienne (dynamique classique) ce que l‚Äôoptique ondulatoire est √† l‚Äôoptique g√©om√©trique.\nSchr√∂dinger C\u0026rsquo;est Schr√∂dinger qui franchira le cap et il va suivre pour cela les pas d\u0026rsquo;Hamilton.\nL\u0026rsquo;√©quation de Hamilton-Jacobi peut s\u0026rsquo;√©crire $\\frac{\\partial S}{\\partial t}+H(q, \\nabla S)=0$ en notant $\\nabla S = \\frac{\\partial S}{\\partial q}$.\nPour une particule libre, $H=p^2 / 2 m$ et on obtient¬†: $\\frac{\\partial S}{\\partial t}+\\frac{1}{2 m}|\\nabla S|^2=0$ (car $\\frac{\\partial S}{\\partial q}$ vaut aussi l\u0026rsquo;impulsion $p$).\nL\u0026rsquo;id√©e-cl√© de Schr√∂dinger est de remplacer dans l‚Äô√©quation de Helmholtz $\\nabla^2 \\Psi+k_0^2 n^2 \\Psi=0$ l‚Äôindice optique $n$ par une fonction li√©e √† l‚Äô√©nergie potentielle $V(q)$ pour trouver l‚Äô√©quation qui r√©git la ¬´ phase ¬ª d‚Äôune onde m√©canique. Pour cela, il va s\u0026rsquo;aider du travail d\u0026rsquo;Hamilton sur l\u0026rsquo;√©ikonale.\nRappelons-nous qu\u0026rsquo;en optique g√©om√©trique, on obtient l\u0026rsquo;√©quation √©ikonale $|\\nabla S|^2=n^2(\\mathbf{r})$ en approximant aux petites longueurs d\u0026rsquo;onde l\u0026rsquo;√©quation de Helmholtz pour une solution scalaire $\\Psi \\propto e^{i k_0 S}$. En faisant la m√™me chose avec une onde m√©canique de phase $S_0(q)$, Schr√∂dinger va trouver l\u0026rsquo;√©quivalent de l\u0026rsquo;indice optique.\n√âquation stationnaire¬†:\nCherchons une fonction d\u0026rsquo;onde $\\Psi(q)$ de la forme $\\Psi(q)=A(q) \\mathrm{e}^{i S_0(q) / \\hbar}$ o√π $S_0$ est l\u0026rsquo;action r√©duite li√©e √† l\u0026rsquo;√©nergie $E$ du syst√®me par $H(q, \\nabla S_0)=E $.\nEn substituant dans Helmoltz, on impose une relation du type $\\nabla^2 \\Psi+K(q) \\Psi=0$ et on identifie $K(q)$ pour que la limite $\\hbar\\rightarrow 0$ redonne le bon $|\\nabla S|^2$.\nPour une particule soumise √† un potentiel $V(q)$, on obtient l\u0026rsquo;√©quation stationnaire¬†:\n$$\\color{#D41876}-\\frac{\\hbar^2}{2 m} \\nabla^2 \\Psi(q)+V(q) \\Psi(q)=E \\Psi(q)$$\n‚ñ∫ d√©monstration : Calcul de $\\nabla^2\\Psi$¬†:\n$\\nabla \\Psi=\\mathrm{e}^{\\frac{\\mathrm{i}}{\\hbar} S_0}\\left(\\nabla A+\\frac{\\mathrm{i}}{\\hbar} A \\nabla S_0\\right)$\n$\\nabla^2 \\Psi=\\nabla \\cdot\\left[\\mathrm{e}^{\\frac{\\mathrm{i}}{\\hbar} S_0}\\left(\\nabla A+\\frac{\\mathrm{i}}{\\hbar} A \\nabla S_0\\right)\\right]=\\mathrm{e}^{\\frac{\\mathrm{i}}{\\hbar} S_0}\\left[\\nabla^2 A+\\frac{2 \\mathrm{i}}{\\hbar} \\nabla A \\cdot \\nabla S_0+\\frac{\\mathrm{i}}{\\hbar} A \\nabla^2 S_0-\\frac{1}{\\hbar^2} A|\\nabla S_0|^2\\right]$ En ne conservant que le terme d\u0026rsquo;ordre dominant en $\\hbar^{-1}$, on obtient $\\nabla^2 \\Psi=-\\mathrm{e}^{\\frac{\\mathrm{i}}{\\hbar} S_0}A \\frac{|\\nabla S_0|^2}{\\hbar^2}$.\nEn substituant dans $\\nabla^2 \\Psi+K(q) \\Psi=0$ et apr√®s factorisation par $\\mathrm{e}^{\\frac{\\mathrm{i}}{\\hbar} S_0}$, on a¬†:\n$-A \\frac{|\\nabla S_0|^2}{\\hbar^2}+K(q) A=0 \\quad \\Longrightarrow \\quad K(q)=\\frac{|\\nabla S_0(q)|^2}{\\hbar^2}$ Or pour une particule soumise √† un potentiel $V(q)$, on a classiquement $H(q,p)=\\frac{1}{2 m}|\\nabla S_0(q)|^2+V(q)=E$, d\u0026rsquo;o√π $|\\nabla W(q)|^2=2 m(E-V(q))$.\nPour que dans la limite des petites longueurs d\u0026rsquo;onde, on retrouve $|\\nabla W(q)|^2=2 m(E-V(q))$, il faut donc poser $K(q)=\\frac{2 m}{\\hbar^2}(E-V(q))$.\nCette √©quation est l‚Äôanalogue directe de l‚Äô√©quation de Helmholtz pour la lumi√®re, o√π $E-V$ joue le r√¥le de $k^2$.\n√âquation d√©pendant du temps¬†:\nSchr√∂dinger remarque que pour d√©crire les √©tats non stationnaires, il suffit de remplacer dans l‚Äô√©quation stationnaire $E$ par $\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t}$. En effet, pour une onde monochromatique classique de pulsation $\\omega$, l‚Äôamplitude √©volue dans le temps comme $\\mathrm{e}^{\\mathrm{i}\\omega t}$. Et avec $E=h\\nu=\\hbar\\omega$, on peut √©crire $\\Psi(q,t)=\\Psi(q)\\mathrm{e}^{\\frac{\\mathrm{i}}{\\hbar}E t}$.\nEn d√©rivant temporellement cette solution, on obtient¬†: $\\frac{\\partial}{\\partial t} \\Psi(q, t)=-\\frac{i E}{\\hbar} \\Psi(q, t) \\Longrightarrow E \\Psi=i \\hbar \\frac{\\partial \\Psi}{\\partial t}$.\nPar identification, on voit que l\u0026rsquo;√©nergie $E$ agit sur la fonction d\u0026rsquo;onde comme un op√©rateur d√©riv√©e.\nOn obtient alors l‚Äô√©quation de Schr√∂dinger d√©pendant du temps¬†:\n$$\\color{#D41876}\\mathrm{i} \\hbar \\frac{\\partial}{\\partial t} \\Psi(q, t)=\\left(-\\frac{\\hbar^2}{2 m} \\nabla^2+V(q)\\right) \\Psi(q, t)$$\nVotre navigateur ne prend pas en charge la balise video. Sur la vid√©o pr√©c√©dente, j\u0026rsquo;ai envoy√© un paquet d\u0026rsquo;onde gaussien √† la rencontre d\u0026rsquo;une marche de potentiel (un petit potentiel uniforme positif r√®gne √† droite alors qu\u0026rsquo;il est nul √† gauche).\nVotre navigateur ne prend pas en charge la balise video. Et dans cette vid√©o, on constate que le blob r√©fract√© s\u0026rsquo;√©loigne de la normale au dioptre comme le ferait un rayon dans un milieu d\u0026rsquo;indice optique inf√©rieur. On pourrait s\u0026rsquo;inqui√©ter que le blob aille moins vite dans ce milieu (alors qu\u0026rsquo;en optique, un indice plus faible correspond √† une vitesse sup√©rieure). Mais non, tout va bien puisque l\u0026rsquo;\u0026ldquo;indice optique\u0026rdquo; de la m√©canique est donn√© par $\\sqrt{2m(E-V)}$ (qui n\u0026rsquo;est autre que $p$) or $E-V$ est plus faible √† droite vu que le potentiel y est plus grand.\nFeynman La contribution de Feynman √† la physique est √† la confluence de cette longue √©volution de l\u0026rsquo;optique et de la m√©canique. Le prix Nobel vient en effet r√©compenser son travail sur l\u0026rsquo;√©lectrodynamique quantique, th√©orie d√©crivant l\u0026rsquo;interaction entre la mati√®re et le rayonnement, o√π il a en particulier remis l\u0026rsquo;action au centre du jeu.\nFeynman peut pourtant para√Ætre √† contre-courant du mouvement qui semble s\u0026rsquo;√™tre op√©r√© dans l\u0026rsquo;histoire des sciences des particules vers les ondes. En effet, pour lui, tout est particule, m√™me la lumi√®re. Il √©crit ainsi dans QED: The Strange Theory of Light and Matter (un introduction tout public √† l\u0026rsquo;√©lectrodynamique quantique √† partir duquel j\u0026rsquo;ai fait un texte il y a tr√®s longtemps)¬†:\nI want to emphasize to you that light comes in this form ‚Äì particles. It is very important to know light behaves like particles, especially for those of you who have gone to school where you were probably told something about light behaving like waves. I‚Äôm going to tell you the way it does behave like particles.\nN√©anmoins, ses particules ont des propri√©t√©s essentiellement ondulatoires. Selon Feynman, lorsqu\u0026rsquo;une particule emprunte un chemin pour aller d\u0026rsquo;un point I √† un point F, elle emporte avec elle une phase qui oscille tout le long du chemin. Que vaut la phase finale pour un chemin entier¬†? $\\frac{S}{\\hbar}$ o√π l\u0026rsquo;action $S$ est calcul√©e sur le chemin consid√©r√©¬†!\nOn peut retrouver ce $\\frac{S}{\\hbar}$ √† partir de la formule de la phase de l\u0026rsquo;optique ondulatoire $kq-\\omega t$ et des relations d\u0026rsquo;Einstein $E=\\hbar\\omega$ et de de Broglie $p=\\hbar k$.\nD√©coupons le chemin en tron√ßons infinit√©simaux¬†: sur un de ces intervalles, la phase $\\phi$ varie de $\\mathrm{d}\\phi=k \\mathrm{d}q - \\omega\\mathrm{d}t$. En utilisant les √©quivalences avec l\u0026rsquo;√©nergie et l\u0026rsquo;impulsion, la petite variation de phase devient $\\mathrm{d}\\phi=\\frac{1}{\\hbar}(p\\mathrm{d}q-E\\mathrm{d}t)$.\nOn force ensuite $\\mathrm{d}t$ en facteur¬†: $\\mathrm{d}\\phi=\\frac{1}{\\hbar} ( p \\frac{ \\mathrm{d} q }{ \\mathrm{d} t } - E ) \\mathrm{d}t=\\frac{1}{\\hbar}(p\\dot{q}-E)\\mathrm{d}t$. Plus qu\u0026rsquo;√† substituer $E$ par l\u0026rsquo;hamiltonien $H$ pour retrouver la formule du lagrangien¬†: $\\mathrm{d}\\phi=\\frac{1}{\\hbar}\\mathcal{L}\\mathrm{d}t$.\nEt pour obtenir la variation de phase sur l\u0026rsquo;enti√®ret√© du chemin, on somme les petits d√©phasages de chaque tron√ßon infinit√©simal, ce qui revient √† int√©grer le lagrangien le long du chemin¬†: $\\Delta\\phi=\\frac{1}{\\hbar}\\int_\\text{chemin}\\mathcal{L}\\mathrm{d}t$. C\u0026rsquo;est bien l\u0026rsquo;action qu\u0026rsquo;on retrouve ici et la variation de phase sur le chemin s\u0026rsquo;√©crit finalement $\\Delta\\phi=\\frac{1}{\\hbar}S_\\text{chemin}$.\nMais pour Feynman, la particule n\u0026rsquo;emprunte pas qu\u0026rsquo;un seul chemin pour aller de I √† F, elle emprunte tous les chemins possibles¬†! Chaque chemin contribue √† la probabilit√© finale de retrouver la particule au point F apr√®s √™tre partie du point I.\nPour obtenir la probabilit√© totale, on se retrouve √† additionner entre eux les d√©phasages obtenus sur chacun des chemins (cela revient √† additionner des termes $\\mathrm{e}^{\\mathrm{i}\\frac{S_\\text{chemin}}{\\hbar}}$). Cela ressemble pas mal au principe de superposition de Huygens-Fresnel\u0026hellip;\nFeynman aime d√©crire cette op√©ration comme une somme de petites fl√®ches dans un plan (l\u0026rsquo;angle de la fl√®che repr√©sente le d√©phasage). La grande majorit√© des chemins fournissent des d√©phasages variant largement les uns par rapport aux autres (la division par $\\hbar$ fait gigoter √ßa tr√®s tr√®s vite) et sommer des fl√®ches dont l\u0026rsquo;angle varie quasi al√©atoirement ne m√™ne pas tr√®s loin\u0026hellip; Seuls les chemins o√π la phase varie tr√®s peu de l\u0026rsquo;un √† l\u0026rsquo;autre vont contribuer efficacement, constructivement, √† la somme finale. Or les chemins o√π la phase varie peu sont les chemins o√π l\u0026rsquo;action varie peu. On se retrouve √† chercher les trajectoires o√π l\u0026rsquo;action est stationnaire red√©couvrant le principe de moindre action via le principe de superposition¬†!\nFeynman marie ainsi d\u0026rsquo;une fa√ßon nouvelle l\u0026rsquo;optique ondulatoire √† la m√©canique en passant directement par la phase et le principe de superposition de Huygens-Fresnel plut√¥t que par l\u0026rsquo;√©quation d\u0026rsquo;onde d\u0026rsquo;Helmholtz et met dans le m√™me mouvement l\u0026rsquo;action au centre de son formalisme.\nBouclons la boucle en reprenant un exemple donn√© par Feynman dans QED.\nOn a une source de photons monochromatiques (leur √©nergie $E$ est donc fixe) et un d√©tecteur. Un √©cran cache la source au d√©tecteur mais un miroir est plac√© en dessous.\nLa probabilit√© que le d√©tecteur capte un photon doit √™tre calcul√©e en utilisant tous les chemins possibles. Pour simplifier, Feynman d√©coupe le miroir en 13 et ne s\u0026rsquo;autorise que les trajets rectilignes.\nVotre navigateur ne prend pas en charge la balise video. Dans cette situation l\u0026rsquo;action obtenue sur chaque chemin d√©pend seulement de la longueur du trajet (en effet, √† √©nergie constante l\u0026rsquo;action se r√©duit $\\int p\\mathrm{d}q$ (action r√©duite) et pour un photon, $p=\\frac{E}{c}$. Donc $E\\frac{L}{c}$ ou encore $E\\tau$, l\u0026rsquo;action et donc le d√©phasage de chaque chemin est proportionnel au temps de parcours.\nEn sommant sur les chemins, on constate bien que les contributions les plus importantes (car de phases quasi constantes) sont celles proche de la trajectoire la plus courte (on a retrouv√© Fermat¬†!).\nSources :\nMesli, A. ¬´¬†Gen√®se et d√©veloppement du principe de moindre action. Premi√®re partie : de Fermat (1655) √† Lagrange (1756)¬†¬ª, Photoniques n¬∞ 80, p. 34‚Äì40 (janvier 2016). Mesli, A. ¬´¬†Gen√®se et d√©veloppement du principe de moindre action. Deuxi√®me partie : de Hamilton (1834) √† Feynman (1942)¬†¬ª, Photoniques n¬∞ 81, p. 37‚Äì45 (avril 2016). üåê Vaquero Vallina, M. ¬´ On the Geometry of the Hamilton‚ÄìJacobi Equation ¬ª, th√®se de doctorat, Universidad Aut√≥noma de Madrid ‚Äì Instituto de Ciencias Matem√°ticas (ICMAT), soutenue le 27 novembre 2015. üåê Houchmandzadeh, B. ‚ÄúThe Hamilton‚ÄìJacobi equation: An alternative approach‚Äù, American Journal of Physics 88 (5), 353‚Äì359 (Mai 2020) üåê Wheeler, N. ‚ÄúSchr√∂dinger‚Äôs train of thought‚Äù, Reed College Physics Department essay, avril 2006. üåê les portraits sont plus ou moins hallucin√©s par IA. le calcul infinit√©simal et int√©gral fut d√©velopp√© parall√®lement par Newton et Leibniz √† la fin du 17e si√®cle.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAmusant ce partage des attributions suivant les pays entre th√©oricien et exp√©rimentateur.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nCe n\u0026rsquo;est pas sans rappeler comment Millikan confirma √† son corps d√©fendant l\u0026rsquo;id√©e du photon d\u0026rsquo;Einstein pour expliquer l\u0026rsquo;effet photo√©lectrique.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nLa dynamique lagrangienne prend place dans le fibr√© tangent √† l\u0026rsquo;espace des configurations $(q,\\dot{q})$, celle d\u0026rsquo;Hamilton est tr√®s bien dans l\u0026rsquo;espace des phases $(q,p)$ qui est donc l\u0026rsquo;unique cadre g√©om√©trique (techniquement il s\u0026rsquo;agit du fibr√© cotangent de l\u0026rsquo;espace des configurations).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nAvec $S_0$, on retrouve au final l\u0026rsquo;action de Maupertuis qui est renomm√©e action r√©duite $W=\\int_{q_A}^q p \\,dq$. En effet, $S_M=\\int_{A\\rightarrow B}mv\\,\\mathrm{d}s=\\int_{A\\rightarrow B}p\\,\\mathrm{d}q=\\int_{A\\rightarrow B}(p\\dot{q} - H +H)\\,\\mathrm{d}t=S_H+E(t_B-t_A)$ ($E$ est fixe ici).\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique3/",
	"title": "Propositions - Preuve",
	"tags": [],
	"description": "",
	"content": " info\nNotes de lecture du livre La logique pas √† pas de Jacques Duparc que je paraphrase all√©grement.\nCalcul des propositions Syntaxe S√©mantique Preuve Th√©orie de la d√©monstration Une preuve ou une d√©monstration\u0026nbsp;:\nest de longueur finie, est v√©rifiable m√©caniquement, s\u0026rsquo;appuie sur des hypoth√®ses, aboutit √† une conclusion. Une d√©monstration permet de mettre au jour une v√©rit√© syntaxique (s\u0026rsquo;appuyant sur un jeu de r√®gles) plut√¥t que s√©mantique (faisant intervenir les mod√®les d\u0026rsquo;une th√©orie).\nOn a vu que $\\phi$ est une cons√©quence s√©mantique de la th√©orie $\\mathcal{T}$, not√© $\\mathcal{T}\\models \\phi$, si $\\phi$ est vraie dans tous les mod√®les de $\\mathcal{T}$.\nOn va maintenant se pencher sur la cons√©quence syntaxique de la th√©orie $\\mathcal{T}$. $\\phi$ est une cons√©quence syntaxique de $\\mathcal{T}$, not√© $\\mathcal{T} \\vdash \\phi$, s\u0026rsquo;il existe une d√©monstration de $\\phi$ sur la base d\u0026rsquo;hypoth√®ses contenues dans la th√©orie $\\mathcal{T}$. Fini les mod√®les, on ne s\u0026rsquo;int√©resse plus qu\u0026rsquo;√† la syntaxe par un jeu de r√©√©criture.\nQuel est le lien entre les cons√©quences s√©mantiques et syntaxiques¬†?\nSi on veut que nos d√©monstrations aboutissent √† des r√©sultats ayant du sens, il faut qu\u0026rsquo;ils correspondent aux cons√©quences s√©mantiques. Et √† l\u0026rsquo;inverse, on souhaite qu\u0026rsquo;une cons√©quence s√©mantique puisse se d√©montrer m√©caniquement. Le syst√®me de r√®gles de d√©monstrations va √™tre consciencieusement mis au point afin d\u0026rsquo;aboutir √† l\u0026rsquo;√©quivalence entre les deux cons√©quences¬†: $\\mathcal{T}\\models \\phi$ si et seulement si $\\mathcal{T} \\vdash \\phi$. Et en particulier, on veut qu\u0026rsquo;une formule soit une tautologie si et seulement si elle est prouvable sans hypoth√®se¬†: $\\models \\phi$ ssi $\\vdash \\phi$.\nOn va distinguer trois familles de syst√®mes de d√©monstration diff√©rents pour le calcul des propositions\u0026nbsp;:\nles syst√®mes axiomatiques, la d√©duction naturelle, le calcul des s√©quents. Les syst√®mes axiomatiques Les syst√®mes axiomatiques ou syst√®mes \u0026ldquo;√† la Hilbert\u0026rdquo; reposent sur un petit nombre de v√©rit√©s √©l√©mentaires, les axiomes, et sur des r√®gles de construction qui pr√©servent la v√©rit√©.\nLes d√©monstrations obtenues sont concises mais tr√®s peu explicatives. √áa marche, mais on ne sait pas pourquoi, faute au manque de structure de ces preuves.\nExemple d\u0026rsquo;un syst√®me axiomatique reposant sur le syst√®me complet de connecteur $\\set{\\neg,\\rightarrow}$ et comportant 3 axiomes pour une seule r√®gle¬†:\nAxiomes $(1)\\quad\\phi\\rightarrow (\\psi\\rightarrow\\phi)$$(2)\\quad(\\phi\\rightarrow (\\psi\\rightarrow\\theta))\\rightarrow ((\\phi\\rightarrow\\psi)\\rightarrow(\\phi\\rightarrow\\theta))$$(3)\\quad(\\neg\\psi\\rightarrow \\neg\\phi) \\rightarrow ((\\neg \\psi\\rightarrow\\phi)\\rightarrow \\psi)$ R√®gle modus ponens\u0026nbsp;: de $\\phi$ et $\\phi\\rightarrow \\psi$ on d√©duit $\\psi$ La d√©duction de $\\phi$ √† partir d'un ensemble de formules $\\Gamma$ est une suite finie de formules $\\langle \\phi_1,\\phi_2,\\ldots,\\phi_n\\rangle$ telle que\u0026nbsp;:\nchaque $\\phi_i$ v√©rifie une des trois conditions suivantes¬†: $\\phi_i$ est un axiome, $\\phi_i$ est une hypoth√®se ($\\phi_i \\in \\Gamma$), $\\phi_i$ est obtenue √† partir de l\u0026rsquo;application de la r√®gle du modus ponens √† deux formules $\\phi_j$ et $\\phi_k$ telles que $j,k‚â§i$. $\\phi_n=\\phi$ Exemple de la d√©monstration de la formule $(\\phi\\rightarrow\\phi)$ sans hypoth√®se\u0026nbsp;:\n$ \\begin{array}{llr} (1)\u0026amp; (\\phi\\rightarrow ((\\phi\\rightarrow \\phi)\\rightarrow\\phi))\\rightarrow ((\\phi\\rightarrow (\\phi\\rightarrow \\phi))\\rightarrow(\\phi\\rightarrow\\phi)) \u0026amp; \\text{(axiome 2 avec } \\psi = (\\phi\\rightarrow \\phi) \\text{ et }\\theta = \\phi )\\\\ (2)\u0026amp; \\phi\\rightarrow ((\\phi\\rightarrow \\phi)\\rightarrow\\phi) \u0026amp; \\text{(axiome 1 avec } \\psi = (\\phi\\rightarrow \\phi) )\\\\ (3)\u0026amp; (\\phi\\rightarrow (\\phi\\rightarrow \\phi))\\rightarrow(\\phi\\rightarrow\\phi) \u0026amp; \\text{(\\it{modus ponens} \\text{ (1) - (2))}}\\\\ (4)\u0026amp; \\phi\\rightarrow (\\phi\\rightarrow \\phi) \u0026amp; \\text{(axiome 1 avec } \\psi = \\phi )\\\\ (5)\u0026amp; \\phi\\rightarrow\\phi \u0026amp; \\text{(\\it{modus ponens} \\text{ (3) - (4))}} \\end{array} $\nOn ne peut pas dire que la d√©monstration nous √©claire beaucoup sur la v√©rit√© de $\\phi\\rightarrow \\phi$\u0026hellip;\nEn conclusion, les syst√®mes √† la Hilbert sont utiles pour d√©montrer des trucs mais pas pour √©tudier les d√©monstrations elles-m√™mes.\nLa d√©duction naturelle Un s√©quent, not√© $\\Gamma\\vdash\\phi$, est un couple o√π\u0026nbsp;:\n$\\Gamma$ est un ensemble fini de formules, $\\phi$ est une formule. Remarques\u0026nbsp;:\n$\\Gamma$ repr√©sente les hypoth√®ses que l\u0026rsquo;on veut utiliser. $\\phi$ est la conclusion du s√©quent, la formule √† d√©montrer. $\\vdash$ se lit \u0026ldquo;d√©montre\u0026rdquo; ou \u0026ldquo;prouve\u0026rdquo;. On √©crit $\\vdash\\phi$ si $\\phi$ se d√©montre sans hypoth√®se ($\\Gamma=\\empty$). Un s√©quent est prouvable s'il peut √™tre obtenu par une application finie de r√®gles de d√©monstration. Une formule $\\phi$ est prouvable si le s√©quent $\\vdash\\phi$ est prouvable. On √©crit $\\Gamma\\nvdash\\phi$ si $\\Gamma\\vdash\\phi$ n\u0026rsquo;est pas prouvable.\nLa r√®gle est la brique de base de la d√©monstration. Une d√©monstration est ainsi un assemblage de r√®gles g√©n√©ralement repr√©sent√© sous forme d\u0026rsquo;arbre.\nChaque r√®gle est compos√©e\u0026nbsp;:\nd\u0026rsquo;un ensemble de pr√©misses (de 0 √† 3), chacune √©tant un s√©quent. d\u0026rsquo;un s√©quent conclusion. d\u0026rsquo;une barre horizontale qui s√©pare les deux (on identifie la r√®gle √† droite de la barre). Pour chaque connecteur logique, on a deux types de r√®gles\u0026nbsp;:\nles r√®gles d\u0026rsquo;introduction, et les r√®gles d\u0026rsquo;√©limination. On va dans la suite consid√©rer le syst√®me complet de connecteur $\\set{\\neg,\\land,\\lor,\\rightarrow}$.\nEn dehors des axiomes repr√©sent√©s par le s√©quent $\\phi \\vdash\\phi$, les r√®gles peuvent √™tre regroup√©es en deux cat√©gories\u0026nbsp;:\nles r√®gles logiques (r√®gles d\u0026rsquo;introduction et d\u0026rsquo;√©limination des diff√©rents connecteurs). les r√®gles structurelles permettant de manipuler les hypoth√®ses¬†; l\u0026rsquo;affaiblissement permet d\u0026rsquo;ajouter de nouvelles hypoth√®ses et la contraction permet de fusionner deux occurrences de la m√™me hypoth√®se. Logique minimale R√®gles de la logique minimale¬†:\nAxiome\n$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\end{prooftree} $ Un s√©quent dont la conclusion est aussi l'hypoth√®se est prouvable. Introduction de la conjonction\n$ \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi$} \\AxiomC{$\\Gamma'\\vdash\\psi$} \\RightLabel{$\\;\\scriptsize \\land i$} \\BinaryInfC{$\\Gamma,\\Gamma' \\vdash\\phi\\land\\psi$} \\end{prooftree} $ Si $\\phi$ et $\\psi$ sont prouv√©es, alors on prouve $\\phi\\land\\psi$. √âlimination de la conjonction\n$ \\begin{array}{cc} \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi\\land\\psi$} \\RightLabel{$\\;\\scriptsize \\land e_g$} \\UnaryInfC{$\\Gamma \\vdash\\phi $} \\end{prooftree} \u0026 \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi\\land\\psi$} \\RightLabel{$\\;\\scriptsize \\land e_d$} \\UnaryInfC{$\\Gamma \\vdash\\psi $} \\end{prooftree} \\end{array} $ De $\\phi\\land\\psi$, on peut d√©duire $\\phi$ et $\\psi$. Introduction de la disjonction\n$ \\begin{array}{cc} \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi$} \\RightLabel{$\\;\\scriptsize \\lor i_g$} \\UnaryInfC{$\\Gamma \\vdash \\phi\\lor\\psi $} \\end{prooftree} \u0026 \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\psi$} \\RightLabel{$\\;\\scriptsize \\lor i_d$} \\UnaryInfC{$\\Gamma \\vdash \\phi\\lor\\phi $} \\end{prooftree} \\end{array} $ Si on a prouv√© une formule, alors a fortiori, on a prouv√© cette formule ou une autre. √âlimination de la disjonction\n$ \\begin{prooftree} \\AxiomC{$\\Gamma \\vdash \\phi \\lor \\psi$} \\AxiomC{$\\Gamma',\\phi \\vdash \\theta$} \\AxiomC{$\\Gamma'',\\psi \\vdash \\theta$} \\RightLabel{$\\;\\scriptsize \\lor e$} \\TrinaryInfC{$\\Gamma,\\Gamma',\\Gamma''\\vdash \\theta$} \\end{prooftree} $ Si on a prouv√© $\\phi\\lor\\psi$, alors pour prouver $\\theta$, il suffit de prouver $\\theta$ en supposant $\\phi$ ou prouver $\\theta$ en supposant $\\psi$. Introduction de l'implication\n$ \\begin{prooftree} \\AxiomC{$\\Gamma,\\phi\\vdash\\psi$} \\RightLabel{$\\;\\scriptsize \\rightarrow i$} \\UnaryInfC{$\\Gamma \\vdash\\phi\\rightarrow\\psi$} \\end{prooftree} $ Pour prouver $\\phi\\rightarrow \\psi$, il suffit de prouver $\\psi$ avec $\\phi$ ajout√©e aux hypoth√®ses. √âlimination de l'implication (modus ponens)\n$ \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi\\rightarrow\\psi$} \\AxiomC{$\\Gamma'\\vdash\\phi$} \\RightLabel{$\\;\\scriptsize \\rightarrow e$} \\BinaryInfC{$\\Gamma,\\Gamma'\\vdash\\psi$} \\end{prooftree} $ Si on a prouv√© $\\phi$ et $\\phi\\rightarrow\\psi$, alors on a prouv√© $\\psi$. Introduction de la n√©gation\n$ \\begin{prooftree} \\AxiomC{$\\Gamma,\\phi \\vdash\\bot$} \\RightLabel{$\\;\\scriptsize \\neg i$} \\UnaryInfC{$\\Gamma\\vdash\\neg\\phi$} \\end{prooftree} $ Pour montrer $\\neg\\phi$, il suffit d'aboutir √† une contradiction en supposant $\\phi$. √âlimination de la n√©gation\n$ \\begin{prooftree} \\AxiomC{$\\Gamma \\vdash \\neg\\phi$} \\AxiomC{$\\Gamma' \\vdash \\phi$} \\RightLabel{$\\;\\scriptsize \\neg e$} \\BinaryInfC{$\\Gamma,\\Gamma'\\vdash \\bot$} \\end{prooftree} $ Si on a prouv√© √† la fois $\\phi$ et $\\neg\\phi$, alors on a prouv√© une contradiction. Affaiblissement\n$ \\begin{prooftree} \\AxiomC{$\\Gamma \\vdash \\phi$} \\RightLabel{$\\;\\scriptsize aff$} \\UnaryInfC{$\\Gamma,\\psi\\vdash \\phi$} \\end{prooftree} $ Si on peut prouver $\\phi$ avec les hypoth√®ses $\\Gamma$, alors on peut prouver $\\phi$ en ajoutant d'autres hypoth√®ses √† $\\Gamma$ (il peut y avoir des hypoth√®ses qui ne servent pas). Contraction\n$ \\begin{prooftree} \\AxiomC{$\\Gamma,\\psi,\\psi \\vdash \\phi$} \\RightLabel{$\\;\\scriptsize ctr$} \\UnaryInfC{$\\Gamma,\\psi\\vdash \\phi$} \\end{prooftree} $ L'ensemble d'hypoth√®ses $\\Gamma\\cup\\set{\\psi,\\psi}$ est le m√™me que $\\Gamma\\cup\\set{\\psi}$. Chaque r√®gle peut √™tre regard√©e comme un arbre simple lu de haut en bas et dont la racine est en bas (pour une fois). Le s√©quent conclusion est donc la racine et les pr√©misses sont les feuilles.\nL\u0026rsquo;axiome n\u0026rsquo;est qu\u0026rsquo;une racine. Les √©liminations de la conjonction, les introductions de la disjonctions, l\u0026rsquo;introduction de l\u0026rsquo;implication, l\u0026rsquo;introduction de la n√©gation, l\u0026rsquo;affaiblissement et la contraction sont des arbres √† une racine et une feuille. l\u0026rsquo;introduction de la conjonction, l\u0026rsquo;√©limination de l\u0026rsquo;implication et l\u0026rsquo;√©limination de la n√©gation sont des arbres √† une racine et deux feuilles. l\u0026rsquo;√©limination de la disjonction et un arbre √† une racine et trois feuilles. Pour construire une preuve, on va assembler ces arbres.\nUne d√©duction (en logique minimale) dans le syst√®me de la d√©duction naturelle est un arbre fini dont les n≈ìuds sont des s√©quents $\\left(S_i\\right)_{i‚â§k}$, et tel que pour chaque n≈ìud $S_i$ de la d√©duction¬†:\n$S_i$ est une feuille si et seulement si $S_i$ est un axiome, si $S_i$ n'est pas une feuille, alors l'arbre dont $S_i$ est la racine et les enfants de $S_i$ sont les feuilles est une instance de l'une des r√®gles du tableau. Une formule $\\phi$ est d√©ductible des hypoth√®ses $\\Gamma$ s\u0026rsquo;il existe une d√©duction dont la racine soit le s√©quent $\\Delta\\vdash\\phi$ pour un ensemble d\u0026rsquo;hypoth√®ses $\\Delta \\subseteq \\Gamma$. On note alors $\\Gamma\\vdash_m \\phi$.\nExemples de preuves¬†:\nProuver que $(\\phi\\rightarrow (\\psi\\rightarrow\\theta))\\rightarrow ((\\phi\\rightarrow\\psi)\\rightarrow(\\phi\\rightarrow\\theta))$ est bien un th√©or√®me du calcul des propositions revient √† obtenir une d√©duction du s√©quent $\\vdash_{\\! m} \\;\\phi\\rightarrow (\\psi\\rightarrow\\theta))\\rightarrow ((\\phi\\rightarrow\\psi)\\rightarrow(\\phi\\rightarrow\\theta)$¬†:\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\phi\\rightarrow(\\phi\\rightarrow\\theta)\\vdash\\phi(\\psi\\rightarrow\\theta)$} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\RightLabel{$\\;\\scriptsize \\rightarrow e$} \\BinaryInfC{$\\phi\\rightarrow(\\psi\\rightarrow\\theta),\\phi\\vdash\\psi\\rightarrow\\theta$} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\phi\\rightarrow\\psi\\vdash\\phi\\rightarrow\\psi$} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\RightLabel{$\\;\\scriptsize \\rightarrow e$} \\BinaryInfC{$\\phi\\rightarrow\\psi,\\phi\\vdash\\psi$} \\RightLabel{$\\;\\scriptsize \\rightarrow e$} \\BinaryInfC{$\\phi\\rightarrow(\\psi\\rightarrow\\theta),\\phi\\rightarrow\\psi,\\phi,\\phi\\vdash\\theta$} \\RightLabel{$\\;\\scriptsize ctr$} \\UnaryInfC{$\\phi\\rightarrow(\\psi\\rightarrow\\theta),\\phi\\rightarrow\\psi,\\phi\\vdash\\theta$} \\RightLabel{$\\;\\scriptsize \\rightarrow i$} \\UnaryInfC{$\\phi\\rightarrow(\\psi\\rightarrow\\theta),\\phi\\rightarrow\\psi\\vdash(\\phi\\rightarrow\\theta)$} \\RightLabel{$\\;\\scriptsize \\rightarrow i$} \\UnaryInfC{$\\phi\\rightarrow(\\psi\\rightarrow\\theta)\\vdash(\\phi\\rightarrow\\psi)\\rightarrow(\\phi\\rightarrow\\theta)$} \\RightLabel{$\\;\\scriptsize \\rightarrow i$} \\UnaryInfC{$\\vdash (\\phi\\rightarrow(\\psi\\rightarrow\\theta))\\rightarrow((\\phi\\rightarrow\\psi)\\rightarrow(\\phi\\rightarrow\\theta))$} \\end{prooftree} \\phantom{--------} $$ Preuve du s√©quent $\\vdash_{\\!m}\\; \\phi\\rightarrow\\neg\\neg\\phi$¬†:\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\neg\\phi\\vdash\\neg\\phi$} \\AxiomC{} \\RightLabel{$\\;\\scriptsize ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\RightLabel{$\\;\\scriptsize \\neg e$} \\BinaryInfC{$\\neg\\phi,\\phi\\vdash\\bot$} \\RightLabel{$\\;\\scriptsize \\neg i$} \\UnaryInfC{$\\phi\\vdash\\neg\\neg\\phi$} \\RightLabel{$\\;\\scriptsize \\rightarrow i$} \\UnaryInfC{$\\vdash \\phi\\rightarrow\\neg\\neg\\phi$} \\end{prooftree} $$ Par contre, on ne peut pas prouver l\u0026rsquo;implication inverse avec les r√®gles de la logique minimale ($\\nvdash_{\\!m} \\neg\\neg\\phi\\rightarrow\\phi$). On ne peut donc pas √©liminer les doubles n√©gations (on verra plus loin qu\u0026rsquo;il manque la r√®gle de l\u0026rsquo;absurdit√© classique pour y parvenir).\nMontrons que $\\vdash_{\\!m} \\,\\neg\\phi\\leftrightarrow (\\phi\\rightarrow\\bot)$¬†:\nRq¬†: pour raccourcir les preuves, on va int√©grer dor√©navant les affaiblissements et contractions aux autres r√®gles.\nLa notion de preuve telle qu\u0026rsquo;on l\u0026rsquo;a d√©fini ne s\u0026rsquo;appuie que sur un encha√Ænement de r√®gles pour aboutir √† la conclusion. Il s\u0026rsquo;agit donc d\u0026rsquo;une cons√©quence syntaxique des formules en hypoth√®se.\nDe la m√™me mani√®re qu\u0026rsquo;on a d√©fini l\u0026rsquo;√©quivalence s√©mantique √† partir de la notion de cons√©quence s√©mantique, on va d√©finir une √©quivalence au sens syntaxique √† partir des s√©quents.\nDeux formules $\\phi$ et $\\psi$ sont √©quivalentes pour la logique minimale si on a √† la fois $\\phi\\vdash_{\\!m}\\psi$ et $\\psi\\vdash_{\\!m}\\phi$, et on √©crit $\\phi\\equiv_m\\psi$\nLogique intuitionniste Certains raisonnements ne sont pas possibles avec la logique minimale. On va enrichir le pouvoir expressif de notre syst√®me d√©ductif gr√¢ce √† de nouvelles r√®gles. Ces r√®gles concernent la contradiction $\\bot$, et plus pr√©cis√©ment son √©limination.\nAbsurdit√© intuitionniste (ou √©limination de la contradiction)\n$ \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\bot$} \\RightLabel{$\\;\\scriptsize \\bot e_i$} \\UnaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $ Si une contradiction d√©coule d\u0026rsquo;un jeu d\u0026rsquo;hypoth√®ses, alors n\u0026rsquo;importe quelle formule est d√©montrable avec ces m√™ms hypoth√®ses.\nOn obtient ainsi un nouveau syst√®me d√©ductif¬†; celui de la logique intuitionniste.\nLa r√®gle de l\u0026rsquo;absurdit√© intuitionniste n\u0026rsquo;est pas d√©montrable dans le cadre de la logique minimale. Les d√©ductions qui utilisent cette r√®gle ne sont donc pas possibles en logique minimale. Il existe par cons√©quent des formules qui sont des th√©or√®mes en logique intuitionniste (des s√©quents d√©montrables), mais restent non prouvables en logique minimale.\nDeux formules $\\phi$ et $\\psi$ sont √©quivalentes pour la logique intuitionniste si on a √† la fois $\\phi\\vdash_{\\!i}\\psi$ et $\\psi\\vdash_{\\!i}\\phi$, et on √©crit $\\phi\\equiv_i\\psi$.\nExemple d'une formule d√©montrable en logique intuitionniste et non d√©montrable en logique minimale\u0026nbsp;: $\\neg\\neg(\\neg\\neg\\phi\\rightarrow\\phi)$ $$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\neg\\neg\\phi\\vdash\\neg\\neg\\phi$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax+aff$} \\UnaryInfC{$\\phi,\\neg\\neg\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\phi\\vdash\\neg\\neg\\phi\\rightarrow\\phi$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\neg(\\neg\\neg\\phi\\rightarrow\\phi)$} \\RightLabel{$\\scriptsize\\;\\neg e$} \\BinaryInfC{$\\neg(\\neg\\neg\\phi\\rightarrow\\phi),\\phi\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\neg i$} \\UnaryInfC{$\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\neg\\phi$} \\RightLabel{$\\scriptsize\\;\\neg e$} \\BinaryInfC{$\\neg\\neg\\phi,\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\bot e_i$} \\UnaryInfC{$\\neg\\neg\\phi,\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\neg\\neg\\phi\\rightarrow\\phi$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\neg(\\neg\\neg\\phi\\rightarrow\\phi)$} \\RightLabel{$\\scriptsize\\;\\neg e + ctr$} \\BinaryInfC{$\\neg(\\neg\\neg\\phi\\rightarrow\\phi)\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\neg i$} \\UnaryInfC{$\\vdash \\neg\\neg(\\neg\\neg\\phi\\rightarrow\\phi)$} \\end{prooftree} $$ La logique classique Absurdit√© classique\n$ \\begin{prooftree} \\AxiomC{$\\Gamma,\\neg\\phi\\vdash\\bot$} \\RightLabel{$\\;\\scriptsize \\bot e_c$} \\UnaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $ Si une contradiction d√©coule d\u0026rsquo;un jeu d\u0026rsquo;hypoth√®se auquel on ajoute la n√©gation d\u0026rsquo;une formule, alors la formule est d√©montrable √† partir du jeu d\u0026rsquo;hypoth√®ses de d√©part.\nEn ajoutant √† la logique minimale non pas la r√®gle de l\u0026rsquo;absurdit√© intuitionniste mais cette r√®gle de l\u0026rsquo;absurdit√© classique, on obtient la logique classique.\nL\u0026rsquo;absurdit√© classique n\u0026rsquo;est d√©montrable ni en logique minimale, ni en logique intuitionniste, alors que l\u0026rsquo;absurdit√© intuitionniste n\u0026rsquo;est qu\u0026rsquo;un cas particulier de l\u0026rsquo;absurdit√© classique. $$ \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;aff$} \\UnaryInfC{$\\Gamma,\\neg\\phi\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\bot e_c$} \\UnaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $$ Cela montre que la logique classique est un upgrade par rapport √† la logique intuitionniste.\nDes raisonnements comme la loi du tiers exclu ou l\u0026rsquo;√©limination des doubles n√©gations deviennent enfin d√©montrables (ils ne l\u0026rsquo;√©taient pas en logique intuitionniste et a fortiori pas non plus en logique minimale).\nLoi du tiers exclu $\\vdash\\phi\\lor\\neg\\phi$ √âlimination des doubles n√©gations $\\vdash\\neg\\neg\\phi\\rightarrow\\phi$ Loi de Peirce $\\vdash(\\neg\\phi\\rightarrow\\phi)\\rightarrow\\phi$ La contraposition $\\vdash(\\neg\\psi\\rightarrow\\neg\\phi)\\rightarrow(\\phi\\rightarrow\\psi)$ L\u0026rsquo;ajout de l\u0026rsquo;absurdit√© classique aux r√®gles de la logique minimale n\u0026rsquo;est pas le seul chemin pour obtenir la logique classique. En utilisant chacun des quatre diff√©rents s√©quents que l\u0026rsquo;on vient de d√©montrer comme axiome, on peut augmenter soit la logique minimale, soit la logique intuitionniste en logique classique.\nlogique classique = logique intuitionniste + principe du tiers exclu logique classique = logique intuitionniste + loi de Peirce logique classique = logique minimale + √©limination des doubles n√©gations logique classique = logique minimale + contraposition Nous allons montr√© que les inclusions sont v√©rifi√©es dans les deux sens.\nPour le sens $\\supset$, c'est d√©j√† fait.\nEn effet, on a d√©montr√© plus haut que chacun des axiomes ajout√©s peut se d√©montrer en logique classique sans hypoth√®se suppl√©mentaire. Sens $\\subset$\u0026nbsp;: il suffit de v√©rifier √† chaque fois que la r√®gle de l'absurde classique peut √™tre obtenue √† partir de la logique consid√©r√©e √† laquelle on a ajout√© l'axiome. logique classique $\\subset$ logique intuitionniste + principe du tiers exclu\u0026nbsp;:\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\color{#970E53}\\vdash\\phi\\lor\\neg\\phi$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\AxiomC{$\\Gamma,\\neg\\phi\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\bot e_i$} \\UnaryInfC{$\\Gamma,\\neg\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\lor e$} \\TrinaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $$ logique classique $\\subset$ logique intuitionniste + loi de Peirce\u0026nbsp;:\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\color{#970E53}\\neg\\phi\\rightarrow\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\vdash (\\neg\\phi\\rightarrow\\phi)\\rightarrow\\phi$} \\AxiomC{$\\Gamma,\\neg\\phi\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\bot e_i$} \\UnaryInfC{$\\Gamma,\\neg\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\Gamma\\vdash \\neg\\phi\\rightarrow\\phi$} \\RightLabel{$\\scriptsize\\;\\lor e$} \\BinaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $$ logique classique $\\subset$ logique minimale + √©limination des doubles n√©gations\u0026nbsp;:\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\color{#970E53}\\vdash\\neg\\neg\\phi\\rightarrow\\phi$} \\AxiomC{$\\Gamma,\\neg\\phi\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\neg i$} \\UnaryInfC{$\\Gamma\\vdash\\neg\\neg\\phi$} \\RightLabel{$\\scriptsize\\;\\lor e$} \\BinaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $$ logique classique $\\subset$ logique minimale + contraposition\nOn va utiliser une instance de la contraposition o√π $\\psi:=\\phi$ et $\\phi:=\\neg\\bot$\u0026nbsp;:\n$$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\color{#970E53}\\neg\\phi\\rightarrow\\neg\\neg\\bot\\vdash\\neg\\bot\\rightarrow\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\vdash (\\neg\\phi\\rightarrow\\neg\\neg\\bot)\\rightarrow(\\neg\\bot\\rightarrow\\phi)$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\bot\\vdash\\bot$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\neg\\bot\\vdash\\neg\\bot$} \\RightLabel{$\\scriptsize\\;\\rightarrow e$} \\BinaryInfC{$\\bot,\\neg\\bot\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\neg i$} \\UnaryInfC{$\\bot\\vdash\\neg\\neg\\bot$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\vdash \\bot\\rightarrow\\neg\\neg\\bot$} \\AxiomC{$\\Gamma,\\neg\\phi\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\rightarrow e$} \\BinaryInfC{$\\Gamma,\\neg\\phi\\vdash\\neg\\neg\\bot$} \\RightLabel{$\\scriptsize\\;\\rightarrow i$} \\UnaryInfC{$\\Gamma\\vdash\\neg\\phi\\rightarrow\\neg\\neg\\bot$} \\RightLabel{$\\scriptsize\\;\\rightarrow e$} \\BinaryInfC{$\\Gamma\\vdash\\neg\\bot\\rightarrow\\phi$} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\bot\\vdash\\bot$} \\RightLabel{$\\scriptsize\\;\\neg i$} \\UnaryInfC{$\\vdash\\neg\\bot$} \\RightLabel{$\\scriptsize\\;\\rightarrow e$} \\BinaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $$ info\nLa logique intuitionniste, contrairement √† la logique classique, vise √† obtenir des preuves constructives. √âtablir la v√©rit√© de $\\phi$ ne suffit pas, il faut la construire √©tape par √©tape. Or les raisonnements par l\u0026rsquo;absurde classique ne construisent pas r√©ellement la v√©rit√© de $\\phi$ puisqu\u0026rsquo;ils se contentent d\u0026rsquo;√©tablir une contradiction mettant en jeu $\\neg\\phi$. De m√™me, le tiers exclu nous affirme que $\\phi=\\psi\\lor\\neg\\psi$ sans jamais √©tablir la v√©rit√© de $\\psi$ ou $\\neg\\psi$.\nExemple classique de l\u0026rsquo;utilisation du tiers exclu en math√©matique¬†:\nprouvons qu\u0026rsquo;il existe un couple de nombres irrationnels $(a,b)$ ($a,b\\in\\mathbb{R}\\setminus \\mathbb{Q}$) tels que $a^b$ soit rationnel ($a^b\\in\\mathbb{Q}$).\nSachant que $\\sqrt{2}$ est irrationnel, on distingue deux cas¬†:\nsi $\\sqrt{2}^\\sqrt{2}$ est rationnel, alors on prend $a=b=\\sqrt{2}$\nsi $\\sqrt{2}^\\sqrt{2}$ est irrationnel, alors on prend $a=\\sqrt{2}^\\sqrt{2}$ et $b=\\sqrt{2}$ puisqu\u0026rsquo;ainsi $a^b=2$.\nOn a donc bien r√©pondu √† la question puisque d\u0026rsquo;apr√®s le principe du tiers exclu, on est dans un cas ou dans l\u0026rsquo;autre. Mais pour savoir laquelle des deux options est la bonne, il faudrait savoir si $\\sqrt{2}^\\sqrt{2}$ est rationnel ou non (et d√©montrer qu\u0026rsquo;il ne l\u0026rsquo;est pas est beaucoup plus lourd)\u0026hellip;\nEn logique intuitionniste, si l\u0026rsquo;on a r√©ussi √† d√©montrer le s√©quent $\\vdash_{\\! i}\\phi\\lor\\psi$, c\u0026rsquo;est qu\u0026rsquo;on a d√©montr√© un des deux s√©quents $\\vdash_{\\! i}\\phi$ ou $\\vdash_{\\! i}\\psi$. Il n\u0026rsquo;y a pas de v√©rit√© interm√©diaire ind√©termin√©e.\nUn autre grief contre la logique classique concerne l\u0026rsquo;implication mat√©rielle¬†: $\\phi\\rightarrow\\psi$ est consid√©r√©e comme vraie si $\\neg\\phi$ sans qu\u0026rsquo;on n\u0026rsquo;ait jamais eu √† construire de lien entre $\\phi$ et $\\psi$. La philosophe Dorothy Edgington en a tir√© une d√©monstration de l\u0026rsquo;existence de Dieu gr√¢ce √† la formule suivante¬†: si Dieu n\u0026rsquo;existe pas ($\\neg G$), il n\u0026rsquo;est pas vrai que si je prie ($P$), alors mes pri√®res seront entendues ($A$). L\u0026rsquo;affirmation ne semble pas choquante. Sous forme de formule, cela donne¬†: $(\\neg G)\\rightarrow \\neg(P\\rightarrow A)$. La formule peut se r√©√©crire en √©liminant les implications¬†: $G\\lor\\neg(\\neg P \\lor A)$. Et voil√† maintenant le coup de gr√¢ce, sous la forme d\u0026rsquo;une deuxi√®me formule¬†: $\\neg P$, \u0026ldquo;je ne prie pas\u0026rdquo;. Et si $P$ est faux, seul $G$ survit dans la formule principale¬†; \u0026ldquo;Dieu existe\u0026rdquo;.\nLes r√®gles de la logique classique permettent de faire co√Øncider les notions de cons√©quence syntaxique et de cons√©quence s√©mantique.\nCorrection de la logique classique¬†:\nSoient $\\Gamma$ un ensemble fini de formules et $\\phi$ une formule du calcul des propositions,\nSi $\\Gamma\\vdash_{\\!c}\\phi$ alors $\\Gamma\\models\\phi$\nUne preuve √©tant une suite finie de s√©quent, on va proc√©der par induction sur la longueur de la preuve.\nSi la preuve a une longueur 1, le s√©quent ne peut √™tre qu'un axiome de la forme $\\phi\\vdash\\phi$.\nReste √† montrer que $\\phi\\models\\phi$ est vraie. Et c'est bien s√ªr le cas\u0026nbsp;: par d√©finition, $\\phi\\models\\phi$ est vraie si $\\phi$ est vraie dans chaque mod√®le o√π $\\phi$ est vraie...\nOn suppose maintenant la propri√©t√© vraie pour toutes les preuves de longueur $n$.\nUne preuve de longueur $n+1$ est une preuve de longueur $n$ √† laquelle on ajoute un s√©quent qui est soit un axiome (on revient au cas pr√©c√©dent), soit une des r√®gles.\nIl faut montrer que les r√®gles ont √©t√© judicieusement choisies pour \"conserver\" la notion de cons√©quence s√©mantique entre els pr√©misses et la conclusion.\nIci, les pr√©misses sont bien des cons√©quences s√©mantiques par application de l'hypoth√®se d'induction (elles correspondent √† des s√©quents de longueur $‚â§n$).\nOn doit maintenant v√©rifier sur chaque r√®gle que le s√©quent conclusion est une cons√©quence s√©mantique des pr√©misses.\nMontrons-le pour la r√®gle d'√©limination de l'implication $ \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi\\rightarrow\\psi$} \\AxiomC{$\\Gamma'\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow e$} \\BinaryInfC{$\\Gamma,\\Gamma'\\vdash\\psi$} \\end{prooftree} $\u0026nbsp;: Par hypoth√®se d'induction, on sait que $\\Gamma\\models\\phi\\rightarrow\\psi$ et $\\Gamma'\\models\\phi$. Donc dans les mod√®les de $\\Gamma \\cup \\Gamma'$ (ensemble des mod√®les o√π les formules de $\\Gamma$ et $\\Gamma'$ sont vraies simultan√©ment), √† la fois $\\phi\\rightarrow\\psi$ et $\\phi$ sont v√©rifi√©es. Et comme lorsque $\\phi$ est vraie, $\\phi\\rightarrow\\psi$ n'est vraie que si $\\psi$ est vraie, il en r√©sulte que $\\psi$ est vraie dans ces mod√®les\u0026nbsp;: $\\Gamma,\\Gamma'\\models\\psi$.\nOn peut montrer de mani√®re √©quivalente que cela marche pour chaque r√®gle... Une cons√©quence de la correction de la logique classique est que pour toute th√©orie finie $\\Gamma$ et pour tout mod√®le $\\mathcal{M}$ de cette th√©orie, si une formule est d√©montrable √† partir des hypoth√®ses $\\Gamma$, alors elle est vraie dans $\\mathcal{M}$.\nTout ce que l\u0026rsquo;on peut d√©duire syntaxiquement d\u0026rsquo;une th√©orie est vrai dans un mod√®le quelconque de cette th√©orie.\nCompl√©tude de la logique classique¬†:\nSoient $\\Gamma$ un ensemble fini de formules et $\\phi$ une formule du calcul des propositions,\nSi $\\Gamma\\models\\phi$ alors $\\Gamma\\vdash_{\\!c}\\phi$\nSoit $\\phi$, une formule quelconque du calcul des propositions dont les variables sont parmis $\\set{P_1,\\ldots,P_n}$, et $\\mathcal{M}$, un mod√®le possible de $\\phi$. On note¬†:\n$P_i^\\mathcal{M} = P_i$ si $P_i$ est vraie dans $\\mathcal{M}$, $P_i^\\mathcal{M} = \\neg P_i$ sinon. On pose √©galement¬†:\n$\\phi^\\mathcal{M} = \\phi$ si $\\mathcal{M}\\models\\phi$, $\\phi^\\mathcal{M} = \\neg\\phi$ sinon. Par induction sur la hauteur de $\\phi$ on veut montrer le r√©sultat suivant¬†: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\phi^{\\mathcal{M}}$\nsi $ht(\\phi)=0$, $\\phi$ est une variable propositionnelle $P_i$ et il est alors imm√©diat que $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}P_i^{\\mathcal{M}}$ si $ht(\\phi)=n+1$, alors on a une des possibilit√©s suivantes\u0026nbsp;: $\\phi=\\neg\\psi$, $\\phi=\\psi_1\\rightarrow\\psi_2$, $\\phi=\\psi_1\\lor\\psi_2$ ou $\\phi=\\psi_1\\land\\psi_2$.\nChaque $\\psi$, de hauteur $n$, respecte par hypoth√®se la propri√©t√© et on doit montrer dans chaque cas que cela implique le m√™me respect de la propri√©t√© pour $\\phi$. Prenons par exemple le cas $\\phi=\\psi_1\\rightarrow\\psi_2$ pour voir le type de raisonnement qu\u0026rsquo;il faut mener¬†: Si $\\mathcal{M}\\not\\models\\psi_1$, alors $\\mathcal{M}\\models\\phi$ et donc $\\phi^\\mathcal{M}=\\phi$, et $\\psi_1^\\mathcal{M}=\\neg\\psi_1$.\nPar hypoth√®se d'induction, $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\neg\\psi_1$\nConsid√©rons la r√®gle suivante\u0026nbsp;:\nL'utilisation de cette r√®gle et du modus ponens permet d'obtenir\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1\\rightarrow\\psi_2$, ce qui est bien ce que l'on voulait montrer puisque $\\psi_1\\rightarrow\\psi_2=\\phi^\\mathcal{M}$ Si $\\mathcal{M}\\models\\psi_1$, alors\u0026nbsp;: Si $\\mathcal{M}\\models\\psi_2$, alors $\\mathcal{M}\\models\\phi$ et donc $\\phi^\\mathcal{M}=\\phi$, $\\psi_1^\\mathcal{M}=\\psi_1$ et $\\psi_2^\\mathcal{M}=\\psi_2$.\nPar hypoth√®se d'induction, on a\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1^\\mathcal{M}$ et $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_2^\\mathcal{M}$\nC'est-√†-dire\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1$ et $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_2$\nDe $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_2$, on d√©duit par affaiblissement $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}},\\psi_1 \\vdash_{\\!c}\\psi_2$, puis par introduction de l'implication\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1 \\rightarrow\\psi_2$ Si $\\mathcal{M}\\not\\models\\psi_2$, alors $\\mathcal{M}\\not\\models\\phi$ et donc $\\phi^\\mathcal{M}=\\neg\\phi$, $\\psi_1^\\mathcal{M}=\\psi_1$ et $\\psi_2^\\mathcal{M}=\\neg\\psi_2$.\nPar hypoth√®se d'induction, on a\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1^\\mathcal{M}$ et $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_2^\\mathcal{M}$\nC'est-√†-dire\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1$ et $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\neg\\psi_2$\nOn en d√©duit\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\psi_1\\land\\neg\\psi_2$.\nConsid√©rons la r√®gle suivante\u0026nbsp;: On applique ensuite la r√®gle du modus ponens pour obtenir $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\neg(\\psi_1\\rightarrow\\psi_2)$ qui est bien le r√©sultat recherch√©. Supposons qu\u0026rsquo;on ait pass√© en revue tous les autres cas pour finir de montrer que $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\phi^{\\mathcal{M}}$.\nNous allons maintenant prouver que pour toute formule $\\phi$, si $\\phi$ est une tautologie, alors $\\phi$ est un th√©or√®me de la logique classique.\nSoit $\\phi$, une formule quelconque du calcul des propositions dont les variables sont $P_1,\\ldots,P_n$. Et soit $\\mathcal{M}$ un mod√®le possible de $\\phi$. On a montr√© que $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\phi^{\\mathcal{M}}$. Or $\\mathcal{M}\\models\\phi$ est v√©rifi√©, donc $\\phi^\\mathcal{M}=\\phi$. D\u0026rsquo;o√π $\\set{P_1^{\\mathcal{M}},\\ldots,P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\phi$.\nSi $\\phi$ est une tautologie, ce r√©sultat est valable pour n\u0026rsquo;importe quel mod√®le. Choisissons les mod√®les $\\mathcal{M}$ et $\\mathcal{M\u0026rsquo;}$ qui distribuent les m√™mes valeurs de v√©rit√© pour toutes les variables $P_i$ sauf $P_n$. $\\mathcal{M}\\models P_n$ alors que $\\mathcal{M\u0026rsquo;}\\models \\neg P_n$.\nOn a √† la fois $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-1}^{\\mathcal{M}},P_n^{\\mathcal{M}}} \\vdash_{\\!c}\\phi$ et $\\set{P_1^{\\mathcal{M\u0026rsquo;}},\\ldots,P_{n-1}^{\\mathcal{M\u0026rsquo;}},P_n^{\\mathcal{M\u0026rsquo;}}} \\vdash_{\\!c}\\phi$.\nD\u0026rsquo;o√π $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-1}^{\\mathcal{M}},P_n} \\vdash_{\\!c}\\phi$ et $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-1}^{\\mathcal{M}},\\neg P_n} \\vdash_{\\!c}\\phi$ qu\u0026rsquo;on peut aussi √©crire $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-1}^{\\mathcal{M}}},P_n \\vdash_{\\!c}\\phi$ et $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-1}^{\\mathcal{M}}},\\neg P_n \\vdash_{\\!c}\\phi$.\nConstruisons la r√®gle suivante en appliquant le tiers exclu¬†:\n$$ \\begin{prooftree} \\AxiomC{} \\UnaryInfC{$\\vdash \\psi\\lor\\neg\\psi$} \\AxiomC{} \\UnaryInfC{$\\Gamma,\\psi\\vdash\\phi$} \\AxiomC{} \\UnaryInfC{$\\Gamma,\\neg\\psi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\lor e$} \\TrinaryInfC{$\\Gamma\\vdash\\phi$} \\end{prooftree} $$ En appliquant cette r√®gle √† notre cas, on obtient\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-1}^{\\mathcal{M}}} \\vdash_{\\!c}\\phi$ et √† nouveau, ce r√©sultat ne d√©pend pas du mod√®le $\\mathcal{M}$. On peut donc recommencer le m√™me raisonnement, ce qui donne successivement\u0026nbsp;: $\\set{P_1^{\\mathcal{M}},\\ldots,P_{n-2}^{\\mathcal{M}}} \\vdash_{\\!c}\\phi, \\set{P_1^{\\mathcal{M}},\\ldots,P_{n-3}^{\\mathcal{M}}} \\vdash_{\\!c}\\phi, \\set{P_1^{\\mathcal{M}},\\ldots,P_{n-4}^{\\mathcal{M}}} \\vdash_{\\!c}\\phi,\\ldots $\nEt en derni√®re √©tape, on obtient $P_1\\vdash_{\\!c}\\phi$ et $\\neg P_1\\vdash_{\\!c}\\phi$. D'o√π l'on d√©duit $\\vdash_{\\!c}\\phi$ par application du tiers exclu.\nNous avons donc bien r√©ussi √† prouver que $\\models\\phi$ implique $\\vdash_{\\!c}\\phi$. Il ne nous reste plus qu\u0026rsquo;√† montrer que pour toute formule $\\phi$ et pour tout ensemble fini d\u0026rsquo;hypoth√®ses $\\Gamma=\\set{\\psi_1,\\ldots,\\psi_k}$, $\\Gamma\\models\\phi$ implique $\\Gamma\\vdash_{\\!c}\\phi$.\nIl apparait imm√©diatement que $\\Gamma\\models\\phi$ si et seulement si $\\models(\\psi_1\\land\\psi_2\\land\\ldots\\land\\psi_k)\\rightarrow\\phi $.\nLe r√©sultat pr√©c√©dent sur les tautologies nous permet de d√©duire que $\\vdash_{\\!c}(\\psi_1\\land\\psi_2\\land\\ldots\\land\\psi_k)\\rightarrow \\phi$.\nIl est √©vident par ailleurs que $\\Gamma\\vdash_{\\!c}(\\psi_1\\land\\psi_2\\land\\ldots\\land\\psi_k)$.\nPar modus ponens, on obtient donc $\\Gamma\\vdash_{\\!c}\\phi$.\ntip\nDe fa√ßon informelle, la correction stipule que toute formule prouvable est vraie, alors que la compl√©tude, de son c√¥t√©, affirme que toutes les formules vraies sont prouvables.\nEt dit autrement encore, on peut prouver toute la v√©rit√© (compl√©tude) et rien que la v√©rit√© (correction).\nCorollaire¬†:\n$\\models\\phi$ ssi $\\vdash_{\\!c}\\phi$\nToute tautologie est d√©montrable sans utiliser d\u0026rsquo;hypoth√®se et √† l\u0026rsquo;inverse, toute formule d√©montrable sans hypoth√®se est une tautologie.\nCorollaire¬†:\nSoient $\\phi$ et $\\psi$ deux formules du calcul des propositions,\n$\\Gamma\\equiv\\psi$ ssi $\\Gamma\\equiv_c\\psi$\nOn a montr√© finalement que la s√©mantique du calcul des propositions correspond √† la logique classique. On peut d\u0026rsquo;ailleurs parler de \u0026ldquo;s√©mantique classique\u0026rdquo;. Mais qu\u0026rsquo;en est-il de la logique intuitionniste¬†? Peut-on d√©finir une s√©mantique qui lui soit adapt√©e afin d\u0026rsquo;obtenir un th√©or√®me de compl√©tude pour la logique intuitionniste¬†?\nOui, gr√¢ce aux mod√®les de Kripke.\nLes mod√®les de Kripke du calcul des propositions intuitionniste On a vu qu\u0026rsquo;une formule est d√©montrable en logique classique si et seulement si elle est vraie dans tous les mod√®les (classiques) possibles. On va aboutir √† une proposition similaire gr√¢ce aux mod√®les de Kripke¬†: une formule est d√©montrable en logique intuitionniste si et seulement si elle est r√©alis√©e dans tous les mod√®les de Kripke possibles.\nSoit $\\set{P_1,\\ldots,P_k}$ un ensemble de variables propositionnelles.\n$\\mathcal{K}=(\\mathcal{T}_\\mathcal{K},\\Vdash)$ est un mod√®le de Kripke arborescent fini sur les variables $P_1,\\ldots,P_k$ si¬†:\n$\\mathcal{T}_\\mathcal{K}$ est un arbre fini dont la relation d'ancestralit√© entre deux n≈ìuds est not√© $\\alpha‚â§\\beta$ ($\\alpha$ est un anc√™tre de $\\beta$). $\\Vdash$ est une relation binaire appel√©e relation de forcing entre les n≈ìuds de $\\mathcal{T}_\\mathcal{K}$ et les variables propositionnelles qui v√©rifie\u0026nbsp;:\npour tout n≈ìud $\\alpha$\u0026nbsp;: $\\alpha\\not\\Vdash\\bot$ pour tout n≈ìud $\\alpha$, $\\beta$ et variable propositionnelle $P_i$, si √† la fois $\\alpha‚â§\\beta$ et $\\alpha\\Vdash P_i$ alors on a $\\beta\\Vdash P_i$. info\nUn mod√®le de Kripke avec un seul n≈ìud peut √™tre vu comme un mod√®le de la logique classique car les conditions de la relations de forcing ($\\Vdash$) sont alors identiques √† celles de la relation de v√©rit√© s√©mantique ($\\models$).\nLa relation de forcing fait en sorte que si une formule est vraie dans un n≈ìud (qui repr√©sente un monde, comme on le verra en logique modale), alors elle reste vraie dans tous les n≈ìuds accessibles depuis le n≈ìud de d√©part (tous les mondes accessibles).\nSoit $\\mathcal{K}=(\\mathcal{T}_\\mathcal{K},\\Vdash)$ un mod√®le de Kripke arborescent fini sur les variables $P_1,\\ldots,P_k$, $\\alpha$ un n≈ìud de $\\mathcal{T}_\\mathcal{K}$ et $\\phi$, $\\psi$ des formules du calcul des propositions dont les variables sont parmi $\\set{P_1,\\ldots,P_k}$,\n$\\alpha\\Vdash \\phi\\land\\psi$ ssi $\\alpha\\Vdash\\phi$ et $\\alpha\\Vdash\\psi$ $\\alpha\\Vdash \\phi\\lor\\psi$ ssi $\\alpha\\Vdash\\phi$ ou $\\alpha\\Vdash\\psi$ $\\alpha\\Vdash \\phi\\rightarrow\\psi$ ssi pour tout n≈ìud $\\beta$ de $\\mathcal{T}_\\mathcal{K}$ tel que $\\alpha‚â§\\beta$, si $\\beta\\Vdash\\phi$ alors $\\beta\\Vdash\\psi$.\nOn cherche ici √† construire explicitement la v√©rit√© de $\\psi$ √† partir de celle de $\\phi$ (alors qu'en logique classique, l'√©tablissement de la v√©rit√© de $\\phi$ n'est pas n√©cessaire puisque l'implication est consid√©r√©e comme vraie soit par l'absence de $\\phi$, soit par la pr√©sence de $\\psi$). $\\alpha\\Vdash \\neg\\phi$ ssi pour tout n≈ìud $\\beta$ de $\\mathcal{T}_\\mathcal{K}$ tel que $\\alpha‚â§\\beta$, $\\beta\\not\\Vdash\\phi$.\nDit autrement, aucun n≈ìud ne force le faux.\nEn effet, la n√©gation $\\neg\\phi$ est interpr√©t√©e ici comme la formule $\\phi\\rightarrow\\bot$. La d√©finition de la relation de forcing de l'implication dit que\u0026nbsp;: $\\alpha\\Vdash\\phi\\rightarrow\\bot$ ssi pour tout n≈ìud $\\beta$ de $\\mathcal{T}_\\mathcal{K}$ tel que $\\alpha‚â§\\beta$, si $\\beta\\Vdash\\phi$ alors $\\beta\\Vdash\\bot$. Or la condition $\\beta\\Vdash\\bot$ est interdite par d√©finition des mod√®les de la logique intuitionniste, donc $\\beta\\Vdash\\bot$ n'est satisfait nulle part. D'o√π $\\beta\\not\\Vdash\\phi$. Voil√† un exemple de mod√®le de Kripke sur les variables $P$, $Q$, $R$. On y trouve les relations de forcing suivantes¬†:\n$\\theta\\Vdash Q\\land P$ mais $\\beta\\not\\Vdash Q\\land P$ $\\alpha\\Vdash P\\rightarrow Q$ mais $\\alpha\\not\\Vdash Q\\rightarrow P$ $\\beta\\Vdash\\neg P$ mais $\\alpha\\not\\Vdash\\neg P$ $\\alpha\\not\\Vdash P$, $\\alpha\\not\\Vdash \\neg P$ et $\\alpha\\not\\Vdash \\neg\\neg P$ Soit $\\mathcal{K}=(\\mathcal{T}_\\mathcal{K},\\Vdash)$ un mod√®le de Kripke arborescent fini sur les variables $P_1,\\ldots,P_k$, $\\alpha$ un n≈ìud de $\\mathcal{T}_\\mathcal{K}$, $\\phi$ une formule et $\\Gamma$ un ensemble fini de formules dont les variables sont parmi $\\set{P_1,\\ldots,P_k}$,\n$\\alpha\\Vdash \\Gamma$ ssi $\\alpha\\Vdash\\phi$ pour toute formule de $\\Gamma$. $\\phi$ (respectivement $\\Gamma)$ est r√©alis√©e dans $\\mathcal{K}$ si pour tout n≈ìud $\\alpha$ de $\\mathcal{T}_\\mathcal{K}$, $\\alpha\\Vdash\\phi$ (respectivement $\\alpha\\Vdash\\Gamma$). $\\phi$ est une cons√©quence intuitionniste de $\\Gamma$, not√© $\\Gamma\\Vdash_i\\phi$, ssi pour tout mod√®le de Kripke $\\mathcal{K}=(\\mathcal{T}_\\mathcal{K},\\Vdash)$ et tout n≈ìud $\\alpha$ si $\\alpha\\Vdash\\Gamma$ alors $\\alpha\\Vdash\\phi$.\nEn particulier, on note $\\Vdash_i \\phi$ si $\\phi$ est r√©alis√©e dans tout mod√®le de Kripke. Ce mod√®le ne r√©alise ni le tiers exclu $(P\\lor\\neg P)$, ni l\u0026rsquo;√©limination des doubles n√©gations $(\\neg\\neg P\\rightarrow P)$.\nEn effet¬†:\n$\\alpha \\not\\Vdash P$ par d√©finition et $\\alpha\\not\\Vdash \\neg P$ puisque $\\alpha‚â§\\beta$ et $\\beta\\Vdash P$. Donc $\\alpha\\not\\Vdash P\\lor\\neg P$.\n$\\alpha\\not\\Vdash\\neg P$ et $\\beta\\not\\Vdash\\neg P$ d\u0026rsquo;o√π $\\alpha\\Vdash \\neg\\neg P$ et $\\beta\\Vdash \\neg\\neg P$. D\u0026rsquo;o√π $\\beta\\Vdash\\neg\\neg P\\rightarrow P$ et $\\alpha\\not\\Vdash\\neg\\neg P\\rightarrow P$ (puisque $\\alpha \\not\\Vdash P$).\nCela permet de constater l\u0026rsquo;adaptation des mod√®les de Kripke √† la logique intuitionniste¬†; il peut exister des mod√®les o√π ni $\\phi$, ni $\\neg\\phi$ ne sont √©tablis comme vrais.\nCorrection et compl√©tude de la logique intuitionniste¬†:\nSoient $\\Gamma$ un ensemble fini de formules et $\\phi$ une formule du calcul des propositions,\n$\\Gamma\\Vdash_i\\phi$ ssi $\\Gamma\\vdash_{\\!i} \\phi$\nCorollaire¬†:\n$\\Vdash_i\\phi$ ssi $\\vdash_{\\!i} \\phi$\nEn abandonnant la condition \u0026ldquo;pour tout n≈ìud $\\alpha$¬†: $\\alpha\\not\\Vdash\\bot$\u0026rdquo; on obtient le mod√®le de Kripke adapt√©e √† la logique minimale.\nLa condition de forcing de la n√©gation devient alors $\\alpha\\Vdash\\neg\\phi$ ssi $\\alpha\\Vdash\\phi\\rightarrow\\bot$ (pour tout n≈ìud $\\beta$ de $\\mathcal{T}_\\mathcal{K}$ tel que $\\alpha‚â§\\beta$, si $\\beta\\Vdash\\phi$ alors $\\beta\\Vdash\\bot$).\nOn obtient cette fois-ci¬†:\nCorrection et compl√©tude de la logique minimale¬†:\nSoient $\\Gamma$ un ensemble fini de formules et $\\phi$ une formule du calcul des propositions,\n$\\Gamma\\Vdash_m\\phi$ ssi $\\Gamma\\vdash_{\\!m} \\phi$\nD\u0026rsquo;o√π d√©coule $\\Vdash_m\\phi$ ssi $\\vdash_{\\!m} \\phi$\nDans ce mod√®le de Kripke de la logique minimale, $\\neg\\neg(\\neg\\neg P\\rightarrow P)$ n\u0026rsquo;est pas r√©alis√©e.\nEn effet, $\\beta\\not\\Vdash P$ par d√©finition, d\u0026rsquo;o√π $\\beta\\Vdash \\neg P$. Mais on a √©galement $\\beta\\Vdash\\neg\\neg P$ puisque $\\beta\\Vdash \\bot$. Mais alors $\\beta\\not\\Vdash \\neg\\neg P\\rightarrow P$. Par cons√©quent $\\beta\\Vdash\\neg(\\neg\\neg P\\rightarrow P)$ et toujours √† cause du fait que $\\beta\\Vdash \\bot$, on obtient $\\beta\\Vdash \\neg\\neg(\\neg\\neg P\\rightarrow P)$.\n$\\alpha\\not\\Vdash P$ et $\\beta\\not\\Vdash P$ par d√©finition, d\u0026rsquo;o√π $\\beta\\Vdash \\neg P$. Mais $\\alpha\\not\\Vdash \\neg\\neg P$ puisque $\\alpha\\not\\Vdash\\bot$. On a aussi $\\alpha\\not\\Vdash\\neg\\neg P\\rightarrow P$ puisque $\\beta\\Vdash \\neg\\neg P$ et $\\beta\\not\\Vdash P$. Par cons√©quent, $\\alpha\\Vdash \\neg(\\neg\\neg P\\rightarrow P)$ puisqu\u0026rsquo;√† la fois $\\alpha\\not\\Vdash\\neg\\neg P \\rightarrow P$ et $\\beta\\not\\Vdash \\neg\\neg P\\rightarrow P$. D\u0026rsquo;o√π $\\alpha\\not\\Vdash\\neg\\neg(\\neg\\neg P\\rightarrow P)$ puisqu\u0026rsquo;√† la fois $\\alpha\\Vdash\\neg(\\neg\\neg P \\rightarrow P)$ et $\\alpha\\not\\Vdash\\bot$.\nIl existe donc un n≈ìud du mod√®le qui ne force pas la formule $\\neg\\neg(\\neg\\neg P\\rightarrow P)$. Le th√©or√®me de compl√©tude de la logique minimale nous dit alors que $\\neg\\neg(\\neg\\neg P\\rightarrow P)$ n\u0026rsquo;est pas un th√©or√®me de la logique minimale.\nOr on a montr√© que cette formule √©tait un th√©or√®me de la logique intuitionniste. Le s√©quent $\\vdash\\neg\\neg(\\neg\\neg P\\rightarrow P)$ est donc prouvable en logique intuitionniste mais pas en logique minimale. Cela prouve que la logique minimale forme un sous-ensemble strict de la logique intuitionniste, elle-m√™me strictement inclue dans la logique classique. Le calcul des s√©quents Comme la d√©duction naturelle, le calcul des s√©quents a √©t√© mis au point par Gerhard Gentzen. Son but est de mettre encore mieux en lumi√®re les propri√©t√©s math√©matiques de la notion de d√©monstration.\nLes diff√©rences principales avec la d√©duction naturelle sont que la partie droite des s√©quents n\u0026rsquo;est plus une seule formule conclusion mais un ensemble fini de formules, et les r√®gles d\u0026rsquo;√©limination sont remplac√©es par des r√®gles d\u0026rsquo;introduction √† gauche afin de rendre le syst√®me de d√©monstration sym√©trique.\nUn s√©quent (not√© $\\Gamma\\vdash\\Delta$) est un couple o√π¬†:\n$\\Gamma$ est un ensemble fini de formules, appel√© la partie gauche du s√©quent, $\\Delta$ est √©galement un ensemble fini de formules, appel√© la partie droite du s√©quent. Intuitivement, un s√©quent $\\Gamma\\vdash\\Delta$ o√π $\\Gamma=\\set{\\phi_1,\\ldots,\\phi_n}$ et $\\Delta=\\set{\\psi_1,\\ldots,\\psi_k}$ est interpr√©t√© comme $\\bigwedge_{1‚â§i‚â§n}\\phi_i \\vdash \\bigvee_{1‚â§j‚â§k} \\psi_j$. Une conjonction d\u0026rsquo;hypoth√®ses prouve une disjonction de conclusions.\nLa conjonction vide d\u0026rsquo;hypoth√®se est interpr√©t√© comme le vrai, alors que la disjonction vide de conclusion est interpr√©t√©e comme le faux.\n$\\vdash\\Delta$ signifie donc $(\\psi_1\\lor\\psi_2\\lor\\ldots\\lor\\psi_k)$, alors que le s√©quent $\\Gamma\\vdash$ signifie $(\\phi_1\\land\\phi_2\\land\\ldots\\land\\phi_n)\\rightarrow\\bot$, c\u0026rsquo;est-√†-dire $\\neg(\\phi_1\\land\\phi_2\\land\\ldots\\land\\phi_n)$.\nEnfin $\\vdash$, qui signifie \u0026ldquo;vrai implique faux\u0026rdquo;, est interpr√©t√© comme l\u0026rsquo;absurde (il correspond au s√©quent $\\vdash\\bot$ de la d√©duction naturelle).\nLes r√®gles du calcul des s√©quents sont tr√®s proches de celles de la d√©duction naturelle.\nD\u0026rsquo;un ensemble de pr√©misses (0, 1 ou 2), on d√©duit un s√©quent conclusion.\nLes r√®gles d\u0026rsquo;introduction sont conserv√©es, mais elles deviennent des r√®gles d\u0026rsquo;introduction √† droite. Les r√®gles d\u0026rsquo;√©limination deviennent, elles, des r√®gles d\u0026rsquo;introduction √† gauche.\nLes r√®gles structurelles sont sym√©tris√©es et on ajoute un axiome ainsi qu\u0026rsquo;une nouvelle r√®gle, la r√®gle de coupure¬†:\n$$ \\begin{prooftree} \\AxiomC{$\\Gamma\\vdash\\phi,\\Delta$} \\AxiomC{$\\Gamma',\\phi\\vdash\\Delta'$} \\RightLabel{$\\scriptsize\\;cut$} \\BinaryInfC{$\\Gamma,\\Gamma'\\vdash\\Delta,\\Delta'$} \\end{prooftree} $$ En d√©duction naturelle, cela correspond aux deux pr√©misses $\\Gamma\\vdash\\phi$ et $\\phi\\vdash\\psi$ desquels on d√©duit $\\Gamma\\vdash\\psi$.\nLa r√®gle de coupure est souvent utilis√©e dans les preuves pour simplifier la d√©duction en introduisant des lemmes ou des th√©or√®mes interm√©diaires.\n√âtant donn√© qu\u0026rsquo;il n\u0026rsquo;y a plus de r√®gles d\u0026rsquo;√©limination, c\u0026rsquo;est la r√®gle de coupure qui permet de r√©tablir la transitivit√© dans les d√©ductions.\nL\u0026rsquo;affaiblissement √† droite refl√®te l\u0026rsquo;id√©e que si on a une preuve que quelque chose est vrai √† partir d\u0026rsquo;un ensemble de pr√©misses, ajouter une conclusion suppl√©mentaire comme possibilit√© n\u0026rsquo;enl√®ve rien √† la validit√© de cette preuve.\nEn rempla√ßant $\\Delta$ par $\\empty$, on retrouve la r√®gle d\u0026rsquo;absurdit√© intuitionniste.\nPreuve du tiers exclu\u0026nbsp;: $$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\neg_d$} \\UnaryInfC{$\\vdash\\phi,\\neg\\phi$} \\RightLabel{$\\scriptsize\\;\\lor_d$} \\UnaryInfC{$\\vdash\\phi\\lor\\neg\\phi$} \\end{prooftree} $$ Preuve de $\\vdash\\phi\\rightarrow\\neg\\neg\\phi$\u0026nbsp;: $$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\neg_g$} \\UnaryInfC{$\\phi,\\neg\\phi\\vdash$} \\RightLabel{$\\scriptsize\\;\\neg_d$} \\UnaryInfC{$\\phi\\vdash\\neg\\neg\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow_d$} \\UnaryInfC{$\\vdash\\phi\\rightarrow\\neg\\neg\\phi$} \\end{prooftree} $$ Preuve de l'√©limination des doubles n√©gations\u0026nbsp;: $$ \\begin{prooftree} \\AxiomC{} \\RightLabel{$\\scriptsize\\;ax$} \\UnaryInfC{$\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\neg_d$} \\UnaryInfC{$\\vdash\\phi,\\neg\\phi$} \\RightLabel{$\\scriptsize\\;\\neg_g$} \\UnaryInfC{$\\neg\\neg\\phi\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow_g$} \\UnaryInfC{$\\vdash\\neg\\neg\\phi\\rightarrow\\phi$} \\end{prooftree} $$ Les deux formules pr√©c√©dentes se ressemblent fortement mais seule la premi√®re est prouvable en logique intuitionniste.\nLes deux preuves en calcul des s√©quents sont quasiment les m√™mes, mais l\u0026rsquo;ordre d\u0026rsquo;introduction des n√©gations est invers√© (gauche-droite pour la premi√®re et droite-gauche pour la deuxi√®me).\nLa logique intuitionniste est la version du calcul des s√©quents dont les r√®gles sont restreintes aux s√©quents avec au plus une formule √† droite, la contraction √† droite √©tant consid√©r√©e comme implicite.\nL\u0026rsquo;id√©e est d\u0026rsquo;obliger une preuve √† indiquer sp√©cifiquement la conclusion d√©riv√©e, sans ambigu√Øt√©.\nEt la n√©gation est interpr√©t√©e comme l\u0026rsquo;absence de preuve puisque puisque c\u0026rsquo;est maintenant seulement $\\phi\\vdash\\bot$ qui donne $\\vdash\\neg\\phi$. Cela rend par cons√©quent l\u0026rsquo;utilisation de la n√©gation √† droite impossible dans la deuxi√®me preuve alors qu\u0026rsquo;il n\u0026rsquo;y a pas de probl√®me dans la premi√®re.\nLa logique minimale est la version du calcul des s√©quents sans la r√®gle de l\u0026rsquo;affaiblissement √† droite et dont les r√®gles sont restreintes aux s√©quents avec au plus une formule √† droite (la contraction √† droite n\u0026rsquo;est ici pas consid√©r√©e comme implicite).\nPreuve de $\\vdash\\neg\\neg(\\neg\\neg\\phi\\rightarrow\\phi)$\u0026nbsp;: Chaque n≈ìud de l\u0026rsquo;arbre de preuve ne contient pas plus d\u0026rsquo;une formule √† droite du symbole du s√©quent, par contre la r√®gle d\u0026rsquo;affaiblissement √† droite est utilis√©e, ce qui situe cette preuve dans le cadre de la logique intuitionniste mais pas de la logique minimale.\nLa r√®gle de coupure correspond, en d√©duction naturelle, √† l\u0026rsquo;introduction de l\u0026rsquo;implication suivie aussit√¥t de son √©limination¬†:\n$$ \\begin{prooftree} \\AxiomC{$\\Gamma,\\phi\\vdash\\psi$} \\RightLabel{$\\scriptsize\\;\\rightarrow_i$} \\UnaryInfC{$\\Gamma\\vdash\\phi\\rightarrow\\psi$} \\AxiomC{$\\Gamma'\\vdash\\phi$} \\RightLabel{$\\scriptsize\\;\\rightarrow_e$} \\BinaryInfC{$\\Gamma,\\Gamma'\\vdash\\psi$} \\end{prooftree} $$ Cette r√®gle est essentielle pour tenir compte de la mani√®re dont nous raisonnons puisqu\u0026rsquo;il arrive tr√®s souvent que l\u0026rsquo;on utilise les conclusions d\u0026rsquo;un raisonnements ant√©rieur comme hypoth√®ses pour de nouvelles d√©monstrations (hypoth√®ses que nous faisons dispara√Ætre sur la base du fait que nous les avons d√©montr√©es).\nMais si notre objectif est d\u0026rsquo;automatiser les preuves, de les algorithmiser, les r√®gles de coupure deviennent un handicap.\n√âlimination des coupures\nS\u0026rsquo;il existe, en calcul des s√©quents classique (respectivement intuitionniste), une preuve du s√©quent $\\Gamma\\vdash\\Delta$, alors il existe une preuve en calcul des s√©quents classique (respectivement intuitionniste) de ce s√©quent sans utilisation de la r√®gle de coupure.\nTout s√©quent est donc prouvable par utilisation des seuls axiomes, r√®gles logiques et r√®gles structurelles. La d√©monstration du th√©or√®me consiste alors justement √† remplacer dans une preuve donn√©e chaque utilisation de la r√®gle de coupure par d\u0026rsquo;autres r√®gles.\nCorollaire¬†: le s√©quent \u0026ldquo;$\\vdash$\u0026rdquo; n\u0026rsquo;est pas prouvable en logique classique.\n\u0026ldquo;$\\vdash$\u0026rdquo; signifie que le vrai entra√Æne le faux. Nous voil√† plut√¥t soulag√© d\u0026rsquo;√™tre dans l\u0026rsquo;incapacit√© de prouver ce r√©sultat qui ferait s\u0026rsquo;effondrer tout l\u0026rsquo;√©difice logique patiemment construit. Si $\\vdash$ √©tait prouvable, alors par affaiblissement, tout s√©quent $\\vdash\\Delta$ le serait √©galement\u0026hellip;\nS\u0026rsquo;il existait une preuve du s√©quent $\\vdash$, il en existerait aussi une sans utilisation de la r√®gle de coupure. Or toutes les autres r√®gles introduisent une formule soit √† droite, soit √† gauche, soit des deux c√¥t√©s √† la fois. La seule r√®gle permettant de faire dispara√Ætre une formule est la r√®gle de contraction, mais elle ne la fait pas dispara√Ætre compl√®tement puisqu\u0026rsquo;elle se contente d\u0026rsquo;en faire dispara√Ætre les occurences. On ne pourra donc jamais aboutir √† \u0026ldquo;$\\vdash$\u0026rdquo; sans la r√®gle de coupure et donc d\u0026rsquo;apr√®s le th√©or√®me d\u0026rsquo;√©limination des coupures, on ne peut pas d√©montrer $\\vdash$ en calcul des s√©quents. Youpi.\nPropri√©t√© de la sous-formule\nSi le s√©quent $\\Gamma\\vdash\\Delta$ est prouvable en logique classique (respectivement intuitionniste), alors il existe une preuve en logique classique (respectivement intuitionniste) de ce s√©quent dans laquelle n\u0026rsquo;appara√Æssent que des s√©quents constitu√©s de sous-formules des formules de $\\Gamma$ et de $\\Delta$.\nIl existe une preuve sans coupure de $\\Gamma\\vdash\\Delta$. Or on peut v√©rifier r√®gle par r√®gle, par induction sur la hauteur de cette preuve sans coupure, qu\u0026rsquo;elle peut ne faire appara√Ætre que des sous-formules de $\\Gamma$ et de $\\Delta$.\ninfo\nUne cons√©quence de la propri√©t√© de la sous-formule est qu\u0026rsquo;une preuve d\u0026rsquo;une disjonction en logique intuitionniste passe n√©cessairement par une preuve d\u0026rsquo;un des termes de la disjonction¬†: $\\vdash_{\\!i}\\phi\\lor\\psi$ si et seulement si ($\\vdash_{\\!i}\\phi$ ou $\\vdash_{\\!i}\\psi$). Par exemple, prouver une instance du tiers exclu $\\Gamma\\vdash_{\\!i}\\phi\\lor\\neg\\phi$ en logique intuitionniste suppose qu\u0026rsquo;on a √©t√© capable soit de prouver $\\Gamma\\vdash_{\\!i}\\phi$, soit de prouver $\\Gamma\\vdash_{\\!i}\\neg\\phi$, et ce n\u0026rsquo;est pas le cas en logique classique.\nLa cons√©quence majeure de la propri√©t√© de la sous-formule est de permettre (tout particuli√®rement en logique intuitionniste) une recherche de preuve automatique. La production m√©canique de preuve comme par exemple la preuve de correction de programmes informatiques devient en effet possible une fois qu\u0026rsquo;on se d√©barrasse du caract√®re abstrait des preuves par coupures. Cela donne des preuves longues mais simples ne faisant appel qu\u0026rsquo;aux sous-formules des formules du s√©quent qu\u0026rsquo;il s\u0026rsquo;agit de prouver.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/relatres/",
	"title": "Relativit√© restreinte",
	"tags": [],
	"description": "",
	"content": " Relativit√© restreinte Vision g√©om√©trique S√©rie de vid√©os sur la relativit√© restreinte avec un angle plut√¥t g√©om√©trique.\nSource : le g√©nial petit livre \u0026ldquo;Very special relativity - An illustrated guide\u0026rdquo; de Sander Bais.\nDiagrammes d\u0026rsquo;espace-\u0026ldquo;temps propre\u0026rdquo; FloatHeadPhysics communique bien dans la vid√©o suivante la joie de la d√©couverte d\u0026rsquo;un mod√®le explicatif simple.\nIci, elle vient de ce merveilleux petit livre illustr√©¬†: Relativity visualized de Lewis Caroll Epstein.\nnote\nN√©anmoins, si les diagrammes utilisant le temps propre permettent de bonnes intuitions, ils ne peuvent pas r√©ellement se substituer aux diagrammes de Minkowski comme outil de travail.\nCe ne sont m√™me pas vraiment des diagrammes d\u0026rsquo;espace-temps puisque le temps propre n\u0026rsquo;est pas une coordonn√©e globale (il d√©pend du r√©f√©rentiel).\nMath√©matiquement, $\\tau=\\int \\sqrt{g_{\\mu\\nu}dx^\\mu dx^\\nu}$ (avec une signature ($+$,$-$,$-$,$-$)) et on ne peut donc pas √©crire $d\\tau = f_\\mu(x)dx^\\mu$. Cela signifie que $\\tau$ n\u0026rsquo;est pas une diff√©rentielle exacte et par suite qu\u0026rsquo;une int√©grale du temps propre d√©pend du chemin choisi.\nR√©sultat, la notion d\u0026rsquo;√©v√®nement perd son sens¬†! Dans un diagramme de Minkowski, chaque point est un √©v√®nement unique $(t,x)$ et c\u0026rsquo;est quand m√™me un peu ce qu\u0026rsquo;on attend d\u0026rsquo;une description de l\u0026rsquo;espace-temps\u0026hellip; Mais ce n\u0026rsquo;est plus le cas pour les diagrammes d\u0026rsquo;Epstein¬†! Les retrouvailles des jumeaux de Langevin ont lieu en deux points distincts par exemple.\nMasse du photon Paradoxe des jumeaux Champ magn√©tique Il n\u0026rsquo;y a pas de champ magn√©tique, seulement un champ √©lectrique et du mouvement.\nEn cela, le champ magn√©tique est l\u0026rsquo;illustration la plus spectaculaire de la relativit√© restreinte puisque le moindre aimant est une manifestation de la th√©orie d\u0026rsquo;Einstein¬†!\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/geometrie/geo3/",
	"title": "Aires et Volumes",
	"tags": [],
	"description": "",
	"content": " Aires et volumes "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/meca/hamilton/",
	"title": "Bypass vers Hamilton",
	"tags": [],
	"description": "",
	"content": " De Newton √† Hamilton "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/",
	"title": "Physique",
	"tags": [],
	"description": "",
	"content": " Physique M√©canique √âlectromagn√©tisme Optique Relativit√© restreinte Relativit√© g√©n√©rale M√©canique quantique Th√©orie quantique des champs Thermodynamique et Physique statistique Chaos Curiosit√©s "
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/proba/",
	"title": "Probabilit√©s",
	"tags": [],
	"description": "",
	"content": " Probabilit√©s Loi Binomiale D\u0026rsquo;une √©preuve de Bernoulli √† la loi binomiale :\nLe petit programme ci-dessous permet de d√©terminer l\u0026rsquo;intervalle de fluctuation pour un seuil donn√©.\nEt on pourra v√©rifier l\u0026rsquo;intervalle graphiquement avec l\u0026rsquo;applet suivant.\nLoi normale La c√©l√®bre gaussienne (ou courbe en cloche)¬†:\nOn peut √† nouveau tirer profit de l\u0026rsquo;applet g√©ogebra qui pr√©c√®de pour obtenir des intervalles de fluctuation.\nDensit√© de probabilit√©s Nouvelle √©nigme permettant de parler des densit√©s de probabilit√©¬†:\nQuelle est la distance moyenne entre deux points pris au hasard sur un segment¬†?\nOn peut v√©rifier la solution statistiquement avec un petit programme simplissime¬†:\nProbabilit√©s conditionnelles et Bayes Les probabilit√©s conditionnelles au tribunal, chez le m√©decin et dans un vieux jeu t√©l√©vis√© am√©ricain¬†:\nCha√Ænes de Markov Le jeu de Penney a d\u0026rsquo;√©tonnantes propri√©t√©s. Les cha√Ænes de Markov aident √† le d√©mystifier¬†:\nCuriosit√©s et √©nigmes Spaghetti et in√©galit√© triangulaire Une petite √©nigme √† base d\u0026rsquo;in√©galit√© triangulaire :\nQuelle est la probabilit√© de pouvoir faire un triangle avec les 3 bouts obtenus en cassant al√©atoirement un spaghetti en deux endroits ?\nAnniversaires simultan√©s Quelle est la probabilit√© que deux √©l√®ves d\u0026rsquo;une classe est un anniversaire le m√™me jour¬†?\nLorsqu\u0026rsquo;on n\u0026rsquo;a jamais fait le calcul ou entendu parler du r√©sultat, notre intuition nous am√®ne g√©n√©ralement √† soup√ßonner une probabilit√© bien plus petite qu\u0026rsquo;elle ne l\u0026rsquo;est.\nC\u0026rsquo;est d\u0026rsquo;ailleurs assez fr√©quent que notre intuition soit aux fraises lorsqu\u0026rsquo;il s\u0026rsquo;agit d\u0026rsquo;estimer une probabilit√© ou expliquer des statistiques (voir le paradoxe de Simpson).\nParadoxe des deux enfants Le paradoxe des deux enfants repose grandement sur la formulation et provoque encore parfois des d√©bats passionn√©s.\nParadoxe de Cover O√π comment un tirage al√©atoire permet de gagner de l\u0026rsquo;information¬†!\nOn vous pr√©sente deux papiers pli√©s o√π sont √©crits deux nombres choisis al√©atoirement $a$ et $b$.\nVous choisissez un des deux papiers et l\u0026rsquo;ouvrez pour d√©couvrir le nombre $X$ (soit $a$, soit $b$). Puis on vous demande lequel des deux papiers contient le plus grand nombre.\nCela semble du 50-50, et pourtant\u0026hellip; Si vous tirez un nombre al√©atoire $Z$ selon une distribution continue sur $\\mathbb{R}$, vous pouvez augmenter vos chances¬†! Il suffit de comparer $Z$ √† $X$¬†: si $Z\u0026gt;X$ vous choisissez $Y$ comme plus grand nombre (l\u0026rsquo;autre papier) et si $Z\u0026lt;X$, vous choisissez $X$.\nDans le code ci-dessous, on tire deux nombres al√©atoires $a$ et $b$ selon une distribution normale centr√©e sur 0 et d\u0026rsquo;√©cart-type 100, puis on tire au sort le nombre $X$. Petite diff√©rence par rapport √† ce qui pr√©c√®de, on se sert d\u0026rsquo;une fonction logistique pour \u0026ldquo;envoyer\u0026rdquo; $X$ entre 0 et 1 puis on le compare √† un $Z$ tir√© uniform√©ment entre 0 et 1.\nOn voit qu\u0026rsquo;on obtient alors un taux de succ√®s autour de 75%¬†!\nPar quel miracle¬†?\nD√©j√†, comme $Z$ est issue d\u0026rsquo;une loi continues, on n\u0026rsquo;aura jamais $Z=a$ ou $Z=b$ et pour que la m√©thode permette de deviner correctement, il faut que¬†:\n$X=\\max(a,b)$ (une chance sur deux) et $Z\u0026lt;X$ $X=\\min(a,b)$ (une chance sur deux) et $Z\u0026gt;X$ La probabilit√© de succ√®s est donc¬†:\nD'o√π $P_\\text { succ }=\\frac{1}{2}P(Z\u003c\\max (a, b))+\\frac{1}{2}[1-P(Z\u003c\\min (a, b))]=\\frac{1}{2}+\\frac{1}{2}[P(Z\u003c\\max (a, b))-P(Z \u003c \\min (a, b))]=\\frac{1}{2}+\\tfrac12\\color{#970E53}P\\left(\\min(a,b) \u003c Z\u003c\\max(a,b)\\right)$\nPuisque $Z$ suit une loi continue ${\\color{#970E53}P\\left(\\min(a,b) \u003c Z\u003c\\max(a,b)\\right)}\u003e0$. On se retrouve donc bien avec une probabilit√© sup√©rieure √† $\\tfrac12$. Dans une d√©monstration similaire (qu\u0026rsquo;on retrouve dans cet article de John Baez retra√ßant l\u0026rsquo;origine du paradoxe), Greg Egan utilise une fonction $f:\\mathbb{R}\\rightarrow ]0,1[$ strictement croissante (si $x\u0026lt;y$, $f(x)\u0026lt;f(y)$) pour ramener $X$ dans $]0,1[$. C\u0026rsquo;est en suivant cette m√©thode qu\u0026rsquo;on a utilis√© dans le code la fonction logistique $f(x)=\\frac{\\mathrm{e}^x}{\\mathrm{e}^x +1}$.\nLe nombre al√©atoire $Z$ est alors choisi uniform√©ment entre $0$ et $1$ et on compare $Z$ √† $f(X)$ plut√¥t qu\u0026rsquo;√† $X$.\nSupposons $a\u0026gt;b$.\nLa probabilit√© de deviner correctement devient¬†:\nOn obtient ainsi $P_\\text { succ }=\\frac{1}{2}+\\frac{1}{2}(f(a)-f(b))\u003e\\frac{1}{2}$ puisque $f(a)\u003ef(b)$ par croissance stricte de $f$. Les deux d√©monstrations sont √©quivalentes. Suffit de poser $f(x)=P(Z\u0026lt;x)$ (bien strictement croissante) pour s\u0026rsquo;en convaincre. $P_\\text { succ }$ devient alors $\\frac{1}{2}+\\frac{1}{2}(P(Z\u0026lt;\\max(a,b))-P(Z\u0026lt;\\min(a,b))$.\nDans le code ci-dessus, les nombres $a$ et $b$ sont volontairement tr√®s √©loign√©s de 0 ($\\sigma=100$). La fonction logistique donne alors soit $\\approx 0$ (nombre $\\ll 0$), soit $\\approx 1$ (nombre $\\gg 0$). Donc $f(a)-f(b)$ va donner $1$ lorsque $a$ et $b$ sont de signes diff√©rents (50% des cas) et $0$ dans les autres cas. √áa nous donne une esp√©rance de succ√®s de $\\tfrac12+\\tfrac12\\tfrac12=\\tfrac34$.\nEn diminuant $\\sigma$, on diminue le pourcentage de succ√®s (on retrouve que l\u0026rsquo;efficacit√© de la m√©thode est d\u0026rsquo;autant plus grande que $Z$ a de chances d\u0026rsquo;√™tre entre $a$ et $b$).\nSi $a$, $b$ et $Z$ sont tir√©s uniform√©ment dans $[0,1]$, on obtient une esp√©rance de succ√®s de $\\tfrac23$. En effet, on a alors $P_\\text { succ }(a,b)=\\tfrac12 + \\tfrac12|a-b|$. Or comme on l\u0026rsquo;a vu plus haut, $\\mathbb{E}[|a-b|]=\\tfrac13$, donc $\\mathbb{E}[P_\\text{succ}(a,b)]=\\tfrac12+\\tfrac12\\tfrac13=\\tfrac23$.\nCela finit par s√©rieusement perdre de sa magie\u0026hellip; Si la loi d\u0026rsquo;o√π est tir√© $Z$ est adapt√©e aux valeurs de $a$ et $b$, tout va pour le mieux, mais d√®s que ce n\u0026rsquo;est plus le cas, on se retrouve b√™tement autour de 50%\u0026hellip; Il suffit par exemple d\u0026rsquo;un $\\mu\\gg\\sigma$ dans loi normale du code. La fonction logistique va envoyer tout le monde sur $\\approx 1$ et on n\u0026rsquo;aura quasi jamais $Z\u0026gt;f(a)$.\nCertes, le hasard permet d\u0026rsquo;augmenter ses chances, mais tant que la distribution des nombres sur les papiers et celle du nombre al√©atoire tir√© ne se correspondent pas, l\u0026rsquo;avantage obtenu sera infinit√©simal\u0026hellip; Le tirage n\u0026rsquo;apporte donc en soit aucune information importante. Celle-ci viendrait plut√¥t du choix ad√©quat de la distribution de $Z$ puisqu\u0026rsquo;il impliquerait de conna√Ætre celle de $a$ et $b$¬†!\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/qed-light/",
	"title": "QED light",
	"tags": [],
	"description": "",
	"content": " R√©flexion vitreuse Pourquoi lors d‚Äôune r√©flexion sur un milieu transparent (eau, verre, etc.), la lumi√®re semble-t-elle se r√©fl√©chir uniquement sur la surface, alors que la transparence m√™me du milieu assure que la lumi√®re p√©n√®tre bien √† l‚Äôint√©rieur¬†? Qu‚Äôa donc de si particulier la surface¬†? Rien assur√©ment. Mais POURQUOI ALORS crie le curieux d√©sesp√©r√© face au Grand T√©ton.\nphoto : Chascar (Parc national du Grand Teton pr√®s du Jackson Lake Lodge)\nC‚Äôest l√† qu‚Äôun Feynman cap√©e de sa d√©concertante facilit√© d√©barque : ¬´c‚Äôest une histoire de petites fl√®ches pauvre cr√©ature limit√©e¬ª. Et il d√©veloppa ces histoires de fl√®ches dans une s√©rie de conf√©rences tout public consacr√©es √† l‚Äô√©lectrodynamique quantique (QED the strange theory of light and matter).\nL‚Äô√©lectrodynamique quantique, hmm\u0026hellip; D‚Äôapr√®s Wikipedia, cette th√©orie s‚Äôoccupe des interactions entre √©lectrons, donc de la lumi√®re. On lit aussi que la QED s‚Äôinscrit dans un cadre conceptuel plus large : la th√©orie quantique des champs (TQC). L√†, on a peur : la r√©ponse serait-elle aussi inaccessible que le Grand T√©ton¬†? Mais puisque l‚Äôirritant g√©nie te dit que ce n‚Äôest qu‚Äôune histoire de fl√®ches\u0026hellip;\nTentons de colporter Sa parole.\nLong pr√©alable : des fl√®ches qui tournent La quantique nous apprend que l‚Äôobservation d‚Äôun √©v√®nement est probabiliste : on a une certaine probabilit√© d‚Äôobserv√© un photon √† un endroit donn√©, et la probabilit√© compl√©mentaire de ne pas l‚Äôobserver. La TQC ajoute que cette probabilit√© finale r√©sulte d‚Äôune interf√©rence entre de multiples ondelettes de probabilit√© qui se propagent partout dans l‚Äôespace sur un champ myst√©rieux.\nPr√©cisons un peu¬†: quelque soit l‚Äô√©v√®nement, la probabilit√© de le d√©tecter s‚Äôobtient par superposition (ou somme) de toutes les probabilit√©s attach√©es √† chaque r√©alisation possible de l‚Äô√©v√®nement (m√™me la plus saugrenue !). Techniquement, on somme des amplitudes de probabilit√©s qui se propagent en oscillant au cours du temps le long de chaque chemin de r√©alisation consid√©r√©.\nMais sommer des trucs qui oscillent n‚Äôest pas tout √† fait aussi simple que de sommer des carottes. Cela revient en fait √† sommer des fl√®ches qui tournent.\nRemarque¬†: une fl√®che qui tourne peut √™tre mod√©lis√©e par un nombre complexe $\\mathrm{e}^{\\mathrm{i}t}$ (les complexes permettent de repr√©senter un vecteur, une fl√®che, en un seul nombre, la partie imaginaire servant de 2e axe, et $\\mathrm{e}^{\\mathrm{i}t}$ repr√©sente une fl√®che qui fait un tour par seconde). Gr√¢ce aux complexes, on somme presque des ondes comme des carottes.\nAjoutons que le nombre de tours par seconde que font ces fl√®ches (la fr√©quence des ondes) est proportionnel √† l‚Äô√©nergie (plus pr√©cis√©ment √† l‚Äôhamiltonien $H$, ce qui donne une amplitude complexe en $\\mathrm{e}^{\\mathrm{i}Ht}$).\nN‚Äôimporte quel processus, comme aller d‚Äôun point A √† un point B, peut se faire d‚Äôune infinit√© de fa√ßons. La TQC assure que toutes ces possibilit√©s sont √† prendre en compte et √† sommer, y compris la promenade du photon au jardin des tuileries avant de revenir en B. Dans un monde tr√®s simplifi√© o√π le photon n‚Äôinteragit avec personne lors de son parcours (√©nergie constante et uniforme), chacune de ces possibilit√©s a la m√™me probabilit√©r√©flexion de se r√©aliser, et par cons√©quent, chacune correspond √† une fl√®che de m√™me taille (tr√®s tr√®s petite). M√™me taille d‚Äôaccord, mais pas m√™me orientation. Chaque fl√®che aura tourn√© d‚Äôun angle proportionnel au temps du parcours¬†!\nEt cette rotation qui n‚Äôa l‚Äôair de rien est primordiale. En effet, on peut sommer 1000 fl√®ches de m√™me longueur et obtenir une fl√®che de longueur nulle, il suffit que, mises bout √† bout, la derni√®re fl√®che pointe sur le pied de la premi√®re.\nOn ajoute donc des petites fl√®ches les unes aux autres, une fl√®che par possibilit√©, et la probabilit√© final d‚Äôobserver l‚Äô√©v√®nement va d√©pendre de l‚Äôangle dont les fl√®ches tournent d‚Äôun chemin √† l‚Äôautre (le chemin √©tant une possibilit√© de r√©alisation). Rq¬†: cette somme sur l‚Äôensemble des chemins correspond aux int√©grales de chemin de Feynman.\nMais voyons o√π peuvent nous mener ces fl√®ches qui tournent\u0026hellip;\nPropagation rectiligne de la lumi√®re Reprenons l‚Äôhistoire du photon allant d‚Äôun point A √† un point B et dessinons tous les chemins possibles. R√©p√©tons-le, la longueur du chemin est proportionnelle √† l‚Äôangle de rotation de la fl√®che correspondante. En sommant toutes les fl√®ches, on se rend vite compte que seules certaines contributions sont ¬´constructives¬ª dans le sens o√π elles seules permettent d‚Äôaugmenter la longueur de la fl√®che finale. Et elles correspondent √† des parcours rectilignes ou quasi rectilignes.\nEn effet, pr√®s de la ligne droite, les diff√©rents chemins sont quasiment aussi longs et sont donc parcourus en des temps tr√®s proches. Par cons√©quent, les fl√®ches tournent peu et pointent √† peu pr√®s dans la m√™me direction. Leur somme donne donc une fl√®che plus grande ! C‚Äôest une situation d‚Äôinterf√©rence entre ondes en phase. Au contraire, d√®s qu‚Äôon s‚Äô√©carte de la ligne droite, √ßa tourne beaucoup et la somme ne donne plus rien (les ondes sont d√©phas√©es, les interf√©rences destructives).\nOn red√©couvre l√† le principe de Fermat (et tous les autres principes variationnels qui se r√©sument au principe de moindre action puisque c‚Äôest elle, $H\\times t$, qui fait la phase de nos ondes)¬†! Mais la lumi√®re ne ¬´cherche¬ª pas √† minimiser son temps de parcours, c‚Äôest seulement que pr√®s de ce chemin particulier, la quasi stationnarit√© des phases augmente la probabilit√© de pr√©sence. Un extr√©mum dans la variation de parcours correspond √† des fl√®ches variant peu d‚Äôangles pour des parcours proches et donc √† une accumulation d‚Äôamplitudes de probabilit√© dans cette zone. Et √† l‚Äôinverse, si on s‚Äô√©carte de cet extr√©mum (puits ou col) les variations sont grandes et la somme devient vite destructive (les fl√®ches tournent en rond).\nSi le milieu de propagation des photons est inhomog√®ne, faisant varier spatialement la vitesse de la lumi√®re (on verra plus bas que cette variation est artificielle), l‚Äôextr√©misation des temps de trajet ne donne alors plus une ligne droite ! D‚Äôo√π la r√©fraction (et les mirages).\nMais r√©homog√©n√©isons le milieu et revenons sur un point tr√®s important¬†: le parcours en ligne droite ne repr√©sente pas √† lui seul la contribution la plus probable. Il l‚Äôest autant que n‚Äôimporte quel autre parcours (y compris le passage par les Tuileries). C‚Äôest l‚Äôaccumulation de parcours proches variant peu qui augmente la probabilit√©¬†! Mais que peut-il alors bien se passer si on emp√™che les ondes de probabilit√© d‚Äôaller fureter aux alentours¬†?\nDiffraction Envoyons un photon depuis A et comparons la probabilit√© qu‚Äôil arrive en B ou en C. On a compris l‚Äôhistoire : dans le 1er cas, il faut sommer tous les chemins possibles entra A et B, et dans le 2e, entre A et C. D‚Äôun chemin √† l‚Äôautre, il y a plus de variation entre A et C, donc les fl√®ches tournent plus et les ondes de probabilit√© ont donc plus vite fait de se d√©truire.\nR√©sultat¬†: une probabilit√© de d√©tection beaucoup plus faible en C qu‚Äôen B (n√©gligeable m√™me).\nMais si on resserre le passage\u0026hellip;\nMoins de chemins sont maintenant accessibles et les ondes de probabilit√© n‚Äôont donc plus trop le loisir d‚Äôinterf√©rer destructivement. Tous les lieux de d√©tection tendent alors √† se ressembler et la probabilit√© d‚Äô√™tre d√©tect√© en C n‚Äôest ainsi plus du tout n√©gligeable¬†! La lumi√®re est diffract√©e.\nOn peut aussi jouer les vicieux et obturer certains passages de mani√®re √† ce que les fl√®ches arrivent toute en phase en un point o√π, sans cela, la probabilit√© de d√©tection aurait √©t√© tr√®s faible.\nSans bouchons, c‚Äôest pas terrible¬†:\nAvec bouchons, oulala la grosse fl√®che¬†:\nOn vient de fabriquer un r√©seau et prouver du m√™me coup que les probas fouinent bien partout¬†; un endroit plong√© dans le noir peut √™tre √©clabouss√© de lumi√®re en cachant une partie de cette lumi√®re¬†! Il faut se d√©brouiller n√©anmoins pour la cacher astucieusement afin d‚Äôaugmenter la probabilit√© de pr√©sence en cet endroit. Au d√©part, les probas √©taient bien venues voir C mais elles trouvaient l‚Äôendroit pas terrible. Apr√®s disposition des petits caches, C devient tout √† fait fr√©quentable\u0026hellip;\nMais revenons au Grand T√©ton.\nRalentissement de la lumi√®re dans le verre Il nous manque un ingr√©dient pour parler de r√©flexion : la diffusion d‚Äôun photon par un atome (la lumi√®re r√©fl√©chie n‚Äôest pas la lumi√®re incidente, c‚Äôest de la lumi√®re toute neuve, crach√©e par un atome √©clair√©).\nIl va y a voir une certaine probabilit√© d‚Äô√™tre diffus√©, une nouvelle fl√®che\u0026hellip; Sa taille donne la proba de diffusion dans la direction qui nous int√©resse, chose qu‚Äôon ne sait pas calculer dans le cas du verre ou de l‚Äôeau mais qu‚Äôon peut d√©duire exp√©rimentalement comme on le verra plus loin.\nOn a sa taille (enfin non mais on fait comme si), il manque encore la direction¬†:\ndans le cas d‚Äôun milieu transparent, la proba de diffusion fait un quart de tour par rapport √† la proba de passer sans encombre (+90¬∞). Effectivement, la fl√®che finale (√™tre et ne pas √™tre diffus√©e) ne doit pas se trouv√©e r√©duite (dans l‚Äôapproximation o√π le milieu est parfaitement transparent et donc n‚Äôabsorbe pas, √ßa semble logique), ni agrandie (car √ßa serait bizarre). Seule possibilit√© restante¬†: l‚Äôajout d‚Äôune petite fl√®che √† 90¬∞ qui fait seulement tourner la probabilit√© de ne pas √™tre diffus√©.\nEn vrai, bien s√ªr, le milieu absorbe un peu. Il suffit, pour le retranscrire en fl√®ches, de courber √† peu moins de 90¬∞ la fl√®che de la diffusion, ce qui r√©duira la taille de la fl√®che finale.\nFaisons traverser une certaine √©paisseur de verre √† la lumi√®re et regardons la probabilit√© que le r√©cepteur en d√©tecte. On simplifie en supposant que la lumi√®re ne se dirige que dans une direction.\nUn photon d√©tect√© peut √™tre pass√© √† travers le verre sans avoir √©t√© diffus√© (grosse fl√®che bleue) ou avoir √©t√© √©mis par un atome apr√®s diffusion (petite fl√®che verte). Chaque atome sur le trajet est susceptible d‚Äôavoir √©mis un photon d√©tect√© apr√®s avoir √©t√© atteint par le photon incident (diff√©rentes fl√®ches vertes, toutes align√©es car le trajet total est toujours aussi long). On somme tout √ßa et qu‚Äôobtient-on approximativement¬†? Une grosse fl√®che de passage direct qui a tourn√© un peu (fl√®che rouge).\nTr√®s bien, mais n‚Äôy a-t-il pas autre chose qui aurait pu donner une fl√®che semblable¬†? Si, si, un trajet sans verre mais un poil plus long¬†! On peut donc faire semblant et dire que le temps de parcours (et donc l‚Äôangle de rotation de la fl√®che) est rallong√© par le verre, puisque tout se passe comme si. D‚Äôo√π la vilaine et n√©anmoins tr√®s pratique assertion ¬´le verre ralentit la lumi√®re¬ª (alors que les photons continuent tous √† se d√©placer √† c bien s√ªr).\nR√©flexion vitreuse Ay√©, il est temps d‚Äôessayer de r√©pondre √† la question du d√©part\u0026hellip; Il suffit de d√©placer le r√©cepteur au niveau de l‚Äô√©metteur (pour rester avec une seule direction) et on regarde ce qui se passe avec les fl√®ches¬†:\nLa lumi√®re qui retourne au r√©cepteur a n√©cessairement √©t√© diffus√©e (fl√®che verte) et suivant la profondeur de l‚Äôatome diffusant, le parcours total est plus ou moins long, et donc, d‚Äôune possibilit√© de diffusion √† l‚Äôautre, les fl√®ches vertes tournent (la probabilit√© d‚Äô√™tre d√©tect√© apr√®s diffusion sur l‚Äôatome 4 est d√©phas√©e par rapport √† celle correspondant √† l‚Äôatome 3). Mises bout √† bout, les probabilit√©s de diffusion, de m√™me taille et un peu d√©phas√©es, tournent en rond. Le r√©sultat final peut alors se r√©sumer √† la somme de deux rayons de cercle (fl√®che bleue + fl√®che rouge). Suivant l‚Äô√©paisseur de la vitre travers√©e, la probabilit√© de d√©tecter un photon r√©fl√©chi varie donc entre 0 et le diam√®tre du cercle dessin√© par les probabilit√©s. Exp√©rimentalement, cette oscillation de la r√©flexion en fonction d l‚Äô√©paisseur est bien observ√©e et on la trouve comprise entre 0 et 16%. Mais cela suppose une √©paisseur de vitre parfaitement d√©termin√©e (√† une fraction de longueur d‚Äôonde pr√®s). Dans un cas plus ordinaire, la fl√®che rouge s‚Äôaffole et fait pleins de tours, son orientation devient al√©atoire. La probabilit√© d‚Äô√™tre d√©tect√© se r√©duit √† la fl√®che bleue. On mesure donc facilement le rayon du cercle puisqu‚Äôil correspond √† la probabilit√© de r√©flexion moyenne sur une surface vitr√©e (8%).\nOn remarque que la fl√®che verte n¬∞1, correspondant √† la probabilit√© de diffusion sur un atome de la surface (le tout premier), fait un angle de -90¬∞ par rapport √† la fl√®che bleue. La fl√®che bleue a par cons√©quent un angle de rotation oppos√© (+œÄ) √† la fl√®che qui repr√©senterait la probabilit√© de faire le m√™me trajet aller-retour, mais sans avoir √©t√© diffus√© (si c‚Äô√©tait possible), rebond du photon sur la face avant quoi. Par ailleurs, la fl√®che rouge a, elle, une orientation correspondante √† un trajet long comme l‚Äôaller-retour jusqu‚Äô√† la face post√©rieure (la diffusion sur le petit dernier, n¬∞6, fait bien un angle d‚Äôenviron +90¬∞ avec elle).\nOn peut maintenant simplifier notre description. La r√©flexion sur une vitre se r√©sume √† l‚Äôinterf√©rence entre deux trajets possibles pour le photon¬†:\nrebond sur la face avant + d√©phasage de 180¬∞ avec une probabilit√© de 8% (fl√®che bleue), rebond sur la face arri√®re avec aussi une probabilit√© de 8% (fl√®che rouge). Et si la vitre est ordinaire (√©paisseur al√©atoire sur une taille caract√©ristique d‚Äôune fraction de longueur d‚Äôonde), tout se r√©sume √† un rebond sur la face avant\u0026nbsp;! Et l‚Äôagacement nerveux provoqu√© par les deux Grands T√©tons s‚Äôapaisent enfin (les quelques secondes o√π on r√©ussi √† √©touffer le d√©sesp√©rant bourdonnement des millions de questions qui essaiment derri√®re ces fl√®ches).\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/relatge/",
	"title": "Relativit√© g√©n√©rale",
	"tags": [],
	"description": "",
	"content": " Relativit√© g√©n√©rale Le plus impressionnant livre sur la relativit√© g√©n√©rale est sans contexte √† mes yeux \u0026ldquo;A Journey into Gravity and Spacetime\u0026rdquo; du g√©nial John Archibald Wheeler. Il r√©ussit le tour de force d\u0026rsquo;expliquer la relativit√© g√©n√©rale quasiment sans math√©matiques tout en n\u0026rsquo;esquivant rien de la profondeur conceptuelle de la th√©orie, au contraire.\nLe livre d√©marre par un long po√®me dont les derniers vers d√©voilent la notion au c≈ìur de son explication, relation fondamentale de la topologie alg√©brique qui peut sembler de prime abord assez cryptique¬†: le bord d\u0026rsquo;un bord est vide.\nHelp me to make this account\nA radiant testimony\nTo the wonderful simplicity\nOf the principle that the boundary of a boundary is zero,\nHeart of Einstein\u0026rsquo;s great 1915,\nBattle-tested, and still standard Geometric account of your action, oh Gravity.\nWheeler est aussi l\u0026rsquo;un des 3 auteurs (les deux autres sont ses anciens th√©sards Kip Thorne et Charles Misner) de la bible de la relat, alias le \u0026ldquo;gros livre noir\u0026rdquo; ou MTW, bien s√ªr beaucoup plus touffu et math√©matique que son \u0026ldquo;voyage dans la gravit√© et l\u0026rsquo;espace-temps\u0026rdquo;, mais enrob√© d\u0026rsquo;une narration, l√† aussi, assez singuli√®re.\nVid√©o sur les ondes gravitationnelles et sur leur d√©tection par le clignotement des pulsars.\nJe m\u0026rsquo;√©tais lanc√©, il y a longtemps, dans la r√©daction de notes \u0026ldquo;am√©lior√©es\u0026rdquo; apr√®s visionnage de ces vid√©os de L√©onard Susskind sur la relativit√© g√©n√©rale.\nSusskind est un immense physicien th√©oricien et les vid√©os sont des captations de ses cours donn√©s dans le cadre du Stanford Continuing Studies Program (sorte de cours du soir pour enthousiastes).\nLa partie 1 introduit le principe d\u0026rsquo;√©quivalence et les outils math√©matiques n√©cessaires (tenseurs, m√©trique, d√©riv√©e covariante). La partie 2 se concentre sur la courbure. La partie 3 est le plat de r√©sistance. Elle pr√©sente l\u0026rsquo;√©quation d\u0026rsquo;Einstein et d√©taille aussi les ondes gravitationnelles. La partie 4, moins aboutie, pr√©sente Schwarzschild et les trous noirs. La partie 5 se concentre sur les 3 tests historiques de la relativit√© g√©n√©rale. "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/quantique/3ndimensions/",
	"title": "3N dimensions",
	"tags": [],
	"description": "",
	"content": " Une ontologie √† 3N dimensions Ontologie de la fonction d\u0026rsquo;onde Ney, A. ¬´ Three arguments for wave function realism ¬ª, European Journal for Philosophy of Science, vol. 13, art. 50 (24 octobre 2023).\nDonner une r√©alit√© √† la fonction d\u0026rsquo;onde $\\psi$, c\u0026rsquo;est accepter que notre espace 3D ne soit qu\u0026rsquo;une projection de l\u0026rsquo;espace √† 3N dimensions dans lequel $\\psi$ vit.\nPourquoi 3N dimensions ? En m√©canique quantique, la fonction d\u0026rsquo;onde est construite comme un champ sur un espace de $3N$ dimensions o√π $N$ est le nombre de particules.\nPourquoi s\u0026rsquo;emb√™ter avec ces dimensions suppl√©mentaires¬†?\n√Ä cause des intrications.\nSupposons que l\u0026rsquo;on ait deux particules pr√©par√©es dans un √©tat singulet de spin $|\\uparrow\\downarrow\\rangle-|\\downarrow\\uparrow\\rangle$. Les particules sont ensuite envoy√©es dans des directions oppos√©es vers deux Stern-Gerlach qui vont les d√©vier vers le haut ou vers le bas en fonction de la valeur de leur spin selon l‚Äôaxe $z$.\nOn appelle $\\text{A}$, $\\text{B}$, $\\text{C}$ et $\\text{D}$ les 4 localisations possibles apr√®s les d√©flections.\nComme les particules deviennent intriqu√©es avec les appareils de mesure, elles se trouvent au final dans un √©tat intriqu√© non pas seulement par rapport √† leurs spins mais aussi par rapport √† leurs positions¬†:\n$$\\psi=\\frac{1}{\\sqrt{2}}\\left[\\left|\\uparrow_z, A\\right\\rangle_1\\left|\\downarrow_z, D\\right\\rangle_2+\\left|\\downarrow_z, B\\right\\rangle_1\\left|\\uparrow_z, C\\right\\rangle_2\\right]$$ √Ä 3-D, on ne peut pas interpr√©ter cet √©tat quantique comme un seul champ si on veut qu\u0026rsquo;il attribue des propri√©t√©s intrins√®ques et s√©parables √† chaque particule. Il faut donc deux champs dont les composantes correspondront aux diff√©rentes positions.\nSupposons maintenant que les deux particules ne soient pas dans un √©tat intriqu√© mais dans un √©tat produit $|\\uparrow_x\\rangle_1\\otimes |\\uparrow_x\\rangle_2$. Apr√®s les Stern-Gerlach, on a¬†:\n$$\\psi^{\\prime}=\\frac{1}{2}\\left[\\left|\\uparrow_z, A\\right\\rangle_1\\left|\\uparrow_z, C\\right\\rangle_2+\\left|\\uparrow_z, A\\right\\rangle_1\\left|\\downarrow_z, D\\right\\rangle_2+\\left|\\downarrow_z, B\\right\\rangle_1\\left|\\uparrow_z, C\\right\\rangle_2+\\left|\\downarrow_z, B\\right\\rangle_1\\left|\\downarrow_z, D\\right\\rangle_2\\right]$$ On peut essayer √† nouveau de d√©crire cet √©tat par deux champs √† 3-D.\nMais comment alors peut-on distinguer les deux √©tats quantiques¬†? Les deux champs obtenus ont bien, chacun, la m√™me densit√© sur $\\text{A}$, $\\text{B}$, $\\text{C}$ et $\\text{D}$ ; en 3-D on ne voit donc aucune diff√©rence.\nPar contre, si on interpr√®te ces √©tats quantiques non plus comme deux champs √† 3-D mais un champ √† 6-D, on capture la corr√©lation entre les √©tats de $\\psi$ et donc la distinction avec $\\psi^\\prime$.\nIl suffit pour cela de consid√©rer 4 points dans l\u0026rsquo;espace √† 6-D d√©sign√©s de mani√®re transparente par $\\text{AC}$, $\\text{AD}$, $\\text{BC}$ et $\\text{BD}$ (le point $\\text{AC}$ dans l\u0026rsquo;espace 6-D des configurations correspond √† la situation dans l\u0026rsquo;espace 3-D o√π la particule 1 est localis√©e en $\\text{A}$ et la particule 2 est localis√©e en $\\text{C}$).\nDans cet espace √† 3N-D, $\\psi$ d√©crit un champ avec seulement des composantes en $\\text{AD}$ et $\\text{BC}$. $\\psi^\\prime$, lui, d√©crit un champ avec des composantes aux 4 points $\\text{AC}$, $\\text{AD}$, $\\text{BC}$, $\\text{BD}$.\nLe sch√©ma bi-champ efface la corr√©lation ‚Äúsi et seulement si‚Äù entre la localisation de 1 et celle de 2.\nS√©parabilit√© et localit√© Vouloir d√©crire la fonction d\u0026rsquo;onde dans un espace impose d\u0026rsquo;accepter la non s√©parabilit√© et/ou la non localit√© de la th√©orie.\nUne m√©taphysique est s√©parable si les parties (R1 et R2) permettent de d√©duire le tout (R1 $\\cup$ R2) .\nLa localit√© signifie que deux r√©gions non causalement reli√©es ne peuvent pas s\u0026rsquo;influencer.\nUne motivation pour une th√©orie s√©parable et locale est l\u0026rsquo;obtention d\u0026rsquo;une ontologie simple, propre, parcimonieuse (rasoir d\u0026rsquo;Ockham)¬†:\nles faits concernant une entit√© ne d√©pendent pas des faits concernant une autre, et un objet ne peut pas agir l√† o√π il n\u0026rsquo;est pas.\nDans l\u0026rsquo;ontologie de la fonction d\u0026rsquo;onde, la physique est bien s√©parable et locale (mais dans l‚Äôespace √† 3N-D, pas dans l‚Äôespace 3-D). En effet¬†:\nL‚Äôunique objet ontique est un champ $\\psi$ sur l‚Äôespace des configurations $\\mathbb R^{3N}$. L‚Äô√©quation de Schr√∂dinger est une √©quation aux d√©riv√©es partielles locales dans cet espace¬†: $\\partial_t\\psi(q,t)=\\hat H(q)\\psi(q,t)$. Un ¬´ point ¬ª $q=(\\mathbf r_1,\\mathbf r_2,\\dots)$ est par construction une r√©gion √©l√©mentaire¬†; donner $\\psi$ sur deux sous-r√©gions disjointes $Q_1,Q_2\\subset\\mathbb R^{3N}$ suffit √† fixer $\\psi$ sur leur union. Revenons √† l\u0026rsquo;√©tat singulet avec les deux stern-Gerlach.\nLa fonction d\u0026rsquo;onde sur les deux points $q_{\\text{AD}}=\\text{AD}$ et $q_\\text{BC}=\\text{BC}$ dans $\\mathbb R^{6}$ s\u0026rsquo;√©crit $\\psi(q)=\\tfrac1{\\sqrt2}\\bigl[\\delta(q-q_\\text{AD})-\\delta(q-q_\\text{BC})\\bigr]$.\nRestreindre $\\psi$ √† la petite boule $Q_\\text{AD}$ autour de $q_{\\text{AD}}$ et √† $Q_\\text{BC}$ autour de $q_\\text{BC}$ suffit pour conna√Ætre tout $\\psi$. Cela montre la s√©parabilit√©.\n√Ä 3D par contre, comme on l\u0026rsquo;a vu, conna√Ætre $|\\phi_1|^2$ sur les points $\\text{A}$ et $\\text{B}$ et $|\\phi_2|^2$ sur $\\text{C}$ et $\\text{D}$ ne d√©termine pas la corr√©lation ¬´¬†$\\text{A}‚ÄØ\\leftrightarrow‚ÄØ\\text{D},¬†\\text{B}‚ÄØ\\leftrightarrow‚ÄØ\\text{C}‚ÄØ$¬†¬ª. L‚Äô√©tat intriqu√© et l‚Äô√©tat produit restent indistinguables. La th√©orie n‚Äôest donc pas s√©parable si l‚Äôespace 3D est fondamental.\nL‚Äô√©volution de $\\psi$ en $Q_{\\text{AD}}$ d√©pend seulement de $\\psi$ dans $Q_{\\text{AD}}$ (via $\\hat H$ √©valu√© en $q_{\\text{AD}}$). Cela montre la localit√©.\nUn petit d√©placement $(\\delta\\mathbf r_1,\\delta\\mathbf r_2)$ permet de rester dans la boule $Q_\\text{AD}$ dans $\\mathbb R^6$ ($\\text{AD}\\rightarrow \\text{AD}^\\prime$ proche de $\\text{AD}$), mais le m√™me mouvement d√©placerait deux particules simultan√©ment dans des r√©gions de l\u0026rsquo;espace √©loign√©es √† 3D¬†: $\\text{A}\\rightarrow \\text{A}^\\prime$ et $\\text{D}\\rightarrow \\text{D}^\\prime$. On perdrait la localit√©.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/geometrie/geo4/",
	"title": "Dimensions",
	"tags": [],
	"description": "",
	"content": " Curse of dimensionality L\u0026rsquo;expression Curse of dimensionality (Fl√©au de la dimension dans sa traduction fran√ßaise) vient de l\u0026rsquo;am√©ricain Richard Bellman alors qu\u0026rsquo;il consid√©rait des probl√®mes en programmation dynamique (qu\u0026rsquo;il inventa en 1953).\nMais qu\u0026rsquo;est-ce donc que ce \u0026ldquo;fl√©au\u0026rdquo;¬†?\nLe volume d\u0026rsquo;un cube de c√¥t√© $a$ dans un espace √† $n$ dimensions vaut $a^n$. Jusque l√†, √ßa va.\nPour la boule, de rayon $R$, c\u0026rsquo;est plus compliqu√©\u0026hellip;\n√† 1D, la boule se r√©duit √† un segment de longueur $R$. √† 2D, c\u0026rsquo;est un disque de \u0026ldquo;volume\u0026rdquo; $\\pi R^2$. Ce qu\u0026rsquo;on appelle aire dans notre monde 3D est bien le volume (l\u0026rsquo;espace contenu √† l\u0026rsquo;int√©rieur) d\u0026rsquo;un monde 2D. √† 3D, le volume de la boule vaut $\\frac{4}{3}\\pi R^3$. Pour des boules de rayon 1, √ßa nous donne les volumes suivant¬†: $1$ √† 1D, $\\pi$ √† 2D, $\\frac{4\\pi}{3}$ √† 3D. Est-ce que cela continue d\u0026rsquo;augmenter pour les dimensions sup√©rieures¬†?\nAussi bizarre que cela puisse para√Ætre, on va voir que le volume culmine en dimension 5 puis s\u0026rsquo;effondre jusqu\u0026rsquo;√† presque s\u0026rsquo;√©vanouir¬†!\nLa petite √©nigme suivante va nous familiariser avec ces bizarreries.\nPetite √©nigme On prend un $n$-cube (cube de dimension $n$) de c√¥t√© $4$ et on y place √† l\u0026rsquo;int√©rieur autant de $n$-boules de rayon $1$ que l\u0026rsquo;on peut.\nQuel sera le rayon de la plus grande $n$-boule possible que l\u0026rsquo;on pourra placer au centre de la boite, dans l\u0026rsquo;espace laiss√© libre par les autres boules¬†?\nOn peut d√©composer la boite en $2^n$ sous-boites de c√¥t√© $2$ contenant chacune une boule de c√¥t√© $1$.\n√† 1D, il ne reste aucune place pour une sph√®re au milieu¬†: le $1$-cube de c√¥t√© $4$ est un segment de longueur $4$ dans lequel on peut placer deux $1$-boules de rayon $1$ qui sont des segments de longueur $2$. √† 2D, 4 $2$-boules (disques) sont plac√©s dans le $2$-cube (carr√©). Pythagore nous dit que la distance entre le centre de chaque boule et le centre de la boite vaut $\\sqrt{R^2+R^2}=R\\sqrt{2}$. Par cons√©quent, le rayon de la plus grande $2$-boule (disque) que l\u0026rsquo;on peut placer au centre est de $R\\sqrt{2}-R\\Rightarrow\\sqrt{2}-1$. √† 3D, on place 8 $3$-boules dans le $3$-cube. On peut faire le m√™me raisonnement qu\u0026rsquo;√† 2D pour trouver la distance entre le centre des $3$-boules et celui de la boite. C\u0026rsquo;est toujours Pythagore qui nous aide (du moins sa g√©n√©ralisation √† $n$ dimensions) et on obtient comme plus grand rayon possible pour la $3$-boule du milieu¬†: $\\sqrt{R^2+R^2+R^2}-R = R\\sqrt{3}-R \\Rightarrow \\sqrt{3}-1$. On peut sans probl√®me g√©n√©raliser √† $n$ dimensions. Le rayon de la sph√®re du milieu vaudra $\\sqrt{n}-1$. Facile¬†!\nMais regardons maintenant ce que cela signifie.\n√Ä 4D, la sph√®re du milieu est aussi grande que les 16 autres¬†! √Ä 9D, le rayon de la sph√®re centrale vaut 2¬†!!! Elle est deux fois plus grande que les 512 voisines et elle touche les parois de la boite. √Ä partir de 10D, la boule centrale d√©borderait de la boite\u0026hellip; Sorcellerie¬†!\nVolume d\u0026rsquo;une $n$-boule Le volume $V_n$ d\u0026rsquo;une $n$-boule de rayon $R$ peut se d√©terminer gr√¢ce √† la formule de r√©currence suivante¬†$\\displaystyle V_n(R)=\\frac{2\\pi R^2}{n} V_{n-2}(R)$\nEn prenant $V_0(R) =1$ (√† partir de la formule du cube $a^n$) et $V_1(R)=2R$, on peut d√©terminer le volume en toute dimension.\nD√©monstration de la formule de r√©currence\u0026nbsp;: Le volume du $n$-boule est par d√©finition $\\displaystyle V_n(R) = \\underset{x_1^2+x_2^2+\\cdots+x_n^2‚â§R^2}{\\int\\cdots\\int} dx_1 dx_2\\cdots dx_n$.\nS√©parons l'int√©gration en deux\u0026nbsp;: $\\displaystyle V_n(R) = \\underset{x_1^2+x_2^2‚â§R^2}{\\iint}\\left(\\underset{x_3^2+\\cdots+x_n^2‚â§R^2-x_1^2-x_2^2}{\\int\\cdots\\int} dx_3\\cdots dx_n\\right)dx_1dx_2 = \\underset{x_1^2+x_2^2‚â§R^2}{\\iint}V_{n-2}\\left(\\sqrt{R^2-x_1^2-x_2^2}\\right)dx_1dx_2$\nPassons en coordonn√©es polaires pour les deux premi√®res dimensions en posant $x_1 = r\\cos\\theta$ et $x_2 = r\\sin\\theta$.\nOn a alors $x_1^2+x_2^2=r^2$ et $dx_1dx_2 = r dr d\\theta$.\n$V_n(R)$ devient\u0026nbsp;:\n$\\displaystyle V_n(R) = \\int_0^{2\\pi}\\int_0^R V_{n-2}\\left(\\sqrt{R^2-r^2}\\right)rdrd\\theta$\nOr par sym√©trie et analyse dimensionnelle $V_n(a\\times R) = a^nV_n(R)$, et ici, on a $V_{n-2}\\left(\\sqrt{R^2-r^2}\\right)=V_{n-2}\\left(\\left(\\sqrt{1-\\frac{r^2}{R^2}}\\right)R\\right)=\\left(1-\\frac{r^2}{R^2}\\right)^{\\frac{n-2}{2}}V_{n-2}(R)$\nFinalement $V_n$ se r√©duit √†\u0026nbsp;:\n$ \\begin{aligned} V_n(R) \u0026= V_{n-2}(R)\\int_0^{2\\pi}\\int_0^R \\left(1-\\frac{r^2}{R^2}\\right)^{\\frac{n-2}{2}}rdrd\\theta\\\\\\\\ \u0026 = 2\\pi V_{n-2}(R) \\int_0^R \\left(1-\\frac{r^2}{R^2}\\right)^{\\frac{n-2}{2}}rdr\\\\\\\\ \u0026 = 2\\pi V_{n-2}(R) \\left[-\\frac{R^2}{2}\\frac{2}{n}\\left(1-\\frac{r^2}{R^2}\\right)^{\\frac{n}{2}}\\right]_{r=0}^{r=R}\\\\\\\\ \u0026 = \\frac{2\\pi R^2}{n} V_{n-2}(R) \\end{aligned} $ On va ne s\u0026rsquo;int√©resser ici qu\u0026rsquo;aux $n$-boules de rayon un. Le petit programme suivant nous permet d\u0026rsquo;obtenir leurs volumes¬†:\nLe volume culmine en dimension 5 puis ne fait que diminuer. Qu\u0026rsquo;a de particulier la dimension 5¬†? Pas grand chose\u0026hellip;\nRegardons ce que nous dit la formule de r√©currence¬†:\nsi $n$ est pair¬†:\n$\\displaystyle V_n(R) = \\frac{2\\pi R}{n}\\times\\frac{2\\pi R}{n-2}\\times\\cdots\\times \\frac{2\\pi R}{4}\\times\\frac{2\\pi R}{2}\\times V_0(R)$, avec $V_0(R) = 1$\nDonc $\\displaystyle V_n(R) = \\frac{2^{n/2}\\pi^{n/2}R^n}{n\\cdot (n-2)\\cdots 4 \\cdot 2} = \\frac{\\pi^{n/2}R^n}{\\left(\\frac{n}{2}\\right)!}$ si $n$ est impair¬†:\n$\\displaystyle V_n(R) = \\frac{2\\pi R}{n}\\times\\frac{2\\pi R}{n-2}\\times\\cdots\\times \\frac{2\\pi R}{3}\\times\\frac{2\\pi R}{1}\\times V_1(R)$, avec$V_1(R)=2R$\nDonc $\\displaystyle V_n(R) = \\frac{2^{\\frac{n+1}{2}}\\pi^{\\frac{n-1}{2}}R^n}{n\\cdot (n-2)\\cdots 3\\cdot 1} $\nL\u0026rsquo;ensemble des deux formules peuvent se combiner en une seule utilisant la fonction gamma $\\Gamma$¬†:\n$\\displaystyle V_n(R) = \\frac{\\pi^{\\frac{n}{2}}R^n}{\\Gamma\\left(\\frac{n}{2}+1\\right)} $ En tra√ßant l\u0026rsquo;√©volution du volume pour diff√©rents rayons, on constate que le maximum change de dimension.\nLa pr√©sence d\u0026rsquo;un maximum s\u0026rsquo;explique par la course entre la tortue num√©rateur, √† base de puissances, et le li√®vre d√©nominateur fait de factorielles. Si les puissances peuvent d√©marrer plus vite, la factorielle finira toujours fatalement par dominer (dans les deux cas, on a un produit de $n$ facteurs mais pour la factorielle, la taille des facteurs augmente en fonction de $n$). Et c\u0026rsquo;est bien √ßa le plus int√©ressant¬†: cet √©vanouissement du volume de l\u0026rsquo;hyperboule pour des grandes dimensions.\nEssayons de mieux comprendre son origine.\nBoule dans boite Pla√ßons maintenant la $n$-boule de rayon 1 dans un $n$-cube de c√¥t√© 2 et calculons la proportion de volume occup√© par la boule (il suffit de diviser par $2^n$ le volume de la boule).\nLe rapport des deux volumes s\u0026rsquo;√©crabouille tr√®s tr√®s vite. Il faut une √©chelle logarithmique pour y voir plus clair.\n√Ä 11 dimensions, la boule ne repr√©sente plus qu\u0026rsquo;un milli√®me du volume de la boite et c\u0026rsquo;est environ un millioni√®me en dimension 17, soit pas grand chose\u0026hellip; Alors qu\u0026rsquo;il s\u0026rsquo;agit bien √† chaque fois de la plus grande sph√®re possible cal√©e dans la boite¬†! Si on augmentait son rayon d\u0026rsquo;un poil, elle d√©borderait.\nRaison de cette apparente √©tranget√©¬†: la diff√©rence entre le volume du cube et celui de la sph√®re est √† chercher dans les coins¬†!\nOr plus la dimension augmente, plus le cube a de coins\u0026hellip; En effet, un $n$-cube poss√®de $2^n$ coins. Ajoutez √† cela que les diagonales deviennent de plus en plus grandes ($\\sqrt{n}$), et on commense √† se convaincre que le volume de la sph√®re va vite devenir n√©gligeable\u0026hellip;\nConcentration sur les bords En tirant des points au hasard dans un hypercube, on va v√©rifier que plus la dimension augmente, plus le volume se concentre sur les bords.\nDans le petit programme suivant, on tire un million de points au hasard dans un $n$-cube de c√¥t√© 1 et on calcule la proportion qui se trouve √† une distance inf√©rieure √† 0,1 d\u0026rsquo;un bord. C\u0026rsquo;est facile, il suffit de checker si c\u0026rsquo;est le cas pour chaque tirage correspondant √† la coordonn√©e sur chaque axe. D√®s qu\u0026rsquo;une coordonn√©e est inf√©rieure √† 0,1 ou sup√©rieure √† 0,9 (on suppose ici que les coordonn√©es des sommets du cube sont des s√©quences de 0 et de 1), on compte le point.\nLa raison de cette concentration devient √©vidente √† la lecture du programme¬†; pour qu\u0026rsquo;un point soit pr√®s d\u0026rsquo;un bords, il suffit qu\u0026rsquo;une de ses coordonn√©es soit pr√®s d\u0026rsquo;un bord. Or si on augmente le nombre de dimensions, on augmente par la m√™me le nombre de coordonn√©es et donc le nombre de tirages. Plus la dimension augmente moins √ßa devient possible qu\u0026rsquo;aucune des coordonn√©es ne tombe pr√®s d\u0026rsquo;un bord.\nCela permet d\u0026rsquo;ailleurs d\u0026rsquo;expliquer que le volume de l\u0026rsquo;hypersph√®re de rayon 1 se ratatine¬†: ses points doivent √™tre presque tous loin des bords. En effet, la sph√®re touche les bords qu\u0026rsquo;au centre des faces or un hypercube de dimension $n$ compte seulement $2n$ faces (hypercubes de dimension $n-1$) pour $2^n$ sommets¬†!\nThe \u0026ldquo;Average man\u0026rdquo; Ce r√©sultat a une cons√©quence tr√®s importante en science des donn√©es¬†: augmenter le nombre de variables descriptives revient √† augmenter le nombre de dimensions et donc √† renforcer dans le m√™me mouvement la probabilit√© d\u0026rsquo;obtenir des valeurs hors normes. Si ces variables d√©crivent diff√©rents aspects d\u0026rsquo;un individu, on en arrive √† la conclusion que l\u0026rsquo;individu moyen n\u0026rsquo;existe pas puisqu\u0026rsquo;il y aura presque toujours au moins une de ces variables qui sortira des standards.\nL\u0026rsquo;U.S. Air Force l\u0026rsquo;a d√©couvert au prix fort √† la fin des ann√©es 40. Beaucoup de pilotes se crashaient (jusqu\u0026rsquo;√† 17 le m√™me jour¬†!), mais personne ne savait trop pourquoi\u0026hellip; Jusqu\u0026rsquo;√† ce qu\u0026rsquo;un jeune scientifique qui venait d\u0026rsquo;√™tre engag√© par l\u0026rsquo;arm√©e, Gilbert Daniels, se r√©solut √† mesurer tous les pilotes.\nLes cockpits et les casques avaient tout sp√©cialement √©t√© dessin√©s pour s\u0026rsquo;adapter parfaitement au pilote moyen\u0026hellip; sans suspecter alors que le pilote moyen √©tait tout √† fait extraordinaire¬†! Ses mensurations furent obtenues au moyen d\u0026rsquo;une large campagne statistique men√©e sur des milliers de jeunes hommes. Mais lorsque Daniels prit les mesures d\u0026rsquo;un peu plus de 4000 pilotes sur 10 crit√®res diff√©rents (circonf√©rence de la poitrine, longueur d\u0026rsquo;entre-jambes, etc.) et qu\u0026rsquo;il inventoria ceux dont l\u0026rsquo;ensemble des caract√©ristiques s\u0026rsquo;√©talait autour de la moyenne (√† 30% pr√®s), il n\u0026rsquo;en trouva\u0026hellip; aucun¬†! Le pilote moyen n\u0026rsquo;existe pas. R√©sultat, chaque pilote avait au moins un truc qui cloche par rapport au design du cockpit, ce qui rendait plus hasardeuse, semble-t-il, la ma√Ætrise de l\u0026rsquo;avion.\nDaniels reporta ses r√©sultats dans cette √©tude intitul√©e The \u0026ldquo;Average man\u0026rdquo;?. Cela fit changer son fusil d\u0026rsquo;√©paule √† l\u0026rsquo;arm√©e am√©ricaine qui se mit √† adapter l\u0026rsquo;avion au pilote plut√¥t que l\u0026rsquo;inverse.\nDistance entre les points Une autre bizarrerie appara√Æt lorsqu\u0026rsquo;on regarde la distance s√©parant deux points pris au hasard dans un hypercube.\nLes histogrammes suivant repr√©sentent la r√©partition des distances obtenues entre chaque paire pour 1000 points tir√©s au hasard uniform√©ment dans l\u0026rsquo;hypercube. Les pointill√©s indiquent les distances minimales (0) et maximales ($\\sqrt{n}$) possibles pour ces distances.\nEncore de la magie noire¬†: en grande dimension, la distance entre deux points est √† peu pr√®s toujours la m√™me¬†!!\nCe n\u0026rsquo;est finalement pas si dur √† intuiter. Tirer un point au hasard dans l\u0026rsquo;hypercube de dimension $n$ revient √† choisir au hasard $n$ coordonn√©es pour construire un vecteur $\\vec{v_1}=x_1 \\vec{e_1}+x_2\\vec{e_2}+\\cdots + x_n\\vec{e_n}$. Et on fait pareil avec un 2e point : $\\vec{v_2}=x_1^\\prime \\vec{e_1}+x^\\prime_2\\vec{e_2}+\\cdots + x^\\prime_n\\vec{e_n}$.\nPour $n$ grand, la probabilit√© que ces deux vecteurs soient orthogonaux (ce qui signifie que leurs directions sont ind√©pendantes) devient √©norme. En effet, il suffit qu\u0026rsquo;une des coordonn√©es soit quasiment nulle pour qu\u0026rsquo;il y ait orthogonalit√© et en multipliant les tirages, √ßa va finir par arriver.\nEn supposant pour simplifier que chacun des points se ballade √† peu pr√®s au milieu d\u0026rsquo;une demi-diagonale de l\u0026rsquo;hypercube (puisqu\u0026rsquo;il est bourr√© de diagonales en grande dimension), on se retrouve avec deux vecteurs orthogonaux √©loign√©s de $\\frac{\\sqrt{n}}{4}$ du centre. Leur distance sera donc approximativement $\\sqrt{\\left(\\frac{\\sqrt{n}}{4}\\right)^2+\\left(\\frac{\\sqrt{n}}{4}\\right)^2}=\\frac{1}{2\\sqrt{2}}\\sqrt{n}\\approx 0,35\\sqrt{n}$. La distance entre deux points augmenterait ainsi comme la racine carr√©e du nombre de dimensions.\nEn ajustant la courbe obtenue en tra√ßant la distance moyenne par une fonction $a\\sqrt{n}$, l\u0026rsquo;accord est plut√¥t tr√®s bon¬†! On obtient $a=0,41$. Pour le coup, ce n\u0026rsquo;est pas tout √† fait nos $0,35$ mais pour un mod√®le ultra simplifi√©, on n\u0026rsquo;est pas si loin\u0026hellip;\nOn constate par contre que la variabilit√©, incarn√©e par l\u0026rsquo;√©cart-type, n\u0026rsquo;est pas modifi√©e¬†! C\u0026rsquo;est ce qui explique que la variabilit√© relative devient, elle, de plus en plus faible quand le nombre de dimensions augmente.\nLa raison de cette conservation de l\u0026rsquo;√©cart-type est √† chercher du c√¥t√© de son √©volution inversement proportionnelle √† la racine carr√©e du nombre d\u0026rsquo;√©chantillons.\nOr ici, le nombre d\u0026rsquo;√©chantillons correspond √† la dimension. Par cons√©quent l\u0026rsquo;augmentation de la variabilit√© provoqu√©e par l\u0026rsquo;augmentation des distances ($\\propto\\sqrt{n}$) est compens√©e par la diminution de cette m√™me variabilit√© du fait de l\u0026rsquo;augmentation du nombre de tirages ($\\propto 1/\\sqrt{n}$) .\nOn comprend mieux pourquoi un algorithme comme KNN qui vise √† pr√©dire une valeur ou cat√©gorie en regardant celle des plus proches voisins gal√®re √† grande dimension¬†; tous les voisins sont √† peu pr√®s √† la m√™me distance\u0026hellip;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/gifted_amateur/",
	"title": "Gifted Amateur",
	"tags": [],
	"description": "",
	"content": " Notes de lecture Notes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell. Les auteurs nous tiennent assez fermement par la main pour nous aider √† traverser sans se perdre une for√™t peu accueillante o√π s'enchev√™trent math√©matiques et formalismes abscons. Ils tentent de motiver au maximum les raisons de chaque nouveau pas dans cette √©preuve en nous faisant miroiter la clairi√®re au panorama immense sur l'ensemble de la physique qui se trouve de l'autre c√¥t√©.\nPartie\u0026nbsp;0\u0026nbsp;: Lagrangien et principe de moindre action Partie\u0026nbsp;1\u0026nbsp;: Les oscillateurs harmoniques Repr√©sentation en nombre d'occupation Seconde quantification Partie\u0026nbsp;2\u0026nbsp;: Th√©orie classique des champs Klein-Gordon Quelques Lagrangiens de champs classiques Partie\u0026nbsp;3\u0026nbsp;: Passage du temps Transformations continues Th√©or√®me de Noether Partie\u0026nbsp;4\u0026nbsp;: Quantification d'un champ scalaire r√©el Quantification d'un champ scalaire complexe Quantification d'un champ √† plusieurs composantes et sym√©tries internes Partie\u0026nbsp;5\u0026nbsp;: Champs de jauge et th√©orie de jauge Sym√©tries discr√®tes Partie\u0026nbsp;6\u0026nbsp;: Propagateurs et fonctions de Green Propagateurs et champs Notations Les points de $\\mathbb{R}^3$ sont conventionnellement not√©s en gras pour les distinguer des points de l'espace-temps $\\mathbb{R}^4$. Ainsi, le point $x=(x_0,x_1,x_2,x_3)$ d√©signe le point de l'espace $\\boldsymbol{x}=(x_1,x_2,x_3)$ √† l'instant $t=x^0/c$ et donc $x=(x^0,\\boldsymbol{x})$. Toujours par convention, les indices d√©sign√©s par des lettres grecques courent de 0 √† 3 alors que les lettres latines vont de 1 √† 3. On suivra aussi le plus souvent la convention de sommation d'Einstein qui consiste √† sommer sur toutes les valeurs possibles les indices r√©p√©t√©s en positions hautes et basses. On introduit la matrice diagonale 4√ó4 $\\eta_{\\mu\\nu}$ avec $\\eta_{00}=1$ et $\\eta_{ii}=-1$ (ce qui signifie donc que $\\eta_{11}=\\eta_{22}=\\eta_{33}=-1)$.\nLa forme bilin√©aire de Lorentz (g√©n√©ralisation du produit scalaire dans l'espace-temps) s'√©crit alors\u0026nbsp;: $(x,y)=\\eta_{\\mu\\nu}x^\\mu y^\\nu=\\sum_{0‚â§\\mu,\\nu‚â§3}\\eta_{\\mu\\nu}x^\\mu y^\\nu = x^0y^0-x^1 y^1 - x^2 y^2 -x^3 y^3$. On note $\\mathbb{R}^{1,3}$ l'espace $\\mathbb{R}^4$ muni de la forme de Lorentz pour rappeler la signature choisie $(+,-,-,-)$ et on le nomme espace de Minkowski. "
},
{
	"uri": "https://sciencesilencieuse.github.io/logique/logique-c/",
	"title": "Logique",
	"tags": [],
	"description": "",
	"content": " Logique Alg√®bre de Boole L\u0026rsquo;alg√®bre de Boole permet d\u0026rsquo;alg√©briser la logique.\nLogique des propositions L\u0026rsquo;implication $A\\rightarrow B$ signifie que si $A$ est vraie alors $B$ est vraie.\n$A$ $B$ $A\\rightarrow B$ 0 0 1 0 1 1 1 0 0 1 1 1 L\u0026rsquo;implication r√©ciproque s\u0026rsquo;√©crit $B\\rightarrow A$. On peut la lire¬†: $B$ est vraie seulement si $A$ est vraie.\n$A$ $B$ $B\\rightarrow A$ 0 0 1 0 1 0 1 0 1 1 1 1 S\u0026rsquo;il y a conjonction entre l\u0026rsquo;implication ($B$ si $A$) et sa r√©ciproque ($B$ seulement si $A$), ce qui s\u0026rsquo;√©crit $(A\\rightarrow B) \\land (B\\rightarrow A)$, alors les propositions $A$ et $B$ sont dites mat√©riellement √©quivalentes $A \\leftrightarrow B$.\n$A$ $B$ $A\\leftrightarrow B$ 0 0 1 0 1 0 1 0 0 1 1 1 On peut exprimer l\u0026rsquo;√©quivalence entre deux propositions de plusieurs mani√®res¬†:\n$A$ si et seulement si (ssi) $B$¬†; pour que $A$, il faut et il suffit que $B$¬†; une condition n√©cessaire et suffisante pour $A$ est $B$. L\u0026rsquo;implication $A\\rightarrow B$ est logiquement √©quivalente √† sa contrapos√©e $\\neg B\\rightarrow \\neg A$, ce qui signifie que leurs tables de v√©rit√© sont les m√™mes.\n$A$ $B$ $\\neg B\\rightarrow \\neg A$ 0 0 1 0 1 1 1 0 0 1 1 1 On note $A\\rightarrow B \\equiv \\neg B\\rightarrow \\neg A$.\nL\u0026rsquo;√©quivalence logique entre deux propositions revient √† dire que l\u0026rsquo;√©quivalence mat√©rielle entre les deux propositions est une tautologie (toujours vraie). L\u0026rsquo;√©quivalence mat√©rielle est un op√©rateur, c\u0026rsquo;est un √©l√©ment du langage de la logique des propositions. L\u0026rsquo;√©quivalence logique ne fait pas partie de ce langage mais d\u0026rsquo;un m√©ta-langage.\nIl ne faut pas confondre la contrapos√©e avec la n√©gation de l\u0026rsquo;ant√©c√©dent $\\neg A\\rightarrow \\neg B$ qui n\u0026rsquo;est pas logiquement √©quivalente √† l\u0026rsquo;implication.\n$A$ $B$ $\\neg A\\rightarrow \\neg B$ 0 0 1 0 1 0 1 0 1 1 1 1 La n√©gation de l\u0026rsquo;ant√©c√©dant est √©quivalent √† la r√©ciproque de la contrapos√©e. L\u0026rsquo;utiliser en pensant qu\u0026rsquo;il est √©quivalent √† l\u0026rsquo;implication est un raisonnement fallacieux ou sophisme.\nUne proposition $F$ est une cons√©quence logique d\u0026rsquo;un ensemble de formules $\\Gamma$ si lorsque les $\\Gamma$ sont vraies, alors $F$ est vraie. On note $\\Gamma\\models F$.\nCela revient √† dire que $\\Gamma\\rightarrow F$ est une tautologie.\nComme $\\equiv$, $\\models$ est un √©l√©ment du m√©ta-langage.\nExemples :\n$A\\models A\\lor B$\n$A\\land B \\models B$\nModus ponens¬†:\n$A \\land (A\\rightarrow B) \\models B$\nS\u0026rsquo;il a plu ($A$) alors le sol est mouill√© ($B$)\nIl a plu ($A$)\nAlors le sol est mouill√© ($B$)\n$A$ $B$ $A\\rightarrow B$ $A \\land (A\\rightarrow B) $ $A \\land (A\\rightarrow B) \\rightarrow B$ 0 0 1 0 1 0 1 1 0 1 1 0 0 0 1 1 1 1 1 1 Modus tollens¬†:\n$\\neg B \\land (A\\rightarrow B) \\models \\neg A$ S\u0026rsquo;il a plu ($A$) alors le sol est mouill√© ($B$)\nLe sol n\u0026rsquo;est pas mouill√© ($\\neg B$)\nAlors il n\u0026rsquo;a pas plu ($\\neg A$)\n$A$ $B$ $A\\rightarrow B$ $\\neg B \\land (A\\rightarrow B) $ $\\neg B \\land (A\\rightarrow B) \\rightarrow \\neg A$ 0 0 1 1 1 0 1 1 0 1 1 0 0 0 1 1 1 1 0 1 Syllogisme hypoth√©tique¬†:\n$(A\\rightarrow B) \\land (B\\rightarrow C) \\models A\\rightarrow C$\nSi je bois ($A$), je ne peux pas conduire ($B$).\nSi je ne peux pas conduire ($B$), je dois appeler un taxi ($C$).\nPar cons√©quent, si je bois ($A$), alors je dois appeler un taxi ($C$).\nCe syllogisme incarne le principe de transitivit√© de l\u0026rsquo;implication.\n$A$ $B$ $C$ $A\\rightarrow B$ $B\\rightarrow C$ $A\\rightarrow C$ $(A\\rightarrow B) \\land (B\\rightarrow C)$ $(A\\rightarrow B) \\land (B\\rightarrow C) \\rightarrow (A\\rightarrow C)$ 0 0 0 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 1 1 1 1 1 1 1 0 0 0 1 0 0 1 1 0 1 0 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 1 1 1 Syllogisme disjonctif¬†:\n$(A\\lor B) \\land (\\neg A) \\models B$\nC\u0026rsquo;est soit bleu ($A$), soit rouge ($B$)\nCe n\u0026rsquo;est pas bleu ($\\neg A$)\nAlors c\u0026rsquo;est rouge ($B$)\n$A$ $B$ $A\\lor B$ $(A\\lor B)\\land (\\neg A) $ $(A\\lor B)\\land (\\neg A) \\rightarrow B$ 0 0 0 0 1 0 1 1 1 1 1 0 1 0 1 1 1 1 0 1 Deux raisonnements fallacieux sont cousins du modus ponens et du modus tollens¬†:\nla n√©gation de l\u0026rsquo;ant√©c√©dent¬†:\n$\\neg A \\land (A\\rightarrow B) \\not\\models \\neg B$ S\u0026rsquo;il a plu ($A$) alors le sol est mouill√© ($B$)\nIl n\u0026rsquo;a pas plu ($\\neg A$)\nAlors le sol n\u0026rsquo;est pas mouill√© ($\\neg B$)\nC\u0026rsquo;est faux, le sol peut √™tre mouill√© malgr√© l\u0026rsquo;absence de pluie.\n$A$ $B$ $A\\rightarrow B$ $\\neg A \\land (A\\rightarrow B) $ $\\neg A \\land (A\\rightarrow B) \\rightarrow \\neg B$ 0 0 1 1 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 1 La table de v√©rit√© montre que $\\neg A \\land (A\\rightarrow B) \\rightarrow \\neg B$ n\u0026rsquo;est pas une tautologie, ce qui prouve qu\u0026rsquo;il n\u0026rsquo;y a pas cons√©quence logique.\nl\u0026rsquo;affirmation du cons√©quent¬†:\n$B \\land (A\\rightarrow B) \\not\\models A$\nS\u0026rsquo;il a plu ($A$) alors le sol est mouill√© ($B$)\nLe sol est mouill√© ($B$)\nAlors il a plu ($A$)\nC\u0026rsquo;est faux. Le sol peut √™tre mouill√© pour d\u0026rsquo;autres raisons.C\u0026rsquo;est une confusion entre la possibilit√© et la n√©cessit√©.\n$A$ $B$ $A\\rightarrow B$ $B \\land (A\\rightarrow B) $ $B \\land (A\\rightarrow B) \\rightarrow A$ 0 0 1 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 1 1 Logique des pr√©dicats Tous les hommes sont mortels ($A$)\nSocrate est un homme ($B$)\nDonc Socrate est mortel ($C$)\nCe syllogisme s\u0026rsquo;√©crit en logique des propositions $A\\land B \\models C$, ce qui n\u0026rsquo;est pas valide\u0026hellip;\nLa logique des propositions vit dans un monde constitu√© seulement de faits. On va d√©passer √ßa gr√¢ce √† la logique des pr√©dicats qui peuple le monde d\u0026rsquo;objets (chiens, th√©ories, chiffres, Socrate, etc.), de relations (rouge, premier, plus grand que, √™tre mortel, etc.) et de fonctions (ajouter 1, longueur, nom du fr√®re, etc.).\nL\u0026rsquo;upgrade de la logique des propositions en logique des pr√©dicats ou logique du 1er ordre passe ainsi par l\u0026rsquo;introduction des notions de variables, de constantes, de fonctions, de pr√©dicats, et de quantificateurs.\nLes variables ($x$, $y$, $z$, etc.) repr√©sentent les objets dont on parle. Les constantes ($a$, $b$, $c$, etc.) sont des valeurs particuli√®res des objets (ici Socrate). Variables et constantes sont des termes du langage. les fonctions ($f$, $g$, $h$, etc.) permettent de fabriquer de nouveaux termes √† partir d\u0026rsquo;anciens. Le nombre de terme auquels la fonction s\u0026rsquo;applique s\u0026rsquo;appelle sont arit√©. Une constante est une fonction d\u0026rsquo;arit√© 0. les pr√©dicats ($P$, $Q$, $R$, etc.) rendent compte des relation entre les termes (ici \u0026ldquo;√™tre un homme\u0026rdquo; et \u0026ldquo;√™tre mortel\u0026rdquo;). L\u0026rsquo;arit√© d\u0026rsquo;un pr√©dicat est √† nouveau le nombre de termes qu\u0026rsquo;il prend en argument. les quantificateurs sont d\u0026rsquo;une part le quantificateur universel $\\foralll$ qui sinfie (\u0026ldquo;pour tout\u0026rdquo;) et le quantificateur existentiel $\\exists$ qui signifie (\u0026ldquo;il existe\u0026rdquo;). Les constantes et les variables sont issues d‚Äôun meÃÇme ensemble appeleÃÅ le domaine.\nUne formule atomique est de la forme $P(t_1,\\ldots,t_n)$ o√π $P$ est une pr√©dicat d\u0026rsquo;arit√© $n$ et $t_1,\\ldots,t_n$ sont des termes.\nOn construit des formules complexes en combinant des formules atomiques gr√¢ce √† des connecteurs.\nRevenons √† l\u0026rsquo;exemple de d√©part.\nAppelons $x$ la variable homme, $s$ la constante Socrate, $P$ le pr√©dicat \u0026ldquo;√™tre un homme\u0026rdquo; et $Q$ le pr√©dicat \u0026ldquo;√™tre mortel\u0026rdquo;.\nL\u0026rsquo;√©nonc√© devient¬†:\n$(\\forall x P(x)\\rightarrow Q(x))\\land P(s)\\models Q(s)$\nCarr√© d\u0026rsquo;Aristote En combinant deux pr√©dicats $P$ et $Q$, leur n√©gation et les deux quantificateurs, on peut construire 4 propositions donts les relations sont repr√©sent√©es dans le carr√© d\u0026rsquo;Aristote.\nDeux propositions sont contradictoires si elles ne peuvent √™tre ni vraies ni fausses en m√™me temps.\nSont donc contradictoire deux propositions dont l\u0026rsquo;une est la n√©gation de l\u0026rsquo;autre.\nPour mieux le voir, on va r√©√©crire les propositions universelles sous des formes logiquement √©quivalentes utilisant le quantificateur existentiel.\nIl est ainsi √©quivalent de dire $\\forall x(P(x)\\rightarrow Q(x))$ (tout $P$ est $Q$) et $\\neg\\exists x (P(x)\\land\\neg Q(x))$ (il n\u0026rsquo;existe pas de $P$ qui soit non $Q$). On retrouve bien ainsi que A est la n√©gation de O.\nDe m√™me, $\\forall x(P(x)\\rightarrow \\neg Q(x)$ (\u0026ldquo;aucun $P$ n\u0026rsquo;est $Q$\u0026rdquo; ou plus clairement \u0026ldquo;tous les $P$ sont non $Q$\u0026rdquo;) est √©quivalent √† $\\neg\\exists x (P(x)\\land Q(x))$ (\u0026ldquo;il n\u0026rsquo;existe pas de $P$ qui soit $Q$\u0026rdquo;). Donc E nie bien I.\nDeux propositions sont contraires ne peuvent pas √™tre vraies en m√™me temps (comme les contradictoires) mais elles peuvent √™tre fausses en m√™me temps par contre. Pour une proposition donn√©e, on peut trouver plusieurs propositions contraires. Le fait qu\u0026rsquo;une propositions soit fausse n\u0026rsquo;entra√Æne pas qu\u0026rsquo;une de ses propositions contraires soit vraie.\nDes propositions subcontraires peuvent √™tre vraies en m√™me temps mais pas fausses en m√™me temps.\nDes propositions subalternes s\u0026rsquo;opposent par la quantit√©. Si la proposition universelle est vraie, alors la proposition particuli√®re est vraie aussi.\nApart√© sur les syllogismes Aristote s\u0026rsquo;est amus√© √† d√©finir et r√©pertorier tous les syllogismes possibles.\nUn syllogisme est un raisonnement mettant en ≈ìuvre trois propositions¬†: les deux premi√®res sont les pr√©misses et elles conduisent √† une conclusion.\nLa 1re pr√©misse est la majeure. C\u0026rsquo;est une des propositions universelles A ou I. La 2e pr√©misse est la mineure. Elle peut √™tre d\u0026rsquo;une des 4 formes A, E, I ou O, mais elle doit avoir un terme commun avec la majeure. La conclusion peut √™tre d\u0026rsquo;une des 4 formes, mais elle doit avoir un terme commun avec la majeure. De toutes les possibilit√©s, Aristote a isol√© 24 syllogismes valides dont 4 ont un statut particulier puisqu\u0026rsquo;ils peuvent g√©n√©rer les autres. Au Moyen-√Çge, les scolastiques ont fabriqu√© des moyens mn√©motechniques √† base des lettres correspondant aux formes des propositions. Les 4 stars sont alors¬†:\nBarbara (pour AAA) dont l\u0026rsquo;exemple type est celui du d√©but avec Socrate (o√π il faut remplacer \u0026ldquo;Socrate est un homme\u0026rdquo; et \u0026ldquo;Socrate est mortel\u0026rdquo; par \u0026ldquo;tous les Socrates sont des hommes\u0026rdquo; et \u0026ldquo;tous les Socrates sont mortels\u0026rdquo;.\nTout P est Q\nTout R est P\nDonc tout R est Q\nCelarent (pour EAE)¬†:\nAucun P n\u0026rsquo;est Q\nTout R est P\nDonc aucun R n\u0026rsquo;est Q\nDarii (pour AII)¬†:\nTout P est Q\nQuelque R est P\ndonc quelque R est Q\nFerio (pour EIO)¬†:\nAucun P n\u0026rsquo;est Q\nQuelque R est P\ndonc quelque R n\u0026rsquo;est pas Q\nLes syllogismes, c\u0026rsquo;est mignons, mais ils n\u0026rsquo;ont √† peu pr√®s jamais √©t√© utilis√©s par les math√©maticiens pour leurs d√©monstrations\u0026hellip;\nVariables libres et li√©es On dit qu‚Äôune occurrence de la variable $x$ dans une formule $F$ est lieÃÅe si un quantificateur porte sur elle.\nSi aucun quantificateur ne porte sur l\u0026rsquo;occurence de la variable, on parle d‚Äôoccurrence libre. Une variable est libre si toutes ses occurrences sont libres.\nUne variable est lieÃÅe si elle a au moins une occurrence lieÃÅe. On parle aussi de variable muette dans ce dernier cas.\nExemple¬†:\nDans $\\forall x (P(x)\\rightarrow Q(x,y))$, $x$ est li√© et $y$ est libre.\nUne formule sans variable libre est dit close (ou ferm√©e).\nUne th√©orie est un ensemble de formules closes.\nAspect s√©mantique Interpr√©tations et mod√®les Les interpr√©tations visent √† donner une valeur de v√©rit√© aux formules. Pour cela, l\u0026rsquo;interpr√©tation $I$ d\u0026rsquo;une formule $F$ sur un domaine $D$ pioche dans $D$ pour affecter des valeurs aux constantes et associe √† chaque symbole de pr√©dicat une relation sur le domaine (de la bonne arit√©) et √† chaque symbole de fonction une fonction sur le domaine (toujours de la bonne arit√©).\nUne formule atomique $P(t_1,\\ldots,t_n)$ est vraie dans une interpr√©tation $I$ sur le domaine $D$ si et seulement si les √©l√©ments du domaine qui sont l\u0026rsquo;interpr√©tation des termes $t_1,\\ldots,t_n$ sont dans la relation correspondant √† l\u0026rsquo;interpr√©tation du pr√©dicat $P$.\nLa valeur de v√©rit√© d\u0026rsquo;une formule complexe est calcul√©e √† partir des valeurs de v√©rit√©s des atomes dont elle est compos√©e en suivant les r√®gles de la logique des propositions.\nExemples :\nInterpr√©tons $F = \\forall x P(x)\\rightarrow Q(x)$ sur le domaine $\\{a,b,c\\}$.\n$x$ $P_I(x)$ $Q_I(x)$ $a$ 1 1 $b$ 0 1 $c$ 0 0 Dans cette interpr√©tation, $F$ est vraie car $P_I(x)\\rightarrow Q_I(x)$ est bien vraie pour toutes les valeurs de $x$.\nMais il suffit d\u0026rsquo;interpr√©ter $P(x)$ un peu diff√©remment pour que $F$ devienne fausse¬†:\n$x$ $P_{I\u0026rsquo;}(x)$ $Q_{I\u0026rsquo;}(x)$ $a$ 1 1 $b$ 0 1 $c$ 1 0 De mani√®re moins arbitraire, on cherche en g√©n√©ral une interpr√©tation qui a du sens.\nLa formule $F = \\forall x\\forall y (P(x,a)\\land P(y,x)\\rightarrow \\neg P(y,a)$ peut vouloir dire \u0026ldquo;les amis des amis de $a$ ne sont pas amis de $a$\u0026rdquo; en interpr√©tant $P$ comme une relation d\u0026rsquo;amit√©.\nConsid√©rons l\u0026rsquo;interpr√©tation suivante¬†: $D=\\{\\text{Anna}, \\text{Bob}, \\text{Joe}, \\text{Clovis}, \\text{Aline}\\}$, $a_I=\\text{Anna}$, $P_I=\\{ (\\text{Anna},\\text{Joe}), (\\text{Anna},\\text{Bob}), (\\text{Joe},\\text{Clovis}), (\\text{Bob},\\text{Aline}) \\}$.\nAnna est amie avec Joe et Bob, Joe est ami avec Clovis, Bob est ami avec Aline, et ni Clovis, ni Aline ne sont amis avec Anna. Donc la formule est vraie dans cette interpr√©tation.\nOn aurait aussi pu interpr√©ter $P$ comme une relation de sup√©riorit√©.\nImaginons donc maintenant $I\u0026rsquo;$ donn√©e par¬†: $D\u0026rsquo;=\\{1,5,12,52\\}$, $a_{I\u0026rsquo;}=5$ et $P_{I\u0026rsquo;}(x,y)=1$ si $x\u0026gt;y$.\nOn voit maintenant que $F$ est forc√©ment fausse dans cette interpr√©tation puisque la relation $\u0026gt;$ est transitive.\nUne interpr√©tation d\u0026rsquo;une formule sur un domaine est un mod√®le de cette formule si la formule est vraie pour cette interpr√©tation.\nC\u0026rsquo;est le cas de $I$ pour nos exemples, mais pas de $I\u0026rsquo;$.\nUne formule est valide (tautologie) si elle est vraie quelle que soit l\u0026rsquo;interpr√©tation (si toute interpr√©tation est un mod√®le).\nAucune des deux formules en exemple n\u0026rsquo;est valide.\nExemple de formule valide¬†: $\\forall x P(x) \\rightarrow \\exists x P(x)$ (si une propri√©t√© est vraie pour tout $x$, alors elle est vraie pour au moins un $x$).\nUne formule est satisfaisable (ou consistante) s\u0026rsquo;il elle poss√®de un mod√®le.\nRq : une formule peut donc √™tre invalide et consistante.\n√âquivalence et cons√©quence Deux formules $F$ et $F\u0026rsquo;$ sont s√©mantiquement √©quivalentes si pour toute interpr√©tation $I$, $F_I=F_I\u0026rsquo;$ (m√™mes tables de v√©rit√©).\nOn note $F\\equiv F\u0026rsquo;$.\nExemple¬†: $\\neg\\neg P(x) \\equiv P(x)$\nUn ensemble de formules $\\{F_1,\\ldots,F_n\\}$ satisfait une formule $F$ si tout mod√®le de $\\{F_1,\\ldots,F_n\\}$ est aussi mod√®le de $F$.\nOn dit que $F$ est cons√©quence s√©mantique de la th√©orie$T = \\{F_1,\\ldots,F_n\\}$ et on note $\\{F_1,\\ldots,F_n\\} \\models F$.\nExemple¬†: $P(a)\\models \\exists x P(x)$\nSi $F$ est une formule valide, on note $\\models F$ (il y a une quantification universelle implicite sur toutes les repr√©sentations).\n$\\{F_1,\\ldots,F_n\\} \\models F$ √©quivaut √†¬†:\n$\\models (F_1\\land\\ldots\\land F_n)\\rightarrow F$ $\\models \\neg(F_1\\land\\ldots\\land F_n)\\lor F$ si les formules sont closes, $ F_1\\land\\ldots\\land F_n \\land \\neg F$ est insatisfaisable (preuve par l\u0026rsquo;absurde). Aspect syntaxique S\u0026rsquo;assurer de la validit√© d\u0026rsquo;une formule suppose de s\u0026rsquo;assurer qu\u0026rsquo;elle est vraie pour un nombre d\u0026rsquo;interpr√©tations potentiellement infini. Et m√™me lorsqu\u0026rsquo;il est fini, l\u0026rsquo;attribution\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/quantique/",
	"title": "M√©canique quantique",
	"tags": [],
	"description": "",
	"content": " M√©canique quantique Fondations Un jeu d\u0026rsquo;axiomes Information quantique Sph√®re de Bloch Pairs de Qubits Non-clonage quantique T√©l√©portation quantique Paradoxe EPR Th√©orie-jouet de Spekkens Interpr√©tations Le monde de l\u0026rsquo;interpr√©tation de la m√©canique quantique est un grand cirque disparate. P√©n√©trer sous le chapiteau, c\u0026rsquo;est tenter de trouver une solution au probl√®me de la mesure, donner un sens aux probabilit√©s, etc.\nParmi les diff√©rentes familles d\u0026rsquo;interpr√©tation, on trouve¬†:\nla th√©orie des mondes multiples (Everett, Deutsch), la th√©orie de l\u0026rsquo;onde pilote (Bohm, D√ºrr), la th√©orie GRW de l\u0026rsquo;effondrement spontan√© (Ghirardi, Rimini, Weber), l\u0026rsquo;interpr√©tation relationnelle (Rovelli), th√©ories stochastiques r√©alistes (Barandes), etc. Une interpr√©tation fonctionnelle d√©crit l‚Äôapparition de r√©sultats uniques et la statistique qu\u0026rsquo;ils suivent‚ÄØ; on peut toutefois la compl√©ter par un choix ontologique ‚Äì particules, densit√©s, onde, etc. ‚Äì qui pr√©cise la nature de ces r√©sultats. Que consid√®re-t-on comme fondamental¬†? Qu\u0026rsquo;est-ce qui existe¬†? Certaines interpr√©tations vont guider une ontologie donn√©e mais pas n√©cessairement. Et tout choix ontologique semble passer par un marchandage conceptuel, des compromis √† dig√©rer. D√©cr√©ter la r√©alit√© de tel ou tel objet impose en effet d\u0026rsquo;accepter certaines implications.\nCitons quelques exemples¬†:\nLes r√©alistes de la fonction d\u0026rsquo;onde (œà-ontistes) choisissent la fonction d\u0026rsquo;onde de Schr√∂dinger comme description fondamentale de la r√©alit√©. En contrepartie, ils doivent accepter un espace √† 3N dimensions, rel√©guant notre espace 3D au rang d\u0026rsquo;illusion √©mergente¬†; Les r√©alistes de l\u0026rsquo;espace-temps attribuent un √©tat quantique par r√©gion d‚Äôespace-temps qui devient l\u0026rsquo;√©l√©ment central. On perd alors la s√©parabilit√© (les √©tats de r√©gions qui se recouvrent ne se factorisent pas)¬†; Les partisans du holisme relationnel font de l\u0026rsquo;intrication quantique (corr√©lations non locales) un fait premier plut√¥t qu‚Äôun probl√®me √† expliquer. Ils abandonnent clairement la s√©parabilit√©¬†; L\u0026rsquo;ontologie primitive met, elle, en avant les objets mat√©riels localis√©s r√©sultants des exp√©riences (les \u0026ldquo;local beables\u0026rdquo; de Bell) pour rattacher la th√©orie aux \u0026ldquo;choses qui laissent des traces\u0026rdquo;. Ils perdent la localit√©. Epist√©mique vs. ontique Distinction selon Harrigan et Speckens Attraits de la vision √©pist√©mique Th√©or√®me PBR et n√©cessit√© de l\u0026rsquo;ontologie Une ontologie √† 3N dimensions ER = EPR "
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/stat/",
	"title": "Statistiques",
	"tags": [],
	"description": "",
	"content": " Statistiques tip\nLes probabilit√©s sont des calculs a priori, visant √† pr√©dire les issues d‚Äô√©v√®nements n‚Äôayant pas encore eu lieu, alors que les statistiques sont des calculs a posteriori qui analysent les issues d‚Äô√©v√®nements d√©j√† r√©alis√©s.\nLes diff√©rentes moyennes Monte Carlo Dans la duxi√®me partie de la vid√©o, on cherche √† r√©pondre √† la grande question suivante¬†: comment s\u0026rsquo;assurer d\u0026rsquo;avoir suffisamment d\u0026rsquo;√©chantillons pour obtenir des statistiques fiables¬†? Ou plus prosa√Øquement¬†: √† partir de combien est-ce suffisant¬†? On utilise d\u0026rsquo;abord les propri√©t√©s de la loi normale puis on construit un estimateur √† partir de l\u0026rsquo;in√©galit√© de Bienaym√©-Tchebychev.\nParadoxe de Simpson Ce paradoxe a fait des d√©g√¢ts lors de la campagne de vaccination pour le Covid. Les chiffres de la vid√©o sont proches des chiffres communiqu√©s par l\u0026rsquo;√©tat d\u0026rsquo;Isra√´l qui √©tait un des premiers pays √† pouvoir vacciner.\nL\u0026rsquo;article qui a servi en partie pour la vid√©o.\nTest d\u0026rsquo;hypoth√®se Les statistiques au service de la prise de d√©cision en science.\nAnalyse en composante principale L\u0026rsquo;analyse en composante principale permet de faire parler les donn√©es.\nUn autre exemple + tuto avec des donn√©es INSEE d√©partementales p-hacking Extrait traduit du livre \u0026#39;Everything is predictable\u0026#39; de Tom Chivers\nPeut-√™tre que l‚Äôexemple le plus c√©l√®bre est celui du scientifique de l\u0026rsquo;alimentation Brian Wansink, une star de l‚ÄôUniversit√© Cornell qui a re√ßu des millions de dollars de financement du gouvernement f√©d√©ral am√©ricain sous l‚Äôadministration Obama. Il a publi√© beaucoup d‚Äô√©tudes sur notre comportement alimentaire, notamment une sur la fa√ßon dont les hommes mangent plus en compagnie de femmes (probablement pour les impressionner) ; une autre sur comment donner des noms plus ‚Äúattractifs‚Äù aux l√©gumes (appeler les carottes ‚Äúcarottes √† vision X-ray,‚Äù par exemple) permet que les enfants d‚Äô√©cole primaire en mangent deux fois plus. Puis, en 2016, il a commis l‚Äôerreur de publier un billet de blog intitul√© \u0026ldquo;The Grad Student Who Never Said \u0026lsquo;No.\u0026rsquo;\u0026rdquo;. L‚Äô√©tudiante en question √©tait une doctorante turque. Lorsqu‚Äôelle est arriv√©e √† Cornell, Wansink ‚Äúlui a donn√© un ensemble de donn√©es d‚Äôune √©tude autofinanc√©e, sans r√©sultats probants‚Äù - une √©tude qui examinait le comportement alimentaire dans un buffet italien √† volont√© pendant un mois. Selon ses propres termes, il lui a dit : ‚ÄúCela nous a co√ªt√© beaucoup de temps et d\u0026rsquo;argent √† collecter. Il doit y avoir quelque chose ici que nous pouvons sauver parce que cet ensemble de donn√©es est cool (riche et unique).‚Äù L‚Äô√©tudiante s‚Äôest donc mise au travail et a d√©coup√© l‚Äôensemble de donn√©es de nombreuses fa√ßons diff√©rentes. Et, in√©vitablement, elle a trouv√© de nombreuses corr√©lations p \u0026lt; 0,05 - suffisamment pour qu\u0026rsquo;elle et Wansink publient cinq articles (y compris l‚Äôarticle ‚Äúles hommes mangent trop pour impressionner les femmes‚Äù). Cela a √©veill√© la suspicion de certains scientifiques et journalistes scientifiques, et ils ont commenc√© √† passer au crible les autres recherches de Wansink. De plus, Stephanie Lee, une journaliste scientifique de BuzzFeed, a mis la main sur ses emails, dans lesquels - il s‚Äôest av√©r√© - il demanda √† sa doctorante de d√©couper les donn√©es en ‚Äúm√¢les, femelles, mangeurs de d√©jeuner, mangeurs de d√Æner, personnes mangeant seules, personnes mangeant en groupe de 2, personnes mangeant en groupe de 2+, personnes commandant de l‚Äôalcool, personnes commandant des boissons non alcoolis√©es, personnes restant pr√®s du buffet, personnes s‚Äôasseyant loin, etc.‚Äù pour ‚Äúessorer le jeu de donn√©es jusqu\u0026rsquo;√† en extraire tous les r√©sultats significatifs possibles‚Äù et faire en sorte que cela ‚Äúdevienne viral √† grande √©chelle.‚Äù En cons√©quence, dix-huit des articles de Wansink ont √©t√© r√©tract√©s ; sept ont re√ßu des \u0026ldquo;expressions of concern‚Äù que les revues ajoutent aux √©tudes qu‚Äôelles ne pensent pas pouvoir √™tre totalement fiables, mais qu‚Äôelles ne sont pas pr√™tes √† r√©tracter compl√®tement ; et quinze ont √©t√© corrig√©s. Wansink, entre-temps, a d√©missionn√© de Cornell en 2019, apr√®s que l‚Äôuniversit√© a conclu qu‚Äôil avait commis une faute scientifique et lui a interdit l‚Äôenseignement et la recherche. C\u0026rsquo;est un exemple particuli√®rement frappant, mais d‚Äôune certaine mani√®re Wansink a eu la malchance d‚Äô√™tre d√©truit publiquement pour quelque chose qui √©tait presque une pratique standard. Le p-hacking se produit tout le temps, de mani√®re beaucoup moins dramatique - et beaucoup de scientifiques n‚Äôont absolument aucune id√©e qu‚Äôils font quelque chose de mal. Daryl Bem, dans un chapitre d\u0026rsquo;un livre de 1987 √©crit comme un guide pour aider les √©tudiants √† publier leurs recherches, a √©crit qu‚Äôil ‚Äúy a deux articles que vous pouvez √©crire : l‚Äôarticle que vous aviez pr√©vu d‚Äô√©crire lorsque vous avez con√ßu votre √©tude ; l‚Äôarticle qui a le plus de sens maintenant que vous avez vu les r√©sultats. La bonne r√©ponse est le second.‚Äù Il a appel√© les chercheurs √† ‚Äúanalyser les sexes s√©par√©ment, cr√©er de nouveaux index composites‚Ä¶ r√©organiser les donn√©es pour les mettre en relief de mani√®re plus audacieuse‚Ä¶ Les donn√©es peuvent √™tre suffisamment solides pour justifier de recentrer votre article autour des nouvelles d√©couvertes et de subordonner ou m√™me d‚Äôignorer vos hypoth√®ses originales.\u0026rdquo;\nComme on l\u0026rsquo;a vu sur les espaces de donn√©es √† grande dimension , plus on a d\u0026rsquo;axes (de cat√©gories, d\u0026rsquo;entr√©es,\u0026hellip;), plus on a de chance d\u0026rsquo;obtenir un r√©sultat hors norme sur au moins l\u0026rsquo;un d\u0026rsquo;eux.\nCe chouette site met le ph√©nom√®ne √† profit pour d√©busquer des corr√©lations amusantes (avec pour chacune des p-values inf√©rieures √† 1%\u0026hellip;).\n√âcart-type exp√©rimental (estimateur de l\u0026rsquo;√©cart-type non biais√©) L\u0026rsquo;√©cart-type pour une variable quantitative dont les relev√©s $(x_1,\\ldots,x_N)$ sont pris sur la population compl√®te (avec pour moyenne $\\mu$) est donn√© par la racine carr√©e de la variance¬†:\n$$\\sigma = \\sqrt{V}=\\sqrt{\\frac{1}{N}\\sum_{i=1}^N (x_i-\\mu)^2}$$\nC\u0026rsquo;est donc la racine-carr√©e des carr√©s des √©carts √† la moyenne divis√© par $N$. On obtient bien ainsi une moyenne des √©carts √† la moyenne en emp√®chant qu\u0026rsquo;un √©cart en-dessous ne compense une √©cart au-dessus.\nMais si les relev√©s ne concernent qu\u0026rsquo;un √©chantillon $n\u0026lt;N$ de la population et non la population compl√®te (ce qui est le cas pour une s√©rie de mesures exp√©rimentales), on passe des probas aux stats et on a alors besoin d\u0026rsquo;un estimateur de l\u0026rsquo;√©cart-type ($\\sigma\\rightarrow S$).\nL\u0026rsquo;estimateur de l\u0026rsquo;√©cart-type non biais√© (aussi appel√© √©cart-type exp√©rimental) est alors donn√© par¬†:\n$$S_{n-1} =\\sqrt{\\frac{1}{n-1}\\sum_{i=1}^n (x_i-\\bar{x})^2}$$ note\nL\u0026rsquo;estimateur non biais√© de l\u0026rsquo;√©cart-type est parfois appel√© √©cart-type exp√©rimental et not√© $\\sigma_{exp}$. C\u0026rsquo;est particuli√®rement le cas dans l\u0026rsquo;enseignement de la physique au lyc√©e.\nPourquoi \u0026ldquo;non biais√©\u0026rdquo;¬†? Et pourquoi passer de \u0026ldquo;$n$\u0026rdquo; √† \u0026ldquo;$n-1$\u0026quot;¬†? Notre probl√®me est qu\u0026rsquo;on ne conna√Æt plus la valeur vraie de la moyenne $\\mu$ mais seulement son estimation $m=\\frac{1}{n}\\sum_{i=1}^n x_i$.\nEt en substituant $m$ √† $\\mu$ dans la variance, on biaise son esp√©rance\u0026hellip;\nEn effet :\n$$ \\begin{aligned} E\\left(\\frac{1}{n}\\sum_{i=1}^n (x_i-m)^2\\right) \u0026= E \\left( \\frac{1}{n}\\sum_{i=1}^n x_i^2 - m^2 \\right) \u0026\u0026\\text{car } -2 \\frac{1}{n}\\sum_{i=1}^n x_i\\times m = -2 m^2\\\\ \u0026 = \\frac{1}{n}\\sum_{i=1}^n E(x_i^2)-E(m^2)\\\\ \u0026 = E(x^2)-E(m^2) \u0026\u0026\\text{car }E(x_i)=E(x)\\text{ ne d√©pend pas de }i\\\\ \u0026= \\left(V(x)+E(x)^2\\right)-\\left(V(m)+E(m)^2\\right)\u0026\u0026\\text{car }V(x)=E(x^2)-E(x)^2\\\\ \u0026= \\left(V(x)+E(x)^2\\right)-\\left(\\frac{1}{n}V(x)+E(x)^2\\right)\u0026\u0026\\text{car }V(m)=\\frac{1}{n}V(x)\\\\ \u0026= \\frac{n-1}{n}V(x) \\end{aligned} $$ Par cons√©quent, on multiplie par $\\frac{n}{n-1}$ pour se d√©barrasser du biais\u0026hellip;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/tqc/",
	"title": "Th√©orie quantique des champs",
	"tags": [],
	"description": "",
	"content": " Th√©orie quantique des champs \"Particules et ondes ne sont que les excitations d'un champ quantique s'√©tendant dans l'espace temps.\" La th√©orie quantique des champs (TQC) est consid√©r√© √† ce jour comme la meilleure (√† d√©faut d\u0026rsquo;√™tre l\u0026rsquo;ultime) th√©orie explicative de l\u0026rsquo;univers qui nous entoure.\nC\u0026rsquo;est √† la fois la th√©orie test√©e avec la plus grande pr√©cision üòé, mais aussi celle qui a produit le plus grand √©cart entre th√©orie et exp√©rience üò± (catastrophe du vide).\nNotes de lecture du livre Quantum field theory for the gifted amateur de Thomas Lancaster et Stephen Blundell.\nVieux texte qui reprend des id√©es d√©velopp√©es dans le petit livre grand public de Feynman sur l\u0026rsquo;√©lectrodynamique quantique QED: The Strange Theory of Light and Matter.\nChouette article de Quanta Magazine qui r√©sume et illustre le mod√®le standard des particules dont la structure repose sur la th√©orie quantique des champs¬†; l\u0026rsquo;√©tat actuel de notre compr√©hension du monde. "
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/quantique/erepr/",
	"title": "ER = EPR",
	"tags": [],
	"description": "",
	"content": " ER = EPR Le paradoxe EPR est depuis quelques temps au c≈ìur d\u0026rsquo;avanc√©es prometteuses vers le graal de la physique th√©orique¬†: une th√©orie de la gravit√© quantique. En effet, la conjecture ER = EPR propos√©e en 2013 par L√©onard Susskind et Joan Maldacena connecterait les particules intriqu√©es par des trou de vers (pont d‚ÄôEinstein-Rosen)1.\nC\u0026rsquo;est la g√©om√©trie m√™me de l\u0026rsquo;espace-temps qui serait d√©termin√©e par l\u0026rsquo;intrication quantique.\nCes articles de Quanta Magazine permettent de creuser ces questions au centre de la physique th√©orique aujourd\u0026rsquo;hui¬†:\nsur le paradoxe des trous noirs qui a tout lanc√© ER=EPR Un peu fou qu\u0026rsquo;Einstein soit pr√©sent dans les deux acronymes\u0026hellip;\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/geometrie/geo5/",
	"title": "Poursuites",
	"tags": [],
	"description": "",
	"content": " Poursuites note\nCette page est tr√®s inspir√©e du livre Chases and Escapes The Mathematics of Pursuit and Evasion de Paul Nahin.\nParadoxe d\u0026rsquo;Achille et de la tortue Dans La Physique, Aristote rapporte les c√©l√®bres paradoxes de Z√©non d\u0026rsquo;√âl√©e dont celui d\u0026rsquo;Achille et la tortue.¬†:\nCelui qui court le plus lentement ne sera jamais rattrap√© par le plus rapide. Car le poursuivant doit d\u0026rsquo;abord atteindre le point d\u0026rsquo;o√π est parti le poursuivi, si bien que le plus lent doit toujours avoir une longueur d\u0026rsquo;avance.\nImaginons qu\u0026rsquo;Achille aille deux fois plus vite que la tortue et que la tortue ait une distance d\u0026rsquo;avance $d$ au d√©part. Il faut un temps $T$ √† Achille pour parcourir la distance $d$, mais pendant ce temps $T$, la tortue a continu√© d\u0026rsquo;avancer et a parcouru une distance $d/2$. Il faudra alors $T/2$ √† Achille pour parcourir cette distance suppl√©mentaire, mais pendant ce temps, la tortue a avanc√© de $d/4$, etc.\nLe temps mis par Achille pour rattraper la tortue vaut donc¬†:\n$\\displaystyle T+\\frac{T}{2}+\\frac{T}{4}+\\frac{T}{8}+\\cdots = \\sum_{n=0}^{\\infty}\\frac{T}{2^n}$\nLe but de Z√©non d\u0026rsquo;√âl√©e √©tait de prouver l\u0026rsquo;impossibilit√© m√™me du mouvement en pi√©geant le concept avec ces histoires d\u0026rsquo;infini.\nR√©ussir √† imaginer que la somme d\u0026rsquo;un nombre infini de termes positifs puisse donner un r√©sultat fini constitua une montagne math√©matique qu\u0026rsquo;on mis tr√®s longtemps √† gravir.\nAjourd\u0026rsquo;hui, on sait que ce type de s√©rie g√©om√©trique converge si sa raison est comprise dans $]-1;1[$. Et dans notre exemple, comme l\u0026rsquo;illustre l\u0026rsquo;animation ci-dessous, la s√©rie converge pour donner $2T$.\nCourbe de poursuite Ok, on peut rattraper la tortue, mais comment faire en pratique si on devait programmer Achille en supposant que les deux d√©marrent dans les poitions not√©es ci-dessous¬†?\nLe plus rapidement¬†: calculer la direction de la ligne droite √† suivre permettant d\u0026rsquo;intercepter la tortue. Le plus simple¬†: toujours orienter son vecteur vitesse (la direction de son mouvement) vers la cible. Dans le deuxi√®me cas, on obtient une famille de courbes particuli√®res appel√©es courbes de poursuite.\nExtrait d\u0026rsquo;un manuel de l\u0026rsquo;US Air Force distribu√© aux mitrailleurs¬†:\nComme l\u0026rsquo;explique le manuel, si un poursuivant se dirige en permanence vers sa cible (son vecteur vitesse √©tant selon la ligne joignant le poursuivant √† la cible) alors sa trajectoire sera une courbe de poursuite.\nSi on voulait d√©terminer analytiquement l\u0026rsquo;√©quation de cette courbe, on poserait que¬†:\nla pente $p(x)$ de la trajectoire du chasseur est donn√©e par $\\displaystyle p(x)=\\frac{dy}{dx}=\\frac{y-V_{cible}t}{x-x_0}$ d\u0026rsquo;apr√®s le sch√©ma ci-dessous. et le chasseur parcours en un temps $t$ le morceau de trajectoire compris enntre $0$ et $x$¬†: $\\displaystyle V_{chasseur} t = \\int_0^x \\sqrt{1+\\left(\\frac{dy}{dx\u0026rsquo;}\\right)^2}dx\u0026rsquo; = \\int_0^x \\sqrt{1+p(x\u0026rsquo;)^2}dx\u0026rsquo; $ En r√©solvant pour $t$, on obtient $\\displaystyle \\frac{1}{V_{chasseur}}\\int_0^x\\sqrt{1+p(x\u0026rsquo;)^2}dx\u0026rsquo; = \\frac{y}{V_{cible}}-p(x)\\frac{(x-x_0)}{V_{cible}}$\nApr√®s quelques √©tapes, cela s\u0026rsquo;int√®gre en $\\displaystyle p(x)=\\frac{dy}{dx}=\\frac{1}{2}\\left[\\left(1-\\frac{x}{x_0}\\right)^{-n}-\\left(1-\\frac{x}{x_0}\\right)^{n}\\right]$ en posant $\\displaystyle n = \\frac{V_{cible}}{V_{chasseur}}$ qui s\u0026rsquo;int√®gre √† son tour en $\\displaystyle y(x)=\\frac{n}{1-n^2}x_0 + \\frac{1}{2}(x_0-x)\\times\\left[\\frac{(1-x/x_0)^n}{1+n}-\\frac{(1-x/x_0)^{-n}}{1-n}\\right]$.\nLa cible est rattrap√©e au point $(x_0\\,;y(x_0))=\\left(x_0\\,;\\frac{n}{(1-n^2)}x_0\\right)$. Elle a alors parcouru une distance $\\displaystyle \\frac{n}{(1-n^2)}x_0$ et le chasseur a parcouru $\\displaystyle \\frac{V_{chasseur}}{V_{cible}}$ fois plus.\nC\u0026rsquo;est finalement beaucoup plus simple √† programmer qu\u0026rsquo;√† calculer. Et si la cible se met √† remuer, cela devient m√™me mission quasi impossible d\u0026rsquo;obtenir une formule ferm√©e pour la courbe\u0026hellip;\nDans ce premier petit programme, on retrouve la courbe d\u0026rsquo;allure caract√©ristique.\nEt dans ce deuxi√®me programme, on prend le point de vue du passager de l'avion cible pour confirmer la mise-en-garde du manuel\u0026nbsp;; si le chasseur semble glisser dans le ciel tout en grossissant, √ßa n'est pas bon signe...\nInterception Arr√™tons maintenant de poursuivre b√™tement et essayons d\u0026rsquo;intercepter.\nSupposons que le chasseur est en $A$ √† $t=0$ et que la cible est en $B$ au m√™me instant et supposons aussi qu\u0026rsquo;ils ont chacun un mouvement rectiligne uniforme (√† $v_{chasseur}$ et $v_{cible}$). √Ä quelle condition le chasseur intercepte-t-il la cible au point $I$¬†?\nIl faut que le ratio des distances et des vitesses soient les m√™mes¬†: $\\displaystyle \\frac{BI}{AI}=\\frac{v_{cible}}{v_{chasseur}}=n$.\nLes points $I$ solutions doivent donc v√©rifier¬†: $\\displaystyle \\frac{\\sqrt{(x-p)^2+y^2}}{\\sqrt{(x-m)^2+y^2}}=n$.\nCela donne l\u0026rsquo;√©quation d\u0026rsquo;un cercle, le cercle d\u0026rsquo;Apollonius¬†: $\\displaystyle \\left[x-\\frac{n^2 m-p}{n^2-1}\\right]^2 + y^2 = \\left[\\frac{n(p-m)}{1-n^2}\\right]^2$.\nSon centre est sur l\u0026rsquo;axe horizontal $\\displaystyle \\left(\\frac{n^2m-p}{n^2-1};0\\right)$ et son rayon vaut $\\displaystyle \\frac{n(p-m)}{|1-n^2|}$.\nPour d√©terminer la direction √† prendre pour le chasseur, il suffit de regarder o√π la direction de la cible coupe le cercle d\u0026rsquo;Apollonius.\nOn remarque que m√™me si la cible va plus vite que le chasseur $n\u0026gt;1$, il peut y avoir interseption (et pour deux directions diff√©rentes du chasseur). Mais pour cela, l\u0026rsquo;angle $\\theta$ de la direction de la cible ne doit pas √™tre sup√©rieur √† $\\displaystyle \\sin^{-1}\\left(\\frac{v_{chasseur}}{v_{cible}}\\right)$.\nEn effet, la situation limite correspond √† une direction de la cible tangente au cercle d\u0026rsquo;Apollonius (voir dessin ci-dessous).\nOn a bien $\\displaystyle \\sin\\theta = \\frac{CI}{CB} = \\frac{\\frac{n(p-m)}{n^2-1}}{p-\\frac{n^2m-p}{n^2-1}}=\\frac{1}{n}$\nMan≈ìuvre d\u0026rsquo;√©vitement de l\u0026rsquo;Enola Gay Juste apr√®s avoir l√¢cher sa terrible bombe sur Hiroshima, l\u0026rsquo;Enola Gay a man≈ìuvr√© pour tenter d\u0026rsquo;√©viter au maximum le souffle de l\u0026rsquo;explosion atomique.\nLe B-29 avait une vitesse de $\\pu{529 km/h}$ par rapport au sol au moment du largage de la bombe. Sachant que l\u0026rsquo;explosion a eu lieu apr√®s une chute de $\\pu{19700 ft}$, soit $\\pu{9,053 km}$, on peut en d√©duire le d√©lai entre le largage et l\u0026rsquo;explosion¬†: dans l\u0026rsquo;hypoth√®se d\u0026rsquo;une chute libre (poids comme seule force), la distance verticale parcourue est donn√©e par $h=\\frac{1}{2}gt^2$, d\u0026rsquo;o√π $t=\\sqrt{\\frac{2h}{g}}\\approx\\pu{43 s}$. C\u0026rsquo;est en accord avec la valeur g√©n√©ralement admise. Le pilote a donc environ 43 secondes pour s\u0026rsquo;√©loigner le plus possible de la zone d\u0026rsquo;impact.\nPendant ces 43 secondes, la bombe continue d\u0026rsquo;avancer horizontalement √† la vitesse de l\u0026rsquo;avion au moment du largage, ce qui donne une distance parcourue de $\\pu{6,3 km}$. Le pilote parle plut√¥t d\u0026rsquo;une distance horizontale de 3,5 mi, soit $\\pu{5,6 km}$. Vu la forme de la bombe, cela para√Æt logique que son mouvement horizontal soit plus impact√© par les frottements que son mouvement vertical.\nAu moment du largage, le pilote va √©videmment chercher √† s√©loigner le plus possible de la future explosion. Pour cela il entreprend imm√©diatement un virage √† pleine vitesse (350 mph par rapport √† l\u0026rsquo;air, soit $\\pu{563 km/h}$) en braquant ses ailes √† un angle $\\alpha = 60^\\circ$.\nSi le B-29 tourne, c\u0026rsquo;est gr√¢ce √† l\u0026rsquo;inclinaison de la force de portance $\\vec{L}$, perpendiculaire aux ailles. On va supposer que la trajectoire pendant le virage est circulaire de rayon $R$ et que la vitesse $v$ de l\u0026rsquo;avion par rapport √† l\u0026rsquo;air reste constante. L\u0026rsquo;acc√©l√©ration est alors horizontale et purement centrip√®te et vaut $\\displaystyle a = \\frac{v^2}{R}$.\nLe PFD (2e loi de Newton) appliqu√© √† l\u0026rsquo;avion nous dit que¬†: $\\displaystyle\\vec{P}+\\vec{L} = m\\vec{a}$.\nEn projetant verticalement, on obtient¬†: $\\displaystyle-mg+L_y = 0$. On retrouve que la composante verticale de la portance compense le poids. Cela donne¬†: $\\displaystyle L\\times\\cos\\alpha = mg$.\nEt la projection horizontale donne¬†: $\\displaystyle -L_x = -ma$, et donc $\\displaystyle L\\sin\\alpha = ma$.\nPar cons√©quent¬†: $\\displaystyle ma=\\frac{mg}{\\cos\\alpha}\\sin\\alpha \\Rightarrow a = g\\tan\\alpha$.\nComme $\\displaystyle\\tan(60^\\circ) = \\sqrt{3}$, on se retrouve avec une acc√©l√©ration horizontale de l\u0026rsquo;avion valant $\\sqrt{3}g$.\nLes passagers de l\u0026rsquo;avion ressentent, eux, une acc√©l√©ration oblique de $2g$ (acc√©l√©ration du champ de pesanteur √† laquelle s\u0026rsquo;ajoute l\u0026rsquo;acc√©l√©ration perpendiculaire due √† l\u0026rsquo;avion).\nEt puisque $\\displaystyle a=\\frac{v^2}{R}$, on obtient¬†: $\\displaystyle R =\\frac{v^2}{g \\tan\\alpha} \\approx \\pu{1,4 km}$. Un petit coucou ou un gros porteur feront le m√™me virage du moment que vitesse et inclinaison des ailes sont les m√™mes¬†!\nIl nous reste √† trouver quand quitter le virage. $(AH)$ et $(HB)$ sont tangentes au cercle form√© et $AH=AB$. Les triangles $HBC$ et $HAC$ sont donc identiques impliquant l\u0026rsquo;√©galit√© des angles $\\widehat{HCB}$ et $\\widehat{HCA}$ ($\\beta$ sur le sch√©ma). Le bombardier quitte le virage au bout d\u0026rsquo;une d√©viation de $2\\beta$ o√π $\\beta = \\arctan\\left(\\frac{AH}{R}\\right)=\\arctan(5,6/1,4)$. Cela donne une d√©viation totale $2\\beta$ d\u0026rsquo;environ $152^\\circ$.\nLa distance parcourue sur le cercle vaut alors $\\frac{2\\beta}{360}\\times 2\\pi R \\approx \\pu{3,7 km}$ et le temps pour la parcourir vaut $3,7/545\\times 3600 = \\pu{24 s}$. Pour la vitesse de l\u0026rsquo;avion, on fait une moyenne entre la vitesse de l\u0026rsquo;avion par rapport au sol au moment du largage et sa vitesse par rapport √† l\u0026rsquo;air. Il reste alors $43-24=\\pu{19 s}$ pour s\u0026rsquo;√©loigner √† tout berzingue avant que la bombe n\u0026rsquo;explose. Le bombardier sera alors √† une distance $d=5,6+545\\times 19/3600 \\approx \\pu{8,5 km}$.\nAu bout de combien de temps sera-t-il touch√© par le souffle de l\u0026rsquo;explosion, et √† quelle distance de l\u0026rsquo;impact sera-t-il alors¬†? Supposons une onde de choc se d√©pla√ßant √† $v_c$ d\u0026rsquo;environ $\\pu{1300 km/h}$ (un peu plus rapide que le son) et appelons $t_i$ le temps au bout duquel le souffle rattrape le bombardier (de vitesse $v_b = \\pu{545 km/h}$). N\u0026rsquo;oublions pas que le bombardier est √† la hauteur $h$ par rapport au point d\u0026rsquo;impact ($\\approx\\pu{9,1 km}$) au-dessus du point d\u0026rsquo;impact. Pythagore donne alors¬†: $(v_c\\times t_i)^2 = (d+v_b\\times t_i)^2+h^2$. La solution positive de ce trin√¥me du second degr√© en $t_i$ est $\\pu{0,0144 h}$ soit $\\pu{52 s}$ environ. Le pilote parle plut√¥t d\u0026rsquo;un d√©lai de $\\pu{45 s}$ apr√®s la d√©tonation et d\u0026rsquo;une distance de $\\pu{11,5 mi}$ au point d\u0026rsquo;impact. Si notre r√©sultat surestime le d√©lai, il donne une distance √† l\u0026rsquo;impact de $0,0144\\times 1300 \\approx \\pu{18,7 km} = \\pu{11,6 mi}$. L\u0026rsquo;accord n\u0026rsquo;est pas mauvais.\nPoursuite cyclique $n$ tortues sont dispos√©es sur les sommets d\u0026rsquo;un polygone r√©gulier √† $n$ c√¥t√©s. Au top d√©part, chacune se met √† poursuivre √† vitesse constante sa voisine dans le sens trigonom√©trique jusqu\u0026rsquo;√† ce qu\u0026rsquo;elles se rejoignent toutes au centre.\nOn remarque que le polygone form√© par les tortues est √† tout moment une r√©duction et rotation du polygone de d√©part.\nUtilisons les coordonn√©es polaires $(r(t);\\theta(t))$ pour d√©crire une des tortues dont la position initiale servira d\u0026rsquo;origine des angles $(r(0)=r;\\theta(0)=0).$\n$\\displaystyle v_r = \\frac{dr}{dt} = -v\\sin\\left(\\frac{\\pi}{n}\\right)$\n$\\displaystyle v_\\theta = r\\frac{d\\theta}{dt} = v\\cos\\left(\\frac{\\pi}{n}\\right)$\nEn jouant un peu avec ces deux expressions, on obtient¬†:\n$\\displaystyle \\frac{v}{r}\\cos\\left(\\frac{\\pi}{n}\\right) = \\frac{d\\theta}{dt} = \\frac{d\\theta}{dr} \\cdot\\frac{dr}{dt} = -\\frac{d\\theta}{dr}v\\sin\\left(\\frac{\\pi}{n}\\right)$\nOn peut simplifier par $v$, ce qui donne¬†:\n$\\displaystyle \\frac{dr}{d\\theta} = -r\\frac{\\sin(\\pi/n)}{\\cos(\\pi/n)} = -r\\tan\\left(\\frac{\\pi}{n}\\right)$\nEt finalement¬†:\n$\\displaystyle \\frac{dr}{r} = - \\tan\\left(\\frac{\\pi}{n}\\right) d\\theta$\nCe qui s\u0026rsquo;int√®gre en une belle spirale logarithmique¬†:\n$\\displaystyle r(\\theta) = r(0)\\exp\\left(-\\theta\\tan\\left(\\frac{\\pi}{n}\\right)\\right)$\nCherchons maintenant le temps mis par les tortues pour rejoindre le centre du polygone¬†:\n$\\displaystyle T=\\int_0^T dt = \\int_{r(0)}^0 \\frac{dr}{(dr/dt)}=\\int_{r(0)}^0\\frac{dr}{-v\\sin(\\pi/n)}$\nD\u0026rsquo;o√π $\\displaystyle T = \\frac{1}{v}\\int_0^{r(0)}\\frac{dr}{\\sin(\\pi/n)} = \\frac{r(0)}{v \\sin(\\pi/n)}$\nEt la distance parcourue vaut donc $\\displaystyle D = \\frac{r(0)}{ \\sin(\\pi/n)}$.\nDans le cas d\u0026rsquo;un carr√©, les expressions deviennnent encore plus simples¬†:\n$\\displaystyle r(\\theta) = r(0)\\exp\\left(-\\theta \\right)$\nEt $\\displaystyle D = r(0)\\frac{\\sqrt{2}}{2} = a$, en appelant $a$ le c√¥t√© du carr√©. Donc le long de leur spirale, les tortues parcourent au final la longueur d\u0026rsquo;un c√¥t√© du carr√© initial.\nOn peut expliquer √ßa simplement par le fait que la vitesse relative entre deux tortues restent en permanence identique √† $v$ puisque leurs vecteurs vitesse sont √† tout moment orthogonaux.\nOn peut s\u0026rsquo;amuser √† combiner les poursuites pour faire des jolies figures\u0026hellip; Ci-dessous, on assembl√© 6 poursuites √† 3 tortues pour former un hexagone.\nMurmurations Comme le montre cette tr√®s chouette vid√©o de fouloscopie, nos histoires de poursuite participent √† l\u0026rsquo;explication des comportements collectifs fascinants des bancs de possions ou des murmurations d\u0026rsquo;√©tourneaux. Un poisson (ou oiseau) ne ferait en fait que poursuivre les quelques individus imm√©diatement dans son champ de vision.\nLe code suivant utilise les 6 premiers voisins et permet de voyager un peu dans le diagramme de phase.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/maths/jeux/",
	"title": "Th√©orie des jeux",
	"tags": [],
	"description": "",
	"content": " Un peu de th√©orie des jeux note\nCette page est tr√®s inspir√©e du livre Chases and Escapes The Mathematics of Pursuit and Evasion de Paul Nahin.\nAttaque - D√©fense Un petit probl√®me math√©matique qui fleure bon la Guerre Froide¬†:\nDeux bombardiers bleus sont en mission. L\u0026rsquo;un transporte une bombe H et l\u0026rsquo;autre des √©quipements pour brouiller les radars. Leur plan de vol est tel qu\u0026rsquo;un des deux bombardiers est mieux prot√©g√© par les mitrailleuses de l\u0026rsquo;autre. Pour √©viter qu\u0026rsquo;un avion de chasse rouge abatte l\u0026rsquo;avion bleu transportant la bombe, que vaut-il mieux¬†: le placer dans la position la mieux ou la moins bien prot√©g√©e¬†?\nLa r√©ponse semble de prime abord √©vidente et pourtant\u0026hellip;\nListons les strat√©gies possibles pour les Bleus et les Rouges¬†:\nB1 : placer la bombe dans le bombardier le mieux prot√©g√© B2 : placer la bombe dans le bombardier le moins bien prot√©g√© R1 : attaquer le bombardier le mieux prot√©g√© R2 : attaquer le bombardier le moins bien prot√©g√© La matrice de gains ci-dessus peut s\u0026rsquo;interpr√©ter ainsi¬†: les chances de survie du bombardier le mieux prot√©g√© sont de 80% s\u0026rsquo;il est attaqu√© (et de 100% s\u0026rsquo;il ne l\u0026rsquo;est pas\u0026hellip;) alors que le moins bien prot√©g√© n\u0026rsquo;a que 60% de chance de s\u0026rsquo;en sortir (et 100% si on le laisse tranquille).\nOn a imagin√© ainsi un \u0026ldquo;jeu\u0026rdquo; √† somme nulle (les gains d\u0026rsquo;un joueur sont les pertes de l\u0026rsquo;autre). Dans la nomenclature de la th√©orie des jeux, il s\u0026rsquo;agit d\u0026rsquo;un jeu injuste puisque toutes les entr√©e sont positives $\\Rightarrow$ bleu repart forc√©ment \u0026ldquo;gagnant\u0026rdquo; (m√™me si √ßa n\u0026rsquo;a pas vraiment de sens ici).\nLa strat√©gie optimale pour deux joueurs rationnels va consister √† minimiser le \u0026ldquo;score\u0026rdquo; maximum que l\u0026rsquo;autre joueur peut faire en suivant ainsi un principe du moindre mal. En effet, chaque joueur sait que l\u0026rsquo;autre va chercher √† maximiser ses gains et va donc s\u0026rsquo;√©vertuer √† rendre cette maximisation la plus faible possible.\nSi une strat√©gie unique permet d\u0026rsquo;arriver √† cette fin, sans que le joueur soit jamais motiv√© √† changer de strat√©gie lors d\u0026rsquo;un prochain tour, on dit qu\u0026rsquo;il joue une strat√©gie pure.\nC\u0026rsquo;est le cas dans l\u0026rsquo;exemple suivant¬†:\nBleu est incit√© √† choisir la strat√©gie B1 pour maximiser ses gains minimums r√©alisables¬†: dans le pire des cas, il recevra 5 alors qu\u0026rsquo;avec la strat√©gie B2, il recevrait au pire 3\u0026hellip;\nSi Rouge joue sa premi√®re strat√©gie, il perd au pire 6 alors qu\u0026rsquo;avec la seconde, il perd au pire 5. Donc Rouge va joueur la seconde strat√©gie.\nEn r√©sum√©, Bleu choisi la ligne contenant le plus grand des gains minimums et Rouge choisit la colonne contenant le plus petit gain maximum possible.\nLe r√©sultat de la partie est ici un gain de 5 pour Bleu.\nLorsque, comme dans cet exemple, le gain minimal sur la ligne comportant le plus grand minimum possible est √©gal au gain maximal sur la colonne contenant le plus petit maximum possible, on dit que le jeu est stable. Aucun des joueurs ne sera inciter √† changer de strat√©gie. Le jeu poss√®de alors un point col (ou point-selle en r√©f√©rence √† une selle de cheval). On a en effet, un minimum local dans une direction et un maximum local dans l\u0026rsquo;autre.\nChangeons de matrice de gain¬†:\nPour maximiser le gain mimal, Bleu choisit la strat√©gie B2 ($4\u0026gt;3$) alors que pour minimiser le gain maximal pour Bleu, Rouge choisit la strat√©gie R1 ($5\u0026lt;6$). On remarque que le plus grand gain minimal sur une rang√©e ($4$) est maintenant diff√©rent du plus petit gain maximal sur une colonne ($5$). La cons√©quence est que maintenant, chacun des deux joueurs est incit√© √† changer sa strat√©gie. Si Rouge passe de la strat√©gie R1 √† R2 par exemple, il r√©duit le gain de Bleu de $5$ √† $4$ (en imaginant que Bleu ait bien choisi la strat√©gie B2). Mais alors, Bleu est tent√© de passer de la strat√©gie B2 √† B1 pour passer d\u0026rsquo;un gain de $4$ √† un un gain de $6$. Et ainsi de suite\u0026hellip;\nJouer une strat√©gie pure est donc optimal pour aucun des joueurs. Le mieux qu\u0026rsquo;ils puissent faire est jouer une strat√©gie mixte compos√©e d\u0026rsquo;un m√©lange al√©atoire (pour que l\u0026rsquo;autre joueur ne puisse pas anticiper) des strat√©gies 1 et 2.\nCe fut l\u0026rsquo;un des tours de force de John von Neumann de d√©montrer en 1928 qu\u0026rsquo;il existe pour tout jeu √† deux opposants √† somme nulle une strat√©gie mixte optimale pour les deux joueurs telle qu\u0026rsquo;ils puissent esp√©rer le m√™me gain moyen $V$, $V$ √©tant le meilleur possible. C\u0026rsquo;est le th√©or√®me du minimax.\nD√©busquons cette strat√©gie optimale pour le dernier exemple. On va supposer que Bleu joue la strat√©gie B1 avec une probabilit√© $p$ et B2 avec donc une probabilit√© $1-p$. Et Rouge joue R1 avec la probabilit√© $q$ et R2 avec la probabilit√© $1-q$.\nSi bleu joue B1, il gagne $3q + 6(1-q)$. Et s\u0026rsquo;il joue B2, il gagne $5q+4(1-q)$. L\u0026rsquo;esp√©rance de gain de Bleu est donc $V(p,q)=p[3q+6(1-q)]+(1-p)[5q+4(1-q)]=4+2p+q-4pq$.\nDu point de vue de Rouge, on obtient une esp√©rance de gain valant $q[3p+5(1-p)]+(1-q)[6p+4(1-p)]=4+2p+q-4pq$. On obtient donc √† nouveau $V(q,p)$.\nTracer $V(p,q)$ fait appara√Ætre une selle de cheval. La strat√©gie optimale pour Bleu et Rouge va donc consister √† placer leurs strat√©gies sur le point col ; le point minimax.\nBleu veut choisir $p$ tel que $\\displaystyle \\frac{\\partial V}{\\partial q} = 0$. Cela donne $1-4p=0$ et donc $p=1/4$. La distribution de probabilit√© optimale pour Bleu est donc $(1/4\\,;3/4)$.\nRouge veut choisir $q$ tel que $\\displaystyle \\frac{\\partial V}{\\partial p} = 0$. Cela donne $2-4q=0$ et donc $q=1/2$. La distribution de probabilit√© optimale pour Rouge est donc $(1/2\\,;1/2)$.\nLe gain pour les deux au point minimax est alors $V(p=1/4,q=1/2) = 4,5$.\nRevenons maintenant √† notre probl√®me de guerre froide.\nIl n\u0026rsquo;y a clairement pas de point col pour des strat√©gies pures puisque le plus grand minimum pour les lignes est $80$ alors que le plus petit maximum pour les colonnes est $100$. Il faut donc jouer des strat√©gies mixtes.\nSupposons, comme dans l\u0026rsquo;exemple pr√©c√©dent, que la strat√©gie mixte de Bleu soit $(p\\,;1-p)$ et que celle de rouge soit $(q\\,;1-q)$.\nPour Bleu : $V(p,q)= p[80q+100(1-q)]+(1-p)[100q+60(1-q)]$ Pour Rouge : $V(p,q)=q[80p+100(1-p)]+(1-q)[100p+60(1-p)]$ Dans les deux cas, $V(p,q)=60+40p+40q-60pq$. On obtient bien √† nouveau une selle de cheval. Cherchons son point col.\nLe point minimax est tel que $\\displaystyle \\frac{\\partial V}{\\partial p} = \\frac{\\partial V}{\\partial q} = 0$. Cela donne $40-60q=0$ et $40-60p=0$, d\u0026rsquo;o√π $p=q=2/3$. Les strat√©gies mixtes optimales pour Bleu et Rouge ont le m√™me distribution de probabilit√©s¬†: $(2/3\\,;1/3)$.\nContrairement √† l\u0026rsquo;intuition, Bleu doit donc placer la bombe dans l\u0026rsquo;avion le moins prot√©g√© dans 1/3 des cas. Et toujours dans 1/3 des cas, Rouge doit attaquer ce bombardier (le moins prot√©g√©).\nLa valeur du gain au point minimax vaut $\\displaystyle V\\left(\\frac{1}{3},\\frac{2}{3}\\right)=60+40\\left(\\frac{2}{3}\\right)+40\\left(\\frac{2}{3}\\right)-60\\left(\\frac{2}{3}\\right)\\left(\\frac{2}{3}\\right)=\\frac{260}{3}\\approx 86,7$. Le porteur de la bombe va donc survivre avec une probabilit√© de $86,7\\%$ qui est sup√©rieure aux $80\\%$ correspondant √† la strat√©gie √©vidente, mais finalement na√Øve, de toujours placer la bombe dans le bombardier le mieux prot√©g√©.\nEt que se passe-t-il si Bleu applique la strat√©gie mais pas Rouge¬†?\nOn a alors $\\displaystyle V\\left(\\frac{1}{3},\\frac{2}{3}\\right) = 60+40\\left(\\frac{2}{3}\\right)+40q-60\\left(\\frac{2}{3}\\right)q=\\frac{260}{3}+40q-40q=\\frac{260}{3}$. Le r√©sultat est ind√©pendant de $q$¬†! Quelle que soit la strat√©gie de Rouge, le gain est assur√© par Bleu. Et √† l\u0026rsquo;inverse, si Rouge applique la strat√©gie mais pas Bleu, on obtient √† nouveau le m√™me r√©sultat, independant de $p$.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/phistat/",
	"title": "Thermodynamique",
	"tags": [],
	"description": "",
	"content": " Thermodynamique et physique statistique Physique statistique Petite s√©rie de vid√©o sur la physique statistique, de la d√©finition de la temp√©rature √† l\u0026rsquo;entropie en passant par le facteur de Boltzmann et la loi de Planck¬†:\nEntropie note\nPW Atkins a √©crit un livre merveilleux sur l\u0026rsquo;entropie¬†: The Second Law.\nIl fait partie de la vieille collection Scientific American Library qui est bourr√©e de p√©pites.\nL\u0026rsquo;entropie est une notion passionnante qui ramifie dans diff√©rents champs.\nEn th√©orie de l\u0026rsquo;information, Claude Shannon a appel√© entropie la mesure de la quantit√© d\u0026rsquo;information contenu dans un message. Ramen√©e √† la physique, l\u0026rsquo;entropie de Shannon mesure le nombre de questions binaires (dont la r√©ponse est oui ou non) auxquelles il faudrait r√©pondre pour sp√©cifier le micro√©tat du syst√®me pour un macro√©tat donn√©.\nCherchant une r√©ponse au paradoxe de Maxwell, Le√≥ Szil√°rd a formul√© en 1929 une √©quivalence entre l\u0026rsquo;√©nergie et l\u0026rsquo;information gr√¢ce √† une version simplifi√©e du d√©mon de Maxwell. Il en a d√©duit l\u0026rsquo;√©nergie n√©cessaire au d√©mon pour faire varier l\u0026rsquo;information d\u0026rsquo;un bit. Landauer en a fait un principe (en 1961) qui stipule que l\u0026rsquo;√©nergie dissip√©e lors de l\u0026rsquo;effacement d\u0026rsquo;un bit d\u0026rsquo;information vaut au minimum $k_BT \\ln(2)$.\nLien entre l\u0026rsquo;entropie de Shannon et l\u0026rsquo;entropie thermodynamique dans le cas d\u0026rsquo;un gaz parfait √† l\u0026rsquo;√©quilibre (compos√© de $N$ particules) dans une enceinte de volume $V$ √† la temp√©rature $T$¬†:\nL\u0026rsquo;information sur une particule consiste √† conna√Ætre sa position et sa vitesse. L\u0026rsquo;additivit√© de l\u0026rsquo;entropie permet de distinguer ces deux contributions.\nAppelons $H_p(V)$ l\u0026rsquo;entropie de Shannon li√©e la position d\u0026rsquo;une particule du gaz.\nApr√®s avoir doubl√© le volume de l\u0026rsquo;enceinte, on peut revenir √† la situtation de d√©part (en terme d\u0026rsquo;information) en posant √† la particule la question binaire \u0026ldquo;es-tu dans l\u0026rsquo;enceinte de gauche ou celle de droite ?\u0026rdquo;. On a par cons√©quent $H_p(2V) = H_p(V) + 1$ (il faut 1 bit d\u0026rsquo;information en plus). La seule fonction ayant cette propri√©t√© est le logarithme en base deux¬†: $\\log_2$. D\u0026rsquo;o√π $H_p(V) = \\log_2(V)+cste$.\n√Ä l\u0026rsquo;√©quilibre thermique, l\u0026rsquo;√©nergie cin√©tique pour chacun des degr√©s de libert√© possibles (3 pour une particule monatomique, et 5 ou 7 pour une particule diatomique en ajoutant les rotations et vibrations) est proportionnelle √† la temp√©rature. Mais notre information porte sur la vitesse. La vitesse pour chaque r√©servoir d\u0026rsquo;√©nergie cin√©tique est donc proportionnelle √† la racine carr√©e de la temp√©rature. En quadruplant la temp√©rature, on double donc la vitesse dans chacun des r√©servoirs. Il faudra alors autant de questions binaires que de r√©servoirs (3 pour une particule monoatomique) pour revenir √† la situation informationnelle de d√©part.\nEn appelant $H_v(T)$ l\u0026rsquo;entropie de Shannon li√©e √† la vitesse d\u0026rsquo;une particule, on a¬†: $H_v(4T) = H_v(T)+3$. La fonction ayant cette propri√©t√© est $3\\log_4$ ou $\\frac{3}{2}\\log_2$.\nD\u0026rsquo;o√π $\\displaystyle H_v(T)=\\frac{3}{2}\\log_2(T) + cste$.\nL\u0026rsquo;entropie de Shannon totale de la particule vaut donc $\\displaystyle H(V,T)=H_v(V)+H_p(T) = \\frac{3}{2}\\log_2(T)+\\log_2(V)+cste$\nEt pour $N$ particules¬†? Si les particules sont suppos√©es indiscernables, multiplier $H$ par $N$ nous ferait tomber dans le paradoxe de Gibbs. Comme toutes les permutations de particules am√®nent √† la m√™me quantit√© d\u0026rsquo;infomation, il faut retirer $\\log_2(N!)$ √† l\u0026rsquo;entropie obtenue¬†: $\\displaystyle H_{gaz}(V,T) = N H(V,T) - \\log_2(N!) \\approx \\frac{3N}{2}\\log_2(T) + N\\log_2(V) + Ncste - N\\log_2(N) + N$.\nEn utilisant le principe de Landauer, on peut convertir ces bits en √©nergie par unit√© de temp√©rature en multipliant par $k_B \\ln_2$. On passe alors de $H$ √† $S$, l\u0026rsquo;entropie thermodynamique¬†:\n$\\displaystyle S(V,T) = \\frac{3Nk_B}{2}\\ln(T) + Nk_B\\ln(V) - Nk_B\\ln(N)+CNk_B$ (o√π $C$ est une constante).\nSource : vid√©o de L√™ Nguy√™n Hoang sur l\u0026rsquo;entropie\nInformation et thermodynamique dans la physique th√©orique contemporaine On observe depuis quelques temps une convergence de la recherche en informatique et en physique th√©orique sur les questions de l\u0026rsquo;entropie et de la complexit√©.\nJohn Wheeler est un des premiers √† militer pour qu\u0026rsquo;on s\u0026rsquo;oriente vers une explication de l\u0026rsquo;univers bas√©e sur son contenu en information, le bit √©l√©mentaire, ce qu\u0026rsquo;il r√©sume par la formule \u0026ldquo;it from bit\u0026rdquo; (Wheeler a un certain talent pour les formules, en plus du reste\u0026hellip;).\nLe fait qu\u0026rsquo;un trou noir ait une entropie proportionnelle √† son aire et non √† sa masse, comme l\u0026rsquo;ont montr√© Stephen Hawking et Jacob Bekenstein, semble donner corps √† cette id√©e que le bit est l\u0026rsquo;atome de notre compr√©hension de l\u0026rsquo;univers. Tout horizon se pr√©senterait en effet comme une sorte d\u0026rsquo;√©cran sur lequel s\u0026rsquo;√©crit l\u0026rsquo;information qu\u0026rsquo;il contient. C\u0026rsquo;est le principe holographique.\nEt plus r√©cemment, Leonard Suskind a jet√© un nouveau pont entre physique et information en liant le volume d\u0026rsquo;un trou noir et sa complexit√©.\nThermodynamique Fluide supercritique On place un √©chantillon d\u0026rsquo;eau dans un r√©cipient herm√©tique √† 25¬†¬∞C. On fait le vide d\u0026rsquo;air, et on laisse l\u0026rsquo;√©quilibre de vaporisation-condensation s\u0026rsquo;√©tablir. On obtient un m√©lange d\u0026rsquo;eau liquide et de vapeur d\u0026rsquo;eau √† une pression de 0,03¬†atm. Une fronti√®re distincte entre le liquide plus dense et le gaz moins dense est clairement observable. En augmentant la temp√©rature, la pression de la vapeur d\u0026rsquo;eau augmente, comme d√©crit par la courbe liquide-gaz du diagramme de phase de l\u0026rsquo;eau, et un √©quilibre diphasique entre les phases liquide et gazeuse persiste. √Ä une temp√©rature de 374¬†¬∞C, la pression de vapeur a atteint 218¬†atm, et toute augmentation suppl√©mentaire de la temp√©rature entra√Æne la disparition de la fronti√®re entre les phases liquide et vapeur. Toute l\u0026rsquo;eau dans le r√©cipient est maintenant pr√©sente dans une phase unique dont les propri√©t√©s physiques sont interm√©diaires entre celles des √©tats gazeux et liquide, le fluide supercritique. Au-dessus de sa temp√©rature critique, un gaz ne peut pas √™tre liqu√©fi√©, quelle que soit la pression appliqu√©e. La pression requise pour liqu√©fier un gaz √† sa temp√©rature critique est appel√©e pression critique.\nEn contournant le point critique, on peut passer de l\u0026rsquo;√©tat liquide √† l\u0026rsquo;√©tat gazeux (et inversement) continument, sans seuil. On parle alors de transition de phase du deuxi√®me ordre. Au moment du contournement du point critique (lorsque $P\u0026gt;P_C$ et $T\u0026gt;T_C$), on dit que la mati√®re est dans un nouvel √©tat (un 5e √©tat avec les solides, liquides, gaz et plasmas), le fluide supercritique.\nCet √©tat poss√®de √† la fois des propri√©t√©s d\u0026rsquo;un gaz et d\u0026rsquo;un liquide¬†; comme un gaz, il occupe tout l\u0026rsquo;espace disponible, poss√®de une grande diffusivit√© mais pas de tension superficielle, et comme un liquide, il a une grand pouvoir de dissolution de solut√©s non volatils. Il peut donc p√©n√©trer plus efficacement les petites ouvertures d\u0026rsquo;un m√©lange solide et en extraire les composants solubles. Ces propri√©t√©s font des fluides supercritiques des solvants extr√™mement utiles pour un large √©ventail d\u0026rsquo;applications. Par exemple, le dioxyde de carbone supercritique est devenu un solvant tr√®s populaire dans l\u0026rsquo;industrie alimentaire, √©tant utilis√© pour d√©caf√©iner le caf√©, √©liminer les graisses des chips et extraire les compos√©s aromatiques et les fragrances des huiles d\u0026rsquo;agrumes. Il est non toxique et relativement peu co√ªteux. Apr√®s utilisation, le $\\ce{CO2}$ peut √™tre facilement r√©cup√©r√© en r√©duisant la pression et en collectant le gaz r√©sultant.\nOn comprend assez bien les propri√©t√©s du fluide superfluide aux vues des conditions pour y parvenir. La grande temp√©rature rend l\u0026rsquo;agitation cin√©tique sup√©rieure aux liaisons intermol√©culaires, emp√™chant l\u0026rsquo;agglom√©ration, mais la grande pression donne malgr√© tout au fluide une densit√© de liquide. Les mol√©cules sont tr√®s proches mais ne \u0026ldquo;collent\u0026rdquo; pas.\nEn agitant un extincteur par une journ√©e fraiche, on sent et entend le liquide remuer √† l\u0026rsquo;int√©rieur. Mais si on r√©p√®te l\u0026rsquo;exp√©rience par une journ√©e tr√®s chaude, on ne sent plus aucun liquide √† l\u0026rsquo;int√©rieur de l\u0026rsquo;extincteur. O√π est-il pass√©¬†?\n√Ä une temp√©rature sup√©rieure √† la temp√©rature critique du $\\ce{CO2}$ (31¬†¬∞C), aucune pression n\u0026rsquo;est plus suffisante pour le liqu√©fier.\nOn voit dans la vid√©o que la proximit√© du point critique peut se traduire (lors du passage de superfluide √† gaz+liquide) par un ph√©nom√®ne spectaculaire¬†: le fluide devient \u0026ldquo;brouillardeux\u0026rdquo;. On appelle ce ph√©nom√®ne l\u0026rsquo;opalescence critique. Lorsque la temp√©rature ou la pression d\u0026rsquo;un superfluide passe sous sa valeur critique, on assiste en tout point du fluide √† une h√©sitation de la mati√®re entre liquide et gaz provoquant des fluctuations de densit√© de toutes tailles. Ces fluctuations de densit√© entra√Ænent des fluctuations d\u0026rsquo;indice optique et les plus petites fluctuations (√©chelles de taille de la longueur d\u0026rsquo;onde de la lumi√®re visible) vont entra√Æner la forte diffusion qui caract√©rise le ph√©nom√®ne (un peu comme les micro-gouttes d\u0026rsquo;huile dans le lait ou le pastis allong√©).\nTechniquement, la transition de phase du deuxi√®me ordre que subit alors le superfluide est caract√©ris√©e par la divergence de la longueur de corr√©lation $\\xi$ qui traduit la distance sur laquelle une perturbation locale peut influencer le syst√®me. Pr√®s du point critique, les fluctuations deviennent corr√©l√©es sur des distances de plus en plus grandes. Plus pr√©cis√©ment, la longueur de corr√©lation √©volue alors en loi de puissance¬†: $\\xi=|T-T_C|^\\nu$ ($\\nu$ est appel√© exposant critique). Au point critique, la longueur de corr√©lation devient th√©oriquement infinie¬†; il y a des fluctuations (ici de densit√©) √† toutes les √©chelles.\nLes transitions du premier ordre sont des transitions avec sauts alors que celles du deuxi√®me ordre sont des transitions avec divergences.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/chaos/",
	"title": "Chaos",
	"tags": [],
	"description": "",
	"content": " Chaos Au confin des maths et de la physique, une des plus belle chose qui soit¬†: le chaos (avec son cheptel de fractales et d\u0026rsquo;attracteurs √©tranges). Le chaos n\u0026rsquo;a pas grand chose √† voir avec son image \u0026ldquo;grand public\u0026rdquo; puisqu\u0026rsquo;il est finalement tr√®s organis√©. Mais timide, il le cache bien\u0026hellip;\nL\u0026rsquo;int√©gration du syst√®me d\u0026rsquo;√©quations diff√©rentielles suivant donne le fameux attracteur de Lorenz avec sa forme caract√©ristique de papillon¬†: $$\\begin{cases}x\u0026rsquo;=\\sigma(y-x)\\\\y\u0026rsquo;=\\rho x-y-xz\\\\z\u0026rsquo;=xy-\\beta \\end{cases}$$ Avec les valeurs de param√®tres suivantes¬†: $$\\begin{cases}\\sigma = 3\\\\\\rho = 26.5\\\\\\beta = 1\\end{cases}$$ et le point de d√©part $(x_0;y_0;z_0)=(0;1;1,05)$, cela donne¬†:\nCet article relate l\u0026rsquo;histore du chaos en tant qu\u0026rsquo;objet de recherche et insiste en particulier sur le r√¥le fondamental de deux programmeuses¬†: Ellen Fetter et Margaret Hamilton.\nLe livre Nonlinear Dynamics and Chaos de Steven Strogatz introduit le domaine de mani√®re passionnante.\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/physique/jeux/",
	"title": "Curiosit√©s",
	"tags": [],
	"description": "",
	"content": " Curiosit√©s La toupie tippe-top La toupie tippe-top se redresse semblant braver les lois physiques. C\u0026rsquo;est le couple des forces de frottement qui est responsable de cette bizarrerie.\nWolfgang Pauli et Niels Bohr, deux immenses physiciens au c≈ìur de la r√©volution quantique, regardant une toupie tippe-top se relever.\nComme le centre de masse s\u0026rsquo;est √©lev√©, on a envie de s\u0026rsquo;inqui√©ter pour la conservation de l\u0026rsquo;√©nergie Mais il ne faut pas¬†! Elle est solide\u0026hellip; La toupie tourne moins vite √† la fin qu\u0026rsquo;au d√©but. Une partie de l\u0026rsquo;√©nergie de rotation est partie dans les frottements et une autre dans le retournement.\nD\u0026rsquo;o√π vient la force qui am√®ne la toupie √† se retourner ? Pauli et Bohr se sont sans doute pos√©s la question moins d\u0026rsquo;un quart de seconde mais chacun son rythme\u0026hellip;\nComme c\u0026rsquo;est une toupie, on pense rapidement √† un mouvement de pr√©cession. La pr√©cession est un mouvement de rotation lent (par rapport √† la rotation propre) du centre de masse du solide. Or elle na√Æt quand un couple est appliqu√© sur le solide et cette rotation se fait autour de la direction de la force provoquant le couple.\nFormidable vid√©o de VSauce expliquant les gyroscopes¬†:\nPremi√®re candidat pour l\u0026rsquo;origine du couple redressant la toupie¬†: le poids. Bof\u0026hellip; Le moment du poids n\u0026rsquo;a pas la bonne direction puisqu\u0026rsquo;il entra√Æne la toupie dans un mouvement de rotation autour d\u0026rsquo;un axe vertical et non horizontal (et contrairement √† une toupie classique, le centre de masse est tr√®s bas et donc le bras de levier tr√®s court, ce qui donne un couple de faible intensit√©).\nIl nous faudrait une pr√©cession autour d\u0026rsquo;un axe horizontal pour faire pivoter la toupie et pour √ßa, on a besoin d\u0026rsquo;un moment d\u0026rsquo;axe vertical et donc finalement une force elle-m√™me horizontale¬†!\nSon origine devient claire si l\u0026rsquo;on s\u0026rsquo;attarde sur la grande diff√©rence entre cette toupie et les toupies classiques (outre la position du centre de masse)¬†: elle n\u0026rsquo;a pas de pointe ! Le point de contact peut se balader le long de la partie arrondie de la toupie, ce qui le d√©saligne avec l\u0026rsquo;axe de rotation principal.\nD√®s que la toupie bascule un peu, le point de contact $I$ s\u0026rsquo;√©loigne du sommet $H$ et la toupie se met √† pr√©cesser (sous l\u0026rsquo;action du poids) autour d\u0026rsquo;un axe vertical. Cette pr√©cession entra√Æne le point de contact $I$ dans un mouvement circulaire (en premi√®re approximation) et comme la toupie tourne trop vite autour de son axe de sym√©trie pour pouvoir rouler, elle glisse. Donc le point de contact glisse sur la surface et subit des forces de frottement de glissement $\\vec{f}$. On a trouv√© notre force horizontale¬†! Le moment de la force de frottement cr√©e la pr√©cession qui va pivoter notre toupie.\nPlus la toupie penche, plus la rotation autour de l\u0026rsquo;axe vertical s\u0026rsquo;acc√©l√®re tandis que celle autour de l\u0026rsquo;axe de sym√©trie ralentit. C\u0026rsquo;est la conservation du moment cin√©tique qui l\u0026rsquo;impose. Le couple de redressement devient, lui, de plus en plus faible au fur et √† mesure de la diminution de la rotation propre de la toupie.\nLorsque la toupie est horizontale, la rotation de la toupie autour de son axe de sym√©trie s\u0026rsquo;arr√™te et donc le couple de redressement dispara√Æt momentan√©ment. Mais entra√Æn√©e par son √©lan, la toupie ne reste pas horizontale longtemps\u0026hellip;\nUne fois la position horizontale d√©pass√©e, la rotation propre de la toupie red√©marre, mais elle est invers√©e par rapport √† celle de d√©part (c\u0026rsquo;est toujours la conservation du moment cin√©tique qui se manifeste). Et l\u0026rsquo;√©change entre les deux rotations (celle autour de l\u0026rsquo;axe vertical et celle autour de l\u0026rsquo;axe de sym√©trie de la toupie) continue, mais au b√©n√©fice maintenant de la rotation propre qui se fait de plus en plus rapide au fur et √† mesure du basculement. Le moment li√© aux forces de frottement reprend lui aussi du poil de la b√™te et va se renforcer avec la vitesse de rotation propre. Et surtout, il est dans le m√™me sens que pr√©c√©demment puisque l\u0026rsquo;inversion de la rotation propre de la toupie est compens√©e par l\u0026rsquo;inversion du sens des forces de frottement¬†! Il continue donc de redresser la toupie.\nVient ensuite le contact entre le pied et le sol. Le bras de levier du moment des forces de frottement augmente d\u0026rsquo;un coup donnant un petit \u0026ldquo;kick\u0026rdquo; √† la remont√©e. Il faut n√©anmoins une r√©serve d\u0026rsquo;√©nergie suffisante, les frottements ayant dispers√© dans le support une partie non n√©gligeable de l\u0026rsquo;√©nergie initiale et l\u0026rsquo;√©l√©vation du centre de masse ayant lui-aussi aussi puis√© dans la r√©serve cin√©tique.\nUne fois redress√©e, les frottements (sur les bords du pied) continuent √† stabiliser la position de la toupie, le centre de masse etant ramen√© en position verticale.\nnote\nL'inversion du sens de rotation propre de la toupie entre le d√©but et la fin illustre joliment la conservation du moment cin√©tique. L\u0026rsquo;anagyre L\u0026rsquo;anagyre (ou rattleback en anglais) refuse obstin√©ment de tourner dans le sens qu\u0026rsquo;il n\u0026rsquo;aime pas. Si par malheur on l\u0026rsquo;y envoie, il freine, rousp√®te dans un bruit de cr√©celle (rattle en anglais) puis se met √† tourner dans le bon sens, le seul, le vrai.\nEn effet, son Cr√©ateur lui a choisi un sens en le rendant asym√©trique.\nSupposons qu\u0026rsquo;un anagyre ait √©t√© fabriqu√© √† partir d\u0026rsquo;un demi-ellipso√Øde auquel on a ajout√© de la masse en deux endroits sym√©triques par rapport √† son centre. Son axe principal d\u0026rsquo;inertie est alors d√©cal√© par rapport √† ses axes de sym√©trie g√©om√©trique (cf. dessin ci-dessous).\nLa cons√©quence du d√©calage de l\u0026rsquo;axe d\u0026rsquo;inertie est le couplage entre les diff√©rentes rotations de l\u0026rsquo;anagyre. C\u0026rsquo;est tr√®s visible lorsqu\u0026rsquo;on le fait osciller d\u0026rsquo;avant en arri√®re (autour de $\\vec{j}$) puisqu\u0026rsquo;appara√Æt imm√©diatement une oscillation de gauche √† droite (autour de $\\vec{i}$) et une rotation dans le sens des aiguilles d\u0026rsquo;une montre (autour de $\\vec{k}$).\nImaginons dans un premier temps que les deux masses soient sur l\u0026rsquo;axe de sym√©trie ($\\vec{i}$) et regardons leur mouvement si l\u0026rsquo;anagyre oscille dans le sens de la longueur (autour de l\u0026rsquo;axe $\\vec{j}$).\nLa 2e loi de Newton appliqu√©e √† la masse $i$ nous dit que $\\vec{F}_{\\text{anagyre sur masse i}} = m_{\\text{masse i}}\\left(\\vec{a}_{\\text{masse i}}-\\vec{g}\\right)$. Quand $\\vec{a}$ et $\\vec{g}$ sont dans le m√™me sens, la force de l\u0026rsquo;anagyre sur la masse est moins grande. Donc $F$ est plus grande lorsque la masse est au plus bas. De plus, la 3e loi de Newton nous dit que $\\vec{F}_{\\text{masse i sur anagyre}} = -\\vec{F}_{\\text{anagyre sur masse i}} $. On se retrouve donc avec la situation sch√©matis√©e ci-dessous¬†:\nMaintenant si on red√©cale les deux masses par rapport √† l\u0026rsquo;axe de sym√©trie dans la direction de $\\vec{j}$, on comprend mieux comment la rotation selon $\\vec{j}$ se couple avec celle selon $\\vec{i}$. La masse la plus basse exerce sur l\u0026rsquo;anagyre une force plus grande que la plus haute, ce qui provoque un moment auour de $\\vec{i}$.\nFinalement, lorsque l\u0026rsquo;anagyre oscille d\u0026rsquo;avant en arri√®re, la partie sur√©lev√©e va chuter \u0026ldquo;en se tournant\u0026rdquo; du c√¥t√© de sa masse ajout√©e. Imaginons que c\u0026rsquo;est la partie avant qui redescend en entra√Ænant une rotation autour de $\\vec{i}$ dans le sens n√©gatif (cf. vid√©o ci-dessous).\nThere should have been a video here but your browser does not seem to support it. Au point de contact, l\u0026rsquo;anagyre exerce alors sur le sol une force selon $-\\vec{j}$ et r√©ciproquement, le sol exerce une force selon $+\\vec{j}$ sur l\u0026rsquo;anagyre. Lorsque c\u0026rsquo;est la partie arri√®re qui redescend, la rotation selon $\\vec{i}$ se fait dans le sens positif, ce qui donne au final une force du sol selon $-\\vec{j}$. Le sol exerce donc un couple sur l\u0026rsquo;anagyre le faisant tourner autour de $\\vec{k}$ dans le sens positif (anti-horaire).\nLorsqu\u0026rsquo;on envoie l\u0026rsquo;anagyre dans le sens impie (horaire ici), la moindre petite oscillation selon $\\vec{i}$ ou $\\vec{j}$ va √™tre amplifi√©e car elle entra√Æne, comme on l\u0026rsquo;a vu, un mouvement de rotation dans l\u0026rsquo;autre sens qui va freiner l\u0026rsquo;anagyre et le cambrer √† la mani√®re d\u0026rsquo;un freinage sur la roue avant d\u0026rsquo;un v√©lo augmentant ainsi l\u0026rsquo;amplitude des oscillations. L\u0026rsquo;augmentation de l\u0026rsquo;amplitude augmente √† son tour la force du sol sur l\u0026rsquo;anagyre qui augmente le cambrage\u0026hellip; C\u0026rsquo;est une r√©troaction positive. La rotation autour de $\\vec{k}$ finit par √™tre stopp√©e et les importantes oscillations autour de $\\vec{i}$ et $\\vec{j}$ relancent ensuite la rotation autour de $\\vec{k}$ dans l\u0026rsquo;autre sens.\nThere should have been a video here but your browser does not seem to support it. Un deuxi√®me type d\u0026rsquo;anagyre est plus simple √† comprendre car sa forme elle-m√™me est asym√©trique. En effet, la ligne de cr√™te de sa partie basse dessine un S.\nLors d\u0026rsquo;une oscillation d\u0026rsquo;avant en arri√®re, l\u0026rsquo;anagyre va reposer pr√©f√©rentiellement sur sa partie la plus plate ce qui le fait basculer autour de $\\vec{i}$. Le sol s\u0026rsquo;oppose √† ce mouvement faisant tourner l\u0026rsquo;anagyre autour de $\\vec{k}$.\nL\u0026rsquo;oiseau buveur L\u0026rsquo;oiseau buveur estt une star (on le retrouve dans Alien ou les Simpsons par exemple).\nEt c\u0026rsquo;est aussi un bon exercice de thermodynamique (qui a √©t√© donn√© au concours de Centrale il y a un bail).\nExplication par Sixty Symbols en utilisant une cam√©ra thermique¬†:\n"
},
{
	"uri": "https://sciencesilencieuse.github.io/",
	"title": "Science en vrac",
	"tags": [],
	"description": "",
	"content": " En vrac Logique Informatique Math√©matiques Physique Pour toute question : "
},
{
	"uri": "https://sciencesilencieuse.github.io/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://sciencesilencieuse.github.io/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]